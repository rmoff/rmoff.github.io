[{"categories":null,"content":"","keywords":null,"title":"Blogging","uri":"https://rmoff.net/categories/blogging/"},{"categories":null,"content":"","keywords":null,"title":"Categories","uri":"https://rmoff.net/categories/"},{"categories":null,"content":"","keywords":null,"title":"DevRel","uri":"https://rmoff.net/categories/devrel/"},{"categories":null,"content":"","keywords":null,"title":"Posts","uri":"https://rmoff.net/post/"},{"categories":null,"content":"Robin Moffatt works on the DevRel team at Confluent. He likes writing about himself in the third person, eating good breakfasts, and drinking good beer.","keywords":null,"title":"rmoff‚Äôs random ramblings","uri":"https://rmoff.net/"},{"categories":["Blogging","DevRel"],"content":" Some would say that the perfect blog article takes the reader on a journey on in which the development process looks like this:\n","keywords":null,"title":"Write more blogs articles, not fewer (Don‚Äôt leave the scraps on the cutting floor)","uri":"https://rmoff.net/2025/03/11/write-more-blogs-articles-not-fewer-dont-leave-the-scraps-on-the-cutting-floor/"},{"categories":null,"content":"","keywords":null,"title":"Apache Flink","uri":"https://rmoff.net/categories/apache-flink/"},{"categories":null,"content":"","keywords":null,"title":"Apache Flink for Confluent Cloud","uri":"https://rmoff.net/categories/apache-flink-for-confluent-cloud/"},{"categories":["Flink SQL","Apache Flink","Apache Flink for Confluent Cloud"],"content":" The UK Government publishes a lot of its data as open feeds. One that I keep coming back to is the Environment Agency‚Äôs flood-monitoring API that gives access to an estate of sensors that provide information about data such as river levels and rainfall.\nThe data is well-structured and provided across three primary API endpoints. In this blog article I‚Äôm going to show you how I use Flink SQL to explore and wrangle these into the kind of form from which I am then going to build a streaming pipeline using them.\n","keywords":null,"title":"Data Wrangling with Flink SQL","uri":"https://rmoff.net/2025/03/10/data-wrangling-with-flink-sql/"},{"categories":null,"content":"","keywords":null,"title":"Flink SQL","uri":"https://rmoff.net/categories/flink-sql/"},{"categories":["Flink SQL","Apache Flink"],"content":" There was a useful question on the Apache Flink Slack recently about joining data in Flink SQL:\nHow can I join two streams of data by id in Flink, to get a combined view of the latest data?\n","keywords":null,"title":"Joining two streams of data with Flink SQL","uri":"https://rmoff.net/2025/03/06/joining-two-streams-of-data-with-flink-sql/"},{"categories":["Flink SQL","Apache Flink"],"content":" Let‚Äôs imagine we‚Äôve got a source of data with a nested array of multiple values. The data is from an IoT device. Each device has multiple sensors, each sensor provides a reading.\n","keywords":null,"title":"How to explode nested arrays with Flink SQL","uri":"https://rmoff.net/2025/03/03/how-to-explode-nested-arrays-with-flink-sql/"},{"categories":null,"content":"","keywords":null,"title":"DuckDB","uri":"https://rmoff.net/categories/duckdb/"},{"categories":["DuckDB"],"content":"The UK Environment Agency publishes a feed of data relating to rainfall and river levels. As a prelude to building a streaming pipeline with this data, I wanted to understand the model of it first.\n","keywords":null,"title":"Exploring UK Environment Agency data in DuckDB and Rill","uri":"https://rmoff.net/2025/02/28/exploring-uk-environment-agency-data-in-duckdb-and-rill/"},{"categories":["DuckDB"],"content":" I was exploring some new data, joining across multiple tables, and doing a simple SELECT * as I‚Äôd not worked out yet which columns I actually wanted. The issue was, the same field name existing in more than one table. This meant that in the results from the query, it wasn‚Äôt clear which field came from which table:\n","keywords":null,"title":"DuckDB tricks - renaming fields in a SELECT * across tables","uri":"https://rmoff.net/2025/02/27/duckdb-tricks-renaming-fields-in-a-select-across-tables/"},{"categories":null,"content":"","keywords":null,"title":"Interesting links","uri":"https://rmoff.net/categories/interesting-links/"},{"categories":["Interesting links"],"content":" Here‚Äôs a bunch of interesting links and articles about data that I‚Äôve come across recently.\n","keywords":null,"title":"Interesting links - February 2025","uri":"https://rmoff.net/2025/02/03/interesting-links-february-2025/"},{"categories":null,"content":"","keywords":null,"title":"asciidoc","uri":"https://rmoff.net/categories/asciidoc/"},{"categories":["asciidoc","vale"],"content":" I‚Äôm a HUGE fan of Docs as Code in general, and specifically tools like Vale that lint your prose for adherence to style rule.\nOne thing that had been bugging me though was how to selectively disable Vale for particular sections of a document. Usually linting issues should be addressed at root: either fix the prose, or update the style rule. Either it‚Äôs a rule, or it‚Äôs not, right?\nSometimes though I‚Äôve found a need to make a particular exception to a rule, or simply needed to skip linting for a particular file. I was struggling with how to do this in Asciidoc. Despite the documentation showing how to, I could never get it to work reliably. Now I‚Äôve taken some time to dig into it, I think I‚Äôve finally understood :)\n","keywords":null,"title":"Disabling Vale Linting Selectively in Asciidoc","uri":"https://rmoff.net/2024/12/11/disabling-vale-linting-selectively-in-asciidoc/"},{"categories":null,"content":"","keywords":null,"title":"vale","uri":"https://rmoff.net/categories/vale/"},{"categories":null,"content":"","keywords":null,"title":"Current 2024","uri":"https://rmoff.net/categories/current-2024/"},{"categories":["Kafka Summit","Running","Current 2024","5k run/walk"],"content":" At Current 24 a few of us will be going for an early run (or walk) on Tuesday morning. Everyone is very welcome!\n","keywords":null,"title":"Current 2024 - 5k Fun Run (or Walk)","uri":"https://rmoff.net/2024/09/02/current-2024-5k-fun-run-or-walk/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Summit","uri":"https://rmoff.net/categories/kafka-summit/"},{"categories":null,"content":"","keywords":null,"title":"Running","uri":"https://rmoff.net/categories/running/"},{"categories":null,"content":"","keywords":null,"title":"Data","uri":"https://rmoff.net/categories/data/"},{"categories":["RSS","Data","Blogging","Learning","Knowledge"],"content":" I do my best to try and keep, if not abreast of, then at least aware of what‚Äôs going on in the world of data. That includes RDBMS, Event streaming, stream processing, open source data projects, data engineering, object storage, and more. If you‚Äôre interested in the same, then you might find this blog useful, because I‚Äôm sharing my sources :)\n","keywords":null,"title":"How I Try To Keep Up With The Data Tech World (A List of Data Blogs)","uri":"https://rmoff.net/2024/05/22/how-i-try-to-keep-up-with-the-data-tech-world-a-list-of-data-blogs/"},{"categories":null,"content":"","keywords":null,"title":"knowledge","uri":"https://rmoff.net/categories/knowledge/"},{"categories":null,"content":"","keywords":null,"title":"Learning","uri":"https://rmoff.net/categories/learning/"},{"categories":null,"content":"","keywords":null,"title":"RSS","uri":"https://rmoff.net/categories/rss/"},{"categories":null,"content":"","keywords":null,"title":"dns","uri":"https://rmoff.net/categories/dns/"},{"categories":null,"content":"","keywords":null,"title":"ngrok","uri":"https://rmoff.net/categories/ngrok/"},{"categories":["ngrok","dns"],"content":" Let‚Äôs not bury the lede: it was DNS. However, unlike the meme (\"It‚Äôs not DNS, it‚Äôs never DNS. It was DNS\"), I didn‚Äôt even have an inkling that DNS might be the problem.\nI‚Äôm writing a new blog about streaming Apache Kafka data to Apache Iceberg and wanted to provision a local Kafka cluster to pull data from remotely. I got this working nicely just last year using ngrok to expose the broker to the interwebz, so figured I‚Äôd use this again. Simple, right?\nNope.\n","keywords":null,"title":"ngrok DNS headaches","uri":"https://rmoff.net/2024/05/03/ngrok-dns-headaches/"},{"categories":null,"content":"","keywords":null,"title":"aws","uri":"https://rmoff.net/categories/aws/"},{"categories":["aws","pager"],"content":" After a break from using AWS I had reason to reacquaint myself with it again today, and did so via the CLI. The AWS CLI is pretty intuitive and has a good helptext system, but one thing that kept frustrasting me was that after closing the help text, the screen cleared‚Äîso I couldn‚Äôt copy the syntax out to use in my command!\nThe same thing happened when I ran a command that returned output - the screen cleared.\nHere‚Äôs how to fix either, or both, of these\n","keywords":null,"title":"How to stop AWS CLI clearing the screen","uri":"https://rmoff.net/2024/04/26/how-to-stop-aws-cli-clearing-the-screen/"},{"categories":null,"content":"","keywords":null,"title":"pager","uri":"https://rmoff.net/categories/pager/"},{"categories":["Kafka Summit","Running","5k run/walk"],"content":" At this year‚Äôs Kafka Summit I‚Äôm planning to continue the tradition of going for a run (or walk) with anyone who‚Äôd like to join in. This started back at Kafka Summit San Francisco in 2019 over the Golden Gate Bridge and has continued since then. Whilst London‚Äôs Docklands might not offer quite the same experience it‚Äôll be fun nonetheless.\n","keywords":null,"title":"üèÉüö∂ The unofficial Kafka Summit London 2024  Run/Walk üèÉüö∂","uri":"https://rmoff.net/2024/03/15/the-unofficial-kafka-summit-london-2024-run/walk/"},{"categories":["Kafka Summit","Apache Flink"],"content":" This year Kafka Summit London includes a dedicated track for talks about Apache Flink. This reflects the continued rise of interest and use of Apache Flink in the streaming community, as well as the focus that Confluent (the hosts of Kafka Summit) has on it.\nI‚Äôm looking forward to being back at Kafka Summit. I will be speaking on Tuesday afternoon, room hosting on Wednesday morning, and hanging out at the Decodable booth in between too.\nHere‚Äôs a list of all the Flink talks, including the talk, time, and speaker. You find find more details, and the full Kafka Summit agenda, here.\n","keywords":null,"title":"Apache Flink talks at Kafka Summit London 2024","uri":"https://rmoff.net/2024/03/15/apache-flink-talks-at-kafka-summit-london-2024/"},{"categories":null,"content":"","keywords":null,"title":"Antora","uri":"https://rmoff.net/categories/antora/"},{"categories":["Antora","GitHub","Cloudflare","AWS Amplify"],"content":" At Decodable we migrated our docs platform onto Antora. I wrote previously about my escapades in getting cross-repository authentication working using Private Access Tokens (PAT). These are fine for just a single user, but they‚Äôre tied to that user, which isn‚Äôt a good practice for deployment in this case.\nIn this article I‚Äôll show how to use GitHub Apps and Installation Access Tokens (IAT) instead, and go into some detail on how we‚Äôve deployed Antora. Our GitHub repositories are private which makes it extra-gnarly.\n","keywords":null,"title":"Antora Deployment to Cloudflare Across Private Repositories with GitHub Actions","uri":"https://rmoff.net/2024/01/17/antora-deployment-to-cloudflare-across-private-repositories-with-github-actions/"},{"categories":null,"content":"","keywords":null,"title":"AWS Amplify","uri":"https://rmoff.net/categories/aws-amplify/"},{"categories":null,"content":"","keywords":null,"title":"Cloudflare","uri":"https://rmoff.net/categories/cloudflare/"},{"categories":null,"content":"","keywords":null,"title":"GitHub","uri":"https://rmoff.net/categories/github/"},{"categories":["GitHub","DNS"],"content":" A friend messaged me late last night with the scary news that Google had emailed him about a ton of spammy subdomains on his own domain.\nAny idea how this could have happened, he asked?\n","keywords":null,"title":"Hosting on GitHub Pages? Watch out for Subdomain Hijacking","uri":"https://rmoff.net/2024/01/16/hosting-on-github-pages-watch-out-for-subdomain-hijacking/"},{"categories":["DuckDB","1BRC"],"content":"Why should the Java folk have all the fun?!\nMy friend and colleague Gunnar Morling launched a fun challenge this week: how fast can you aggregate and summarise a billion rows of data? Cunningly named The One Billion Row Challenge (1BRC for short), it‚Äôs aimed at Java coders to look at new features in the language and optimisation techniques.\nNot being a Java coder myself, and seeing how the challenge has already unofficially spread to other communities including Rust and Python I thought I‚Äôd join in the fun using what I know best: SQL.\n","keywords":null,"title":"1Ô∏è‚É£üêùüèéÔ∏èü¶Ü (1BRC in SQL with DuckDB)","uri":"https://rmoff.net/2024/01/03/1%EF%B8%8F%E2%83%A3%EF%B8%8F-1brc-in-sql-with-duckdb/"},{"categories":null,"content":"","keywords":null,"title":"1BRC","uri":"https://rmoff.net/categories/1brc/"},{"categories":["Antora","GitHub","Personal Access Token","Cloudflare"],"content":"Antora is a modern documentation site generator with many nice features including sourcing documentation content from one or more separate git repositories. This means that your docs can be kept under source control (yay üéâ) and in sync with the code of the product that they are documenting (double yay üéâüéâ).\nAs you would expect for a documentation tool, the Antora documentation is thorough but there was one sharp edge involving GitHub that caught me out which I‚Äôll detail here.\n","keywords":null,"title":"Deploying Antora with GitHub Actions and a private GitHub repo","uri":"https://rmoff.net/2023/12/19/deploying-antora-with-github-actions-and-a-private-github-repo/"},{"categories":null,"content":"","keywords":null,"title":"Personal Access Token","uri":"https://rmoff.net/categories/personal-access-token/"},{"categories":null,"content":"","keywords":null,"title":"AI","uri":"https://rmoff.net/categories/ai/"},{"categories":null,"content":"","keywords":null,"title":"Productivity Tools","uri":"https://rmoff.net/categories/productivity-tools/"},{"categories":["AI","Blogging","Productivity Tools"],"content":"AI, what a load of hyped-up bollocks, right? Yet here I am, legit writing a blog about it and not for the clickbait but‚Ä¶gasp‚Ä¶because it‚Äôs actually useful.\nUsed correctly, it‚Äôs just like any other tool on your desktop. It helps you get stuff done quicker, better‚Äîor both.\n","keywords":null,"title":"Productivity tools: AI Image Generators","uri":"https://rmoff.net/2023/12/07/productivity-tools-ai-image-generators/"},{"categories":null,"content":"","keywords":null,"title":"docker","uri":"https://rmoff.net/categories/docker/"},{"categories":null,"content":"","keywords":null,"title":"Hugo","uri":"https://rmoff.net/categories/hugo/"},{"categories":["Hugo","Docker","OrbStack"],"content":"I‚Äôve used Hugo for my blog for several years now, and it‚Äôs great. One of the things I love about it is the fast build time coupled with it‚Äôs live-reload feature. Using this I can edit my source (Markdown or Asciidoc) in one window, hit save, and see the preview update in my browser window next to it pretty much instantaneously. For copy-editing, experimenting with images, etc this is really helpful.","keywords":null,"title":"Hugo not detecting changed pages on Mac","uri":"https://rmoff.net/2023/11/16/hugo-not-detecting-changed-pages-on-mac/"},{"categories":null,"content":"","keywords":null,"title":"OrbStack","uri":"https://rmoff.net/categories/orbstack/"},{"categories":null,"content":"","keywords":null,"title":"Flink JDBC","uri":"https://rmoff.net/categories/flink-jdbc/"},{"categories":null,"content":"","keywords":null,"title":"Flink SQL Gateway","uri":"https://rmoff.net/categories/flink-sql-gateway/"},{"categories":null,"content":"","keywords":null,"title":"LAF","uri":"https://rmoff.net/categories/laf/"},{"categories":["Flink JDBC","Flink SQL Gateway","LAF","Apache Flink"],"content":"As a newcomer to Apache Flink one of the first things I did was join the Slack community (which is vendor-neutral and controlled by the Flink PMC). At the moment I‚Äôm pretty much in full-time lurker mode, soaking up the kind of questions that people have and how they‚Äôre using Flink.\nOne question that caught my eye was from Marco Villalobos, in which he asked about the Flink JDBC driver and a SQLDataException he was getting with a particular datatype. Now, unfortunately, I have no idea about the answer to this question‚Äîbut the idea of a JDBC driver through which Flink SQL could be run sounded like a fascinating path to follow after previously looking at the SQL Client.\n","keywords":null,"title":"Learning Apache Flink S01E06: The Flink JDBC Driver","uri":"https://rmoff.net/2023/11/16/learning-apache-flink-s01e06-the-flink-jdbc-driver/"},{"categories":null,"content":"","keywords":null,"title":"Apache Kafka","uri":"https://rmoff.net/categories/apache-kafka/"},{"categories":["ngrok","Apache Kafka"],"content":"Sometimes you might want to access Apache Kafka that‚Äôs running on your local machine from another device not on the same network. I‚Äôm not sure I can think of a production use-case, but there are a dozen examples for sandbox, demo, and playground environments.\nIn this post we‚Äôll see how you can use ngrok to, in their words, Put localhost on the internet. And specifically, your local Kafka broker on the internet.\n","keywords":null,"title":"Using Apache Kafka with ngrok","uri":"https://rmoff.net/2023/11/01/using-apache-kafka-with-ngrok/"},{"categories":["LAF","Apache Flink","PyFlink"],"content":"When I started my journey learning Apache Flink one of the things that several people expressed an interest in hearing more about was PyFlink. This appeals to me too, because whilst Java is just something I don‚Äôt know and feels beyond me to try and learn, Python is something that I know enough of to at least hack my way around it. I‚Äôve previously had fun with PySpark, and whilst Flink SQL will probably be one of my main focusses, I also want to get a feel for PyFlink.\nThe first step to using PyFlink is installing it - which should be simple, right?\n","keywords":null,"title":"Learning Apache Flink S01E05: Installing PyFlink (with some bumps along the way‚Ä¶)","uri":"https://rmoff.net/2023/10/25/learning-apache-flink-s01e05-installing-pyflink-with-some-bumps-along-the-way/"},{"categories":null,"content":"","keywords":null,"title":"PyFlink","uri":"https://rmoff.net/categories/pyflink/"},{"categories":["LAF","Apache Flink","Flink SQL"],"content":"So far I‚Äôve plotted out a bit of a map for my exploration of Apache Flink, looked at what Flink is, and run my first Flink application. Being an absolutely abysmal coder‚Äîbut knowing a thing or two about SQL‚ÄîI figure that Flink SQL is where my focus is going to lie (I‚Äôm also intrigued by PyFlink, but that‚Äôs for another day‚Ä¶).\n","keywords":null,"title":"Learning Apache Flink S01E04: A [Partial] Exploration of the Flink SQL Client","uri":"https://rmoff.net/2023/10/10/learning-apache-flink-s01e04-a-partial-exploration-of-the-flink-sql-client/"},{"categories":["LAF","Apache Flink"],"content":"üéâ I just ran my first Apache Flink cluster and application on it üéâ\n","keywords":null,"title":"Learning Apache Flink S01E03: Running my First Flink Cluster and Application","uri":"https://rmoff.net/2023/10/05/learning-apache-flink-s01e03-running-my-first-flink-cluster-and-application/"},{"categories":["zsh"],"content":"A brief diversion from my journey learning Apache Flink to document an interesting zsh oddity that briefly tripped me up:\ncd: string not in pwd: flink-1.17.1 ","keywords":null,"title":"cd: string not in pwd","uri":"https://rmoff.net/2023/10/04/cd-string-not-in-pwd/"},{"categories":null,"content":"","keywords":null,"title":"zsh","uri":"https://rmoff.net/categories/zsh/"},{"categories":["Apache Flink","LAF"],"content":"My journey with Apache Flink begins with an overview of what Flink actually is.\nWhat better place to start than the Apache Flink website itself:\nApache Flink¬†is a framework and distributed processing engine for stateful computations over¬†unbounded¬†and¬†bounded¬†data streams. Flink has been designed to run in¬†all common cluster environments, perform computations at¬†in-memory¬†speed and at¬†any scale.\n","keywords":null,"title":"Learning Apache Flink S01E02: What \u003cem\u003eis\u003c/em\u003e Flink?","uri":"https://rmoff.net/2023/10/02/learning-apache-flink-s01e02-what-is-flink/"},{"categories":["Apache Flink","LAF"],"content":"Like a fortunate child on Christmas Day, I‚Äôve got a brand new toy! A brand new‚Äîto me‚Äîopen-source technology to unwrap, learn, and perhaps even aspire to master elements of within.\nI joined Decodable two weeks ago, and since Decodable is built on top of Apache Flink it seems like a great time to learn it. After six years learning Apache Kafka and hearing about this ‚ÄúFlink‚Äù thing but‚Äîfor better or worse‚Äînever investigating it, I now have the perfect opportunity to do so.\n","keywords":null,"title":"Learning Apache Flink S01E01: Where Do I Start?","uri":"https://rmoff.net/2023/09/29/learning-apache-flink-s01e01-where-do-i-start/"},{"categories":["Streaming","Career","DevEx"],"content":"This week I joined Decodable. I‚Äôm grateful to my former colleagues at Treeverse for allowing me to join them on the journey with lakeFS - but something about the streaming world was too strong to resist üòÅ.\n","keywords":null,"title":"An Itch That Just Has to Be Scratched‚Ä¶ (Or, Why Am I Joining Decodable?)","uri":"https://rmoff.net/2023/09/21/an-itch-that-just-has-to-be-scratched-or-why-am-i-joining-decodable/"},{"categories":null,"content":"","keywords":null,"title":"Career","uri":"https://rmoff.net/categories/career/"},{"categories":null,"content":"","keywords":null,"title":"devex","uri":"https://rmoff.net/categories/devex/"},{"categories":null,"content":"","keywords":null,"title":"Streaming","uri":"https://rmoff.net/categories/streaming/"},{"categories":["DevRel","Blogging"],"content":"Writing is one of the most powerful forms of communication, and it‚Äôs useful in a multitude of roles and contexts. As a blog-writing, documentation-authoring, twitter-shitposting DevEx engineer I spend a lot of my time writing. Recently, someone paid me a very nice compliment about a blog I‚Äôd written and asked how they could learn to write like me and what resources I‚Äôd recommend.\nNever one to miss a chance to write and share something, here‚Äôs my response to this :)\n","keywords":null,"title":"Blog Writing for Developers","uri":"https://rmoff.net/2023/07/19/blog-writing-for-developers/"},{"categories":["devrel","devex"],"content":"This was originally titled more broadly ‚ÄúWhat Does A DevEx Engineer Do‚Äù, but that made it into a far too tedious and long-winding etymological exploration of the discipline. Instead, I‚Äôm going to tell you what this particular instantiation of the entity does üòÑ\n","keywords":null,"title":"What Does This DevEx Engineer Do?","uri":"https://rmoff.net/2023/05/23/what-does-this-devex-engineer-do/"},{"categories":["Markdown","Wordpress","Google Docs","Blogging"],"content":"Wordpress still, to an extent, rules the blogging world. Its longevity is testament to‚Ä¶something about it ;) However, it‚Äôs not my favourite platform in which to write a blog by a long way. It doesn‚Äôt support Markdown to the extent that I want. Yes, I‚Äôve tried the plugins; no, they didn‚Äôt do what I needed.\nI like to write all my content in a structured format - ideally Asciidoc, but I‚Äôll settle for Markdown too. Here‚Äôs how I stayed [almost] sane whilst composing a blog in Markdown, reviewing it in Google Docs, and then publishing it in Wordpress in a non-lossy way.\n","keywords":null,"title":"Authoring Wordpress blogs in Markdown (with Google Docs for review)","uri":"https://rmoff.net/2023/05/03/authoring-wordpress-blogs-in-markdown-with-google-docs-for-review/"},{"categories":null,"content":"","keywords":null,"title":"Google Docs","uri":"https://rmoff.net/categories/google-docs/"},{"categories":null,"content":"","keywords":null,"title":"Markdown","uri":"https://rmoff.net/categories/markdown/"},{"categories":null,"content":"","keywords":null,"title":"Wordpress","uri":"https://rmoff.net/categories/wordpress/"},{"categories":["Documentation","Jekyll","GitHub Actions"],"content":"One of the most important ways that a project can help its developers is providing them good documentation. Actually, scratch that. Great documentation.\n","keywords":null,"title":"Building Better Docs - Automating Jekyll Builds and Link Checking for PRs","uri":"https://rmoff.net/2023/04/20/building-better-docs-automating-jekyll-builds-and-link-checking-for-prs/"},{"categories":null,"content":"","keywords":null,"title":"Documentation","uri":"https://rmoff.net/categories/documentation/"},{"categories":null,"content":"","keywords":null,"title":"GitHub Actions","uri":"https://rmoff.net/categories/github-actions/"},{"categories":null,"content":"","keywords":null,"title":"Jekyll","uri":"https://rmoff.net/categories/jekyll/"},{"categories":null,"content":"","keywords":null,"title":"Delta Lake","uri":"https://rmoff.net/categories/delta-lake/"},{"categories":null,"content":"","keywords":null,"title":"PySpark","uri":"https://rmoff.net/categories/pyspark/"},{"categories":["PySpark","Delta Lake"],"content":"No great insights in this post, just something for folk who Google this error after me and don‚Äôt want to waste three hours chasing their tails‚Ä¶ üòÑ\n","keywords":null,"title":"Using Delta from pySpark - \u003ccode\u003ejava.lang.ClassNotFoundException: delta.DefaultSource\u003c/code\u003e","uri":"https://rmoff.net/2023/04/05/using-delta-from-pyspark-java.lang.classnotfoundexception-delta.defaultsource/"},{"categories":["DuckDB"],"content":"Here‚Äôs a neat little trick you can use with DuckDB to convert a CSV file into a Parquet file:\nCOPY (SELECT * FROM read_csv('~/data/source.csv',AUTO_DETECT=TRUE)) TO '~/data/target.parquet' (FORMAT 'PARQUET', CODEC 'ZSTD'); ","keywords":null,"title":"Quickly Convert CSV to Parquet with DuckDB","uri":"https://rmoff.net/2023/03/14/quickly-convert-csv-to-parquet-with-duckdb/"},{"categories":null,"content":"","keywords":null,"title":"Alfred","uri":"https://rmoff.net/categories/alfred/"},{"categories":["Alfred","Raycast"],"content":"It all started with a tweet.\n","keywords":null,"title":"Making the move from Alfred to Raycast","uri":"https://rmoff.net/2023/03/03/making-the-move-from-alfred-to-raycast/"},{"categories":null,"content":"","keywords":null,"title":"Raycast","uri":"https://rmoff.net/categories/raycast/"},{"categories":["DuckDB","Parquet"],"content":"What do you do when you want to query over multiple parquet files but the schemas don‚Äôt quite line up? Let‚Äôs find out üëáüèª\n","keywords":null,"title":"Aligning mismatched Parquet schemas in DuckDB","uri":"https://rmoff.net/2023/03/03/aligning-mismatched-parquet-schemas-in-duckdb/"},{"categories":null,"content":"","keywords":null,"title":"Parquet","uri":"https://rmoff.net/categories/parquet/"},{"categories":null,"content":"","keywords":null,"title":"LakeFS","uri":"https://rmoff.net/categories/lakefs/"},{"categories":["Career","DevRel","LakeFS"],"content":" As we enter December and 2022 draws to a close, so does a significant chapter in my working career‚Äîlater this month I‚Äôll be leaving Confluent and onto pastures new.\nIt‚Äôs nearly six years since I wrote a 'moving on' blog entry, and as well as sharing what I‚Äôll be working on next (and why), I also want to reflect on how much I‚Äôve benefited from my time at Confluent and particularly the people with whom I worked.\n","keywords":null,"title":"Looking Forwards, and Looking Backwards","uri":"https://rmoff.net/2022/12/09/looking-forwards-and-looking-backwards/"},{"categories":null,"content":"","keywords":null,"title":"AirByte","uri":"https://rmoff.net/categories/airbyte/"},{"categories":null,"content":"","keywords":null,"title":"Data Engineering","uri":"https://rmoff.net/categories/data-engineering/"},{"categories":["ELT","dbt","Fivetran","AirByte","Data Engineering"],"content":" In my quest to bring myself up to date with where the data \u0026 analytics engineering world is at nowadays, I‚Äôm going to build on my exploration of the storage and access technologies and look at the tools we use for loading and transforming data.\n","keywords":null,"title":"Data Engineering in 2022: ELT tools","uri":"https://rmoff.net/2022/11/08/data-engineering-in-2022-elt-tools/"},{"categories":null,"content":"","keywords":null,"title":"dbt","uri":"https://rmoff.net/categories/dbt/"},{"categories":null,"content":"","keywords":null,"title":"ELT","uri":"https://rmoff.net/categories/elt/"},{"categories":null,"content":"","keywords":null,"title":"Fivetran","uri":"https://rmoff.net/categories/fivetran/"},{"categories":null,"content":"","keywords":null,"title":"Current 2022","uri":"https://rmoff.net/categories/current-2022/"},{"categories":["dbt","DuckDB","Current 2022","Data Engineering"],"content":" I started my dbt journey by poking and pulling at the pre-built jaffle_shop demo running with DuckDB as its data store. Now I want to see if I can put it to use myself to wrangle the session feedback data that came in from Current 2022. I‚Äôve analysed this already, but it struck me that a particular part of it would benefit from some tidying up - and be a good excuse to see what it‚Äôs like using dbt to do so.\n","keywords":null,"title":"Data Engineering in 2022: Wrangling the feedback data from Current 22 with dbt","uri":"https://rmoff.net/2022/10/24/data-engineering-in-2022-wrangling-the-feedback-data-from-current-22-with-dbt/"},{"categories":["Data Engineering","dbt","DuckDB"],"content":" I‚Äôve been wanting to try out dbt for some time now, and a recent long-haul flight seemed like the obvious opportunity to do so. Except many of the tutorials with dbt that I found were based on using Cloud, and airplane WiFi is generally sucky or non-existant. Then I found the DuckDB-based demo of dbt, which seemed to fit the bill (ü¶Ü geddit?!) perfectly, since DuckDB runs locally. In addition, DuckDB had appeared on my radar recently and I was keen to check it out.\n","keywords":null,"title":"Data Engineering in 2022: Exploring dbt with DuckDB","uri":"https://rmoff.net/2022/10/20/data-engineering-in-2022-exploring-dbt-with-duckdb/"},{"categories":["DuckDB","Current 2022","Jupyter"],"content":"At Current 2022 the audience was given the option to submit ratings. Here‚Äôs some analysis I‚Äôve done on the raw data. It‚Äôs interesting to poke about it, and it also gave me an excuse to try using DuckDB in a notebook!\n","keywords":null,"title":"Current 22 - Session Analysis with DuckDB and Jupyter Notebook","uri":"https://rmoff.net/2022/10/14/current-22-session-analysis-with-duckdb-and-jupyter-notebook/"},{"categories":null,"content":"","keywords":null,"title":"Jupyter","uri":"https://rmoff.net/categories/jupyter/"},{"categories":["Data Engineering","dbt","Oracle"],"content":" This is one of those you had to be there moments. If you come into the world of data and analytics engineering today, ELT is just what it is and is pretty much universally understood. But if you‚Äôve been around for ‚Ä¶waves hands‚Ä¶ longer than that, you might be confused by what people are calling ELT and ETL. Well, I was ‚úã.\n","keywords":null,"title":"Data Engineering in 2022: Architectures \u0026 Terminology","uri":"https://rmoff.net/2022/10/02/data-engineering-in-2022-architectures-terminology/"},{"categories":null,"content":"","keywords":null,"title":"Oracle","uri":"https://rmoff.net/categories/oracle/"},{"categories":["Kafka Summit","Running","Current 2022","5k run/walk"],"content":" At Current 22 a few of us will be going for an early run on Tuesday morning. Everyone is very welcome!\n","keywords":null,"title":"Current 2022 - 5k Fun Run","uri":"https://rmoff.net/2022/09/26/current-2022-5k-fun-run/"},{"categories":["Data Engineering","LakeFS","PySpark"],"content":"With my foray into the current world of data engineering I wanted to get my hands dirty with some of the tools and technologies I‚Äôd been reading about. The vehicle for this was trying to understand more about LakeFS, but along the way dabbling with PySpark and S3 (MinIO) too.\nI‚Äôd forgotten how amazingly useful notebooks are. It‚Äôs six years since I wrote about them last (and the last time I tried my hand at PySpark). This blog is basically the notebook, with some more annotations.\n","keywords":null,"title":"Data Engineering in 2022: Exploring LakeFS with Jupyter and PySpark","uri":"https://rmoff.net/2022/09/16/data-engineering-in-2022-exploring-lakefs-with-jupyter-and-pyspark/"},{"categories":["Data Engineering"],"content":" As I‚Äôve been reading and exploring the current world of data engineering I‚Äôve been adding links to my Raindrop.io collection, so check that out. In addition, below are some specific resources that I‚Äôd recommend.\n","keywords":null,"title":"Data Engineering: Resources","uri":"https://rmoff.net/2022/09/14/data-engineering-resources/"},{"categories":null,"content":"","keywords":null,"title":"Apache Hudi","uri":"https://rmoff.net/categories/apache-hudi/"},{"categories":null,"content":"","keywords":null,"title":"Apache Iceberg","uri":"https://rmoff.net/categories/apache-iceberg/"},{"categories":["Data Engineering","Table Formats","Apache Hudi","Apache Iceberg","Delta Lake","LakeFS"],"content":" In this article I look at where we store our analytical data, how we organise it, and how we enable access to it. I‚Äôm considering here potentially large volumes of data for access throughout an organisation. I‚Äôm not looking at data stores that are used for specific purposes (caches, low-latency analytics, graph etc).\nThe article is part of a series in which I explore the world of data engineering in 2022 and how it has changed from when I started my career in data warehousing 20+ years ago. Read the introduction for more context and background.\n","keywords":null,"title":"Data Engineering in 2022: Storage and Access","uri":"https://rmoff.net/2022/09/14/data-engineering-in-2022-storage-and-access/"},{"categories":null,"content":"","keywords":null,"title":"Table Formats","uri":"https://rmoff.net/categories/table-formats/"},{"categories":["Data Engineering"],"content":" For the past 5.5 years I‚Äôve been head-down in the exciting area of stream processing and events, and I realised recently that the world of data and analytics that I worked in up to 2017 which was changing significantly back then (Big Data, y‚Äôall!) has evolved and, dare I say it, matured somewhat - and I‚Äôve not necessarily kept up with it. In this series of posts you can follow along as I start to reacquaint myself with where it‚Äôs got to these days.\n","keywords":null,"title":"Stretching my Legs in the Data Engineering Ecosystem in 2022","uri":"https://rmoff.net/2022/09/14/stretching-my-legs-in-the-data-engineering-ecosystem-in-2022/"},{"categories":null,"content":"","keywords":null,"title":"Airtable","uri":"https://rmoff.net/categories/airtable/"},{"categories":["Airtable"],"content":" Airtable is a rather wonderful tool. It powers the program creation backend process for Kafka Summit and Current. It does, however, have a few frustrating limitations - often where it feels like a feature was built on a Friday afternoon and they didn‚Äôt get chance to finish it before knocking off to head to the pub.\n","keywords":null,"title":"Customising the fields shown in Airtable‚Äôs Calendar .ics export","uri":"https://rmoff.net/2022/09/12/customising-the-fields-shown-in-airtables-calendar-.ics-export/"},{"categories":null,"content":"","keywords":null,"title":"conferences","uri":"https://rmoff.net/categories/conferences/"},{"categories":["DevRel","Program Committee","Current 2022","Conferences"],"content":" If you‚Äôve ever been to a conference, particularly as a speaker whose submitted a paper that may or may not have been accepted, you might wonder quite how conferences choose the talks that get accepted.\nI had the privilege of chairing the program committee for Current and Kafka Summit this year and curating the final program for both. Here‚Äôs a glimpse behind the curtains of how we built the program for Current 2022. It was originally posted as a thread on Twitter.\n","keywords":null,"title":"Inside the Sausage Factory: How we Built the Program for Current 2022","uri":"https://rmoff.net/2022/08/31/inside-the-sausage-factory-how-we-built-the-program-for-current-2022/"},{"categories":null,"content":"","keywords":null,"title":"Program Committee","uri":"https://rmoff.net/categories/program-committee/"},{"categories":["DevRel","Conferences","Lightning talks","Abstracts"],"content":" (src)\nLightning talks are generally 5-10 minutes. As the name implies - they are quick!\nA good lightning talk is not just your breakout talk condensed into a shorter time frame. You can‚Äôt simply deliver the same material faster, or the same material at a higher level, or the same material with a few bits left out\n","keywords":null,"title":"‚ö°Ô∏è Writing an abstract for a lightning talk ‚ö°Ô∏è","uri":"https://rmoff.net/2022/08/31/%EF%B8%8F-writing-an-abstract-for-a-lightning-talk-%EF%B8%8F/"},{"categories":null,"content":"","keywords":null,"title":"abstracts","uri":"https://rmoff.net/categories/abstracts/"},{"categories":null,"content":"","keywords":null,"title":"Lightning talks","uri":"https://rmoff.net/categories/lightning-talks/"},{"categories":["DevRel","Abstracts","Conferences"],"content":" Building the program for any conference is not an easy task. There will always be a speaker disappointed that their talk didn‚Äôt get in‚Äîor perhaps an audience who are disappointed that a particular talk did get in. As the chair of the program committee for Current 22 one of the things that I‚Äôve found really useful in building out the program this time round are the comments that the program committee left against submissions as they reviewed them.\nThere were some common patterns I saw, and I thought it would be useful to share these here. Perhaps you‚Äôre an aspiring conference speaker looking to understand what mistakes to avoid. Maybe you‚Äôre an existing speaker whose abstracts don‚Äôt get accepted as often as you‚Äôd like. Or perhaps you‚Äôre just curious as to what goes on behind the curtains :)\n","keywords":null,"title":"How to Write a Good Tech Conference Abstract - Learn from the Mistakes of Others","uri":"https://rmoff.net/2022/07/20/how-to-write-a-good-tech-conference-abstract-learn-from-the-mistakes-of-others/"},{"categories":["DevRel"],"content":" I‚Äôm convinced that a developer advocate can be effective remotely. As a profession, we‚Äôve all spent two years figuring out how to do just that. Some of it worked out great. Some of it, less so.\nI made the decision during COVID to stop travelling as part of my role as a developer advocate. In this article, I talk about my experience with different areas of advocacy done remotely.\n","keywords":null,"title":"Remote-First Developer Advocacy","uri":"https://rmoff.net/2022/04/07/remote-first-developer-advocacy/"},{"categories":["DevRel"],"content":" I recently started writing an abstract for a conference later this year and realised that I‚Äôm not even sure if I want to do it. Not the conference‚Äîit‚Äôs a great one‚Äîbut just the whole up on stage doing a talk thing. I can‚Äôt work out if this is just nerves from the amount of time off the stage, or something more fundamental to deal with.\n","keywords":null,"title":"Hanging up my Boarding Passes and Jetlag‚Ä¶for now","uri":"https://rmoff.net/2022/04/07/hanging-up-my-boarding-passes-and-jetlagfor-now/"},{"categories":["Hugo","GitHub","Blogging","GitHub Actions"],"content":" This blog is written in Asciidoc, built using Hugo, and hosted on GitHub Pages. I recently wanted to share the draft of a post I was writing with someone and ended up exporting a local preview to a PDF - not a great workflow! This blog post shows you how to create an automagic hosted preview of any draft content on Hugo using GitHub Actions.\nThis is useful for previewing and sharing one‚Äôs own content, but also for making good use of GitHub as a collaborative platform - if someone reviews and amends your PR the post gets updated in the preview too.\n","keywords":null,"title":"Using GitHub Actions to build automagic Hugo previews of draft articles","uri":"https://rmoff.net/2022/04/06/using-github-actions-to-build-automagic-hugo-previews-of-draft-articles/"},{"categories":["Kafka Summit","Running","5k run/walk"],"content":" Kafka Summit London IS BACK! After COVID spoiled everyone‚Äôs fun and fundamentally screwed everything up for the past two years, I cannot wait to be back at an in-person conference. At the last Kafka Summit in the beforetimes (San Francisco, 2019) some of us got together for a run (or walk) across the GoldenGate bridge. I can‚Äôt promise quite the same views, but I thought it would be fun to do something similar when we meet in London later this month.\n","keywords":null,"title":"üèÉüö∂ The unofficial Kafka Summit London 2022  Run/Walk üèÉüö∂","uri":"https://rmoff.net/2022/04/05/the-unofficial-kafka-summit-london-2022-run/walk/"},{"categories":null,"content":"","keywords":null,"title":"Mac","uri":"https://rmoff.net/categories/mac/"},{"categories":["Productivity","Mac"],"content":" This is the software counterpart to my previous article in which I looked at my workstation‚Äôs hardware setup. Some of these are unique or best-of-breed, others may have been sherlocked but I stick with them anyway :)\n","keywords":null,"title":"My Favourite Tools on the Mac (Setting up a new Mac)","uri":"https://rmoff.net/2021/07/29/my-favourite-tools-on-the-mac-setting-up-a-new-mac/"},{"categories":null,"content":"","keywords":null,"title":"Productivity","uri":"https://rmoff.net/categories/productivity/"},{"categories":["Productivity","Mac","Alfred"],"content":"I‚Äôve used Alfred for years, and it‚Äôs one of the first apps I‚Äôll install on a fresh Mac. It‚Äôs like the Cmd-Space search integration that MacOS has, but so much more than that. Here‚Äôs a few of the really powerful features that makes it the first app I‚Äôll reach for to install on any new Mac - and which it‚Äôll feel like I‚Äôm trying to work with one arm tied behind my back if I don‚Äôt have :)","keywords":null,"title":"Why I use Alfred App (and maybe you should too)","uri":"https://rmoff.net/2021/07/29/why-i-use-alfred-app-and-maybe-you-should-too/"},{"categories":["ksqlDB","Confluent Cloud"],"content":" There‚Äôs a bunch of improvements in the works for how ksqlDB handles code deployments and migrations. For now though, for deploying queries there‚Äôs the option of using headless mode (which is limited to one query file and disables subsequent interactive work on the server from a CLI), manually running commands (yuck), or using the REST endpoint to deploy queries automagically. Here‚Äôs an example of doing that.\n","keywords":null,"title":"A bash script to deploy ksqlDB queries automagically","uri":"https://rmoff.net/2021/04/01/a-bash-script-to-deploy-ksqldb-queries-automagically/"},{"categories":null,"content":"","keywords":null,"title":"Confluent Cloud","uri":"https://rmoff.net/categories/confluent-cloud/"},{"categories":null,"content":"","keywords":null,"title":"ksqlDB","uri":"https://rmoff.net/categories/ksqldb/"},{"categories":null,"content":"","keywords":null,"title":"csv","uri":"https://rmoff.net/categories/csv/"},{"categories":null,"content":"","keywords":null,"title":"FilePulse","uri":"https://rmoff.net/categories/filepulse/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Connect","uri":"https://rmoff.net/categories/kafka-connect/"},{"categories":["Confluent Cloud","Kafka Connect","FilePulse","CSV"],"content":" The FilePulse connector from Florian Hussonnois is a really useful connector for Kafka Connect which enables you to ingest flat files including CSV, JSON, XML, etc into Kafka. You can read more it in its overview here. Other connectors for ingested CSV data include kafka-connect-spooldir (which I wrote about previously), and kafka-connect-fs.\nHere I‚Äôll show how to use it to stream CSV data into a topic in Confluent Cloud. You can apply the same config pattern to any other secured Kafka cluster.\n","keywords":null,"title":"Loading CSV data into Confluent Cloud using the FilePulse connector","uri":"https://rmoff.net/2021/03/26/loading-csv-data-into-confluent-cloud-using-the-filepulse-connector/"},{"categories":["ksqlDB","Confluent Cloud"],"content":" Using ksqlDB in Confluent Cloud makes things a whole bunch easier because now you just get to build apps and streaming pipelines, instead of having to run and manage a bunch of infrastructure yourself.\nOnce you‚Äôve got ksqlDB provisioned on Confluent Cloud you can use the web-based editor to build and run queries. You can also connect to it using the REST API and the ksqlDB CLI tool. Here‚Äôs how.\n","keywords":null,"title":"Connecting to managed ksqlDB in Confluent Cloud with REST and ksqlDB CLI","uri":"https://rmoff.net/2021/03/24/connecting-to-managed-ksqldb-in-confluent-cloud-with-rest-and-ksqldb-cli/"},{"categories":null,"content":"","keywords":null,"title":"ActiveMQ","uri":"https://rmoff.net/categories/activemq/"},{"categories":["ActiveMQ","Kafka Connect","ksqlDB"],"content":" The ActiveMQ source connector creates a Struct holding the value of the message from ActiveMQ (as well as its key). This is as would be expected. However, you can encounter challenges in working with the data if the ActiveMQ data of interest within the payload is complex. Things like converters and schemas can get really funky, really quick.\n","keywords":null,"title":"Using ksqlDB to process data ingested from ActiveMQ with Kafka Connect","uri":"https://rmoff.net/2021/03/19/using-ksqldb-to-process-data-ingested-from-activemq-with-kafka-connect/"},{"categories":null,"content":"","keywords":null,"title":"jdbc sink","uri":"https://rmoff.net/categories/jdbc-sink/"},{"categories":["Kafka Connect","JDBC Sink"],"content":" The Kafka Connect JDBC Sink can be used to stream data from a Kafka topic to a database such as Oracle, Postgres, MySQL, DB2, etc.\nIt supports many permutations of configuration around how primary keys are handled. The documentation details these. This article aims to illustrate and expand on this.\n","keywords":null,"title":"Kafka Connect JDBC Sink deep-dive: Working with Primary Keys","uri":"https://rmoff.net/2021/03/12/kafka-connect-jdbc-sink-deep-dive-working-with-primary-keys/"},{"categories":["Kafka Connect","JDBC Sink","MySQL"],"content":" I got the error SQLSyntaxErrorException: BLOB/TEXT column 'MESSAGE_KEY' used in key specification without a key length with Kafka Connect JDBC Sink connector (v10.0.2) and MySQL (8.0.23)\n","keywords":null,"title":"Kafka Connect - SQLSyntaxErrorException: BLOB/TEXT column ‚Ä¶ used in key specification without a key length","uri":"https://rmoff.net/2021/03/11/kafka-connect-sqlsyntaxerrorexception-blob/text-column-used-in-key-specification-without-a-key-length/"},{"categories":null,"content":"","keywords":null,"title":"MySQL","uri":"https://rmoff.net/categories/mysql/"},{"categories":null,"content":"","keywords":null,"title":"Kafkacat","uri":"https://rmoff.net/categories/kafkacat/"},{"categories":["Data","kafkacat","visidata"],"content":" ksqlDB is a fantastically powerful tool for processing and analysing streams of data in Apache Kafka. But sometimes, you just want a quick way to profile the data in a topic in Kafka. I wrote about this previously with a convoluted (but effective) set of bash commands pipelined together to perform a GROUP BY on data. Then someone introduced me to visidata, which makes it all a lot quicker!\n","keywords":null,"title":"Quick profiling of data in Apache Kafka using kafkacat and visidata","uri":"https://rmoff.net/2021/03/04/quick-profiling-of-data-in-apache-kafka-using-kafkacat-and-visidata/"},{"categories":null,"content":"","keywords":null,"title":"visidata","uri":"https://rmoff.net/categories/visidata/"},{"categories":null,"content":"","keywords":null,"title":"kibana","uri":"https://rmoff.net/categories/kibana/"},{"categories":null,"content":"","keywords":null,"title":"Open Data","uri":"https://rmoff.net/categories/open-data/"},{"categories":["Kibana","Open Data"],"content":"Kibana‚Äôs map functionality is a powerful way to visualise data that has a location element in it. I was recently working with data about ships at sea, and whilst the built in Road map is very good it doesn‚Äôt show much maritime detail.\nKibana‚Äôs map visualisation has the option to pull in additional visual information from other places (known as tile servers). I found a list of Tile servers, which had details of OpenSeaMap which includes:","keywords":null,"title":"Using Open Sea Map data in Kibana maps","uri":"https://rmoff.net/2021/03/04/using-open-sea-map-data-in-kibana-maps/"},{"categories":null,"content":"","keywords":null,"title":"Bash","uri":"https://rmoff.net/categories/bash/"},{"categories":null,"content":"","keywords":null,"title":"delimited data","uri":"https://rmoff.net/categories/delimited-data/"},{"categories":null,"content":"","keywords":null,"title":"hexdump","uri":"https://rmoff.net/categories/hexdump/"},{"categories":["kafkacat","csv","ksqldb","delimited data","data engineering","bash","hexdump"],"content":" Whilst Apache Kafka is an event streaming platform designed for, well, streams of events, it‚Äôs perfectly valid to use it as a store of data which perhaps changes only occasionally (or even never). I‚Äôm thinking here of reference data (lookup data) that‚Äôs used to enrich regular streams of events.\nYou might well get your reference data from a database where it resides and do so effectively using CDC - but sometimes it comes down to those pesky CSV files that we all know and love/hate. Simple, awful, but effective. I wrote previously about loading CSV data into Kafka from files that are updated frequently, but here I want to look at CSV files that are not changing. Kafka Connect simplifies getting data in to (and out of) Kafka but even Kafka Connect becomes a bit of an overhead when you just have a single file that you want to load into a topic and then never deal with again. I spent this afternoon wrangling with a couple of CSV-ish files, and building on my previous article about neat tricks you can do in bash with data, I have some more to share with you here :)\n","keywords":null,"title":"Loading delimited data into Kafka - quick \u0026 dirty (but effective)","uri":"https://rmoff.net/2021/02/26/loading-delimited-data-into-kafka-quick-dirty-but-effective/"},{"categories":["ksqlDB"],"content":" Some people learn through doing - and for that there‚Äôs a bunch of good ksqlDB tutorials here and here. Others may prefer to watch and listen first, before getting hands on. And for that, I humbly offer you this little series of videos all about ksqlDB. They‚Äôre all based on a set of demo scripts that you can run for yourself and try out.\nüö® Make sure you subscribe to my YouTube channel so that you don‚Äôt miss more videos like these!\n","keywords":null,"title":"üìº ksqlDB HOWTO - A mini video series üìº","uri":"https://rmoff.net/2021/02/17/ksqldb-howto-a-mini-video-series/"},{"categories":["Data Engineering","Bash","Kafkacat"],"content":" One of the fun things about working with data over the years is learning how to use the tools of the day‚Äîbut also learning to fall back on the tools that are always there for you - and one of those is bash and its wonderful library of shell tools.\nThere‚Äôs an even better way than I‚Äôve described here, and it‚Äôs called visidata. I‚Äôve written about it more over here. I‚Äôve been playing around with a new data source recently, and needed to understand more about its structure. Within a single stream there were multiple message types.\n","keywords":null,"title":"Performing a GROUP BY on data in bash","uri":"https://rmoff.net/2021/02/02/performing-a-group-by-on-data-in-bash/"},{"categories":null,"content":"","keywords":null,"title":"docker","uri":"https://rmoff.net/tag/docker/"},{"categories":null,"content":"","keywords":null,"title":"oracle","uri":"https://rmoff.net/tag/oracle/"},{"categories":null,"content":"","keywords":null,"title":"root","uri":"https://rmoff.net/tag/root/"},{"categories":null,"content":"","keywords":null,"title":"root","uri":"https://rmoff.net/categories/root/"},{"categories":["oracle","docker","sudo","root"],"content":"tl;dr: specify the --user root argument:\ndocker exec --interactive \\ --tty \\ --user root \\ --workdir / \\ container-name bash","keywords":null,"title":"Running as root on Docker images that don‚Äôt use root","uri":"https://rmoff.net/2021/01/13/running-as-root-on-docker-images-that-dont-use-root/"},{"categories":null,"content":"","keywords":null,"title":"sudo","uri":"https://rmoff.net/tag/sudo/"},{"categories":null,"content":"","keywords":null,"title":"sudo","uri":"https://rmoff.net/categories/sudo/"},{"categories":null,"content":"","keywords":null,"title":"Tag","uri":"https://rmoff.net/tag/"},{"categories":null,"content":"","keywords":null,"title":"GCP","uri":"https://rmoff.net/categories/gcp/"},{"categories":["Kafka Connect","GCP","Docker","Confluent Cloud"],"content":" Confluent Cloud is not only a fully-managed Apache Kafka service, but also provides important additional pieces for building applications and pipelines including managed connectors, Schema Registry, and ksqlDB. Managed Connectors are run for you (hence, managed!) within Confluent Cloud - you just specify the technology to which you want to integrate in or out of Kafka and Confluent Cloud does the rest.\n","keywords":null,"title":"Running a self-managed Kafka Connect worker for Confluent Cloud","uri":"https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/"},{"categories":["Kafka Connect"],"content":" When Kafka Connect ingests data from a source system into Kafka it writes it to a topic. If you have set auto.create.topics.enable = true on your broker then the topic will be created when written to. If auto.create.topics.enable = false (as it is on Confluent Cloud and many self-managed environments, for good reasons) then you can tell Kafka Connect to create those topics first. This was added in Apache Kafka 2.6 (Confluent Platform 6.0) - prior to that you had to manually create the topics yourself otherwise the connector would fail.\n","keywords":null,"title":"Creating topics with Kafka Connect","uri":"https://rmoff.net/2021/01/06/creating-topics-with-kafka-connect/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" KIP-66 was added in Apache Kafka 0.10.2 and brought new functionality called Single Message Transforms (SMT). Using SMT you can modify the data and its characteristics as it passes through Kafka Connect pipeline, without needing additional stream processors. For things like manipulating fields, changing topic names, conditionally dropping messages, and more, SMT are a perfect solution. If you get to things like aggregation, joining streams, and lookups then SMT may not be the best for you and you should head over to Kafka Streams or ksqlDB instead.\n","keywords":null,"title":"Kafka Connect - Deep Dive into Single Message Transforms","uri":"https://rmoff.net/2021/01/04/kafka-connect-deep-dive-into-single-message-transforms/"},{"categories":null,"content":"","keywords":null,"title":"Single Message Transform","uri":"https://rmoff.net/categories/single-message-transform/"},{"categories":null,"content":"","keywords":null,"title":"TwelveDaysOfSMT","uri":"https://rmoff.net/categories/twelvedaysofsmt/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":"Apache Kafka ships with many Single Message Transformations (SMT) included - but the great thing about it being an open API is that people can, and do, write their own transformations. Many of these are shared with the wider community, and in this final installment of the series I‚Äôm going to look at some of the transformations written by Jeremy Custenborder and available in kafka-connect-transform-common which can be downloaded and installed from Confluent Hub (or built from source, if you like that kind of thing).","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 12: Community Transformations","uri":"https://rmoff.net/2020/12/23/twelve-days-of-smt-day-12-community-transformations/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" Apache Kafka 2.6 included KIP-585 which adds support for defining predicates against which transforms are conditionally executed, as well as a Filter Single Message Transform to drop messages - which in combination means that you can conditionally drop messages.\nAs part of Apache Kafka, Kafka Connect ships with pre-built Single Message Transforms and Predicates, but you can also write you own. The API for each is documented: Transformation / Predicate. The predicates that ship with Apache Kafka are:\nRecordIsTombstone - The value part of the message is null (denoting a tombstone message)\nHasHeaderKey- Matches if a header exists with the name given\nTopicNameMatches - Matches based on topic\n","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 11: Predicate and Filter","uri":"https://rmoff.net/2020/12/22/twelve-days-of-smt-day-11-predicate-and-filter/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" The ReplaceField Single Message Transform has three modes of operation on fields of data passing through Kafka Connect:\nInclude only the fields specified in the list (whitelist)\nInclude all fields except the ones specified (blacklist)\nRename field(s) (renames)\n","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 10: ReplaceField","uri":"https://rmoff.net/2020/12/21/twelve-days-of-smt-day-10-replacefield/"},{"categories":["Blogging","Hugo","GitHub","GitHub Actions"],"content":" Over the years I‚Äôve used various blogging platforms; after a brief dalliance with Blogger I started for real with the near-inevitable Wordpress.com. From there I decided it would be fun to self-host using Ghost, and then almost exactly two years ago to the day decided it definitely was not fun to spend time patching and upgrading my blog platform instead of writing blog articles, so headed over to my current platform of choice: Hugo hosted on GitHub pages. This has worked extremely well for me during that time, doing everything I want from it until recently.\n","keywords":null,"title":"Scheduling Hugo Builds on GitHub pages with GitHub Actions","uri":"https://rmoff.net/2020/12/20/scheduling-hugo-builds-on-github-pages-with-github-actions/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" The Cast Single Message Transform lets you change the data type of fields in a Kafka message, supporting numerics, string, and boolean.\n","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 9: Cast","uri":"https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" The TimestampConverter Single Message Transform lets you work with timestamp fields in Kafka messages. You can convert a string into a native Timestamp type (or Date or Time), as well as Unix epoch - and the same in reverse too.\nThis is really useful to make sure that data ingested into Kafka is correctly stored as a Timestamp (if it is one), and also enables you to write a Timestamp out to a sink connector in a string format that you choose.\n","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 8: TimestampConverter","uri":"https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" Just like the RegExRouter, the TimeStampRouter can be used to modify the topic name of messages as they pass through Kafka Connect. Since the topic name is usually the basis for the naming of the object to which messages are written in a sink connector, this is a great way to achieve time-based partitioning of those objects if required. For example, instead of streaming messages from Kafka to an Elasticsearch index called cars, they can be routed to monthly indices e.g. cars_2020-10, cars_2020-11, cars_2020-12, etc.\nThe TimeStampRouter takes two arguments; the format of the final topic name to generate, and the format of the timestamp to put in the topic name (based on SimpleDateFormat).\n\"transforms\" : \"addTimestampToTopic\", \"transforms.addTimestampToTopic.type\" : \"org.apache.kafka.connect.transforms.TimestampRouter\", \"transforms.addTimestampToTopic.topic.format\" : \"${topic}_${timestamp}\", \"transforms.addTimestampToTopic.timestamp.format\": \"YYYY-MM-dd\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 7: TimestampRouter","uri":"https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" We kicked off this series by seeing on day 1 how to use InsertField to add in the timestamp to a message passing through the Kafka Connect sink connector. Today we‚Äôll see how to use the same Single Message Transform to add in a static field value, as well as the name of the Kafka topic, partition, and offset from which the message has been read.\n\"transforms\" : \"insertStaticField1\", \"transforms.insertStaticField1.type\" : \"org.apache.kafka.connect.transforms.InsertField$Value\", \"transforms.insertStaticField1.static.field\": \"sourceSystem\", \"transforms.insertStaticField1.static.value\": \"NeverGonna\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 6: InsertField II","uri":"https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" If you want to mask fields of data as you ingest from a source into Kafka, or write to a sink from Kafka with Kafka Connect, the MaskField Single Message Transform is perfect for you. It retains the fields whilst replacing its value.\nTo use the Single Message Transform you specify the field to mask, and its replacement value. To mask the contents of a field called cc_num you would use:\n\"transforms\" : \"maskCC\", \"transforms.maskCC.type\" : \"org.apache.kafka.connect.transforms.MaskField$Value\", \"transforms.maskCC.fields\" : \"cc_num\", \"transforms.maskCC.replacement\" : \"****-****-****-****\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 5: MaskField","uri":"https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" If you want to change the topic name to which a source connector writes, or object name that‚Äôs created on a target by a sink connector, the RegExRouter is exactly what you need.\nTo use the Single Message Transform you specify the pattern in the topic name to match, and its replacement. To drop a prefix of test- from a topic you would use:\n\"transforms\" : \"dropTopicPrefix\", \"transforms.dropTopicPrefix.type\" : \"org.apache.kafka.connect.transforms.RegexRouter\", \"transforms.dropTopicPrefix.regex\" : \"test-(.*)\", \"transforms.dropTopicPrefix.replacement\" : \"$1\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 4: RegExRouter","uri":"https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" The Flatten Single Message Transform (SMT) is useful when you need to collapse a nested message down to a flat structure.\nTo use the Single Message Transform you only need to reference it; there‚Äôs no additional configuration required:\n\"transforms\" : \"flatten\", \"transforms.flatten.type\" : \"org.apache.kafka.connect.transforms.Flatten$Value\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 3: Flatten","uri":"https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" Setting the key of a Kafka message is important as it ensures correct logical processing when consumed across multiple partitions, as well as being a requirement when joining to messages in other topics. When using Kafka Connect the connector may already set the key, which is great. If not, you can use these two Single Message Transforms (SMT) to set it as part of the pipeline based on a field in the value part of the message.\nTo use the ValueToKey Single Message Transform specify the name of the field (id) that you want to copy from the value to the key:\n\"transforms\" : \"copyIdToKey\", \"transforms.copyIdToKey.type\" : \"org.apache.kafka.connect.transforms.ValueToKey\", \"transforms.copyIdToKey.fields\" : \"id\", ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 2: ValueToKey and ExtractField","uri":"https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/"},{"categories":["Kafka Connect","Single Message Transform","TwelveDaysOfSMT"],"content":" You can use the InsertField Single Message Transform (SMT) to add the message timestamp into each message that Kafka Connect sends to a sink.\nTo use the Single Message Transform specify the name of the field (timestamp.field) that you want to add to hold the message timestamp:\n\"transforms\" : \"insertTS\", \"transforms.insertTS.type\" : \"org.apache.kafka.connect.transforms.InsertField$Value\", \"transforms.insertTS.timestamp.field\": \"messageTS\" ","keywords":null,"title":"üéÑ Twelve Days of SMT üéÑ - Day 1: InsertField (timestamp)","uri":"https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/"},{"categories":["DevRel","Virtual Conferences"],"content":"Back in March 2020 the western world came to somewhat of a juddering halt, thanks to COVID-19. No-one knew then what would happen, but there was the impression that whilst the next few months were a write-off for sure, maybe things would pick up again later in the year.\nIt‚Äôs now early December 2020, and nothing is picking up any time soon. Summer provided a respite from the high levels of infection and mortality (in the UK at least), but then numbers spiked again in many places around the world and what was punted down the river back in March is being firmly punted yet again now.","keywords":null,"title":"Life as a Developer Advocate, nine months into a pandemic","uri":"https://rmoff.net/2020/12/03/life-as-a-developer-advocate-nine-months-into-a-pandemic/"},{"categories":null,"content":"","keywords":null,"title":"Virtual Conferences","uri":"https://rmoff.net/categories/virtual-conferences/"},{"categories":null,"content":"","keywords":null,"title":"Home office","uri":"https://rmoff.net/categories/home-office/"},{"categories":["DevRel","Home office"],"content":" Is a blog even a blog nowadays if it doesn‚Äôt include a \"Here is my home office setup\"?\nThanks to conferences all being online, and thus my talks being delivered from my study‚Äîand my habit of posting a #SpeakerSelfie each time I do a conference talk‚ÄîI often get questions about my setup. Plus, I‚Äôm kinda pleased with it so I want to show it off too ;-)\n","keywords":null,"title":"My Workstation - 2020","uri":"https://rmoff.net/2020/12/02/my-workstation-2020/"},{"categories":null,"content":"","keywords":null,"title":"Keynote","uri":"https://rmoff.net/categories/keynote/"},{"categories":["Keynote"],"content":" Very short \u0026 sweet this post, but Google turned up nothing when I was stuck so hopefully I‚Äôll save someone else some head scratching by sharing this.\n","keywords":null,"title":"Keynote - Why is Replace Fonts greyed out?","uri":"https://rmoff.net/2020/11/13/keynote-why-is-replace-fonts-greyed-out/"},{"categories":null,"content":"","keywords":null,"title":"Compacted topic","uri":"https://rmoff.net/categories/compacted-topic/"},{"categories":null,"content":"","keywords":null,"title":"Elasticsearch","uri":"https://rmoff.net/categories/elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"Kafka","uri":"https://rmoff.net/categories/kafka/"},{"categories":["Kafka","ksqlDB","Tombstone","Kafka Connect","Postgres","Elasticsearch","Compacted topic"],"content":" As you may already realise, Kafka is not just a fancy message bus, or a pipe for big data. It‚Äôs an event streaming platform! If this is news to you, I‚Äôll wait here whilst you read this or watch this‚Ä¶\n","keywords":null,"title":"Kafka Connect, ksqlDB, and Kafka Tombstone messages","uri":"https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/"},{"categories":null,"content":"","keywords":null,"title":"Postgres","uri":"https://rmoff.net/categories/postgres/"},{"categories":null,"content":"","keywords":null,"title":"Tombstone","uri":"https://rmoff.net/categories/tombstone/"},{"categories":null,"content":"","keywords":null,"title":"Geo point","uri":"https://rmoff.net/categories/geo-point/"},{"categories":["Kafka Connect","Elasticsearch","Geo point","ksqlDB"],"content":" Streaming data from Kafka to Elasticsearch is easy with Kafka Connect - you can see how in this tutorial and video.\nOne of the things that sometimes causes issues though is how to get location data correctly indexed into Elasticsearch as geo_point fields to enable all that lovely location analysis. Unlike data types like dates and numerics, Elasticsearch‚Äôs Dynamic Field Mapping won‚Äôt automagically pick up geo_point data, and so you have to do two things:\n","keywords":null,"title":"Streaming Geopoint data from Kafka to Elasticsearch","uri":"https://rmoff.net/2020/11/03/streaming-geopoint-data-from-kafka-to-elasticsearch/"},{"categories":["ksqlDB"],"content":" There was a good question on StackOverflow recently in which someone was struggling to find the appropriate ksqlDB DDL to model a source topic in which there was a variable number of fields in a STRUCT.\n","keywords":null,"title":"ksqlDB - How to model a variable number of fields in a nested value (\u003ccode\u003eSTRUCT\u003c/code\u003e)","uri":"https://rmoff.net/2020/10/07/ksqldb-how-to-model-a-variable-number-of-fields-in-a-nested-value-struct/"},{"categories":null,"content":"","keywords":null,"title":"IBM MQ","uri":"https://rmoff.net/categories/ibm-mq/"},{"categories":null,"content":"","keywords":null,"title":"MongoDB","uri":"https://rmoff.net/categories/mongodb/"},{"categories":["XML","Kafka","Kafka Connect","Single Message Transform","MongoDB","IBM MQ"],"content":" Let‚Äôs imagine we have XML data on a queue in IBM MQ, and we want to ingest it into Kafka to then use downstream, perhaps in an application or maybe to stream to a NoSQL store like MongoDB.\nNote This same pattern for ingesting XML will work with other connectors such as JMS and ActiveMQ. ","keywords":null,"title":"Streaming XML messages from IBM MQ into Kafka into MongoDB","uri":"https://rmoff.net/2020/10/05/streaming-xml-messages-from-ibm-mq-into-kafka-into-mongodb/"},{"categories":null,"content":"","keywords":null,"title":"XML","uri":"https://rmoff.net/categories/xml/"},{"categories":["XML","Kafka","Kafka Connect","FilePulse"],"content":" üëâ Ingesting XML data into Kafka - Introduction\nWe saw in the first post how to hack together an ingestion pipeline for XML into Kafka using a source such as curl piped through xq to wrangle the XML and stream it into Kafka using kafkacat, optionally using ksqlDB to apply and register a schema for it.\nThe second one showed the use of any Kafka Connect source connector plus the kafka-connect-transform-xml Single Message Transformation. Now we‚Äôre going to take a look at a source connector from the community that can also be used to ingest XML data into Kafka.\n","keywords":null,"title":"Ingesting XML data into Kafka - Option 3: Kafka Connect FilePulse connector","uri":"https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-3-kafka-connect-filepulse-connector/"},{"categories":["XML","Kafka","Kafka Connect","Single Message Transform"],"content":" We previously looked at the background to getting XML into Kafka, and potentially how [not] to do it. Now let‚Äôs look at the proper way to build a streaming ingestion pipeline for XML into Kafka, using Kafka Connect.\nIf you‚Äôre unfamiliar with Kafka Connect, check out this quick intro to Kafka Connect here. Kafka Connect‚Äôs excellent plugable architecture means that we can pair any source connector to read XML from wherever we have it (for example, a flat file, or a MQ, or anywhere else), with a Single Message Transform to transform the XML into a payload with a schema, and finally a converter to serialise the data in a form that we would like to use such as Avro or Protobuf.\n","keywords":null,"title":"Ingesting XML data into Kafka - Option 2: Kafka Connect plus Single Message Transform","uri":"https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/"},{"categories":["XML","kafkacat","xq"],"content":" üëâ Ingesting XML data into Kafka - Introduction\nWhat would a blog post on rmoff.net be if it didn‚Äôt include the dirty hack option? üòÅ\nThe secret to dirty hacks is that they are often rather effective and when needs must, they can suffice. If you‚Äôre prototyping and need to JFDI, a dirty hack is just fine. If you‚Äôre looking for code to run in Production, then a dirty hack probably is not fine.\n","keywords":null,"title":"Ingesting XML data into Kafka - Option 1: The Dirty Hack","uri":"https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/"},{"categories":null,"content":"","keywords":null,"title":"xq","uri":"https://rmoff.net/categories/xq/"},{"categories":["XML","Kafka"],"content":"XML has been around for 20+ years, and whilst other ways of serialising our data have gained popularity in more recent times (such as JSON, Avro, and Protobuf), XML is not going away soon. Part of that is down to technical reasons (clearly defined and documented schemas), and part of it is simply down to enterprise inertia - having adopted XML for systems in the last couple of decades, they‚Äôre not going to be changing now just for some short-term fad.","keywords":null,"title":"Ingesting XML data into Kafka - Introduction","uri":"https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/"},{"categories":["abcde","CD ripping"],"content":" Short \u0026 sweet to help out future Googlers. Trying to use abcde I got the error:\n[WARNING] something went wrong while querying the CD... Maybe a DATA CD or the CD is not loaded? [WARNING] Error trying to calculate disc ids without lead-out information. ","keywords":null,"title":"\u003ccode\u003eabcde\u003c/code\u003e - Error trying to calculate disc ids without lead-out information","uri":"https://rmoff.net/2020/10/01/abcde-error-trying-to-calculate-disc-ids-without-lead-out-information/"},{"categories":null,"content":"","keywords":null,"title":"abcde","uri":"https://rmoff.net/categories/abcde/"},{"categories":null,"content":"","keywords":null,"title":"CD ripping","uri":"https://rmoff.net/categories/cd-ripping/"},{"categories":["IBM MQ","Docker"],"content":" Running IBM MQ in a Docker container and the client connecting to it was throwing repeated Channel was blocked errors.\n","keywords":null,"title":"IBM MQ on Docker - Channel was blocked","uri":"https://rmoff.net/2020/10/01/ibm-mq-on-docker-channel-was-blocked/"},{"categories":null,"content":"","keywords":null,"title":"jq","uri":"https://rmoff.net/categories/jq/"},{"categories":["jq","kafkacat"],"content":" One of my favourite hacks for getting data into Kafka is using kafkacat and stdin, often from jq. You can see this in action with Wi-Fi data, IoT data, and data from a REST endpoint. This is fine for getting values into a Kafka message - but Kafka messages are key/value, and being able to specify a key is can often be important.\nHere‚Äôs a way to do that, using a separator and some jq magic. Note that at the moment kafkacat only supports single byte separator characters, so you need to choose carefully. If you pick a separator that also appears in your data, it‚Äôs possibly going to have unintended consequences.\n","keywords":null,"title":"Setting key value when piping from jq to kafkacat","uri":"https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/"},{"categories":null,"content":"","keywords":null,"title":"Datasets","uri":"https://rmoff.net/categories/datasets/"},{"categories":null,"content":"","keywords":null,"title":"Sample data","uri":"https://rmoff.net/categories/sample-data/"},{"categories":["Datasets","Sample data"],"content":" Readers of a certain age and RDBMS background will probably remember northwind, or HR, or OE databases - or quite possibly not just remember them but still be using them. Hardcoded sample data is fine, and it‚Äôs great for repeatable tutorials and examples - but it‚Äôs boring as heck if you want to build an example with something that isn‚Äôt using the same data set for the 100th time.\n","keywords":null,"title":"Some of my favourite public data sets","uri":"https://rmoff.net/2020/09/25/some-of-my-favourite-public-data-sets/"},{"categories":["Apache Kafka","ksqlDB","Talks","Presentations"],"content":" Here‚Äôs a collection of Kafka-related talks, just for you.\nEach one has üçøüé• a recording, üìî slides, and üëæ code to go and try out.¬†","keywords":null,"title":"üìå    üéÅ A collection of Kafka-related talks üíù","uri":"https://rmoff.net/2020/09/23/a-collection-of-kafka-related-talks/"},{"categories":null,"content":"","keywords":null,"title":"Presentations","uri":"https://rmoff.net/categories/presentations/"},{"categories":null,"content":"","keywords":null,"title":"Talks","uri":"https://rmoff.net/categories/talks/"},{"categories":null,"content":"","keywords":null,"title":"Confluent Hub","uri":"https://rmoff.net/categories/confluent-hub/"},{"categories":null,"content":"","keywords":null,"title":"Debezium","uri":"https://rmoff.net/categories/debezium/"},{"categories":null,"content":"","keywords":null,"title":"Docker Compose","uri":"https://rmoff.net/categories/docker-compose/"},{"categories":null,"content":"","keywords":null,"title":"MS SQL","uri":"https://rmoff.net/categories/ms-sql/"},{"categories":["MS SQL","Debezium","Confluent Hub","ksqlDB","Docker Compose"],"content":"Prompted by a question on StackOverflow I thought I‚Äôd take a quick look at setting up ksqlDB to ingest CDC events from Microsoft SQL Server using Debezium. Some of this is based on my previous article, Streaming data from SQL Server to Kafka to Snowflake ‚ùÑÔ∏è with Kafka Connect.\nSetting up the Docker Compose I like standalone, repeatable, demo code. For that reason I love using Docker Compose and I embed everything in there - connector installation, the kitchen sink - the works.","keywords":null,"title":"Using the Debezium MS SQL connector with ksqlDB embedded Kafka Connect","uri":"https://rmoff.net/2020/09/18/using-the-debezium-ms-sql-connector-with-ksqldb-embedded-kafka-connect/"},{"categories":["Hugo","Asciidoc"],"content":"I use Hugo for my blog, hosted on GitHub pages. One of the reasons I‚Äôm really happy with it is that I can use Asciidoc to author my posts. I was writing a blog recently in which I wanted to include some code that‚Äôs hosted on GitHub. I could have copied \u0026 pasted it into the blog but that would be lame!\nWith Asciidoc you can use the include:: directive to include both local files:","keywords":null,"title":"Including content from external links with Asciidoc in Hugo","uri":"https://rmoff.net/2020/09/18/including-content-from-external-links-with-asciidoc-in-hugo/"},{"categories":null,"content":"","keywords":null,"title":"Video","uri":"https://rmoff.net/categories/video/"},{"categories":["Kafka Connect","Video"],"content":"Kafka Connect is the integration API for Apache Kafka. Check out this video for an overview of what Kafka Connect enables you to do, and how to do it.\n","keywords":null,"title":"What is Kafka Connect?","uri":"https://rmoff.net/2020/09/11/what-is-kafka-connect/"},{"categories":["Kafkacat","ksqlDB"],"content":" There‚Äôs ways, and then there‚Äôs ways, to count the number of records/events/messages in a Kafka topic. Most of them are potentially inaccurate, or inefficient, or both. Here‚Äôs one that falls into the potentially inefficient category, using kafkacat to read all the messages and pipe to wc which with the -l will tell you how many lines there are, and since each message is a line, how many messages you have in the Kafka topic:\n$ kafkacat -b broker:29092 -t mytestopic -C -e -q| wc -l 3 ","keywords":null,"title":"Counting the number of messages in a Kafka topic","uri":"https://rmoff.net/2020/09/08/counting-the-number-of-messages-in-a-kafka-topic/"},{"categories":null,"content":"","keywords":null,"title":"Google Chrome","uri":"https://rmoff.net/categories/google-chrome/"},{"categories":["sqlite","Google Chrome"],"content":" Google Chrome automagically adds sites that you visit which support searching to a list of custom search engines. For each one you can set a keyword which activates it, so based on the above list if I want to search Amazon I can just type a \u003ctab\u003e and then my search term\n","keywords":null,"title":"Poking around the search engines in Google Chrome","uri":"https://rmoff.net/2020/09/07/poking-around-the-search-engines-in-google-chrome/"},{"categories":null,"content":"","keywords":null,"title":"sqlite","uri":"https://rmoff.net/categories/sqlite/"},{"categories":null,"content":"","keywords":null,"title":"Go","uri":"https://rmoff.net/categories/go/"},{"categories":null,"content":"","keywords":null,"title":"Golang","uri":"https://rmoff.net/categories/golang/"},{"categories":null,"content":"","keywords":null,"title":"REST API","uri":"https://rmoff.net/categories/rest-api/"},{"categories":["Go","Golang","ksqlDB","REST API","DevRel","Talks"],"content":" I had the pleasure of presenting at DataEngBytes recently, and am delighted to share with you the üóíÔ∏è slides, üëæ code, and üé• recording of my ‚ú®brand new talk‚ú®:\nü§ñBuilding a Telegram bot with Apache Kafka, Go, and ksqlDB\n","keywords":null,"title":"ü§ñBuilding a Telegram bot with Apache Kafka, Go, and ksqlDB","uri":"https://rmoff.net/2020/08/20/building-a-telegram-bot-with-apache-kafka-go-and-ksqldb/"},{"categories":null,"content":"","keywords":null,"title":"Telegram","uri":"https://rmoff.net/categories/telegram/"},{"categories":["Telegram"],"content":" A tiny snippet since I wasted 10 minutes going around the houses on this one‚Ä¶\ntl;dr: If you try to create a command that is not in lower case (e.g. Alert not alert) then the setMyCommands API will return BOT_COMMAND_INVALID\n","keywords":null,"title":"Telegram bot - BOT_COMMAND_INVALID","uri":"https://rmoff.net/2020/07/23/telegram-bot-bot_command_invalid/"},{"categories":null,"content":"","keywords":null,"title":"Chunked Response","uri":"https://rmoff.net/categories/chunked-response/"},{"categories":["Go","Golang","Chunked Response","ksqlDB"],"content":" The server sends Transfer-Encoding: chunked data, and you want to work with the data as you get it, instead of waiting for the server to finish, the EOF to fire, and then process the data?\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E09 - Processing chunked responses before EOF is reached","uri":"https://rmoff.net/2020/07/23/learning-golang-some-rough-notes-s02e09-processing-chunked-responses-before-eof-is-reached/"},{"categories":null,"content":"","keywords":null,"title":"advertised.listeners","uri":"https://rmoff.net/categories/advertised.listeners/"},{"categories":["Go","Golang","advertised.listeners"],"content":"At the beginning of all this my aim was to learn something new (Go), and use it to write a version of a utility that I‚Äôd previously hacked together in Python that checks your Apache Kafka broker configuration for possible problems with the infamous advertised.listeners setting. Check out a blog that I wrote which explains all about Apache Kafka and listener configuration.\nYou can find the code at https://github.com/rmoff/kafka-listeners It‚Äôs been a fun journey, and now I am pleased to be able to show the results of it.","keywords":null,"title":"Learning Golang (some rough notes) - S02E08 - Checking Kafka advertised.listeners with Go","uri":"https://rmoff.net/2020/07/17/learning-golang-some-rough-notes-s02e08-checking-kafka-advertised.listeners-with-go/"},{"categories":["Go","Golang","Packages"],"content":"So far I‚Äôve been running all my code either in the Go Tour sandbox, using Go Playground, or from a single file in VS Code. My explorations in the previous article ended up with a a source file that was starting to get a little bit unwieldily, so let‚Äôs take a look at how that can be improved.\nWithin my most recent code, I have the main function and the doProduce function, which is fine when collapsed down:","keywords":null,"title":"Learning Golang (some rough notes) - S02E07 - Splitting Go code into separate source files and building a binary executable","uri":"https://rmoff.net/2020/07/16/learning-golang-some-rough-notes-s02e07-splitting-go-code-into-separate-source-files-and-building-a-binary-executable/"},{"categories":null,"content":"","keywords":null,"title":"Packages","uri":"https://rmoff.net/categories/packages/"},{"categories":null,"content":"","keywords":null,"title":"Go routine","uri":"https://rmoff.net/categories/go-routine/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Producer API","uri":"https://rmoff.net/categories/kafka-producer-api/"},{"categories":["Go","Golang","Kafka","Kafka Producer API","Go routine"],"content":" When I set out to learn Go one of the aims I had in mind was to write a version of this little Python utility which accompanies a blog I wrote recently about understanding and diagnosing problems with Kafka advertised listeners. Having successfully got Producer, Consumer, and AdminClient API examples working, it is now time to turn to that task.\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E06 - Putting the Producer in a function and handling errors in a Go routine","uri":"https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e06-putting-the-producer-in-a-function-and-handling-errors-in-a-go-routine/"},{"categories":null,"content":"","keywords":null,"title":"Kafka AdminClient API","uri":"https://rmoff.net/categories/kafka-adminclient-api/"},{"categories":["Go","Golang","Kafka","Kafka AdminClient API"],"content":" Having ticked off the basics with an Apache Kafka producer and consumer in Go, let‚Äôs now check out the AdminClient. This is useful for checking out metadata about the cluster, creating topics, and stuff like that.\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E05 - Kafka Go AdminClient","uri":"https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e05-kafka-go-adminclient/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Consumer API","uri":"https://rmoff.net/categories/kafka-consumer-api/"},{"categories":["Go","Golang","Kafka","Kafka Consumer API"],"content":"Last time I looked at creating my first Apache Kafka consumer in Go, which used the now-deprecated channel-based consumer. Whilst idiomatic for Go, it has some issues which mean that the function-based consumer is recommended for use instead. So let‚Äôs go and use it!\nInstead of reading from the Events() channel of the consumer, we read events using the Poll() function with a timeout. The way we handle events (a switch based on their type) is the same:","keywords":null,"title":"Learning Golang (some rough notes) - S02E04 - Kafka Go Consumer (Function-based)","uri":"https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e04-kafka-go-consumer-function-based/"},{"categories":["Go","Golang","Kafka","Kafka Consumer API"],"content":" Having written my first Kafka producer in Go, and even added error handling to it, the next step was to write a consumer. It follows closely the pattern of Producer code I finished up with previously, using the channel-based approach for the Consumer:\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E03 - Kafka Go Consumer (Channel-based)","uri":"https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e03-kafka-go-consumer-channel-based/"},{"categories":null,"content":"","keywords":null,"title":"Error handling","uri":"https://rmoff.net/categories/error-handling/"},{"categories":["Go","Golang","Kafka","Kafka Producer API","Error handling","Type Assertion"],"content":" I looked last time at the very bare basics of writing a Kafka producer using Go. It worked, but only with everything lined up and pointing the right way. There was no error handling of any sorts. Let‚Äôs see about fixing this now.\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E02 - Adding error handling to the Producer","uri":"https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/"},{"categories":null,"content":"","keywords":null,"title":"Type Assertion","uri":"https://rmoff.net/categories/type-assertion/"},{"categories":["Go","Golang","Kafka","Kafka Producer API"],"content":"","keywords":null,"title":"Learning Golang (some rough notes) - S02E01 - My First Kafka Go Producer","uri":"https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e01-my-first-kafka-go-producer/"},{"categories":["Go","Golang","Kafka"],"content":" With the first leg of my journey with Go done (starting from a very rudimentary base), the next step for me was to bring it into my current area of interest and work - Apache Kafka.\n","keywords":null,"title":"Learning Golang (some rough notes) - S02E00 - Kafka and Go","uri":"https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e00-kafka-and-go/"},{"categories":["Go","Golang","Wait Group","Mutex"],"content":" üëâ A Tour of Go : sync.Mutex\nIn the previous exercise I felt my absence of a formal CompSci background with the introduction of Binary Sorted Trees, and now I am concious of it again with learning about mutex. I‚Äôd heard of them before, mostly when Oracle performance folk were talking about wait types - TIL it stands for mutual exclusion!\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E10 - Concurrency (Web Crawler)","uri":"https://rmoff.net/2020/07/03/learning-golang-some-rough-notes-s01e10-concurrency-web-crawler/"},{"categories":null,"content":"","keywords":null,"title":"Mutex","uri":"https://rmoff.net/categories/mutex/"},{"categories":null,"content":"","keywords":null,"title":"Wait Group","uri":"https://rmoff.net/categories/wait-group/"},{"categories":null,"content":"","keywords":null,"title":"json","uri":"https://rmoff.net/categories/json/"},{"categories":null,"content":"","keywords":null,"title":"JSON Schema","uri":"https://rmoff.net/categories/json-schema/"},{"categories":["kafkacat","hexdump","ksqlDB","JSON","JSON Schema"],"content":" I‚Äôve been playing around with the new SerDes (serialisers/deserialisers) that shipped with Confluent Platform 5.5 - Protobuf, and JSON Schema (these were added to the existing support for Avro). The serialisers (and associated Kafka Connect converters) take a payload and serialise it into bytes for sending to Kafka, and I was interested in what those bytes look like. For that I used my favourite Kafka swiss-army knife: kafkacat.\n","keywords":null,"title":"Why JSON isn‚Äôt the same as JSON Schema in Kafka Connect converters and ksqlDB (Viewing Kafka messages bytes as hex)","uri":"https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-and-ksqldb-viewing-kafka-messages-bytes-as-hex/"},{"categories":null,"content":"","keywords":null,"title":"Channels","uri":"https://rmoff.net/categories/channels/"},{"categories":null,"content":"","keywords":null,"title":"Goroutines","uri":"https://rmoff.net/categories/goroutines/"},{"categories":["Go","Golang","Channels","Goroutines"],"content":" A Tour of Go : Goroutines was OK but as with some previous material I headed over to Go by example for clearer explanations.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E09 - Concurrency (Channels, Goroutines)","uri":"https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e09-concurrency-channels-goroutines/"},{"categories":null,"content":"","keywords":null,"title":"Images","uri":"https://rmoff.net/categories/images/"},{"categories":["Go","Golang","Images"],"content":" üëâ A Tour of Go : Exercise: Images\nThis is based on the Picture generator from the Slices exercise.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E08 - Images","uri":"https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e08-images/"},{"categories":["Go","Golang","Readers"],"content":" üëâ A Tour of Go : Readers\nI‚Äôm not intending to pick holes in the Tour‚Ä¶but it‚Äôs not helping itself ;-)\nFor an introductory text, it makes a ton of assumptions about the user. Here it introduces Readers, and the explanation is good‚Äîbut the example code looks like this:\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E07 - Readers","uri":"https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e07-readers/"},{"categories":null,"content":"","keywords":null,"title":"Readers","uri":"https://rmoff.net/categories/readers/"},{"categories":null,"content":"","keywords":null,"title":"Errors","uri":"https://rmoff.net/categories/errors/"},{"categories":["Go","Golang","Errors"],"content":" üëâ A Tour of Go : Exercise: Errors\nLike Interfaces, the Tour didn‚Äôt really do it for me on Errors either. Too absract, and not enough explanation of the code examples for my liking. It also doesn‚Äôt cover the errors package which other tutorial do. I‚Äôm not clear if that‚Äôs because the errors package isn‚Äôt used much, or the Tour focusses only on teaching the raw basics.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E06 - Errors","uri":"https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e06-errors/"},{"categories":null,"content":"","keywords":null,"title":"Interfaces","uri":"https://rmoff.net/categories/interfaces/"},{"categories":["Go","Golang","Interfaces"],"content":" üëâ A Tour of Go : Interfaces\nThis page really threw me, for several reasons:\nThe text notes that there‚Äôs an error (so why don‚Äôt they fix it?)\nThe provided code doesn‚Äôt run (presumably because of the above error)\nIt‚Äôs not clear if this is a deliberate error to illustrate a point, or just a snafu\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E05 - Interfaces","uri":"https://rmoff.net/2020/06/30/learning-golang-some-rough-notes-s01e05-interfaces/"},{"categories":null,"content":"","keywords":null,"title":"Function closures","uri":"https://rmoff.net/categories/function-closures/"},{"categories":["Go","Golang","Function closures"],"content":" üëâ A Tour of Go : Function Closures\nSo far the Tour has been ü§î and üßê and even ü§® but function closures had me ü§Ø ‚Ä¶\nEach of the words on the page made sense but strung together in a sentence didn‚Äôt really make any sense to me.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E04 - Function Closures","uri":"https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e04-function-closures/"},{"categories":["Go","Golang","Maps"],"content":" üëâ A Tour of Go : Exercise - Maps\nImplement WordCount\nThis is probably bread-and-butter for any seasoned programmer, but I enjoyed the simple process and satisfaction of breaking the problem down into steps to solve using what the tutorial had just covered. Sketching out the logic in pseudo-code first, I figured that I wanted to do this:\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E03 - Maps","uri":"https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e03-maps/"},{"categories":null,"content":"","keywords":null,"title":"Maps","uri":"https://rmoff.net/categories/maps/"},{"categories":["Go","Golang","Slices"],"content":" Note Learning Go : Background üëâ A Tour of Go : Slices\nSlices made sense, until I got to Slice length and capacity. Two bits puzzled me in this code:\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E02 - Slices","uri":"https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e02-slices/"},{"categories":null,"content":"","keywords":null,"title":"Slices","uri":"https://rmoff.net/categories/slices/"},{"categories":["Go","Golang","Pointers"],"content":" Note Learning Go : Background üëâ A Tour of Go : Pointers\nI‚Äôve never used pointers before. Found plenty of good resources about what they are, e.g.\nhttps://www.callicoder.com/golang-pointers/\nhttps://dave.cheney.net/2017/04/26/understand-go-pointers-in-less-than-800-words-or-your-money-back\nBut why? It‚Äôs like explaining patiently to someone that 2+2 = 4, without really explaining why would we want to add two numbers together in the first place.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E01 - Pointers","uri":"https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e01-pointers/"},{"categories":null,"content":"","keywords":null,"title":"Pointers","uri":"https://rmoff.net/categories/pointers/"},{"categories":["Go","Golang","Learning"],"content":" My background is not a traditional CompSci one. I studied Music at university, and managed to wangle my way into IT through various means, ending up doing what I do now with no formal training in coding, and a grab-bag of hacky programming attempts on my CV. My weapons of choice have been BBC Basic, VBA, ASP, and more recently some very unpythonic-Python. It‚Äôs got me by, but I figured recently I‚Äôd like to learn something new, and several people pointed to Go as a good option.\n","keywords":null,"title":"Learning Golang (some rough notes) - S01E00","uri":"https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/"},{"categories":["Kafka Connect","Docker"],"content":" Kafka Connect (which is part of Apache Kafka) supports pluggable connectors, enabling you to stream data between Kafka and numerous types of system, including to mention just a few:\nDatabases\nMessage Queues\nFlat files\nObject stores\nThe appropriate plugin for the technology which you want to integrate can be found on Confluent Hub.\n","keywords":null,"title":"How to install connector plugins in Kafka Connect","uri":"https://rmoff.net/2020/06/19/how-to-install-connector-plugins-in-kafka-connect/"},{"categories":["Kafka Connect","CSV","ksqlDB","Postgres"],"content":" For whatever reason, CSV still exists as a ubiquitous data interchange format. It doesn‚Äôt get much simpler: chuck some plaintext with fields separated by commas into a file and stick .csv on the end. If you‚Äôre feeling helpful you can include a header row with field names in.\norder_id,customer_id,order_total_usd,make,model,delivery_city,delivery_company,delivery_address 1,535,190899.73,Dodge,Ram Wagon B350,Sheffield,DuBuque LLC,2810 Northland Avenue 2,671,33245.53,Volkswagen,Cabriolet,Edinburgh,Bechtelar-VonRueden,1 Macpherson Crossing In this article we‚Äôll see how to load this CSV data into Kafka, without even needing to write any code\n","keywords":null,"title":"Loading CSV data into Kafka","uri":"https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/"},{"categories":["Kafka REST Proxy","Kafka topics"],"content":" In v5.5 of Confluent Platform the REST Proxy added new Admin API capabilities, including functionality to list, and create, topics on your cluster.\nCheck out the docs here and download Confluent Platform here. The REST proxy is Confluent Community Licenced.\n","keywords":null,"title":"How to list and create Kafka topics using the REST Proxy API","uri":"https://rmoff.net/2020/06/05/how-to-list-and-create-kafka-topics-using-the-rest-proxy-api/"},{"categories":null,"content":"","keywords":null,"title":"Kafka REST Proxy","uri":"https://rmoff.net/categories/kafka-rest-proxy/"},{"categories":null,"content":"","keywords":null,"title":"Kafka topics","uri":"https://rmoff.net/categories/kafka-topics/"},{"categories":["ksqlDB"],"content":" Question from the Confluent Community Slack group:\nHow can I access the data in object in an array like below using ksqlDB stream\n\"Total\": [ { \"TotalType\": \"Standard\", \"TotalAmount\": 15.99 }, { \"TotalType\": \"Old Standard\", \"TotalAmount\": 16, \" STID\":56 } ] ","keywords":null,"title":"Working with JSON nested arrays in ksqlDB - example","uri":"https://rmoff.net/2020/05/26/working-with-json-nested-arrays-in-ksqldb-example/"},{"categories":["Alfred","Productivity"],"content":" Alfred is one of my favourite productivity apps for the Mac. It‚Äôs a file indexer, a clipboard manager, a snippet expander - and that‚Äôs just scratching the surface really. I recently got a new machine without it installed and realised just how much I rely on Alfred, particularly its clipboard manager.\n","keywords":null,"title":"Searching Alfred‚Äôs Clipboard history programatically","uri":"https://rmoff.net/2020/05/18/searching-alfreds-clipboard-history-programatically/"},{"categories":["ksqlDB","Telegram"],"content":"Imagine you‚Äôve got a stream of data; it‚Äôs not ‚Äúbig data,‚Äù but it‚Äôs certainly a lot. Within the data, you‚Äôve got some bits you‚Äôre interested in, and of those bits, you‚Äôd like to be able to query information about them at any point. Sounds fun, right?\nWhat if you didn‚Äôt need any datastore other than Apache Kafka itself to be able to do this? What if you could ingest, filter, enrich, aggregate, and query data with just Kafka?","keywords":null,"title":"Building a Telegram bot with Apache Kafka and ksqlDB","uri":"https://rmoff.net/2020/05/18/building-a-telegram-bot-with-apache-kafka-and-ksqldb/"},{"categories":["Youtube","Screenflow"],"content":"Screenflow has a useful Markers feature for adding notes to the timeline.\nYou can use these to helpfully add a table of contents to your Youtube video, but unfortunately Screenflow doesn‚Äôt have the option to export them directly. Instead, use the free Subler program as an intermediary (download it from here).\nExport from Screenflow with a chapters track\nOpen the file in Subler and export to text file\nFrom there, tidy up the text file from the source","keywords":null,"title":"Add Markers list from Screenflow to Youtube Table of Contents","uri":"https://rmoff.net/2020/05/04/add-markers-list-from-screenflow-to-youtube-table-of-contents/"},{"categories":null,"content":"","keywords":null,"title":"Screenflow","uri":"https://rmoff.net/categories/screenflow/"},{"categories":null,"content":"","keywords":null,"title":"Youtube","uri":"https://rmoff.net/categories/youtube/"},{"categories":null,"content":"","keywords":null,"title":"replicator","uri":"https://rmoff.net/categories/replicator/"},{"categories":["Confluent Cloud","Replicator"],"content":"‚òÅÔ∏èConfluent Cloud is a great solution for a hosted and managed Apache Kafka service, with the additional benefits of Confluent Platform such as ksqlDB and managed Kafka Connect connectors. But as a developer, you won‚Äôt always have a reliable internet connection. Train, planes, and automobiles‚Äînot to mention crappy hotel or conference Wi-Fi. Wouldn‚Äôt it be useful if you could have a replica of your Cloud data on your local machine?","keywords":null,"title":"Using Confluent Cloud when there is no Cloud (or internet)","uri":"https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/"},{"categories":null,"content":"","keywords":null,"title":"fedora","uri":"https://rmoff.net/categories/fedora/"},{"categories":["kafkacat","fedora"],"content":"kafkacat is one of my go-to tools when working with Kafka. It‚Äôs a producer and consumer, but also a swiss-army knife of debugging and troubleshooting capabilities. So when I built a new Fedora server recently, I needed to get it installed. Unfortunately there‚Äôs no pre-packed install available on yum, so here‚Äôs how to do it manually.\nNote kafkacat is now known as kcat (ref). When invoking the command you will need to use kcat in place of kafkacat.","keywords":null,"title":"How to install kafkacat on Fedora","uri":"https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/"},{"categories":["pandoc","asciidoc","docx","Google Docs","MS Word"],"content":"Updated 16 April 2020 to cover formatting tricks \u0026 add import to Google Docs info\nShort and sweet this one. I‚Äôve written in the past how I love Markdown but I‚Äôve actually moved on from that and now firmly throw my hat in the AsciiDoc ring. I‚Äôll write another post another time explaining why in more detail, but in short it‚Äôs just more powerful whilst still simple and readable without compilation.","keywords":null,"title":"Converting from AsciiDoc to Google Docs and MS Word","uri":"https://rmoff.net/2020/04/16/converting-from-asciidoc-to-google-docs-and-ms-word/"},{"categories":null,"content":"","keywords":null,"title":"docx","uri":"https://rmoff.net/categories/docx/"},{"categories":null,"content":"","keywords":null,"title":"MS Word","uri":"https://rmoff.net/categories/ms-word/"},{"categories":null,"content":"","keywords":null,"title":"pandoc","uri":"https://rmoff.net/categories/pandoc/"},{"categories":["Kafka","kafkacat","Telegram","Monitoring"],"content":" I‚Äôve been poking around recently with capturing Wi-Fi packet data and streaming it into Apache Kafka, from where I‚Äôm processing and analysing it. Kafka itself is rock-solid - because I‚Äôm using ‚òÅÔ∏èConfluent Cloud and someone else worries about provisioning it, scaling it, and keeping it running for me. But whilst Kafka works just great, my side of the setup‚Äîtshark running on a Raspberry Pi‚Äîis less than stable. For whatever reason it sometimes stalls and I have to restart the Raspberry Pi and restart the capture process.\n","keywords":null,"title":"A quick and dirty way to monitor data arriving on Kafka","uri":"https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/"},{"categories":null,"content":"","keywords":null,"title":"Monitoring","uri":"https://rmoff.net/categories/monitoring/"},{"categories":["DevRel","Conferences"],"content":"ü¶†COVID-19 has well and truly hit the tech scene this week. As well as being full of \"WFH tips\" for all the tech workers suddenly banished from their offices, my particular Twitter bubble is full of DevRel folk musing and debating about what this interruption means to our profession. For sure, in the short term, the Spring conference season is screwed‚Äî all the conferences are cancelled (or postponed).\nBut what about the future?","keywords":null,"title":"Are Tech Conferences Dead?","uri":"https://rmoff.net/2020/03/13/are-tech-conferences-dead/"},{"categories":null,"content":"","keywords":null,"title":"Raspberry pi","uri":"https://rmoff.net/categories/raspberry-pi/"},{"categories":["Raspberry pi","kafkacat","Confluent Cloud","wireshark","tshark","tcpdump"],"content":"Wi-fi is now ubiquitous in most populated areas, and the way the devices communicate leaves a lot of 'digital exhaust'. Usually a computer will have a Wi-Fi device that‚Äôs configured to connect to a given network, but often these devices can be configured instead to pick up the background Wi-Fi chatter of surrounding devices.\nThere are good reasons‚Äîand bad‚Äîfor doing this. Just like taking apart equipment to understand how it works teaches us things, so being able to dissect and examine protocol traffic lets us learn about this.","keywords":null,"title":"Streaming Wi-Fi trace data from Raspberry Pi to Apache Kafka with Confluent Cloud","uri":"https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/"},{"categories":null,"content":"","keywords":null,"title":"tcpdump","uri":"https://rmoff.net/categories/tcpdump/"},{"categories":null,"content":"","keywords":null,"title":"tshark","uri":"https://rmoff.net/categories/tshark/"},{"categories":null,"content":"","keywords":null,"title":"wireshark","uri":"https://rmoff.net/categories/wireshark/"},{"categories":["Kafka Connect","JDBC Sink"],"content":" I wanted to get some data from a Kafka topic:\nksql\u003e PRINT PERSON_STATS FROM BEGINNING; Key format: KAFKA (STRING) Value format: AVRO rowtime: 2/25/20 1:12:51 PM UTC, key: robin, value: {\"PERSON\": \"robin\", \"LOCATION_CHANGES\":1, \"UNIQUE_LOCATIONS\": 1} into Postgres, so did the easy thing and used Kafka Connect with the JDBC Sink connector.\n","keywords":null,"title":"Kafka Connect JDBC Sink - setting the key field name","uri":"https://rmoff.net/2020/02/25/kafka-connect-jdbc-sink-setting-the-key-field-name/"},{"categories":["Docker Compose","AWS ECS","Kafka","ksqlDB","Elasticsearch"],"content":" My name‚Äôs Robin, and I‚Äôm a Developer Advocate. What that means in part is that I build a ton of demos, and Docker Compose is my jam. I love using Docker Compose for the same reasons that many people do:\nSpin up and tear down fully-functioning multi-component environments with ease. No bespoke builds, no cloning of VMs to preserve \"that magic state where everything works\"\nRepeatability. It‚Äôs the same each time.\nPortability. I can point someone at a docker-compose.yml that I‚Äôve written and they can run the same on their machine with the same results almost guaranteed.\n","keywords":null,"title":"Adventures in the Cloud, Part 94: ECS","uri":"https://rmoff.net/2020/02/13/adventures-in-the-cloud-part-94-ecs/"},{"categories":null,"content":"","keywords":null,"title":"AWS ECS","uri":"https://rmoff.net/categories/aws-ecs/"},{"categories":["ksqlDB","Debezium","Single Message Transform","Kafka Connect","kafkacat"],"content":" ksqlDB 0.7 will add support for message keys as primitive data types beyond just STRING (which is all we‚Äôve had to date). That means that Kafka messages are going to be much easier to work with, and require less wrangling to get into the form in which you need them. Take an example of a database table that you‚Äôve ingested into a Kafka topic, and want to join to a stream of events. Previously you‚Äôd have had to take the Kafka topic into which the table had been ingested and run a ksqlDB processor to re-key the messages such that ksqlDB could join on them. Friends, I am here to tell you that this is no longer needed!\n","keywords":null,"title":"Primitive Keys in ksqlDB","uri":"https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/"},{"categories":null,"content":" Very simple to fix: go to https://calendar.google.com/calendar/syncselect and select the calendars that you want. Click save.\n","keywords":null,"title":"Fantastical / Mac Calendar not showing Google Shared Calendar","uri":"https://rmoff.net/2020/01/24/fantastical-/-mac-calendar-not-showing-google-shared-calendar/"},{"categories":null,"content":"","keywords":null,"title":"InfluxDB","uri":"https://rmoff.net/categories/influxdb/"},{"categories":["Kafka Connect","InfluxDB","kafkacat","Serialisation"],"content":" You can download the InfluxDB connector for Kafka Connect here. Documentation for it is here.\nWhen a message from your source Kafka topic is written to InfluxDB the InfluxDB values are set thus:\nTimestamp is taken from the Kafka message timestamp (which is either set by your producer, or the time at which it was received by the broker)\nTag(s) are taken from the tags field in the message. This field must be a map type - see below\nValue fields are taken from the rest of the message, and must be numeric or boolean\nMeasurement name can be specified as a field of the message, or hardcoded in the connector config.\n","keywords":null,"title":"Notes on getting data into InfluxDB from Kafka with Kafka Connect","uri":"https://rmoff.net/2020/01/23/notes-on-getting-data-into-influxdb-from-kafka-with-kafka-connect/"},{"categories":null,"content":"","keywords":null,"title":"Serialisation","uri":"https://rmoff.net/categories/serialisation/"},{"categories":null,"content":"","keywords":null,"title":"Converters","uri":"https://rmoff.net/categories/converters/"},{"categories":["Kafka Connect","Converters","Schemas"],"content":" Here‚Äôs a fun one that Kafka Connect can sometimes throw out:\njava.lang.ClassCastException: java.util.HashMap cannot be cast to org.apache.kafka.connect.data.Struct HashMap? Struct? HUH?\n","keywords":null,"title":"Kafka Connect and Schemas","uri":"https://rmoff.net/2020/01/22/kafka-connect-and-schemas/"},{"categories":null,"content":"","keywords":null,"title":"Schemas","uri":"https://rmoff.net/categories/schemas/"},{"categories":null,"content":"","keywords":null,"title":"Grafana","uri":"https://rmoff.net/categories/grafana/"},{"categories":["ksqlDB","Kafka Connect","Sonos","InfluxDB","Grafana","kafkacat"],"content":"I‚Äôm quite a fan of Sonos audio equipment but recently had some trouble with some of the devices glitching and even cutting out whilst playing. Under the covers Sonos stuff is running Linux (of course) and exposes some diagnostics through a rudimentary frontend that you can access at http://\u003csonos player IP\u003e:1400/support/review:\nWhilst this gives you the current state, you can‚Äôt get historical data on it. It felt like the problems were happening \"all the time\", but were they actually?","keywords":null,"title":"Monitoring Sonos with ksqlDB, InfluxDB, and Grafana","uri":"https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/"},{"categories":null,"content":"","keywords":null,"title":"Sonos","uri":"https://rmoff.net/categories/sonos/"},{"categories":null,"content":"","keywords":null,"title":"jdk","uri":"https://rmoff.net/categories/jdk/"},{"categories":null,"content":"","keywords":null,"title":"maven","uri":"https://rmoff.net/categories/maven/"},{"categories":["maven","jdk"],"content":" This article is just for Googlers and my future self encountering this error. Recently I was building a Docker image from the ksqlDB code base, and whilst it built successfully the ksqlDB server process in the Docker container when instantiated failed with a UnsupportedClassVersionError:\n","keywords":null,"title":"UnsupportedClassVersionError: \u003ccode\u003e\u003cx\u003e\u003c/code\u003e has been compiled by a more recent version of the Java Runtime","uri":"https://rmoff.net/2020/01/21/unsupportedclassversionerror-x-has-been-compiled-by-a-more-recent-version-of-the-java-runtime/"},{"categories":["Kafka Connect","Troubleshooting","Log4j","Logging"],"content":"Logs are magical things. They tell us what an application is doing‚Äîor not doing. They help us debug problems. As it happens, they also underpin the entire philosophy of Apache Kafka, but that‚Äôs a story for another day. Today we‚Äôre talking about logs written by Kafka Connect, and how we can change the amount of detail written.\nBy default, Kafka Connect will write logs at INFO and above. So when it starts up, the settings that it‚Äôs using, and any WARN or ERROR messages along the way - a missing configuration, a broken connector, and so on.","keywords":null,"title":"Changing the Logging Level for Kafka Connect Dynamically","uri":"https://rmoff.net/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/"},{"categories":null,"content":"","keywords":null,"title":"Log4j","uri":"https://rmoff.net/categories/log4j/"},{"categories":null,"content":"","keywords":null,"title":"Logging","uri":"https://rmoff.net/categories/logging/"},{"categories":null,"content":"","keywords":null,"title":"Troubleshooting","uri":"https://rmoff.net/categories/troubleshooting/"},{"categories":["DevRel","abstracts","presenting","conferences"],"content":"Just over a year ago, I put together the crudely-titled \"Quick Thoughts on Not Writing a Crap Abstract\" after reviewing a few dozen conference abstracts. This time around I‚Äôve had the honour of being on a conference programme committee and with it the pleasure of reading 250+ abstracts‚Äîfrom which I have some more snarky words of wisdom to impart on the matter.\nRemind me‚Ä¶how does this conference game work? Before we really get into it, let‚Äôs recap how this whole game works, because plenty of people are new to conference speaking.","keywords":null,"title":"How to win [or at least not suck] at the conference abstract submission game","uri":"https://rmoff.net/2020/01/16/how-to-win-or-at-least-not-suck-at-the-conference-abstract-submission-game/"},{"categories":null,"content":"","keywords":null,"title":"presenting","uri":"https://rmoff.net/categories/presenting/"},{"categories":null,"content":"","keywords":null,"title":"Aggregation","uri":"https://rmoff.net/categories/aggregation/"},{"categories":["ksqlDB","Windowing","Aggregation"],"content":"Prompted by a question on StackOverflow I had a bit of a dig into how windows behave in ksqlDB, specifically with regards to their start time. This article shows also how to create test data in ksqlDB and create data to be handled with a timestamp in the past.\nFor a general background to windowing in ksqlDB see the excellent docs.\nThe nice thing about recent releases of ksqlDB/KSQL is that you can create and populate streams directly with CREATE STREAM and INSERT INTO respectively.","keywords":null,"title":"Exploring ksqlDB window start time","uri":"https://rmoff.net/2020/01/09/exploring-ksqldb-window-start-time/"},{"categories":null,"content":"","keywords":null,"title":"Windowing","uri":"https://rmoff.net/categories/windowing/"},{"categories":null,"content":"","keywords":null,"title":"ByteArrayConverter","uri":"https://rmoff.net/categories/bytearrayconverter/"},{"categories":null,"content":"","keywords":null,"title":"RabbitMQ","uri":"https://rmoff.net/categories/rabbitmq/"},{"categories":null,"content":"","keywords":null,"title":"Schema","uri":"https://rmoff.net/categories/schema/"},{"categories":["RabbitMQ","Kafka Connect","ksqlDB","postgres","Schema","ByteArrayConverter"],"content":"This was prompted by a question on StackOverflow to which I thought the answer would be straightforward, but turned out not to be so. And then I got a bit carried away and ended up with a nice example of how you can handle schema-less data coming from a system such as RabbitMQ and apply a schema to it.\nNote This same pattern for ingesting bytes and applying a schema will work with other connectors such as MQTT What?","keywords":null,"title":"Streaming messages from RabbitMQ into Kafka with Kafka Connect","uri":"https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/"},{"categories":["Kafka","ksqlDB","syslog","Unifi","Kafka Connect","MongoDB","Neo4j"],"content":"In this post I want to build on my previous one and show another use of the Syslog data that I‚Äôm capturing. Instead of looking for SSH attacks, I‚Äôm going to analyse the behaviour of my networking components.\nNote You can find all the code to run this on GitHub. Getting Syslog data into Kafka As before, let‚Äôs create ourselves a syslog connector in ksqlDB:\nCREATE SOURCE CONNECTOR SOURCE_SYSLOG_UDP_01 WITH ( 'tasks.","keywords":null,"title":"Analysing network behaviour with ksqlDB and MongoDB","uri":"https://rmoff.net/2019/12/20/analysing-network-behaviour-with-ksqldb-and-mongodb/"},{"categories":null,"content":"","keywords":null,"title":"Neo4j","uri":"https://rmoff.net/categories/neo4j/"},{"categories":null,"content":"","keywords":null,"title":"syslog","uri":"https://rmoff.net/categories/syslog/"},{"categories":null,"content":"","keywords":null,"title":"Unifi","uri":"https://rmoff.net/categories/unifi/"},{"categories":["Kafka","ksqlDB","syslog","SSH","Kafka Connect","Elasticsearch","Neo4j"],"content":"I‚Äôve written previously about ingesting Syslog into Kafka and using KSQL to analyse it. I want to revisit the subject since it‚Äôs nearly two years since I wrote about it and some things have changed since then.\nksqlDB now includes the ability to define connectors from within it, which makes setting things up loads easier.\nYou can find the full rig to run this on GitHub.\nCreate and configure the Syslog connector To start with, create a source connector:","keywords":null,"title":"Detecting and Analysing SSH Attacks with ksqlDB","uri":"https://rmoff.net/2019/12/18/detecting-and-analysing-ssh-attacks-with-ksqldb/"},{"categories":null,"content":"","keywords":null,"title":"ssh","uri":"https://rmoff.net/categories/ssh/"},{"categories":["unifi","ubiquiti","mongoDB","ssh","hacks"],"content":"This is revisiting the blog I wrote a while back, which showed using mongodump and mongorestore to copy a MongoDB database from one machine (a Unifi CloudKey) to another. This time instead of a manual lift and shift, I wanted a simple way to automate the update of the target with changes made on the source.\nThe source is as before, Unifi‚Äôs CloudKey, which runs MongoDB to store its data about the network - devices, access points, events, and so on.","keywords":null,"title":"Copy MongoDB collections from remote to local instance","uri":"https://rmoff.net/2019/12/17/copy-mongodb-collections-from-remote-to-local-instance/"},{"categories":null,"content":"","keywords":null,"title":"hacks","uri":"https://rmoff.net/categories/hacks/"},{"categories":null,"content":"","keywords":null,"title":"ubiquiti","uri":"https://rmoff.net/categories/ubiquiti/"},{"categories":["Kafka Connect","Kafka"],"content":" A short \u0026 sweet blog post to help people Googling for this error, and me next time I encounter it.\nThe scenario: trying to create a connector in Kafka Connect (running in distributed mode, one worker) failed with the curl response\nHTTP/1.1 500 Internal Server Error Date: Fri, 29 Nov 2019 14:33:53 GMT Content-Type: application/json Content-Length: 48 Server: Jetty(9.4.18.v20190429) {\"error_code\":500,\"message\":\"Request timed out\"} ","keywords":null,"title":"Kafka Connect - Request timed out","uri":"https://rmoff.net/2019/11/29/kafka-connect-request-timed-out/"},{"categories":["Docker","tcpdump"],"content":" I was doing some troubleshooting between two services recently and wanting to poke around to see what was happening in the REST calls between them. Normally I‚Äôd reach for tcpdump to do this but imagine my horror when I saw:\nroot@ksqldb-server:/# tcpdump bash: tcpdump: command not found ","keywords":null,"title":"Using tcpdump With Docker","uri":"https://rmoff.net/2019/11/29/using-tcpdump-with-docker/"},{"categories":["Kafka Connect"],"content":"Kafka Connect can be deployed in two modes: Standalone or Distributed. You can learn more about them in my Kafka Summit London 2019 talk.\nI usually recommend Distributed for several reasons:\nIt can scale\nIt is fault-tolerant\nIt can be run on a single node sandbox or a multi-node production environment\nIt is the same configuration method however you run it\nI usually find that Standalone is appropriate when:","keywords":null,"title":"Common mistakes made when configuring multiple Kafka Connect workers","uri":"https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/"},{"categories":null,"content":"","keywords":null,"title":"Snowflake","uri":"https://rmoff.net/categories/snowflake/"},{"categories":null,"content":"","keywords":null,"title":"SQL Server","uri":"https://rmoff.net/categories/sql-server/"},{"categories":["Kafka Connect","Snowflake","SQL Server","Confluent Cloud","Debezium"],"content":"Snowflake is the data warehouse built for the cloud, so let‚Äôs get all ‚òÅÔ∏è cloudy and stream some data from Kafka running in Confluent Cloud to Snowflake!\nWhat I‚Äôm showing also works just as well for an on-premises Kafka cluster. I‚Äôm using SQL Server as an example data source, with Debezium to capture and stream and changes from it into Kafka.\nI‚Äôm assuming that you‚Äôve signed up for Confluent Cloud and Snowflake and are the proud owner of credentials for both.","keywords":null,"title":"Streaming data from SQL Server to Kafka to Snowflake ‚ùÑÔ∏è with Kafka Connect","uri":"https://rmoff.net/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/"},{"categories":["Kafka Connect","Confluent Cloud","Docker","GCP"],"content":" I talk and write about Kafka and Confluent Platform a lot, and more and more of the demos that I‚Äôm building are around Confluent Cloud. This means that I don‚Äôt have to run or manage my own Kafka brokers, Zookeeper, Schema Registry, KSQL servers, etc which makes things a ton easier. Whilst there are managed connectors on Confluent Cloud (S3 etc), I need to run my own Kafka Connect worker for those connectors not yet provided. An example is the MQTT source connector that I use in this demo. Up until now I‚Äôd either run this worker locally, or manually build a cloud VM. Locally is fine, as it‚Äôs all Docker, easily spun up in a single docker-compose up -d command. I wanted something that would keep running whilst my laptop was off, but that was as close to my local build as possible‚Äîenter GCP and its functionality to run a container on a VM automagically.\nYou can see the full script here. The rest of this article just walks through the how and why.\n","keywords":null,"title":"Running Dockerised Kafka Connect worker on GCP","uri":"https://rmoff.net/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/"},{"categories":["Debezium","MySQL"],"content":" I started hitting problems when trying Debezium against MySQL v8. When creating the connector:\n","keywords":null,"title":"Debezium \u0026 MySQL v8 : Public Key Retrieval Is Not Allowed","uri":"https://rmoff.net/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/"},{"categories":["Kafka Connect","Debezium","kafkacat","Confluent Cloud"],"content":" This is based on using Confluent Cloud to provide your managed Kafka and Schema Registry. All that you run yourself is the Kafka Connect worker.\nOptionally, you can use this Docker Compose to run the worker and a sample MySQL database.\n","keywords":null,"title":"Using Kafka Connect and Debezium with Confluent Cloud","uri":"https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/"},{"categories":null,"content":"","keywords":null,"title":"Consumer Group","uri":"https://rmoff.net/categories/consumer-group/"},{"categories":["Kafka Connect","JDBC sink","Consumer Group","kafkacat"],"content":" The Kafka Connect framework provides generic error handling and dead-letter queue capabilities which are available for problems with [de]serialisation and Single Message Transforms. When it comes to errors that a connector may encounter doing the actual pull or put of data from the source/target system, it‚Äôs down to the connector itself to implement logic around that. For example, the Elasticsearch sink connector provides configuration (behavior.on.malformed.documents) that can be set so that a single bad record won‚Äôt halt the pipeline. Others, such as the JDBC Sink connector, don‚Äôt provide this yet. That means that if you hit this problem, you need to manually unblock it yourself. One way is to manually move the offset of the consumer on past the bad message.\nTL;DR : You can use kafka-consumer-groups --reset-offsets --to-offset \u003cx\u003e to manually move the connector past a bad message\n","keywords":null,"title":"Skipping bad records with the Kafka Connect JDBC sink connector","uri":"https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/"},{"categories":["Kafka Connect","Elasticsearch"],"content":" I use the Elastic stack for a lot of my talks and demos because it complements Kafka brilliantly. A few things have changed in recent releases and this blog is a quick note on some of the errors that you might hit and how to resolve them. It was inspired by a lot of the comments and discussion here and here.\n","keywords":null,"title":"Kafka Connect and Elasticsearch","uri":"https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/"},{"categories":["kafkacat","Confluent Cloud"],"content":" kafkacat gives you Kafka super powers üòé I‚Äôve written before about kafkacat and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in a recent demo that I built in which data needed manipulating in a way that I couldn‚Äôt easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it‚Äôs getting data from Confluent Cloud down to a local cluster.\n","keywords":null,"title":"Copying data between Kafka clusters with Kafkacat","uri":"https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/"},{"categories":["Kafka Summit","Running","5k run/walk"],"content":" Coming to Kafka Summit in San Francisco next week? Inspired by similar events at Oracle OpenWorld in past years, I‚Äôm proposing an unofficial run (or walk) across the GoldenGate bridge on the morning of Tuesday 1st October. We should be up and out and back in plenty of time to still attend the morning keynotes. Some people will run, some may prefer to walk, it‚Äôs open to everyone :)\n","keywords":null,"title":"Kafka Summit GoldenGate bridge run/walk","uri":"https://rmoff.net/2019/09/23/kafka-summit-goldengate-bridge-run/walk/"},{"categories":["DevRel"],"content":"I‚Äôve been a full-time Developer Advocate for nearly 1.5 years now, and have learnt lots along the way. The stuff I‚Äôve learnt about being an advocate I‚Äôve written about elsewhere (here/here/here); today I want to write about something that‚Äôs just as important: staying sane and looking after yourself whilst on the road. This is also tangentially related to another of my favourite posts that I‚Äôve written: Travelling for Work, with Kids at Home.","keywords":null,"title":"Staying sane on the road as a Developer Advocate","uri":"https://rmoff.net/2019/09/19/staying-sane-on-the-road-as-a-developer-advocate/"},{"categories":null,"content":"","keywords":null,"title":"meetups","uri":"https://rmoff.net/categories/meetups/"},{"categories":null,"content":"","keywords":null,"title":"travel","uri":"https://rmoff.net/categories/travel/"},{"categories":["DevRel","conferences","meetups","travel"],"content":"I‚Äôve had a relaxing couple of weeks off work over the summer, and came back today to realise that I‚Äôve got a fair bit of conference and meetup travel to wrap my head around for the next few months :)\nIf you‚Äôre interested in where I‚Äôll be and want to come and say hi, hear about Kafka‚Äîor just grab a coffee or beer, herewith my itinerary as it currently stands.","keywords":null,"title":"Where I‚Äôll be on the road for the remainder of 2019","uri":"https://rmoff.net/2019/09/02/where-ill-be-on-the-road-for-the-remainder-of-2019/"},{"categories":null,"content":"","keywords":null,"title":"offsets","uri":"https://rmoff.net/categories/offsets/"},{"categories":["Kafka Connect","offsets","kafkacat"],"content":"Kafka Connect in distributed mode uses Kafka itself to persist the offsets of any source connectors. This is a great way to do things as it means that you can easily add more workers, rebuild existing ones, etc without having to worry about where the state is persisted. I personally always recommend using distributed mode, even if just for a single worker instance - it just makes things easier, and more standard.","keywords":null,"title":"Reset Kafka Connect Source Connector Offsets","uri":"https://rmoff.net/2019/08/15/reset-kafka-connect-source-connector-offsets/"},{"categories":null,"content":"","keywords":null,"title":"offset management","uri":"https://rmoff.net/categories/offset-management/"},{"categories":["Kafka Connect","offset management"],"content":" When you create a sink connector in Kafka Connect, by default it will start reading from the beginning of the topic and stream all of the existing‚Äîand new‚Äîdata to the target. The setting that controls this behaviour is auto.offset.reset, and you can see its value in the worker log when the connector runs:\n[2019-08-05 23:31:35,405] INFO ConsumerConfig values: allow.auto.create.topics = true auto.commit.interval.ms = 5000 auto.offset.reset = earliest ‚Ä¶ ","keywords":null,"title":"Starting a Kafka Connect sink connector at the end of a topic","uri":"https://rmoff.net/2019/08/09/starting-a-kafka-connect-sink-connector-at-the-end-of-a-topic/"},{"categories":null,"content":"","keywords":null,"title":"consumer","uri":"https://rmoff.net/categories/consumer/"},{"categories":["Kafka Connect","kafka","consumer","consumer group","replicator"],"content":"I‚Äôve been using Replicator as a powerful way to copy data from my Kafka rig at home onto my laptop‚Äôs Kafka environment. It means that when I‚Äôm on the road I can continue to work with the same set of data and develop pipelines etc. With a VPN back home I can even keep them in sync directly if I want to.\nI hit a problem the other day where Replicator was running, but I had no data in my target topics on my laptop.","keywords":null,"title":"Resetting a Consumer Group in Kafka","uri":"https://rmoff.net/2019/08/09/resetting-a-consumer-group-in-kafka/"},{"categories":["alfred","productivity"],"content":"Alfred is one of my favourite productivity tools. One of its best features is the clipboard history, which when I moved laptops and it didn‚Äôt transfer I realised quite how much I rely on this functionality in my day-to-day work.\nWhilst Alfred has the options to syncronise its preferences across machines, it seems that it doesn‚Äôt synchronise the clipboard database. To get it to work I did the following:","keywords":null,"title":"Migrating Alfred Clipboard to New Laptop","uri":"https://rmoff.net/2019/08/07/migrating-alfred-clipboard-to-new-laptop/"},{"categories":null,"content":"","keywords":null,"title":"concepts","uri":"https://rmoff.net/tag/concepts/"},{"categories":null,"content":"","keywords":null,"title":"devrel","uri":"https://rmoff.net/tag/devrel/"},{"categories":null,"content":"","keywords":null,"title":"diagrams","uri":"https://rmoff.net/tag/diagrams/"},{"categories":null,"content":"","keywords":null,"title":"ios","uri":"https://rmoff.net/tag/ios/"},{"categories":null,"content":"","keywords":null,"title":"ipad","uri":"https://rmoff.net/tag/ipad/"},{"categories":null,"content":"","keywords":null,"title":"presenting","uri":"https://rmoff.net/tag/presenting/"},{"categories":null,"content":"I write and speak lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:\nHow do you make those cool diagrams?\nI wrote about this originally last year but since then have evolved my approach. I‚Äôve now pretty much ditched Paper, in favour of Concepts. It was recommended to me after I published the previous post.","keywords":null,"title":"So how DO you make those cool diagrams? July 2019 update","uri":"https://rmoff.net/2019/07/11/so-how-do-you-make-those-cool-diagrams-july-2019-update/"},{"categories":null,"content":"","keywords":null,"title":"tools","uri":"https://rmoff.net/tag/tools/"},{"categories":null,"content":"","keywords":null,"title":"DevRelLife","uri":"https://rmoff.net/categories/devrellife/"},{"categories":["DevRelLife","Travel"],"content":"This week I was scheduled in to a couple of meetups, in Vienna and Munich. Flying is an inevitable part of travel since I also happen to like being home seeing my family and airplanes are usually the quickest way to make this happen. I don‚Äôt particularly enjoy flying, and there‚Äôs the environmental impact of it too‚Äîso when I realised that Vienna and Munich are relatively close to each other I looked at getting the train.","keywords":null,"title":"Taking the Vienna-Munich sleeper train","uri":"https://rmoff.net/2019/07/03/taking-the-vienna-munich-sleeper-train/"},{"categories":["Kafka Connect","kafkacat"],"content":" Kafka Connect has as REST API through which all config should be done, including removing connectors that have been created. Sometimes though, you might have reason to want to manually do this‚Äîand since Kafka Connect running in distributed mode uses Kafka as its persistent data store, you can achieve this by manually writing to the topic yourself.\n","keywords":null,"title":"Manually delete a connector from Kafka Connect","uri":"https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/"},{"categories":["Kafka Connect"],"content":" Here‚Äôs a hacky way to automatically restart Kafka Connect connectors if they fail. Restarting automatically only makes sense if it‚Äôs a transient failure; if there‚Äôs a problem with your pipeline (e.g. bad records or a mis-configured server) then you don‚Äôt gain anything from this. You might want to check out Kafka Connect‚Äôs error handling and dead letter queues too.\n","keywords":null,"title":"Automatically restarting failed Kafka Connect tasks","uri":"https://rmoff.net/2019/06/06/automatically-restarting-failed-kafka-connect-tasks/"},{"categories":["Kafka Connect"],"content":" Kafka Connect configuration is easy - you just write some JSON! But what if you‚Äôve got credentials that you need to pass? Embedding those in a config file is not always such a smart idea. Fortunately with KIP-297 which was released in Apache Kafka 2.0 there is support for external secrets. It‚Äôs extendable to use your own ConfigProvider, and ships with its own for just putting credentials in a file - which I‚Äôll show here. You can read more here.\n","keywords":null,"title":"Putting Kafka Connect passwords in a separate file / externalising secrets","uri":"https://rmoff.net/2019/05/24/putting-kafka-connect-passwords-in-a-separate-file-/-externalising-secrets/"},{"categories":["Kafka Connect","REST","kafkacat"],"content":" Kafka Connect exposes a REST interface through which all config and monitoring operations can be done. You can create connectors, delete them, restart them, check their status, and so on. But, I found a situation recently in which I needed to delete a connector and couldn‚Äôt do so with the REST API. Here‚Äôs another way to do it, by amending the configuration Kafka topic that Kafka Connect in distributed mode uses to persist configuration information for connectors. Note that this is not a recommended way of working with Kafka Connect‚Äîthe REST API is there for a good reason :)\n","keywords":null,"title":"Deleting a Connector in Kafka Connect without the REST API","uri":"https://rmoff.net/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/"},{"categories":null,"content":"","keywords":null,"title":"REST","uri":"https://rmoff.net/categories/rest/"},{"categories":["ksqlDB"],"content":" There is an open issue for support of EXPLODE/UNNEST functionality in KSQL, and if you need it then do up-vote the issue. Here I detail a hacky, but effective, workaround for exploding arrays into multiple messages‚Äîso long as you know the upper-bound on your array.\n","keywords":null,"title":"A poor man‚Äôs KSQL EXPLODE/UNNEST technique","uri":"https://rmoff.net/2019/05/09/a-poor-mans-ksql-explode/unnest-technique/"},{"categories":["Kafka Connect"],"content":" Kafka Connect is a API within Apache Kafka and its modular nature makes it powerful and flexible. Converters are part of the API but not always fully understood. I‚Äôve written previously about Kafka Connect converters, and this post is just a hands-on example to show even further what they are‚Äîand are not‚Äîabout.\nNote To understand more about Kafka Connect in general, check out my talk from Kafka Summit London From Zero to Hero with Kafka Connect. ","keywords":null,"title":"When a Kafka Connect converter is not a \u003cem\u003econverter\u003c/em\u003e","uri":"https://rmoff.net/2019/05/08/when-a-kafka-connect-converter-is-not-a-_converter_/"},{"categories":["Kafka Connect","Kafka REST Proxy"],"content":" When you run Kafka Connect in distributed mode it uses a Kafka topic to store the offset information for each connector. Because it‚Äôs just a Kafka topic, you can read that information using any consumer.\n","keywords":null,"title":"Reading Kafka Connect Offsets via the REST Proxy","uri":"https://rmoff.net/2019/05/02/reading-kafka-connect-offsets-via-the-rest-proxy/"},{"categories":null,"content":"","keywords":null,"title":"Data wrangling","uri":"https://rmoff.net/categories/data-wrangling/"},{"categories":["ksqlDB","Data wrangling"],"content":"Prompted by a question on StackOverflow, the requirement is to take a series of events related to a common key and for each key output a series of aggregates derived from a changing value in the events. I‚Äôll use the data from the question, based on ticket statuses. Each ticket can go through various stages, and the requirement was to show, per customer, how many tickets are currently at each stage.\n","keywords":null,"title":"Pivoting Aggregates in Ksql","uri":"https://rmoff.net/2019/04/17/pivoting-aggregates-in-ksql/"},{"categories":["ksqlDB","confluent cloud"],"content":" See also : https://docs.confluent.io/current/ksql/docs/installation/server-config/security.html#configuring-ksql-for-secured-sr-long\nConfluent Cloud now includes a secured Schema Registry, which you can use from external applications, including KSQL.\nTo configure KSQL for it you need to set:\nksql.schema.registry.url=https://\u003cSchema Registry endpoint\u003e ksql.schema.registry.basic.auth.credentials.source=USER_INFO ksql.schema.registry.basic.auth.user.info=\u003cSchema Registry API Key\u003e:\u003cSchema Registry API Secret\u003e ","keywords":null,"title":"Connecting KSQL to a Secured Schema Registry","uri":"https://rmoff.net/2019/04/12/connecting-ksql-to-a-secured-schema-registry/"},{"categories":["ksqlDB","Stream processing"],"content":" Introduction What can you use stream-stream joins for? Can you use them to join between a stream of orders and stream of related shipments to do useful things? What‚Äôs not supported in KSQL, where are the cracks?\n","keywords":null,"title":"Exploring KSQL Stream-Stream Joins","uri":"https://rmoff.net/2019/03/28/exploring-ksql-stream-stream-joins/"},{"categories":null,"content":"","keywords":null,"title":"stream processing","uri":"https://rmoff.net/categories/stream-processing/"},{"categories":["ksqlDB","rest"],"content":" Before you can drop a stream or table that‚Äôs populated by a query in KSQL, you have to terminate any queries upon which the object is dependent. Here‚Äôs a bit of jq \u0026 xargs magic to terminate all queries that are currently running\n","keywords":null,"title":"Terminate All KSQL Queries","uri":"https://rmoff.net/2019/03/25/terminate-all-ksql-queries/"},{"categories":["presenting","conferences","DevRel"],"content":" This post is the companion to an earlier one that I wrote about conference abstracts. In the same way that the last one was inspired by reviewing a ton of abstracts and noticing a recurring pattern in my suggestions, so this one comes from reviewing a bunch of slide decks for a forthcoming conference. They all look like good talks, but in several cases these great talks are fighting to get out from underneath the deadening weight of slides.\nHerewith follows my highly-opinionated, fairly-subjective, and extremely-terse advice and general suggestions for slide decks. You can also find relating ramblings in this recent post too. My friend and colleague Vik Gamov also wrote a good post on this same topic, and linked to a good video that I‚Äôd recommend you watch.\n","keywords":null,"title":"Quick Thoughts on Not Making a Crap Slide Deck","uri":"https://rmoff.net/2019/03/19/quick-thoughts-on-not-making-a-crap-slide-deck/"},{"categories":null,"content":"","keywords":null,"title":"httpie","uri":"https://rmoff.net/categories/httpie/"},{"categories":["httpie","Kafka REST Proxy"],"content":" This shows how to use httpie with the Confluent REST Proxy.\nSend data echo '{\"records\":[{\"value\":{\"foo\":\"bar\"}}]}' | \\ http POST http://localhost:8082/topics/jsontest \\ Content-Type:application/vnd.kafka.json.v2+json Accept:application/vnd.kafka.v2+json ","keywords":null,"title":"Using httpie with the Kafka REST Proxy","uri":"https://rmoff.net/2019/03/08/using-httpie-with-the-kafka-rest-proxy/"},{"categories":["presenting","devrel"],"content":"I‚Äôve written quite a few talks over the years, but usually as a side-line to my day job. In my role as a Developer Advocate, talks are part of What I Do, and so I can dedicate more time to it. A lot of the talks I‚Äôve done previously have evolved through numerous iterations, and with a new talk to deliver for the \"Spring Season\" of conferences, I thought it would be interesting to track what it took from concept to actual delivery.","keywords":null,"title":"Preparing a New Talk","uri":"https://rmoff.net/2019/03/01/preparing-a-new-talk/"},{"categories":null,"content":"","keywords":null,"title":"personal","uri":"https://rmoff.net/categories/personal/"},{"categories":["personal","travel","career","DevRel"],"content":"I began travelling for my job when my first child was three months old. But don‚Äôt mistake correlation for causation‚Ä¶it wasn‚Äôt the broken nights' sleep that forced me onto the road, but an excellent job opportunity that seemed worth the risk. Nearly eight years later and I‚Äôm in a different job but still with a bunch of travel involved. How much I travel has varied. It‚Äôs tended to average around 30%, but has peaked at way more than that.","keywords":null,"title":"Travelling for Work, with Kids at Home","uri":"https://rmoff.net/2019/02/09/travelling-for-work-with-kids-at-home/"},{"categories":["Kafka Connect"],"content":"By default Kafka Connect sends its output to stdout, so you‚Äôll see it on the console, Docker logs, or wherever. Sometimes you might want to route it to file, and you can do this by reconfiguring log4j. You can also change the configuration to get more (or less) detail in the logs by changing the log level.\nFinding the log configuration file The configuration file is called connect-log4j.properties and usually found in etc/kafka/connect-log4j.","keywords":null,"title":"Kafka Connect Change Log Level and Write Log to File","uri":"https://rmoff.net/2019/01/29/kafka-connect-change-log-level-and-write-log-to-file/"},{"categories":["bash","hexdump"],"content":" A script I‚Äôd batch-run on my Markdown files had inserted a UTF-8 non-breaking-space between Markdown heading indicator and the text, which meant that # My title actually got rendered as that, instead of an H3 title.\nLooking at the file contents, I could see it wasn‚Äôt just a space between the # and the text, but a non-breaking space.\n","keywords":null,"title":"Replacing UTF8 non-breaking-space with bash/sed on the Mac","uri":"https://rmoff.net/2019/01/21/replacing-utf8-non-breaking-space-with-bash/sed-on-the-mac/"},{"categories":["ksqlDB"],"content":" KSQL is generally case-sensitive. Very sensitive, at times ;-)\n","keywords":null,"title":"How KSQL handles case","uri":"https://rmoff.net/2019/01/21/how-ksql-handles-case/"},{"categories":["ksqlDB","rest"],"content":" Full reference is here\n","keywords":null,"title":"KSQL REST API cheatsheet","uri":"https://rmoff.net/2019/01/17/ksql-rest-api-cheatsheet/"},{"categories":["rest","Schema Registry"],"content":" The Schema Registry support a REST API for finding out information about the schemas within it. Here‚Äôs a quick cheatsheat with REST calls that I often use.\n","keywords":null,"title":"Confluent Schema Registry REST API cheatsheet","uri":"https://rmoff.net/2019/01/17/confluent-schema-registry-rest-api-cheatsheet/"},{"categories":null,"content":"","keywords":null,"title":"schema registry","uri":"https://rmoff.net/categories/schema-registry/"},{"categories":["docker"],"content":" I use Docker and Docker Compose a lot. Like, every day. It‚Äôs a fantastic way to build repeatable demos and examples, that can be torn down and spun up in a repeatable way. But‚Ä¶what happens when the demo that was working is spun up and then tail spins down in a blaze of flames?\n","keywords":null,"title":"What to Do When Docker on the Mac Runs Out of Space","uri":"https://rmoff.net/2019/01/09/what-to-do-when-docker-on-the-mac-runs-out-of-space/"},{"categories":["DevRel","abstracts","presenting","conferences"],"content":"I‚Äôve reviewed a bunch of abstracts in the last couple of days, here are some common suggestions I made:\nNo need to include your company name in the abstract text. Chances are I‚Äôve not heard of your company, and even if I have, what does it add to my comprehension of your abstract and what you‚Äôre going to talk about? Possible exception would be the \"hot\" tech companies where people will see a talk just because it‚Äôs Netflix etc","keywords":null,"title":"Quick Thoughts on Not Writing a Crap Abstract","uri":"https://rmoff.net/2018/12/19/quick-thoughts-on-not-writing-a-crap-abstract/"},{"categories":null,"content":"","keywords":null,"title":"asciidoc","uri":"https://rmoff.net/tag/asciidoc/"},{"categories":null,"content":"","keywords":null,"title":"blogging","uri":"https://rmoff.net/tag/blogging/"},{"categories":null,"content":"","keywords":null,"title":"ghost","uri":"https://rmoff.net/tag/ghost/"},{"categories":null,"content":"","keywords":null,"title":"ghost","uri":"https://rmoff.net/categories/ghost/"},{"categories":null,"content":"","keywords":null,"title":"hugo","uri":"https://rmoff.net/tag/hugo/"},{"categories":null,"content":"","keywords":null,"title":"Markdown","uri":"https://rmoff.net/tag/markdown/"},{"categories":["blogging","ghost","hugo","Markdown","asciidoc"],"content":"Why? I‚Äôve been blogging for quite a few years now, starting on Blogger, soon onto WordPress, and then to Ghost a couple of years ago. Blogger was fairly lame, WP yucky, but I really do like Ghost. It‚Äôs simple and powerful and was perfect for my needs. My needs being, an outlet for technical content that respected formatting, worked with a markup language (Markdown), and didn‚Äôt f**k things up in the way that WP often would in its WYSIWYG handling of content.","keywords":null,"title":"Moving from Ghost to Hugo","uri":"https://rmoff.net/2018/12/17/moving-from-ghost-to-hugo/"},{"categories":["docker"],"content":" Tiny little snippet this one. Given a list of images:\n$ docker images|grep confluent confluentinc/cp-enterprise-kafka 5.0.0 d0c5528d7f99 3 months ago 600MB confluentinc/cp-kafka 5.0.0 373a4e31e02e 3 months ago 558MB confluentinc/cp-zookeeper 5.0.0 3cab14034c43 3 months ago 558MB confluentinc/cp-ksql-server 5.0.0 691bc3c1991f 4 months ago 493MB confluentinc/cp-ksql-cli 5.0.0 e521f3e787d6 4 months ago 488MB ‚Ä¶ Now there‚Äôs a new version available, and you want to pull down all the latest ones for it:\ndocker images|grep \"^confluentinc\"|awk '{print $1}'|xargs -Ifoo docker pull foo:5.1.0 ","keywords":null,"title":"Pull new version of multiple Docker images","uri":"https://rmoff.net/2018/12/17/pull-new-version-of-multiple-docker-images/"},{"categories":null,"content":"","keywords":null,"title":"Docker Compose","uri":"https://rmoff.net/tag/docker-compose/"},{"categories":["docker","Docker Compose","ksql","ksql-cli","ksql-server","kafka connect"],"content":"A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad‚Ä¶how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.","keywords":null,"title":"Docker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka","uri":"https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/"},{"categories":null,"content":"","keywords":null,"title":"kafka connect","uri":"https://rmoff.net/tag/kafka-connect/"},{"categories":null,"content":"","keywords":null,"title":"ksql","uri":"https://rmoff.net/tag/ksql/"},{"categories":null,"content":"","keywords":null,"title":"ksql","uri":"https://rmoff.net/categories/ksql/"},{"categories":null,"content":"","keywords":null,"title":"ksql-cli","uri":"https://rmoff.net/tag/ksql-cli/"},{"categories":null,"content":"","keywords":null,"title":"ksql-cli","uri":"https://rmoff.net/categories/ksql-cli/"},{"categories":null,"content":"","keywords":null,"title":"ksql-server","uri":"https://rmoff.net/tag/ksql-server/"},{"categories":null,"content":"","keywords":null,"title":"ksql-server","uri":"https://rmoff.net/categories/ksql-server/"},{"categories":null,"content":"","keywords":null,"title":"cdc","uri":"https://rmoff.net/categories/cdc/"},{"categories":null,"content":"","keywords":null,"title":"goldengate","uri":"https://rmoff.net/categories/goldengate/"},{"categories":null,"content":"","keywords":null,"title":"logminer","uri":"https://rmoff.net/categories/logminer/"},{"categories":["oracle","cdc","debezium","goldengate","xstream","logminer","ksqldb"],"content":"This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018 (refreshed June 2020). For a more detailed background to why and how at a broader level for all databases (not just Oracle) see this blog and this talk.\nWhat techniques \u0026 tools are there? Franck Pachot has written up an excellent analysis of the options available here.\nAs of June 2020, this is what the line-up looks like:","keywords":null,"title":"Streaming data from Oracle into Kafka","uri":"https://rmoff.net/2018/12/12/streaming-data-from-oracle-into-kafka/"},{"categories":null,"content":"","keywords":null,"title":"xstream","uri":"https://rmoff.net/categories/xstream/"},{"categories":null,"content":"","keywords":null,"title":"apple keyboard","uri":"https://rmoff.net/tag/apple-keyboard/"},{"categories":null,"content":"","keywords":null,"title":"apple keyboard","uri":"https://rmoff.net/categories/apple-keyboard/"},{"categories":null,"content":"","keywords":null,"title":"apple pencil","uri":"https://rmoff.net/tag/apple-pencil/"},{"categories":null,"content":"","keywords":null,"title":"apple pencil","uri":"https://rmoff.net/categories/apple-pencil/"},{"categories":null,"content":"","keywords":null,"title":"ipad pro","uri":"https://rmoff.net/tag/ipad-pro/"},{"categories":null,"content":"","keywords":null,"title":"ipad pro","uri":"https://rmoff.net/categories/ipad-pro/"},{"categories":null,"content":"","keywords":null,"title":"keynote","uri":"https://rmoff.net/tag/keynote/"},{"categories":["ipad pro","apple pencil","apple keyboard","keynote"],"content":"I‚Äôve written recently about how I create the diagrams in my blog posts and talks, and from discussions around that, a couple of people were interested more broadly in how I use my iPad Pro. So, on the basis that if two people are interested maybe others are (and if no-one else is, I have a copy-and-paste answer to give to those two people) here we go.\nKit iPad Pro 10.","keywords":null,"title":"Tools I Use: iPad Pro","uri":"https://rmoff.net/2018/12/11/tools-i-use-ipad-pro/"},{"categories":null,"content":"","keywords":null,"title":"diagrams","uri":"https://rmoff.net/categories/diagrams/"},{"categories":null,"content":"","keywords":null,"title":"ios","uri":"https://rmoff.net/categories/ios/"},{"categories":null,"content":"","keywords":null,"title":"ipad","uri":"https://rmoff.net/categories/ipad/"},{"categories":null,"content":"","keywords":null,"title":"paper","uri":"https://rmoff.net/tag/paper/"},{"categories":null,"content":"","keywords":null,"title":"paper","uri":"https://rmoff.net/categories/paper/"},{"categories":["paper","ios","diagrams","presenting","ipad","tools"],"content":"I write and speak lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:\nHow do you make those cool diagrams?\nSo here‚Äôs a short, and longer, answer!\nUpdate July 2019 I‚Äôve moved away from Paper -\u003e read more here\ntl;dr An iOS app called Paper, from a company called FiftyThree\nSo, how DO you make those cool diagrams?","keywords":null,"title":"So how DO you make those cool diagrams?","uri":"https://rmoff.net/2018/12/10/so-how-do-you-make-those-cool-diagrams/"},{"categories":null,"content":"","keywords":null,"title":"tools","uri":"https://rmoff.net/categories/tools/"},{"categories":null,"content":"","keywords":null,"title":"brew","uri":"https://rmoff.net/tag/brew/"},{"categories":null,"content":"","keywords":null,"title":"brew","uri":"https://rmoff.net/categories/brew/"},{"categories":["mtr","mac","brew"],"content":"Install Not sure why the brew doesn‚Äôt work as it used to, but here‚Äôs how to get it working:\nbrew install mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr /usr/local/bin/mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr-packet /usr/local/bin/mtr-packet (If you don‚Äôt do the two symbolic links (ln) you‚Äôll get mtr: command not found or mtr: Failure to start mtr-packet: Invalid argument)\nRun sudo mtr google.com ","keywords":null,"title":"Get mtr working on the Mac","uri":"https://rmoff.net/2018/12/08/get-mtr-working-on-the-mac/"},{"categories":null,"content":"","keywords":null,"title":"mac","uri":"https://rmoff.net/tag/mac/"},{"categories":null,"content":"","keywords":null,"title":"mtr","uri":"https://rmoff.net/tag/mtr/"},{"categories":null,"content":"","keywords":null,"title":"mtr","uri":"https://rmoff.net/categories/mtr/"},{"categories":null,"content":"","keywords":null,"title":"bash","uri":"https://rmoff.net/tag/bash/"},{"categories":null,"content":"","keywords":null,"title":"jq","uri":"https://rmoff.net/tag/jq/"},{"categories":["kafka connect","bash","jq","peco","xargs","rest api"],"content":"I do lots of work with Kafka Connect, almost entirely in Distributed mode‚Äîeven just with 1 node -\u003e makes scaling out much easier when/if needed. Because I‚Äôm using Distributed mode, I use the Kafka Connect REST API to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.","keywords":null,"title":"Kafka Connect CLI tricks","uri":"https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/"},{"categories":null,"content":"","keywords":null,"title":"peco","uri":"https://rmoff.net/tag/peco/"},{"categories":null,"content":"","keywords":null,"title":"peco","uri":"https://rmoff.net/categories/peco/"},{"categories":null,"content":"","keywords":null,"title":"rest api","uri":"https://rmoff.net/tag/rest-api/"},{"categories":null,"content":"","keywords":null,"title":"xargs","uri":"https://rmoff.net/tag/xargs/"},{"categories":null,"content":"","keywords":null,"title":"xargs","uri":"https://rmoff.net/categories/xargs/"},{"categories":["docker","docker-compose"],"content":"Doing some funky Docker Compose stuff, including:\n","keywords":null,"title":"ERROR: Invalid interpolation format for ‚Äúcommand‚Äù option in service‚Ä¶","uri":"https://rmoff.net/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/"},{"categories":["cdc","ksql","Apache Kafka","jdbc sink"],"content":"The problem - nested messages in Kafka Data comes into Kafka in many shapes and sizes. Sometimes it‚Äôs from CDC tools, and may be nested like this:\n","keywords":null,"title":"Flatten CDC records in KSQL","uri":"https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/"},{"categories":["Apache Kafka","ksql","jmx","jmxterm"],"content":" Check out the jmxterm repository Download jmxterm from https://docs.cyclopsgroup.org/jmxterm ","keywords":null,"title":"Exploring JMX with jmxterm","uri":"https://rmoff.net/2018/09/19/exploring-jmx-with-jmxterm/"},{"categories":null,"content":"","keywords":null,"title":"jmx","uri":"https://rmoff.net/categories/jmx/"},{"categories":null,"content":"","keywords":null,"title":"jmxterm","uri":"https://rmoff.net/categories/jmxterm/"},{"categories":["jmx","Apache Kafka","docker"],"content":"See also docs.\nTo help future Googlers‚Ä¶ with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables: \u003cx\u003e_JMX_HOSTNAME and \u003cx\u003e_JMX_PORT, prefixed by a component name.\n\u003cx\u003e_JMX_HOSTNAME - the hostname/IP of the JMX host machine, as accessible from the JMX Client.\nThis is used by the JMX client to connect back into JMX, so must be accessible from the host machine running the JMX client.","keywords":null,"title":"Accessing Kafka Docker containers‚Äô JMX from host","uri":"https://rmoff.net/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/"},{"categories":null,"content":"","keywords":null,"title":"multiline","uri":"https://rmoff.net/categories/multiline/"},{"categories":["kafkacat","Apache Kafka","multiline"],"content":"(SO answer repost)\nYou can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):\nkafkacat -b kafka:29092 \\ -t test_topic_01 \\ -D/ \\ -P \u003c\u003cEOF this is a string message with a line break/this is another message with two line breaks! EOF Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140","keywords":null,"title":"Sending multiline messages to Kafka","uri":"https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/"},{"categories":null,"content":"","keywords":null,"title":"aggregate","uri":"https://rmoff.net/tag/aggregate/"},{"categories":null,"content":"","keywords":null,"title":"aggregate","uri":"https://rmoff.net/categories/aggregate/"},{"categories":null,"content":"","keywords":null,"title":"elasticsearch","uri":"https://rmoff.net/tag/elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"kibana","uri":"https://rmoff.net/tag/kibana/"},{"categories":null,"content":"","keywords":null,"title":"timestamp","uri":"https://rmoff.net/tag/timestamp/"},{"categories":null,"content":"","keywords":null,"title":"timestamp","uri":"https://rmoff.net/categories/timestamp/"},{"categories":null,"content":"","keywords":null,"title":"window","uri":"https://rmoff.net/tag/window/"},{"categories":null,"content":"","keywords":null,"title":"window","uri":"https://rmoff.net/categories/window/"},{"categories":["ksql","window","aggregate","timestamp","elasticsearch","kibana"],"content":"KSQL provides the ability to create windowed aggregations. For example, count the number of messages in a 1 minute window, grouped by a particular column:\nCREATE TABLE RATINGS_BY_CLUB_STATUS AS \\ SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \\ FROM RATINGS_WITH_CUSTOMER_DATA \\ WINDOW TUMBLING (SIZE 1 MINUTES) \\ GROUP BY CLUB_STATUS; How KSQL, and Kafka Streams, stores the window timestamp associated with an aggregate, has recently changed. See #1497 for details.\nWhereas previously the Kafka message timestamp (accessible through the KSQL ROWTIME system column) stored the start of the window for which the aggregate had been calculated, this changed in July 2018 to instead be the timestamp of the latest message to update that aggregate value.","keywords":null,"title":"Window Timestamps in KSQL / Integration with Elasticsearch","uri":"https://rmoff.net/2018/09/03/window-timestamps-in-ksql-/-integration-with-elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"calendar","uri":"https://rmoff.net/tag/calendar/"},{"categories":null,"content":"","keywords":null,"title":"calendar","uri":"https://rmoff.net/categories/calendar/"},{"categories":null,"content":"","keywords":null,"title":"devoxx","uri":"https://rmoff.net/tag/devoxx/"},{"categories":null,"content":"","keywords":null,"title":"devoxx","uri":"https://rmoff.net/categories/devoxx/"},{"categories":null,"content":"","keywords":null,"title":"javazone","uri":"https://rmoff.net/tag/javazone/"},{"categories":null,"content":"","keywords":null,"title":"javazone","uri":"https://rmoff.net/categories/javazone/"},{"categories":null,"content":"","keywords":null,"title":"lisa","uri":"https://rmoff.net/tag/lisa/"},{"categories":null,"content":"","keywords":null,"title":"lisa","uri":"https://rmoff.net/categories/lisa/"},{"categories":null,"content":"","keywords":null,"title":"schedule","uri":"https://rmoff.net/tag/schedule/"},{"categories":null,"content":"","keywords":null,"title":"schedule","uri":"https://rmoff.net/categories/schedule/"},{"categories":null,"content":"","keywords":null,"title":"speaking","uri":"https://rmoff.net/tag/speaking/"},{"categories":null,"content":"","keywords":null,"title":"speaking","uri":"https://rmoff.net/categories/speaking/"},{"categories":null,"content":"","keywords":null,"title":"usenix","uri":"https://rmoff.net/tag/usenix/"},{"categories":null,"content":"","keywords":null,"title":"usenix","uri":"https://rmoff.net/categories/usenix/"},{"categories":["speaking","schedule","calendar","javazone","lisa","usenix","devoxx"],"content":"There‚Äôs lots going on in the next few months :-)\nI‚Äôm particularly excited to be speaking at several notable conferences for the first time, including JavaZone, USENIX LISA, and Devoxx.\nAs always, if you‚Äôre nearby then hope to see you there, and let me know if you want to meet for a coffee or beer!\nSeptember üá™üá∏ Madrid, Spain 6th Sept: Madrid Kafka Meetup üá≥üá¥ Oslo, Norway 10th Sept: Oslo Kafka Meetup 11th Sept: JavaZone üáßüá™ Antwerp/Brussels, Belgium 25th Sept: Brussels Kafka Meetup üá™üá∏ Barcelona, Spain 26th Sept: Barcelona Kafka Meetup October üá¨üáß Leeds, UK 4th Oct: The JVM Thing meetup üá∫üá∏ Nashville (TN), USA 31st Oct: LISA18 November üá©üá™ M√ºnich, Germany 7th Nov: W-JAX üáßüá™ Antwerp, Belgium 13th Nov: Devoxx Belgium üáµüá± Krakow, Poland 26th Nov: CoreDump December üá¨üáß Liverpool, UK 4th Dec: UKOUG TECH 18 üá©üá™ Frankfurt, Germany 10th Dec: Apache Kafka Meetup 11th Dec: IT Days ","keywords":null,"title":"Where I‚Äôm speaking in the rest of 2018","uri":"https://rmoff.net/2018/08/21/where-im-speaking-in-the-rest-of-2018/"},{"categories":null,"content":"","keywords":null,"title":"ec2","uri":"https://rmoff.net/categories/ec2/"},{"categories":["apache kafka","docker","advertised.listeners","listeners","aws","ec2","KAFKA_ADVERTISED_LISTENERS"],"content":"(This was cross-posted on the Confluent.io blog)\nThis question comes up on StackOverflow and such places a lot, so here‚Äôs something to try and help.\ntl;dr : You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you‚Äôre using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they‚Äôll try to connect to the internal host address‚Äìand if that‚Äôs not reachable then problems ensue.\nPut another way, courtesy of Spencer Ruport:\nLISTENERS are what interfaces Kafka binds to. ADVERTISED_LISTENERS are how clients can connect.\n","keywords":null,"title":"Kafka Listeners - Explained","uri":"https://rmoff.net/2018/08/02/kafka-listeners-explained/"},{"categories":null,"content":"","keywords":null,"title":"KAFKA_ADVERTISED_LISTENERS","uri":"https://rmoff.net/categories/kafka_advertised_listeners/"},{"categories":null,"content":"","keywords":null,"title":"listeners","uri":"https://rmoff.net/categories/listeners/"},{"categories":null,"content":"","keywords":null,"title":"asciinema","uri":"https://rmoff.net/tag/asciinema/"},{"categories":null,"content":"","keywords":null,"title":"asciinema","uri":"https://rmoff.net/categories/asciinema/"},{"categories":null,"content":"","keywords":null,"title":"pygmentize","uri":"https://rmoff.net/tag/pygmentize/"},{"categories":null,"content":"","keywords":null,"title":"pygmentize","uri":"https://rmoff.net/categories/pygmentize/"},{"categories":null,"content":"","keywords":null,"title":"pygments","uri":"https://rmoff.net/tag/pygments/"},{"categories":null,"content":"","keywords":null,"title":"pygments","uri":"https://rmoff.net/categories/pygments/"},{"categories":null,"content":"","keywords":null,"title":"syntax highlighting","uri":"https://rmoff.net/tag/syntax-highlighting/"},{"categories":null,"content":"","keywords":null,"title":"syntax highlighting","uri":"https://rmoff.net/categories/syntax-highlighting/"},{"categories":["pygments","pygmentize","jq","syntax highlighting","keynote","presenting","asciinema"],"content":"So you‚Äôve got a code sample you want to share in a presentation, but whilst it looks beautiful in your text-editor with syntax highlighting, it‚Äôs fugly in Keynote? You could screenshot it and paste the image into your slide, but you just know that you‚Äôll want to change that code, and end up re-snapshotting it‚Ä¶what a PITA.\nBetter to have a nicely syntax-highlighted code snippet that you can paste as formatted text into Keynote and amend from there as needed.","keywords":null,"title":"Syntax highlighting code for presentation slides","uri":"https://rmoff.net/2018/06/20/syntax-highlighting-code-for-presentation-slides/"},{"categories":["elasticsearch","ksql","apache kafka","ubiquiti","espressi","slack","python","stream processing"],"content":"In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we‚Äôll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana‚Äîand how KSQL again came into use for building some stream processing as a result of the discovery made.\nThe data came from my home Ubiquiti router, and took two forms:","keywords":null,"title":"Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch","uri":"https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"apache kafka","uri":"https://rmoff.net/tag/apache-kafka/"},{"categories":null,"content":"","keywords":null,"title":"espressi","uri":"https://rmoff.net/tag/espressi/"},{"categories":null,"content":"","keywords":null,"title":"espressi","uri":"https://rmoff.net/categories/espressi/"},{"categories":null,"content":"","keywords":null,"title":"python","uri":"https://rmoff.net/tag/python/"},{"categories":null,"content":"","keywords":null,"title":"python","uri":"https://rmoff.net/categories/python/"},{"categories":null,"content":"","keywords":null,"title":"slack","uri":"https://rmoff.net/tag/slack/"},{"categories":null,"content":"","keywords":null,"title":"slack","uri":"https://rmoff.net/categories/slack/"},{"categories":null,"content":"","keywords":null,"title":"stream processing","uri":"https://rmoff.net/tag/stream-processing/"},{"categories":null,"content":"","keywords":null,"title":"ubiquiti","uri":"https://rmoff.net/tag/ubiquiti/"},{"categories":["diff","patch"],"content":"Hacky way to keep config files in sync when there‚Äôs a new version of some software.\nCaveat : probably completely wrong, may not pick up config entries added in the new version, etc. But, works for me right here right now ;-)\nSo let‚Äôs say we have two folders:\nconfluent-4.1.0 confluent-4.1.1 Same structures, different versions. 4.1.0 was set up with our local config in ./etc, that we want to preserve. We can use diff to easily see what‚Äôs changed:","keywords":null,"title":"Compare and apply a diff / patch recursively","uri":"https://rmoff.net/2018/06/07/compare-and-apply-a-diff-/-patch-recursively/"},{"categories":null,"content":"","keywords":null,"title":"diff","uri":"https://rmoff.net/tag/diff/"},{"categories":null,"content":"","keywords":null,"title":"diff","uri":"https://rmoff.net/categories/diff/"},{"categories":null,"content":"","keywords":null,"title":"patch","uri":"https://rmoff.net/tag/patch/"},{"categories":null,"content":"","keywords":null,"title":"patch","uri":"https://rmoff.net/categories/patch/"},{"categories":["kafka connect","oracle","number","timestamp"],"content":"The Kafka Connect JDBC Connector by default does not cope so well with:\nNUMBER columns with no defined precision/scale. You may end up with apparent junk (bytes) in the output, or just errors. TIMESTAMP WITH LOCAL TIME ZONE. Throws JDBC type -102 not currently supported warning in the log. Read more about NUMBER data type in the Oracle docs.\ntl;dr : How do I make it work? There are several options:","keywords":null,"title":"Kafka Connect and Oracle data types","uri":"https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/"},{"categories":null,"content":"","keywords":null,"title":"number","uri":"https://rmoff.net/tag/number/"},{"categories":null,"content":"","keywords":null,"title":"number","uri":"https://rmoff.net/categories/number/"},{"categories":null,"content":"","keywords":null,"title":"join","uri":"https://rmoff.net/tag/join/"},{"categories":null,"content":"","keywords":null,"title":"join","uri":"https://rmoff.net/categories/join/"},{"categories":null,"content":"","keywords":null,"title":"stream","uri":"https://rmoff.net/tag/stream/"},{"categories":null,"content":"","keywords":null,"title":"stream","uri":"https://rmoff.net/categories/stream/"},{"categories":["ksql","stream","table","join"],"content":"(preserving this StackOverflow answer for posterity and future Googlers)\ntl;dr When doing a stream-table join, your table messages must already exist (and must be timestamped) before the stream messages. If you re-emit your source stream messages, after the table topic is populated, the join will succeed.\nExample data Use kafakcat to populate topics:\nkafkacat -b localhost:9092 -P -t sessionDetails \u003c\u003cEOF {\"Media\":\"Foo\",\"SessionIdTime\":\"2018-05-17 11:25:33 BST\",\"SessionIdSeq\":1} {\"Media\":\"Foo\",\"SessionIdTime\":\"2018-05-17 11:26:33 BST\",\"SessionIdSeq\":2} EOF kafkacat -b localhost:9092 -P -t voipDetails \u003c\u003cEOF {\"SessionIdTime\":\"2018-05-17 11:25:33 BST\",\"SessionIdSeq\":1,\"Details\":\"Bar1a\"} {\"SessionIdTime\":\"2018-05-17 11:25:33 BST\",\"SessionIdSeq\":1,\"Details\":\"Bar1b\"} {\"SessionIdTime\":\"2018-05-17 11:26:33 BST\",\"SessionIdSeq\":2,\"Details\":\"Bar2\"} EOF Validate topic contents:","keywords":null,"title":"Stream-Table Joins in KSQL: Stream events must be timestamped after the Table messages","uri":"https://rmoff.net/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/"},{"categories":null,"content":"","keywords":null,"title":"table","uri":"https://rmoff.net/tag/table/"},{"categories":null,"content":"","keywords":null,"title":"table","uri":"https://rmoff.net/categories/table/"},{"categories":null,"content":"","keywords":null,"title":"mockaroo","uri":"https://rmoff.net/categories/mockaroo/"},{"categories":["Apache Kafka","kafkacat","mockaroo","testing"],"content":"tl;dr Use curl to pull data from the Mockaroo REST endpoint, and pipe it into kafkacat, thus:\ncurl -s \"https://api.mockaroo.com/api/d5a195e0?count=2\u0026key=ff7856d0\"| \\ kafkacat -b localhost:9092 -t purchases -P ","keywords":null,"title":"Quick ‚Äôn Easy Population of Realistic Test Data into Kafka","uri":"https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/"},{"categories":null,"content":"","keywords":null,"title":"testing","uri":"https://rmoff.net/categories/testing/"},{"categories":["blogging"],"content":"So the last post here was in 2011‚Ä¶seven years later I should probably post again, just to point random Google visitors to :\nMy new blog :¬†https://rmoff.net (Wordpress is icky; Ghost FTW!) My employer‚Äôs blog on which I also write:¬†http://cnfl.io/rmoff ","keywords":null,"title":"Blogging v2","uri":"https://rmoff.net/2018/05/09/blogging-v2/"},{"categories":null,"content":"","keywords":null,"title":"debezium","uri":"https://rmoff.net/tag/debezium/"},{"categories":null,"content":"","keywords":null,"title":"mongodb","uri":"https://rmoff.net/tag/mongodb/"},{"categories":null,"content":"","keywords":null,"title":"replica set","uri":"https://rmoff.net/tag/replica-set/"},{"categories":null,"content":"","keywords":null,"title":"replica set","uri":"https://rmoff.net/categories/replica-set/"},{"categories":["mongodb","debezium","kafka connect","apache kafka","replica set"],"content":"Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I‚Äôm doing something wrong\nMongoDB config - enabling replica sets For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:\nDocs: Replication / Convert a Standalone to a Replica Set\nStop Mongo:\nrmoff@proxmox01 ~\u003e sudo service mongod stop Add replica set config to /etc/mongod.","keywords":null,"title":"Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium","uri":"https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/"},{"categories":["mongodb","ubiquiti","ubnt","mongorestore","mongodump"],"content":"DISCLAIMER: I am not a MongoDB person (even if it is Web Scale X-D) - below instructions may work for you, they may not. Use with care!\nFor some work I‚Äôve been doing I wanted to access the data in Ubiquiti‚Äôs Unifi controller which it stores in MongoDB. Because I didn‚Äôt want to risk my actual Unifi device by changing local settings to enable remote access, and also because the version of MongoDB on it is older than ideal, I wanted to clone the data elsewhere.","keywords":null,"title":"Cloning Ubiquiti‚Äôs MongoDB instance to a separate server","uri":"https://rmoff.net/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/"},{"categories":null,"content":"","keywords":null,"title":"mongodump","uri":"https://rmoff.net/tag/mongodump/"},{"categories":null,"content":"","keywords":null,"title":"mongodump","uri":"https://rmoff.net/categories/mongodump/"},{"categories":null,"content":"","keywords":null,"title":"mongorestore","uri":"https://rmoff.net/tag/mongorestore/"},{"categories":null,"content":"","keywords":null,"title":"mongorestore","uri":"https://rmoff.net/categories/mongorestore/"},{"categories":null,"content":"","keywords":null,"title":"ubnt","uri":"https://rmoff.net/tag/ubnt/"},{"categories":null,"content":"","keywords":null,"title":"ubnt","uri":"https://rmoff.net/categories/ubnt/"},{"categories":["debezium","Apache Kafka","kafka connect","mysql"],"content":"Debezium is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we‚Äôll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.\nThe software versions used here are:\nConfluent Platform 4.0 Debezium 0.","keywords":null,"title":"Streaming Data from MySQL into Kafka with Kafka Connect and Debezium","uri":"https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/"},{"categories":null,"content":"io.confluent.ksql.exception.KafkaTopicException: Topic 'KSQL_NOTIFY' does not conform to the requirements Partitions:1 v 4. Replication: 1 v 1 Why? Because the topic KSQL creates to underpin a CREATE STREAM AS SELECT or CREATE TABLE AS SELECT already exists, and doesn‚Äôt match what it expects. By default it will create partitions \u0026 replicas based on the same values of the input topic.\nOptions:\nUse a different topic, via the WITH (KAFKA_TOPIC='FOO') syntax, e.g.","keywords":null,"title":"KSQL: Topic ‚Ä¶ does not conform to the requirements","uri":"https://rmoff.net/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/"},{"categories":["kafka connect","elasticsearch","Apache Kafka","oracle","streaming etl"],"content":"This article is part of a series exploring Streaming ETL in practice. You can read about setting up the ingest of realtime events from a standard Oracle platform, and building streaming ETL using KSQL.\nThis post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.","keywords":null,"title":"Streaming data from Kafka into Elasticsearch","uri":"https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"streaming etl","uri":"https://rmoff.net/categories/streaming-etl/"},{"categories":null,"content":"","keywords":null,"title":"apt-get","uri":"https://rmoff.net/categories/apt-get/"},{"categories":null,"content":"","keywords":null,"title":"confluent","uri":"https://rmoff.net/categories/confluent/"},{"categories":["Apache Kafka","confluent","python","apt-get"],"content":"System:\nrmoff@proxmox01:~$ uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux rmoff@proxmox01:~$ head -n1 /etc/os-release PRETTY_NAME=\"Debian GNU/Linux 8 (jessie)\" rmoff@proxmox01:~$ python --version Python 2.7.9 Following:\nhttps://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/ https://github.com/confluentinc/confluent-kafka-python Install librdkafka, which is a pre-req for the Python library:\nwget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add - sudo add-apt-repository \"deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main\" sudo apt-get install librdkafka-dev python-dev Setup virtualenv:\nsudo apt-get install virtualenv virtualenv kafka_push_notify source .","keywords":null,"title":"Installing the Python Kafka library from Confluent - troubleshooting some silly errors‚Ä¶","uri":"https://rmoff.net/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/"},{"categories":null,"content":"","keywords":null,"title":"confluent platform","uri":"https://rmoff.net/tag/confluent-platform/"},{"categories":null,"content":"","keywords":null,"title":"confluent platform","uri":"https://rmoff.net/categories/confluent-platform/"},{"categories":null,"content":"","keywords":null,"title":"etl","uri":"https://rmoff.net/tag/etl/"},{"categories":null,"content":"","keywords":null,"title":"etl","uri":"https://rmoff.net/categories/etl/"},{"categories":null,"content":"","keywords":null,"title":"streaming","uri":"https://rmoff.net/tag/streaming/"},{"categories":["streaming","etl","apache kafka","confluent platform"],"content":"(This is an expanded version of the intro to an article I posted over on the Confluent blog. Here I get to be as verbose as I like ;))\nMy first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe.","keywords":null,"title":"Why Do We Need Streaming ETL?","uri":"https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/"},{"categories":null,"content":"","keywords":null,"title":"goldengate","uri":"https://rmoff.net/tag/goldengate/"},{"categories":["apache kafka","schema registry","swingbench","goldengate","oracle"],"content":"This is the detailed step-by-step if you want to recreate the process I describe in the Confluent blog here\nI used Oracle‚Äôs Oracle Developer Days VM, which comes preinstalled with Oracle 12cR2. You can see the notes on how to do this here. These notes take you through installing and configuring:\nSwingbench, to create a sample ‚ÄúOrder Entry‚Äù schema and simulate events on the Oracle database Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith) I‚Äôm using Oracle GoldenGate 12.","keywords":null,"title":"HOWTO: Oracle GoldenGate + Apache Kafka + Schema Registry + Swingbench","uri":"https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/"},{"categories":null,"content":"","keywords":null,"title":"schema registry","uri":"https://rmoff.net/tag/schema-registry/"},{"categories":null,"content":"","keywords":null,"title":"swingbench","uri":"https://rmoff.net/tag/swingbench/"},{"categories":null,"content":"","keywords":null,"title":"swingbench","uri":"https://rmoff.net/categories/swingbench/"},{"categories":null,"content":"","keywords":null,"title":"adminclient","uri":"https://rmoff.net/categories/adminclient/"},{"categories":["Apache Kafka","adminclient","networking"],"content":"See also Kafka Listeners - Explained\nA short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:\nWARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) KSQL was throwing a similar error:\nKSQL cannot initialize AdminCLient. I had correctly set the machine‚Äôs hostname in my Kafka server.","keywords":null,"title":"Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available","uri":"https://rmoff.net/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/"},{"categories":null,"content":"","keywords":null,"title":"networking","uri":"https://rmoff.net/categories/networking/"},{"categories":["goldengate","oracle","Apache Kafka","confluent platform","swingbench"],"content":"Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!\nI used the Oracle Developer Days VM for this - it‚Äôs preinstalled with Oracle 12cR2. Big Data Lite is nice but currently has an older version of GoldenGate.\nLogin to the VM (oracle/oracle) and then install some useful things:\nsudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop sudo su - cd /etc/yum.","keywords":null,"title":"Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform","uri":"https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/"},{"categories":null,"content":"","keywords":null,"title":"oaktable world","uri":"https://rmoff.net/tag/oaktable-world/"},{"categories":null,"content":"","keywords":null,"title":"oaktable world","uri":"https://rmoff.net/categories/oaktable-world/"},{"categories":null,"content":"","keywords":null,"title":"openworld","uri":"https://rmoff.net/tag/openworld/"},{"categories":null,"content":"","keywords":null,"title":"openworld","uri":"https://rmoff.net/categories/openworld/"},{"categories":["oracle","openworld","oaktable world"],"content":"Here‚Äôs where I‚Äôll be!\nIf you use Google Calendar you can click on individual entries above and select copy to my calendar - which of course you‚Äôll want to do for all the ones I‚Äôve marked as [SPEAKING] :-)\nHere‚Äôs a list of all the Apache Kafka talks at OpenWorld and JavaOne, most of which I‚Äôll be trying to get to.","keywords":null,"title":"Where will I be at OpenWorld / Oak Table World?","uri":"https://rmoff.net/2017/09/29/where-will-i-be-at-openworld-/-oak-table-world/"},{"categories":["Apache Kafka","openworld","javaone","oow","san francisco","oaktable world"],"content":"There‚Äôs an impressive 19 sessions that cover Apache Kafka‚Ñ¢ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for OOW, JavaOne, and Oak Table World. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!\nCheck out the writeup of my previous visit to OOW including useful tips here.","keywords":null,"title":"Apache Kafka‚Ñ¢ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017","uri":"https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/"},{"categories":null,"content":"","keywords":null,"title":"javaone","uri":"https://rmoff.net/categories/javaone/"},{"categories":null,"content":"","keywords":null,"title":"oow","uri":"https://rmoff.net/categories/oow/"},{"categories":null,"content":"","keywords":null,"title":"san francisco","uri":"https://rmoff.net/categories/san-francisco/"},{"categories":null,"content":"","keywords":null,"title":"avro","uri":"https://rmoff.net/tag/avro/"},{"categories":null,"content":"","keywords":null,"title":"avro","uri":"https://rmoff.net/categories/avro/"},{"categories":["goldengate","kafka connect","avro","schema registry","oracle"],"content":"The Replicat was kapput:\nGGSCI (localhost.localdomain) 3\u003e info rkconnoe REPLICAT RKCONNOE Last Started 2017-09-12 17:06 Status ABENDED Checkpoint Lag 00:00:00 (updated 00:46:34 ago) Log Read Checkpoint File /u01/app/ogg/dirdat/oe000000 First Record RBA 0 So checking the OGG error log ggserr.log showed\n2017-09-12T17:06:17.572-0400 ERROR OGG-15051 Oracle GoldenGate Delivery, rkconnoe.prm: Java or JNI exception: oracle.goldengate.util.GGException: Error detected handling operation added event. 2017-09-12T17:06:17.572-0400 ERROR OGG-01668 Oracle GoldenGate Delivery, rkconnoe.prm: PROCESS ABENDING. So checking the replicat log dirrpt/RKCONNOE_info_log4j.","keywords":null,"title":"Oracle GoldenGate / Kafka Connect Handler troubleshooting","uri":"https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/"},{"categories":null,"content":"","keywords":null,"title":"emacs","uri":"https://rmoff.net/tag/emacs/"},{"categories":null,"content":"","keywords":null,"title":"emacs","uri":"https://rmoff.net/categories/emacs/"},{"categories":null,"content":"","keywords":null,"title":"marked2","uri":"https://rmoff.net/tag/marked2/"},{"categories":null,"content":"","keywords":null,"title":"marked2","uri":"https://rmoff.net/categories/marked2/"},{"categories":null,"content":"","keywords":null,"title":"vi","uri":"https://rmoff.net/tag/vi/"},{"categories":null,"content":"","keywords":null,"title":"vi","uri":"https://rmoff.net/categories/vi/"},{"categories":["markdown","marked2","emacs","vi"],"content":"Markdown is a plain-text formatting syntax. It enables you write documents in plain text, readable by others in plain text, and optionally rendered into nicely formatted PDF, HTML, DOCX etc.\nIt‚Äôs used widely in software documentation, particularly open-source, because it enables richer formatting than plain-text alone, but without constraining authors or readers to a given software platform.\nPlatforms such as github natively support Markdown rendering - so you write your README etc in markdown, and when viewed on github it is automagically rendered - without you needing to actually do anything.","keywords":null,"title":"What is Markdown, and Why is it Awesome?","uri":"https://rmoff.net/2017/09/12/what-is-markdown-and-why-is-it-awesome/"},{"categories":null,"content":"","keywords":null,"title":"conferences","uri":"https://rmoff.net/tag/conferences/"},{"categories":["conferences","meetups","speaking","doag","ukoug","oow"],"content":"I‚Äôm excited to be speaking at several conferences and meetups over the next few months. Unsurprisingly, the topic will be Apache Kafka!\nIf you‚Äôre at any of these, please do come and say hi :)\nApache Kafka Meetup - London My first time talking at the London Apache Kafka Meetup - always a sold-out crowd, this will be fun!\nSeptember 20th, 19:00 : Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka Slides are available here Oracle OpenWorld - San Francisco This will be my second time at OOW - I wrote up my previous trip here","keywords":null,"title":"Conferences \u0026 Meetups at which I‚Äôll be speaking - 2017","uri":"https://rmoff.net/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/"},{"categories":null,"content":"","keywords":null,"title":"doag","uri":"https://rmoff.net/tag/doag/"},{"categories":null,"content":"","keywords":null,"title":"doag","uri":"https://rmoff.net/categories/doag/"},{"categories":null,"content":"","keywords":null,"title":"meetups","uri":"https://rmoff.net/tag/meetups/"},{"categories":null,"content":"","keywords":null,"title":"oow","uri":"https://rmoff.net/tag/oow/"},{"categories":null,"content":"","keywords":null,"title":"ukoug","uri":"https://rmoff.net/tag/ukoug/"},{"categories":null,"content":"","keywords":null,"title":"ukoug","uri":"https://rmoff.net/categories/ukoug/"},{"categories":null,"content":"","keywords":null,"title":"JsonDeserializer","uri":"https://rmoff.net/tag/jsondeserializer/"},{"categories":null,"content":"","keywords":null,"title":"JsonDeserializer","uri":"https://rmoff.net/categories/jsondeserializer/"},{"categories":["kafka connect","JsonDeserializer"],"content":"An error that I see coming up frequently in the Kafka Connect community (e.g. mailing list, Slack group, StackOverflow) is:\nJsonDeserializer with schemas.enable requires \"schema\" and \"payload\" fields and may not contain additional fields or\nNo fields found using key and value schemas for table: foo-bar You can see an explanation, and solution, for the issue in my StackOverflow answer here: https://stackoverflow.com/a/45940013/350613\nIf you‚Äôre using schemas.enable in the Connector configuration, you must have schema and payload as the root-level elements of your JSON message ( Which is pretty much verbatim what the error says üòÅ), like this:","keywords":null,"title":"Kafka Connect - JsonDeserializer with schemas.enable requires ‚Äúschema‚Äù and ‚Äúpayload‚Äù fields","uri":"https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/"},{"categories":null,"content":"","keywords":null,"title":"curl","uri":"https://rmoff.net/tag/curl/"},{"categories":null,"content":"","keywords":null,"title":"curl","uri":"https://rmoff.net/categories/curl/"},{"categories":null,"content":"","keywords":null,"title":"grafana","uri":"https://rmoff.net/tag/grafana/"},{"categories":["curl","grafana","bash","jq"],"content":"Grafana API Reference\nExport all Grafana data sources to data_sources folder mkdir -p data_sources \u0026\u0026 curl -s \"http://localhost:3000/api/datasources\" -u admin:admin|jq -c -M '.[]'|split -l 1 - data_sources/ This exports each data source to a separate JSON file in the data_sources folder.\nLoad data sources back in from folder This submits every file that exists in the data_sources folder to Grafana as a new data source definition.\nfor i in data_sources/*; do \\ curl -X \"POST\" \"http://localhost:3000/api/datasources\" \\ -H \"Content-Type: application/json\" \\ --user admin:admin \\ --data-binary @$i done ","keywords":null,"title":"Simple export/import of Data Sources in Grafana","uri":"https://rmoff.net/2017/08/08/simple-export/import-of-data-sources-in-grafana/"},{"categories":["lsblk","uas","usb","mount"],"content":"Usually connecting external disks in Linux is easy. Plug it in, run fdisk -l or lsblk | grep disk to identify the device ID, and then mount it.\nUnfortunately in this instance, plugging in my Seagate 2TB wasn‚Äôt so simple. The server is running Proxmox:\n# uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux No device showed up on lsblk or fdisk -l. In dmesg I saw:","keywords":null,"title":"Linux - USB disk connection problems - uas: probe failed with error -12","uri":"https://rmoff.net/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/"},{"categories":null,"content":"","keywords":null,"title":"lsblk","uri":"https://rmoff.net/tag/lsblk/"},{"categories":null,"content":"","keywords":null,"title":"lsblk","uri":"https://rmoff.net/categories/lsblk/"},{"categories":null,"content":"","keywords":null,"title":"mount","uri":"https://rmoff.net/tag/mount/"},{"categories":null,"content":"","keywords":null,"title":"mount","uri":"https://rmoff.net/categories/mount/"},{"categories":null,"content":"","keywords":null,"title":"uas","uri":"https://rmoff.net/tag/uas/"},{"categories":null,"content":"","keywords":null,"title":"uas","uri":"https://rmoff.net/categories/uas/"},{"categories":null,"content":"","keywords":null,"title":"usb","uri":"https://rmoff.net/tag/usb/"},{"categories":null,"content":"","keywords":null,"title":"usb","uri":"https://rmoff.net/categories/usb/"},{"categories":["Apache Kafka","log4j","kafka connect"],"content":"Kafka‚Äôs Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from the many available, it‚Äôs possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the Confluent Control Center, for both management and monitoring:\nBUT ‚Ä¶ there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured.","keywords":null,"title":"Configuring Kafka Connect to log REST HTTP messages to a separate file","uri":"https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/"},{"categories":["Apache Kafka","key","spelling","pebcak"],"content":"A very silly PEBCAK problem this one, but Google hits weren‚Äôt so helpful so here goes.\nRunning a console producer, specifying keys:\nkafka-console-producer \\ --broker-list localhost:9092 \\ --topic test_topic \\ --property parse.key=true \\ --property key.seperator=, Failed when I entered a key/value:\n1,foo kafka.common.KafkaException: No key found on line 1: 1,foo at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314) at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55) at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala) kafka.common.KafkaException: No key found on line ‚Ä¶ but I specified the key, didn‚Äôt I?","keywords":null,"title":"kafka.common.KafkaException: No key found on line 1","uri":"https://rmoff.net/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/"},{"categories":null,"content":"","keywords":null,"title":"key","uri":"https://rmoff.net/categories/key/"},{"categories":null,"content":"","keywords":null,"title":"pebcak","uri":"https://rmoff.net/categories/pebcak/"},{"categories":null,"content":"","keywords":null,"title":"spelling","uri":"https://rmoff.net/categories/spelling/"},{"categories":null,"content":"","keywords":null,"title":"books","uri":"https://rmoff.net/tag/books/"},{"categories":null,"content":"","keywords":null,"title":"books","uri":"https://rmoff.net/categories/books/"},{"categories":null,"content":"","keywords":null,"title":"career","uri":"https://rmoff.net/tag/career/"},{"categories":null,"content":"","keywords":null,"title":"data","uri":"https://rmoff.net/tag/data/"},{"categories":["reading","books","twitter","podcasts","knowledge","data","career"],"content":"How do you try and stay current on technical affairs, given only 24 hours in a day and a job to do as well? Here‚Äôs my take on it‚Ä¶\nOne of the many things that has changed perceptibly since the beginning of this century when I started working in IT is the amount of information freely available, and being created all the time. Back then, printed books and manuals were still the primary source of definitive information about a piece of software.","keywords":null,"title":"Keeping Up with the Deluge","uri":"https://rmoff.net/2017/03/11/keeping-up-with-the-deluge/"},{"categories":null,"content":"","keywords":null,"title":"knowledge","uri":"https://rmoff.net/tag/knowledge/"},{"categories":null,"content":"","keywords":null,"title":"podcasts","uri":"https://rmoff.net/tag/podcasts/"},{"categories":null,"content":"","keywords":null,"title":"podcasts","uri":"https://rmoff.net/categories/podcasts/"},{"categories":null,"content":"","keywords":null,"title":"reading","uri":"https://rmoff.net/tag/reading/"},{"categories":null,"content":"","keywords":null,"title":"reading","uri":"https://rmoff.net/categories/reading/"},{"categories":null,"content":"","keywords":null,"title":"twitter","uri":"https://rmoff.net/tag/twitter/"},{"categories":null,"content":"","keywords":null,"title":"twitter","uri":"https://rmoff.net/categories/twitter/"},{"categories":null,"content":"","keywords":null,"title":"aws","uri":"https://rmoff.net/tag/aws/"},{"categories":null,"content":"","keywords":null,"title":"centos","uri":"https://rmoff.net/tag/centos/"},{"categories":null,"content":"","keywords":null,"title":"centos","uri":"https://rmoff.net/categories/centos/"},{"categories":null,"content":"","keywords":null,"title":"ec2","uri":"https://rmoff.net/tag/ec2/"},{"categories":["qemu","aws","ec2","centos","rhel"],"content":"Mucking about with virtual disks, I wanted to install qemu on a AWS EC2 instance in order to use qemu-img.\nNot finding it in a yum repo, I built it from scratch:\n$ uname -a Linux ip-10-0-1-238 4.4.41-36.55.amzn1.x86_64 #1 SMP Wed Jan 18 01:03:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux Steps:\nsudo yum install -y ghc-glib-devel ghc-glib autoconf autogen intltool libtool wget http://download.qemu-project.org/qemu-2.8.0.tar.xz tar xvJf qemu-2.8.0.tar.xz cd qemu-2.8.0 ./configure make sudo make install I hit a few errors, recorded here for passing Googlers:","keywords":null,"title":"Install qemu on AWS EC2 Amazon Linux","uri":"https://rmoff.net/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/"},{"categories":null,"content":"","keywords":null,"title":"qemu","uri":"https://rmoff.net/tag/qemu/"},{"categories":null,"content":"","keywords":null,"title":"qemu","uri":"https://rmoff.net/categories/qemu/"},{"categories":null,"content":"","keywords":null,"title":"rhel","uri":"https://rmoff.net/tag/rhel/"},{"categories":null,"content":"","keywords":null,"title":"rhel","uri":"https://rmoff.net/categories/rhel/"},{"categories":null,"content":"","keywords":null,"title":"img","uri":"https://rmoff.net/tag/img/"},{"categories":null,"content":"","keywords":null,"title":"img","uri":"https://rmoff.net/categories/img/"},{"categories":null,"content":"","keywords":null,"title":"losetup","uri":"https://rmoff.net/tag/losetup/"},{"categories":null,"content":"","keywords":null,"title":"losetup","uri":"https://rmoff.net/categories/losetup/"},{"categories":null,"content":"","keywords":null,"title":"lvm","uri":"https://rmoff.net/tag/lvm/"},{"categories":null,"content":"","keywords":null,"title":"lvm","uri":"https://rmoff.net/categories/lvm/"},{"categories":["vmdk","vgscan","lvm","mount","raw","img","aws","losetup"],"content":"So you‚Äôve got a Linux VM that you want to access the contents of in EC2 - how do you do it? Let‚Äôs see how. First up, convert the VMDK to raw image file. If you‚Äôve got a ova/ovf then just untar it first (tar -xvf my_vm.ova), from which you should get the VMDK. With that, convert it using qemu-img:\n$ time qemu-img convert -f vmdk -O raw SampleAppv607p-appliance-disk1.vmdk SampleAppv607p-appliance-disk1.raw real 16m36.","keywords":null,"title":"Mount VMDK/OVF/OVA on Amazon Web Services (AWS) EC2","uri":"https://rmoff.net/2017/03/11/mount-vmdk/ovf/ova-on-amazon-web-services-aws-ec2/"},{"categories":null,"content":"","keywords":null,"title":"raw","uri":"https://rmoff.net/tag/raw/"},{"categories":null,"content":"","keywords":null,"title":"raw","uri":"https://rmoff.net/categories/raw/"},{"categories":null,"content":"","keywords":null,"title":"vgscan","uri":"https://rmoff.net/tag/vgscan/"},{"categories":null,"content":"","keywords":null,"title":"vgscan","uri":"https://rmoff.net/categories/vgscan/"},{"categories":null,"content":"","keywords":null,"title":"vmdk","uri":"https://rmoff.net/tag/vmdk/"},{"categories":null,"content":"","keywords":null,"title":"vmdk","uri":"https://rmoff.net/categories/vmdk/"},{"categories":null,"content":"","keywords":null,"title":"apple","uri":"https://rmoff.net/tag/apple/"},{"categories":null,"content":"","keywords":null,"title":"apple","uri":"https://rmoff.net/categories/apple/"},{"categories":null,"content":"","keywords":null,"title":"apple watch","uri":"https://rmoff.net/tag/apple-watch/"},{"categories":null,"content":"","keywords":null,"title":"apple watch","uri":"https://rmoff.net/categories/apple-watch/"},{"categories":null,"content":"","keywords":null,"title":"headphones","uri":"https://rmoff.net/tag/headphones/"},{"categories":null,"content":"","keywords":null,"title":"headphones","uri":"https://rmoff.net/categories/headphones/"},{"categories":null,"content":"","keywords":null,"title":"headset","uri":"https://rmoff.net/tag/headset/"},{"categories":null,"content":"","keywords":null,"title":"headset","uri":"https://rmoff.net/categories/headset/"},{"categories":["headphones","microphone","headset","wireless","logitech","microsoft","apple","strava","running","podcasts","apple watch"],"content":"Wireless Headset for VOIP With No 30-Minute Dalek Timebomb A lot of my work is done remotely, with colleagues and customers. Five years ago I bought a Microsoft LifeChat LX-3000 which plugged into the USB port on my Mac. It did the job kinda fine, with two gripes:\nit wasn‚Äôt wireless. I like to wander whilst I chat, and I didn‚Äôt like being tethered. But this in itself wasn‚Äôt a reason to ditch it After c.","keywords":null,"title":"Little Technology Wins","uri":"https://rmoff.net/2017/03/11/little-technology-wins/"},{"categories":null,"content":"","keywords":null,"title":"logitech","uri":"https://rmoff.net/tag/logitech/"},{"categories":null,"content":"","keywords":null,"title":"logitech","uri":"https://rmoff.net/categories/logitech/"},{"categories":null,"content":"","keywords":null,"title":"microphone","uri":"https://rmoff.net/tag/microphone/"},{"categories":null,"content":"","keywords":null,"title":"microphone","uri":"https://rmoff.net/categories/microphone/"},{"categories":null,"content":"","keywords":null,"title":"microsoft","uri":"https://rmoff.net/tag/microsoft/"},{"categories":null,"content":"","keywords":null,"title":"microsoft","uri":"https://rmoff.net/categories/microsoft/"},{"categories":null,"content":"","keywords":null,"title":"running","uri":"https://rmoff.net/tag/running/"},{"categories":null,"content":"","keywords":null,"title":"strava","uri":"https://rmoff.net/tag/strava/"},{"categories":null,"content":"","keywords":null,"title":"strava","uri":"https://rmoff.net/categories/strava/"},{"categories":null,"content":"","keywords":null,"title":"wireless","uri":"https://rmoff.net/tag/wireless/"},{"categories":null,"content":"","keywords":null,"title":"wireless","uri":"https://rmoff.net/categories/wireless/"},{"categories":["career"],"content":"After 5 years at Rittman Mead, 126 blog posts, 16 conferences, four published OTN articles, an Oracle ACE award - not to mention, of course, a whole heap of interesting and challenging client work - I‚Äôve decided that it‚Äôs time to do something different.\nLater this month I‚Äôll be joining Confluent as a Partner Technology Evangelist, helping spread the good word of Apache Kafka and the Confluent platform.\nAs always you can find me on Twitter @rmoff, for beer tweets, fried breakfast pics - and lots of Apache Kafka!","keywords":null,"title":"Time For a Change","uri":"https://rmoff.net/2017/03/10/time-for-a-change/"},{"categories":null,"content":"","keywords":null,"title":"bigdatalite","uri":"https://rmoff.net/tag/bigdatalite/"},{"categories":null,"content":"","keywords":null,"title":"bigdatalite","uri":"https://rmoff.net/categories/bigdatalite/"},{"categories":null,"content":"","keywords":null,"title":"hbase","uri":"https://rmoff.net/tag/hbase/"},{"categories":null,"content":"","keywords":null,"title":"hbase","uri":"https://rmoff.net/categories/hbase/"},{"categories":["hbase","bigdatalite","virtualbox","ova"],"content":"I use BigDataLite for a lot of my sandboxing work. This is a OVA provided by Oracle which can be run on VirtualBox, VMWare, etc and has the Cloudera Hadoop platform (CDH) along with all of Oracle‚Äôs Big Data goodies including Big Data Discovery and Big Data Spatial and Graph (BDSG).\nSomething that kept tripping me up during my work with BDSG was that HBase would become unavailable. Not being an HBase expert and simply using it as a data store for my property graph data, I wrote it off as mistakes on my part.","keywords":null,"title":"HBase crash after resuming suspended VM","uri":"https://rmoff.net/2017/01/20/hbase-crash-after-resuming-suspended-vm/"},{"categories":null,"content":"","keywords":null,"title":"ova","uri":"https://rmoff.net/tag/ova/"},{"categories":null,"content":"","keywords":null,"title":"ova","uri":"https://rmoff.net/categories/ova/"},{"categories":null,"content":"","keywords":null,"title":"virtualbox","uri":"https://rmoff.net/tag/virtualbox/"},{"categories":null,"content":"","keywords":null,"title":"virtualbox","uri":"https://rmoff.net/categories/virtualbox/"},{"categories":null,"content":"","keywords":null,"title":"holt","uri":"https://rmoff.net/tag/holt/"},{"categories":null,"content":"","keywords":null,"title":"holt","uri":"https://rmoff.net/categories/holt/"},{"categories":["kibana","timelion","holt"],"content":"Using the holt function in Timelion to do anomaly detection on Metricbeat data in Kibana:\nExpression:\n$thres=0.02, .es(index='metricbeat*',metric='max:system.cpu.user.pct').lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).color(#eee).lines(10).label('Prediction'), .es(index='metricbeat*',metric='max:system.cpu.user.pct').color(#666).lines(1).label(Actual), .es(index='metricbeat*',metric='max:system.cpu.user.pct').lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).subtract(.es(index='metricbeat*',metric='max:system.cpu.user.pct')).abs().if(lt, $thres, null, .es(index='metricbeat*',metric='max:system.cpu.user.pct')).points(10,3,0).color(#c66).label('Anomaly').title('max:system.cpu.user.pct / @rmoff') References:\nhttps://twitter.com/rashidkpc/status/762754396111327232 https://github.com/elastic/timelion/issues/87 https://github.com/elastic/timelion/blob/master/FUNCTIONS.md ","keywords":null,"title":"Kibana Timelion - Anomaly Detection","uri":"https://rmoff.net/2017/01/18/kibana-timelion-anomaly-detection/"},{"categories":null,"content":"","keywords":null,"title":"timelion","uri":"https://rmoff.net/tag/timelion/"},{"categories":null,"content":"","keywords":null,"title":"timelion","uri":"https://rmoff.net/categories/timelion/"},{"categories":null,"content":"The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Data architectures for streaming applications SE-Radio Episode 272: Frances Perry on Apache Beam (img credit)","keywords":null,"title":"Streaming / Unbounded Data - Resources","uri":"https://rmoff.net/2017/01/16/streaming-/-unbounded-data-resources/"},{"categories":null,"content":"","keywords":null,"title":"kafka-avro-console-producer","uri":"https://rmoff.net/categories/kafka-avro-console-producer/"},{"categories":["Apache Kafka","schema registry","kafka-avro-console-producer"],"content":"By default, the kafka-avro-console-producer will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 already!\n[oracle@bigdatalite tmp]$ kafka-avro-console-producer \\ \u003e --broker-list localhost:9092 --topic kudu_test \\ \u003e --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"random_field\", \"type\": \"string\"}]}' {\"id\": 999, \"random_field\": \"foo\"} org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"random_field\",\"type\":\"string\"}]} Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character ('\u003c' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null') at [Source: sun.","keywords":null,"title":"kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException","uri":"https://rmoff.net/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-/-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/"},{"categories":null,"content":"","keywords":null,"title":"ogg","uri":"https://rmoff.net/tag/ogg/"},{"categories":null,"content":"","keywords":null,"title":"ogg","uri":"https://rmoff.net/categories/ogg/"},{"categories":["ogg","kafka connect","avro"],"content":"tl;dr Make sure that key.converter.schema.registry.url and value.converter.schema.registry.url are specified, and that there are no trailing whitespaces.\nI‚Äôve been building on previous work I‚Äôve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the sample configuration gives.\nSimply changing the Kafka Connect OGG configuration file (confluent.properties) from\nvalue.converter=org.apache.kafka.connect.json.JsonConverter key.","keywords":null,"title":"Oracle GoldenGate -\u003e Kafka Connect - ‚ÄúFailed to serialize Avro data‚Äù","uri":"https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/"},{"categories":null,"content":"","keywords":null,"title":"classpath","uri":"https://rmoff.net/categories/classpath/"},{"categories":null,"content":"","keywords":null,"title":"IncompatibleClassChangeError","uri":"https://rmoff.net/categories/incompatibleclasschangeerror/"},{"categories":["Apache Kafka","kafka connect","IncompatibleClassChangeError","classpath"],"content":"I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:\n[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties [...] Exception in thread \"main\" java.lang.IncompatibleClassChangeError: Implementing class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) The fix was to unset the CLASSPATH first:\nunset CLASSPATH ","keywords":null,"title":"Kafka Connect - java.lang.IncompatibleClassChangeError","uri":"https://rmoff.net/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/"},{"categories":null,"content":"","keywords":null,"title":"boto","uri":"https://rmoff.net/tag/boto/"},{"categories":null,"content":"","keywords":null,"title":"boto","uri":"https://rmoff.net/categories/boto/"},{"categories":["boto","s3","aws","python"],"content":"Presented without comment, warranty, or context - other than these might help a wandering code hacker.\nWhen using SigV4, you must specify a ‚Äòhost‚Äô parameter boto.s3.connection.HostRequiredError: BotoClientError: When using SigV4, you must specify a 'host' parameter. To fix, switch\nconn_s3 = boto.connect_s3() for\nconn_s3 = boto.connect_s3(host='s3.amazonaws.com') You can see a list of endpoints here.\nboto.exception.S3ResponseError: S3ResponseError: 400 Bad Request Make sure you‚Äôre specifying the correct hostname (see above) for the bucket‚Äôs region.","keywords":null,"title":"boto / S3 errors","uri":"https://rmoff.net/2016/10/14/boto-/-s3-errors/"},{"categories":null,"content":"","keywords":null,"title":"s3","uri":"https://rmoff.net/tag/s3/"},{"categories":null,"content":"","keywords":null,"title":"s3","uri":"https://rmoff.net/categories/s3/"},{"categories":null,"content":"","keywords":null,"title":"ogg-15051","uri":"https://rmoff.net/categories/ogg-15051/"},{"categories":["Apache Kafka","goldengate","ogg","ogg-15051"],"content":"Similar to the previous issue, the sample config in the docs causes another snafu:\nOGG-15051 Java or JNI exception: oracle.goldengate.util.GGException: Class not found: \"kafkahandler\". kafkahandler Class not found: \"kafkahandler\". kafkahandler This time it‚Äôs in the kafka.props file:\ngg.handler.kafkahandler.Type = kafka Should be\ngg.handler.kafkahandler.type = kafka No capital T in Type!\n(Image credit: https://unsplash.com/@vanschneider)","keywords":null,"title":"OGG-15051 oracle.goldengate.util.GGException:  Class not found: ‚Äúkafkahandler‚Äù","uri":"https://rmoff.net/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/"},{"categories":["ogg","goldengate","Apache Kafka"],"content":"In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there‚Äôs a helpful sample configuration, which isn‚Äôt so helpful ‚Ä¶\n[...] gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord [...] This value for gg.handler.kafkahandler.ProducerRecordClass will cause a failure when you start the replicat:\n[...] Class not found: \"com.company.kafka.CustomProducerRecord\" [...] If you comment this configuration item out, it‚Äôll use the default (oracle.goldengate.handler.kafka.DefaultProducerRecord) and work swimingly!\n(Image credit: https://unsplash.com/@vanschneider)","keywords":null,"title":"OGG -  Class not found: ‚Äúcom.company.kafka.CustomProducerRecord‚Äù","uri":"https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/"},{"categories":null,"content":"","keywords":null,"title":"jdbc","uri":"https://rmoff.net/categories/jdbc/"},{"categories":["Apache Kafka","kafka connect","jdbc","oracle","log4j"],"content":"There are various reasons for this error, but the one I hit was that the table name is case sensitive, and returned from Oracle by the JDBC driver in uppercase.\nIf you specify the tablename in your connecter config in lowercase, it won‚Äôt be matched, and this error is thrown. You can validate this by setting debug logging (edit etc/kafka/connect-log4j.properties to set log4j.rootLogger=DEBUG, stdout), and observe: (I‚Äôve truncated some of the output for legibility)","keywords":null,"title":"Kafka Connect JDBC - Oracle - Number of groups must be positive","uri":"https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/"},{"categories":["Apache Kafka","avro","kafka connect","SchemaProjectorException"],"content":"I‚Äôve been doing some noodling around with Confluent‚Äôs Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you‚Äôre not familiar with Kafka Connect this page gives a good idea of the thinking behind it.\nOne issue that I hit defeated my Google-fu so I‚Äôm recording it here to hopefully help out fellow n00bs.\nThe pipeline that I‚Äôd set up looked like this:\nEneco‚Äôs Twitter Source streaming tweets to a Kafka topic Confluent‚Äôs HDFS Sink to stream tweets to HDFS and define Hive table automagically over them It worked great, but only if I didn‚Äôt enable the Hive integration part.","keywords":null,"title":"Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required","uri":"https://rmoff.net/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/"},{"categories":null,"content":"","keywords":null,"title":"SchemaProjectorException","uri":"https://rmoff.net/categories/schemaprojectorexception/"},{"categories":null,"content":"","keywords":null,"title":"apcupsd","uri":"https://rmoff.net/tag/apcupsd/"},{"categories":null,"content":"","keywords":null,"title":"apcupsd","uri":"https://rmoff.net/categories/apcupsd/"},{"categories":["apcupsd","ups"],"content":"With my new server I bought a UPS, partly just as a Good Thing, but also because I suspect a powercut fried the motherboard on a previous machine that I had, and this baby is too precious to lose ;)\nThe idea is that the UPS will smooth out the power supply to my server, protecting it from surges or temporarily blips in power loss. If there‚Äôs a proper power cut, the UPS is connected to my server and can initiate a graceful shutdown instead of system crash.","keywords":null,"title":"Configuring UPS/apcupsd","uri":"https://rmoff.net/2016/07/18/configuring-ups/apcupsd/"},{"categories":null,"content":"","keywords":null,"title":"ups","uri":"https://rmoff.net/tag/ups/"},{"categories":null,"content":"","keywords":null,"title":"ups","uri":"https://rmoff.net/categories/ups/"},{"categories":null,"content":"","keywords":null,"title":"json","uri":"https://rmoff.net/tag/json/"},{"categories":null,"content":"","keywords":null,"title":"spark","uri":"https://rmoff.net/tag/spark/"},{"categories":null,"content":"","keywords":null,"title":"spark","uri":"https://rmoff.net/categories/spark/"},{"categories":["spark","sparksql","json"],"content":"Trying to use SparkSQL to read a JSON file, from either pyspark or spark-shell, I got this error:\njava.io.IOException: No input paths specified in job scala\u003e sqlContext.read.json(\"/u02/custom/twitter/twitter.json\") java.io.IOException: No input paths specified in job at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202) Despite the reference articles that I found using this local path syntax (/u02/custom/twitter/twitter.json), it turned out that I needed to prefix it with file://:\nscala\u003e sqlContext.read.json(\"file:///u02/custom/twitter/twitter.json\") res3: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: string, created_at: string, entities: struct\u003chashtags:array\u003cstruct\u003cindices:array\u003cbigint\u003e,text:string\u003e\u003e,media:array\u003cstruct\u003cdisplay_url:string,expanded_url:string,id:bigint,id_str:string,indices:array\u003cbigint\u003e,media_url:string,media_url_https:string,sizes:struct\u003clarge:struct\u003ch:bigint,resize:string,w:bigint\u003e,medium:struct\u003ch:bigint,resize:string,w:bigint\u003e,small:struct\u003ch:bigint,resize:string,w:bigint\u003e,thumb:struct\u003ch:bigint,resize:string,w:bigint\u003e\u003e,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string\u003e\u003e,symbols:array\u003cstruct\u003cindices:array\u003cbigint\u003e,text:string\u003e\u003e,urls:array\u003cstruct\u003cdisplay_url:string,expanded_url:string.","keywords":null,"title":"Spark sqlContext.read.json - java.io.IOException: No input paths specified in job","uri":"https://rmoff.net/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/"},{"categories":null,"content":"","keywords":null,"title":"sparksql","uri":"https://rmoff.net/tag/sparksql/"},{"categories":null,"content":"","keywords":null,"title":"sparksql","uri":"https://rmoff.net/categories/sparksql/"},{"categories":null,"content":"","keywords":null,"title":"cidr","uri":"https://rmoff.net/tag/cidr/"},{"categories":null,"content":"","keywords":null,"title":"cidr","uri":"https://rmoff.net/categories/cidr/"},{"categories":null,"content":"","keywords":null,"title":"lxc","uri":"https://rmoff.net/tag/lxc/"},{"categories":null,"content":"","keywords":null,"title":"lxc","uri":"https://rmoff.net/categories/lxc/"},{"categories":null,"content":"","keywords":null,"title":"networking","uri":"https://rmoff.net/tag/networking/"},{"categories":null,"content":"","keywords":null,"title":"proxmox","uri":"https://rmoff.net/tag/proxmox/"},{"categories":null,"content":"","keywords":null,"title":"proxmox","uri":"https://rmoff.net/categories/proxmox/"},{"categories":["proxmox","cidr","networking","lxc"],"content":"TL;DR When defining networking on Proxmox 4 LXC containers, use an appropriate CIDR suffix (e.g. 24) - don‚Äôt use 32!\nOn my Proxmox 4 server I‚Äôm running a whole load of lovely LXC containers. Unfortunately, I had trouble connecting to them. From a client machine, I got the error\nssh_exchange_identification: read: Connection reset by peer On the server I was connecting to (which I could get a console for through the Proxmox GUI, or a session on using pct enter from the Proxmox host) I ran a SSHD process with debug to see what was happening:","keywords":null,"title":"Proxmox 4 Containers - ssh - ssh_exchange_identification: read: Connection reset by peer","uri":"https://rmoff.net/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/"},{"categories":null,"content":"","keywords":null,"title":"cdh","uri":"https://rmoff.net/tag/cdh/"},{"categories":null,"content":"","keywords":null,"title":"cdh","uri":"https://rmoff.net/categories/cdh/"},{"categories":null,"content":"","keywords":null,"title":"django","uri":"https://rmoff.net/tag/django/"},{"categories":null,"content":"","keywords":null,"title":"django","uri":"https://rmoff.net/categories/django/"},{"categories":null,"content":"","keywords":null,"title":"hue","uri":"https://rmoff.net/tag/hue/"},{"categories":null,"content":"","keywords":null,"title":"hue","uri":"https://rmoff.net/categories/hue/"},{"categories":["hue","django","python","cdh"],"content":"(Ref)\nThe bit that caught me out was this kept failing with\nError: Password not present\tand a Python stack trace that ended with\nsubprocess.CalledProcessError: Command '/var/run/cloudera-scm-agent/process/78-hue-HUE_SERVER/altscript.sh sec-1-secret_key' returned non-zero exit status 1 The answer (it seems) is to ensure that HUE_SECRET_KEY is set (to any value!)\nLaunch shell:\nexport HUE_SECRET_KEY=foobar /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/hue/build/env/bin/hue shell Reset password for hue, activate account and make it superuser\nfrom django.contrib.auth.models import User user = User.objects.get(username='hue') user.","keywords":null,"title":"Reset Hue password","uri":"https://rmoff.net/2016/07/05/reset-hue-password/"},{"categories":null,"content":"","keywords":null,"title":"apache drill","uri":"https://rmoff.net/tag/apache-drill/"},{"categories":null,"content":"","keywords":null,"title":"apache drill","uri":"https://rmoff.net/categories/apache-drill/"},{"categories":["apache drill"],"content":"Vanilla download of Apache Drill 1.6, attempting to follow the Followed the Drill in 10 Minutes tutorial - but kept just getting the error No current connection. Here‚Äôs an example:\n[oracle@bigdatalite apache-drill-1.6.0]$ ./bin/drill-embedded Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 com.fasterxml.jackson.databind.JavaType.isReferenceType()Z apache drill 1.6.0 \"the only truly happy people are children, the creative minority and drill users\" 0: jdbc:drill:zk=local\u003e SELECT version FROM sys.version; No current connection 0: jdbc:drill:zk=local\u003e Whether SELECT version FROM sys.","keywords":null,"title":"Apache Drill - conflicting jar problem - ‚ÄúNo current connection‚Äù","uri":"https://rmoff.net/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/"},{"categories":null,"content":"","keywords":null,"title":"classnotfoundexception","uri":"https://rmoff.net/tag/classnotfoundexception/"},{"categories":null,"content":"","keywords":null,"title":"classnotfoundexception","uri":"https://rmoff.net/categories/classnotfoundexception/"},{"categories":["mogodb","hive","jar","classnotfoundexception"],"content":"I wasted literally two hours on this one, so putting down a note to hopefully help future Googlers.\nSymptom Here‚Äôs all the various errors that I got in the hive-server2.log during my attempts to get a CREATE EXTERNABLE TABLE to work against a MongoDB table in Hive:\nCaused by: java.lang.ClassNotFoundException: com.mongodb.hadoop.io.BSONWritable Caused by: java.lang.ClassNotFoundException: com.mongodb.util.JSON Caused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson Caused by: java.lang.ClassNotFoundException: org.bson.io.OutputBuffer Whilst Hive would throw errors along the lines of:","keywords":null,"title":"ClassNotFoundException with MongoDB-Hadoop in Hive","uri":"https://rmoff.net/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/"},{"categories":null,"content":"","keywords":null,"title":"hive","uri":"https://rmoff.net/tag/hive/"},{"categories":null,"content":"","keywords":null,"title":"hive","uri":"https://rmoff.net/categories/hive/"},{"categories":null,"content":"","keywords":null,"title":"jar","uri":"https://rmoff.net/tag/jar/"},{"categories":null,"content":"","keywords":null,"title":"jar","uri":"https://rmoff.net/categories/jar/"},{"categories":null,"content":"","keywords":null,"title":"mogodb","uri":"https://rmoff.net/tag/mogodb/"},{"categories":null,"content":"","keywords":null,"title":"mogodb","uri":"https://rmoff.net/categories/mogodb/"},{"categories":null,"content":"","keywords":null,"title":"cloudera","uri":"https://rmoff.net/tag/cloudera/"},{"categories":null,"content":"","keywords":null,"title":"cloudera","uri":"https://rmoff.net/categories/cloudera/"},{"categories":["lxc","proxmox","swapfree","cdh","cloudera","yarn","readProcMemInfoFile","/proc/meminfo"],"content":"Installing CDH 5.7 on Linux Containers (LXC) hosted on Proxmox 4. Everything was going well until Cluster Setup, and which point it failed on Start YARN (MR2 included)\nCompleted only 0/1 steps. First failure: Failed to execute command Start on service YARN (MR2 Included) Log /var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh57-01-node-02.moffatt.me.log.out showed:\norg.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.NumberFormatException: For input string: \"18446744073709550364\" java.lang.NumberFormatException: For input string: \"18446744073709550364\" Looking down the stack trace, this came from org.","keywords":null,"title":"Erroneous SwapFree on LXC causes problems with CDH install","uri":"https://rmoff.net/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/"},{"categories":null,"content":"","keywords":null,"title":"readProcMemInfoFile","uri":"https://rmoff.net/tag/readprocmeminfofile/"},{"categories":null,"content":"","keywords":null,"title":"readProcMemInfoFile","uri":"https://rmoff.net/categories/readprocmeminfofile/"},{"categories":null,"content":"","keywords":null,"title":"swapfree","uri":"https://rmoff.net/tag/swapfree/"},{"categories":null,"content":"","keywords":null,"title":"swapfree","uri":"https://rmoff.net/categories/swapfree/"},{"categories":null,"content":"","keywords":null,"title":"yarn","uri":"https://rmoff.net/tag/yarn/"},{"categories":null,"content":"","keywords":null,"title":"yarn","uri":"https://rmoff.net/categories/yarn/"},{"categories":null,"content":"","keywords":null,"title":"edgemax","uri":"https://rmoff.net/tag/edgemax/"},{"categories":null,"content":"","keywords":null,"title":"edgemax","uri":"https://rmoff.net/categories/edgemax/"},{"categories":null,"content":"","keywords":null,"title":"edgerouter lite","uri":"https://rmoff.net/tag/edgerouter-lite/"},{"categories":null,"content":"","keywords":null,"title":"edgerouter lite","uri":"https://rmoff.net/categories/edgerouter-lite/"},{"categories":null,"content":"","keywords":null,"title":"erl","uri":"https://rmoff.net/tag/erl/"},{"categories":null,"content":"","keywords":null,"title":"erl","uri":"https://rmoff.net/categories/erl/"},{"categories":["edgemax","erl","edgerouter lite","tftp","rj45","screen","squashfs","router","networking"],"content":"I‚Äôve got an EdgeRouter LITE (ERL) which I used as my home router until a powercut fried it a while ago (looks like I‚Äôm not the only one to have this issue). The symptoms were it powering on but not giving any DHCP addresses, or after a factory reset responding on the default IP of 192.168.1.1. It was a real shame, because it had been a great bit of kit up until then.","keywords":null,"title":"Reviving a bricked EdgeRouter Lite (ERL) from a Mac","uri":"https://rmoff.net/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/"},{"categories":null,"content":"","keywords":null,"title":"rj45","uri":"https://rmoff.net/tag/rj45/"},{"categories":null,"content":"","keywords":null,"title":"rj45","uri":"https://rmoff.net/categories/rj45/"},{"categories":null,"content":"","keywords":null,"title":"router","uri":"https://rmoff.net/tag/router/"},{"categories":null,"content":"","keywords":null,"title":"router","uri":"https://rmoff.net/categories/router/"},{"categories":null,"content":"","keywords":null,"title":"screen","uri":"https://rmoff.net/tag/screen/"},{"categories":null,"content":"","keywords":null,"title":"screen","uri":"https://rmoff.net/categories/screen/"},{"categories":null,"content":"","keywords":null,"title":"squashfs","uri":"https://rmoff.net/tag/squashfs/"},{"categories":null,"content":"","keywords":null,"title":"squashfs","uri":"https://rmoff.net/categories/squashfs/"},{"categories":null,"content":"","keywords":null,"title":"tftp","uri":"https://rmoff.net/tag/tftp/"},{"categories":null,"content":"","keywords":null,"title":"tftp","uri":"https://rmoff.net/categories/tftp/"},{"categories":null,"content":"","keywords":null,"title":"bittorrent sync","uri":"https://rmoff.net/tag/bittorrent-sync/"},{"categories":null,"content":"","keywords":null,"title":"bittorrent sync","uri":"https://rmoff.net/categories/bittorrent-sync/"},{"categories":null,"content":"","keywords":null,"title":"dropbox","uri":"https://rmoff.net/tag/dropbox/"},{"categories":null,"content":"","keywords":null,"title":"dropbox","uri":"https://rmoff.net/categories/dropbox/"},{"categories":["docker","proxmox","bittorrent sync","dropbox"],"content":"(Previously, previously, previously)\nSince Proxmox 4 has a recent Linux kernel and mainline one at that, it means that Docker can be run on it. I‚Äôve yet to really dig into Docker and work out when it makes sense in place of Linux Containers (LXC), so this is going to be a learning experience for me.\nTo install Docker, add Backports repo to apt:\nroot@proxmox01:~# cat /etc/apt/sources.list.d/backports.list deb http://ftp.debian.org/debian jessie-backports main And then install:","keywords":null,"title":"Running a Docker Container on Proxmox for BitTorrent Sync","uri":"https://rmoff.net/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/"},{"categories":["proxmox","vmware","virtualbox","qcow2"],"content":"(Previously, previously)\nI‚Äôve got a bunch of existing VirtualBox and VMWare VMs that I want to run on Proxmox. Eventually I‚Äôll migrate them to containers, but for the time being run them as ‚Äúfat‚Äù VMs using Proxmox‚Äôs KVM virtualisation. After copying the OVA files that I had to the server, I uncompressed them:\nroot@proxmox01:/data04/vms/bdl44-biwa# cd ../bdl44 root@proxmox01:/data04/vms/bdl44# ll total 27249328 -rw------- 1 root root 27903306752 Jun 1 10:14 BigDataLite440.ova root@proxmox01:/data04/vms/bdl44# tar -xf BigDataLite440.","keywords":null,"title":"Importing VMWare and VirtualBox VMs to Proxmox","uri":"https://rmoff.net/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/"},{"categories":null,"content":"","keywords":null,"title":"qcow2","uri":"https://rmoff.net/tag/qcow2/"},{"categories":null,"content":"","keywords":null,"title":"qcow2","uri":"https://rmoff.net/categories/qcow2/"},{"categories":null,"content":"","keywords":null,"title":"vmware","uri":"https://rmoff.net/tag/vmware/"},{"categories":null,"content":"","keywords":null,"title":"vmware","uri":"https://rmoff.net/categories/vmware/"},{"categories":null,"content":"","keywords":null,"title":"bootable usb","uri":"https://rmoff.net/tag/bootable-usb/"},{"categories":null,"content":"","keywords":null,"title":"bootable usb","uri":"https://rmoff.net/categories/bootable-usb/"},{"categories":["proxmox","ext4","e2label","lsblk","hdiutil","bootable usb","memtest"],"content":"(Previously)\nWith my server in place, I ran a memtest on it ‚Ä¶ which with 128G took a while ;)\nAnd then installed Proxmox 4, using a bootable USB that I‚Äôd created on my Mac from the ISO downloaded from Proxmox‚Äôs website. To create the bootable USB, create the img file:\nhdiutil convert -format UDRW -o target.img source.iso and then burn it to USB:\nsudo dd if=target.img of=/dev/rdiskN bs=1m Replace N with the correct device based on diskutil list output.","keywords":null,"title":"Commissioning my Proxmox Server - OS and filesystems","uri":"https://rmoff.net/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/"},{"categories":null,"content":"","keywords":null,"title":"e2label","uri":"https://rmoff.net/tag/e2label/"},{"categories":null,"content":"","keywords":null,"title":"e2label","uri":"https://rmoff.net/categories/e2label/"},{"categories":null,"content":"","keywords":null,"title":"ext4","uri":"https://rmoff.net/tag/ext4/"},{"categories":null,"content":"","keywords":null,"title":"ext4","uri":"https://rmoff.net/categories/ext4/"},{"categories":null,"content":"","keywords":null,"title":"hdiutil","uri":"https://rmoff.net/tag/hdiutil/"},{"categories":null,"content":"","keywords":null,"title":"hdiutil","uri":"https://rmoff.net/categories/hdiutil/"},{"categories":null,"content":"","keywords":null,"title":"memtest","uri":"https://rmoff.net/tag/memtest/"},{"categories":null,"content":"","keywords":null,"title":"memtest","uri":"https://rmoff.net/categories/memtest/"},{"categories":["proxmox","home server","lxc","docker","vmware esxi"],"content":"After a long and painful delivery, I‚Äôm delighted to announce the arrival of a new addition to my household ‚Ä¶ :\nThis custom-build from Scan 3XS is sat in my study quietly humming away. I‚Äôm going to use it for hosting VMs for R\u0026D on OBIEE, Big Data Lite, Elastic, InfluxDB, Kafka, etc. I‚Äôll blog various installations that I‚Äôve done on it as a reference for myself, and anyone else interested.","keywords":null,"title":"A New Arrival","uri":"https://rmoff.net/2016/06/07/a-new-arrival/"},{"categories":null,"content":"","keywords":null,"title":"home server","uri":"https://rmoff.net/tag/home-server/"},{"categories":null,"content":"","keywords":null,"title":"home server","uri":"https://rmoff.net/categories/home-server/"},{"categories":null,"content":"","keywords":null,"title":"vmware esxi","uri":"https://rmoff.net/tag/vmware-esxi/"},{"categories":null,"content":"","keywords":null,"title":"vmware esxi","uri":"https://rmoff.net/categories/vmware-esxi/"},{"categories":["oracle","bigdatalite","vm"],"content":"Oracle‚Äôs excellent Big Data Lite VM has been updated, to version 4.5.\nDownload it here\nChanges:\nCDH 5.5 -\u003e 5.7 Big Data Spatial and Graph 1.1 -\u003e 1.2 Big Data Discovery 1.1 -\u003e 1.2 Oracle Big Data Connectors 4.4 -\u003e 4.5 Oracle NoSQL 3.5 -\u003e 4.0 GoldenGate 12.2.0.1 -\u003e 12.2.0.1.1 ","keywords":null,"title":"New version of BigDataLite VM from Oracle","uri":"https://rmoff.net/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/"},{"categories":null,"content":"","keywords":null,"title":"vm","uri":"https://rmoff.net/tag/vm/"},{"categories":null,"content":"","keywords":null,"title":"vm","uri":"https://rmoff.net/categories/vm/"},{"categories":["obiee12c"],"content":"I‚Äôve been spending some interesting hours digging into OBIEE 12c recently, with some interesting blog posts to show for it. Some of it is just curiosities discovered along the way, but the real meaty stuff is the in the RESTful APIs - lots of potential here for cool integrations I think‚Ä¶\nLifting the Lid on OBIEE 12c Web Services - Part 1 Lifting the Lid on OBIEE 12c Web Services - Part 2 Extended Subject Areas (XSA) and the Data Set Service Changes in BI Server Cache Behaviour in OBIEE 12c : OBIS_REFRESH_CACHE Dynamic Naming of OBIEE 12c Service Instance Exports OBIEE 12c - ‚ÄúAdd Data Source‚Äù in Answers (Photo credit: https://unsplash.","keywords":null,"title":"OBIEE 12c blog posts","uri":"https://rmoff.net/2016/06/01/obiee-12c-blog-posts/"},{"categories":null,"content":"","keywords":null,"title":"obiee12c","uri":"https://rmoff.net/tag/obiee12c/"},{"categories":null,"content":"","keywords":null,"title":"obiee12c","uri":"https://rmoff.net/categories/obiee12c/"},{"categories":null,"content":"","keywords":null,"title":"diagnostics","uri":"https://rmoff.net/tag/diagnostics/"},{"categories":null,"content":"","keywords":null,"title":"diagnostics","uri":"https://rmoff.net/categories/diagnostics/"},{"categories":null,"content":"","keywords":null,"title":"logging","uri":"https://rmoff.net/tag/logging/"},{"categories":["sawserver","logging","diagnostics"],"content":"Presentation Services can provide some very detailed logs, useful for troubleshooting, performance tracing, and general poking around. See here for details.\nThere‚Äôs no bi-init.sh in 12c, so need to set up the LD_LIBRARY_PATH ourselves:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/oracle/biee/bi/bifoundation/web/bin/:/app/oracle/biee/bi/lib/:/app/oracle/biee/lib/:/app/oracle/biee/bi/bifoundation/odbc/lib/ Run sawserver with flag to list all log sources\n/app/oracle/biee/bi/bifoundation/web/bin/sawserver -logsources \u003e saw_logsources_12.2.1.txt Full list: https://gist.github.com/rmoff/e3be9009da6130839c71181cb58509a0","keywords":null,"title":"Presentation Services Logsources in OBIEE 12c","uri":"https://rmoff.net/2016/06/01/presentation-services-logsources-in-obiee-12c/"},{"categories":null,"content":"","keywords":null,"title":"sawserver","uri":"https://rmoff.net/tag/sawserver/"},{"categories":null,"content":"","keywords":null,"title":"sawserver","uri":"https://rmoff.net/categories/sawserver/"},{"categories":null,"content":"","keywords":null,"title":"data-model-cmd","uri":"https://rmoff.net/tag/data-model-cmd/"},{"categories":null,"content":"","keywords":null,"title":"data-model-cmd","uri":"https://rmoff.net/categories/data-model-cmd/"},{"categories":null,"content":"","keywords":null,"title":"downloadrpd","uri":"https://rmoff.net/tag/downloadrpd/"},{"categories":null,"content":"","keywords":null,"title":"downloadrpd","uri":"https://rmoff.net/categories/downloadrpd/"},{"categories":["obiee","obiee12c","rpd","downloadrpd","uploadrpd","data-model-cmd","web service","rest","curl","sysdig"],"content":"In OBIEE 12c data-model-cmd is a wrapper for some java code which ultimately calls an internal RESTful web service in OBIEE 12c, bi-lcm. We saw in the previous post how these internal web services can be opened up slightly, and we‚Äôre going to do the same again here. Which means, time for the same caveat:\nNone of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle.","keywords":null,"title":"Lifting the Lid on OBIEE 12c Web Services - Part 2","uri":"https://rmoff.net/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/"},{"categories":null,"content":"","keywords":null,"title":"obiee","uri":"https://rmoff.net/tag/obiee/"},{"categories":null,"content":"","keywords":null,"title":"obiee","uri":"https://rmoff.net/categories/obiee/"},{"categories":null,"content":"","keywords":null,"title":"rest","uri":"https://rmoff.net/tag/rest/"},{"categories":null,"content":"","keywords":null,"title":"rpd","uri":"https://rmoff.net/tag/rpd/"},{"categories":null,"content":"","keywords":null,"title":"rpd","uri":"https://rmoff.net/categories/rpd/"},{"categories":null,"content":"","keywords":null,"title":"sysdig","uri":"https://rmoff.net/tag/sysdig/"},{"categories":null,"content":"","keywords":null,"title":"sysdig","uri":"https://rmoff.net/categories/sysdig/"},{"categories":null,"content":"","keywords":null,"title":"uploadrpd","uri":"https://rmoff.net/tag/uploadrpd/"},{"categories":null,"content":"","keywords":null,"title":"uploadrpd","uri":"https://rmoff.net/categories/uploadrpd/"},{"categories":null,"content":"","keywords":null,"title":"web service","uri":"https://rmoff.net/tag/web-service/"},{"categories":null,"content":"","keywords":null,"title":"web service","uri":"https://rmoff.net/categories/web-service/"},{"categories":["obiee","obiee12c","exportserviceinstance","wlst"],"content":"exportServiceInstance will export the RPD, Presentation Catalog, and Security model (application roles \u0026 policies etc ‚Äì but not WLS LDAP) into a single .bar file, from which they can be imported to another environment, or restored to the same one at a later date (e.g. for backup/restore).\nTo run exportServiceInstance you need to launch WLST first. The following demonstrates how to call it, and embeds the current timestamp \u0026 machine details in the backup (useful info, and also makes the backup name unique each time).","keywords":null,"title":"Dynamic Naming of OBIEE 12c Service Instance Exports","uri":"https://rmoff.net/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/"},{"categories":null,"content":"","keywords":null,"title":"exportserviceinstance","uri":"https://rmoff.net/tag/exportserviceinstance/"},{"categories":null,"content":"","keywords":null,"title":"exportserviceinstance","uri":"https://rmoff.net/categories/exportserviceinstance/"},{"categories":null,"content":"","keywords":null,"title":"wlst","uri":"https://rmoff.net/tag/wlst/"},{"categories":null,"content":"","keywords":null,"title":"wlst","uri":"https://rmoff.net/categories/wlst/"},{"categories":null,"content":"","keywords":null,"title":"dataset","uri":"https://rmoff.net/tag/dataset/"},{"categories":null,"content":"","keywords":null,"title":"dataset","uri":"https://rmoff.net/categories/dataset/"},{"categories":null,"content":"","keywords":null,"title":"datasetsvc","uri":"https://rmoff.net/tag/datasetsvc/"},{"categories":null,"content":"","keywords":null,"title":"datasetsvc","uri":"https://rmoff.net/categories/datasetsvc/"},{"categories":["obiee","obiee12c","xsa","dataset","datasetsvc"],"content":"So this had me scratching my head for a good hour today. Comparing SampleApp v511 against a vanilla OBIEE 12c install I‚Äôd done, one had ‚ÄúAdd Data Source‚Äù as an option in Answers, the other didn‚Äôt. The strange thing was that the option wasn‚Äôt there in SampleApp ‚Äì and usually that has all the bells and whistles enabled.\nAfter checking and re-checking the Manage Privileges option, and even the Application Policy grants, and the manual, I hit MoS - and turned up Doc ID 2093886.","keywords":null,"title":"OBIEE 12c - ‚ÄúAdd Data Source‚Äù in Answers","uri":"https://rmoff.net/2016/05/27/obiee-12c-add-data-source-in-answers/"},{"categories":null,"content":"","keywords":null,"title":"xsa","uri":"https://rmoff.net/tag/xsa/"},{"categories":null,"content":"","keywords":null,"title":"xsa","uri":"https://rmoff.net/categories/xsa/"},{"categories":null,"content":"","keywords":null,"title":"black pudding","uri":"https://rmoff.net/tag/black-pudding/"},{"categories":null,"content":"","keywords":null,"title":"black pudding","uri":"https://rmoff.net/categories/black-pudding/"},{"categories":null,"content":"","keywords":null,"title":"fried slice","uri":"https://rmoff.net/tag/fried-slice/"},{"categories":null,"content":"","keywords":null,"title":"fried slice","uri":"https://rmoff.net/categories/fried-slice/"},{"categories":null,"content":"","keywords":null,"title":"fryup","uri":"https://rmoff.net/tag/fryup/"},{"categories":null,"content":"","keywords":null,"title":"fryup","uri":"https://rmoff.net/categories/fryup/"},{"categories":null,"content":"","keywords":null,"title":"fullenglish","uri":"https://rmoff.net/tag/fullenglish/"},{"categories":null,"content":"","keywords":null,"title":"fullenglish","uri":"https://rmoff.net/categories/fullenglish/"},{"categories":null,"content":"","keywords":null,"title":"york","uri":"https://rmoff.net/tag/york/"},{"categories":null,"content":"","keywords":null,"title":"york","uri":"https://rmoff.net/categories/york/"},{"categories":["fullenglish","fryup","york","black pudding","fried slice"],"content":"I had the pleasure of not one but two fry-ups in York, UK last weekend.\nThe first was courtesy of Bill‚Äôs Restaurant\nOverall, pretty good, and I‚Äôve had much worse. All the ingredients seemed decent. The black pudding was overcooked and almost biscuit-like, but that‚Äôs my only serious grumble. The bacon was cooked well. That black pudding, beans and the mashed/friend potato thing were each extra charges annoyed me. Particularly with a hangover, I just want to be able to order a full english, without playing Mastermind to work out what‚Äôs in or not.","keywords":null,"title":"York Fry Ups","uri":"https://rmoff.net/2016/05/24/york-fry-ups/"},{"categories":null,"content":"","keywords":null,"title":"api","uri":"https://rmoff.net/tag/api/"},{"categories":null,"content":"","keywords":null,"title":"api","uri":"https://rmoff.net/categories/api/"},{"categories":null,"content":"","keywords":null,"title":"command substitution","uri":"https://rmoff.net/tag/command-substitution/"},{"categories":null,"content":"","keywords":null,"title":"command substitution","uri":"https://rmoff.net/categories/command-substitution/"},{"categories":["obiee","obiee12c","rest","paw","postman","api","visual analyzer","process substitution","command substitution","sysdig"],"content":"Architecturally, OBIEE 12c is - on the surface - pretty similar to OBIEE 11g. Sure, we‚Äôve lost OPMN in favour of Node Manager, but all the old favourites are there - WebLogic Servers, BI Server (nqsserver / OBIS), Presentation Services (sawserver / OBIPS), and so on.\nBut, scratch beneath the surface, or have a gander at slide decks such as this one from BIWA this year, and you realise that change is afoot.","keywords":null,"title":"Lifting the Lid on OBIEE 12c Web Services - Part 1","uri":"https://rmoff.net/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/"},{"categories":null,"content":"","keywords":null,"title":"paw","uri":"https://rmoff.net/tag/paw/"},{"categories":null,"content":"","keywords":null,"title":"paw","uri":"https://rmoff.net/categories/paw/"},{"categories":null,"content":"","keywords":null,"title":"postman","uri":"https://rmoff.net/tag/postman/"},{"categories":null,"content":"","keywords":null,"title":"postman","uri":"https://rmoff.net/categories/postman/"},{"categories":null,"content":"","keywords":null,"title":"process substitution","uri":"https://rmoff.net/tag/process-substitution/"},{"categories":null,"content":"","keywords":null,"title":"process substitution","uri":"https://rmoff.net/categories/process-substitution/"},{"categories":null,"content":"","keywords":null,"title":"visual analyzer","uri":"https://rmoff.net/tag/visual-analyzer/"},{"categories":null,"content":"","keywords":null,"title":"visual analyzer","uri":"https://rmoff.net/categories/visual-analyzer/"},{"categories":null,"content":"","keywords":null,"title":"bar","uri":"https://rmoff.net/tag/bar/"},{"categories":null,"content":"","keywords":null,"title":"bar","uri":"https://rmoff.net/categories/bar/"},{"categories":null,"content":"","keywords":null,"title":"formatting","uri":"https://rmoff.net/tag/formatting/"},{"categories":null,"content":"","keywords":null,"title":"formatting","uri":"https://rmoff.net/categories/formatting/"},{"categories":["timelion","kibana","offset","formatting","bar","line"],"content":"I wrote recently about Kibana‚Äôs excellent Timelion feature, which brings time-series visualisations to Kibana. In the comments Ben Huang asked:\ndo you know how to show whats the difference between this Friday and last Friday by Timelion?\nSo I thought I‚Äôd answer properly here.\nTimelion includes mathematical functions including add and subtract, as well as the ability to show data offset by an amount of time. So to answer Ben‚Äôs query, we combine the two.","keywords":null,"title":"Kibana Timelion - Series Calculations - Difference from One Week Ago","uri":"https://rmoff.net/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/"},{"categories":null,"content":"","keywords":null,"title":"line","uri":"https://rmoff.net/tag/line/"},{"categories":null,"content":"","keywords":null,"title":"line","uri":"https://rmoff.net/categories/line/"},{"categories":null,"content":"","keywords":null,"title":"offset","uri":"https://rmoff.net/tag/offset/"},{"categories":null,"content":"","keywords":null,"title":"offset","uri":"https://rmoff.net/categories/offset/"},{"categories":null,"content":"","keywords":null,"title":"boot.properties","uri":"https://rmoff.net/tag/boot.properties/"},{"categories":null,"content":"","keywords":null,"title":"boot.properties","uri":"https://rmoff.net/categories/boot.properties/"},{"categories":null,"content":"","keywords":null,"title":"hang","uri":"https://rmoff.net/tag/hang/"},{"categories":null,"content":"","keywords":null,"title":"hang","uri":"https://rmoff.net/categories/hang/"},{"categories":["obiee","obiee12c","hang","boot.properties","start.cmd"],"content":"Running the OBIEE 12c startup on Windows:\nC:\\app\\oracle\\fmw\\user_projects\\domains\\bi\\bitools\\bin\\start.cmd Just hangs at:\nStarting AdminServer ... No CPU being consumed, very odd. But then ‚Ä¶ looking at DOMAIN_HOME\\servers\\AdminServer\\logs\\AdminServer.out shows the last log entry was:\nEnter username to boot WebLogic server: And that‚Äôs bad news, cos that‚Äôs an interactive prompt, but not echo‚Äôd to the console output of the startup command, and there‚Äôs no way to interact with it.\nThe start.cmd was being called by adding it to the Startup folder (C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\StartUp), and I guess it was something about this that stopped the prompt coming back to the console, because when I ran it manually from the command prompt, I got this:","keywords":null,"title":"OBIEE 12c hangs at startup - Starting AdminServer ‚Ä¶","uri":"https://rmoff.net/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../"},{"categories":null,"content":"","keywords":null,"title":"start.cmd","uri":"https://rmoff.net/tag/start.cmd/"},{"categories":null,"content":"","keywords":null,"title":"start.cmd","uri":"https://rmoff.net/categories/start.cmd/"},{"categories":["obiee","obiee12c","bar"],"content":"Another quick note on OBIEE 12c, this time on the importServiceInstance command. If you run it with a BAR file that doesn‚Äôt exist, it‚Äôll fail (obviously), but the error at the end of the stack trace is slightly confusing:\noracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release. Scrolling back up the stack trace does show the error message:\nSEVERE: Failed in reading bar file.","keywords":null,"title":"oracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.","uri":"https://rmoff.net/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./"},{"categories":null,"content":"","keywords":null,"title":"baseline validation tool","uri":"https://rmoff.net/tag/baseline-validation-tool/"},{"categories":null,"content":"","keywords":null,"title":"baseline validation tool","uri":"https://rmoff.net/categories/baseline-validation-tool/"},{"categories":null,"content":"","keywords":null,"title":"bvt","uri":"https://rmoff.net/tag/bvt/"},{"categories":null,"content":"","keywords":null,"title":"bvt","uri":"https://rmoff.net/categories/bvt/"},{"categories":["obiee","bvt","regression testing","baseline validation tool","obiee12c"],"content":"Interesting quirk in running Baseline Validation Tool for OBIEE here. If you invoke obibvt from the bin folder, it errors with Parameter ‚Äòdirectory‚Äô is not a directory\nPS C:\\OracleBI-BVT\u003e cd bin PS C:\\OracleBI-BVT\\bin\u003e .\\obibvt -config C:\\OracleBI-BVT\\bin\\bvt-config.xml -deployment current INFO: Result folder: Results\\current Throwable: Parameter 'directory' is not a directory Thread[main,5,main] SEVERE: Unhandled Exception SEVERE: java.lang.IllegalArgumentException: Parameter 'directory' is not a directory at org.apache.commons.io.FileUtils.validateListFilesParameters(FileUtils.java:545) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:521) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:691) at com.oracle.biee.bvt.UpgradeTool.loadPlugins(UpgradeTool.java:537) at com.","keywords":null,"title":"OBIEE Baseline Validation Tool - Parameter ‚Äòdirectory‚Äô is not a directory","uri":"https://rmoff.net/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/"},{"categories":null,"content":"","keywords":null,"title":"regression testing","uri":"https://rmoff.net/tag/regression-testing/"},{"categories":null,"content":"","keywords":null,"title":"regression testing","uri":"https://rmoff.net/categories/regression-testing/"},{"categories":null,"content":"","keywords":null,"title":"ingest","uri":"https://rmoff.net/tag/ingest/"},{"categories":null,"content":"","keywords":null,"title":"ingest","uri":"https://rmoff.net/categories/ingest/"},{"categories":null,"content":"","keywords":null,"title":"logstash","uri":"https://rmoff.net/tag/logstash/"},{"categories":null,"content":"","keywords":null,"title":"logstash","uri":"https://rmoff.net/categories/logstash/"},{"categories":null,"content":"","keywords":null,"title":"monitoring","uri":"https://rmoff.net/tag/monitoring/"},{"categories":["logstash","timelion","kibana","elasticsearch","monitoring","ingest"],"content":"Yesterday I wrote about Monitoring Logstash Ingest Rates with InfluxDB and Grafana, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.\nMark Walkom reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it‚Äôll integrate back into the Elastic stack:\n@rmoff nice, LS 5.0 will have APIs exposing metrics too. they‚Äôll be integrated back into Marvel/Monitoring!","keywords":null,"title":"Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion","uri":"https://rmoff.net/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/"},{"categories":null,"content":"","keywords":null,"title":"graphite","uri":"https://rmoff.net/tag/graphite/"},{"categories":null,"content":"","keywords":null,"title":"graphite","uri":"https://rmoff.net/categories/graphite/"},{"categories":null,"content":"","keywords":null,"title":"influxdb","uri":"https://rmoff.net/tag/influxdb/"},{"categories":["influxdb","grafana","logstash","graphite","monitoring","ingest"],"content":"In this article I‚Äôm going to show you how to easily monitor the rate at which Logstash is ingesting data, as well as in future articles the rate at which Elasticsearch is indexing it. It‚Äôs a nice little touch to add to any project involving Logstash, and it‚Äôs easy to do.\nLogstash is powerful tool for data ingest, processing, and distribution. It originated as simply the pipe to slurp at log files and put them into Elasticsearch, but has evolved into a whole bunch more.","keywords":null,"title":"Monitoring Logstash Ingest Rates with InfluxDB and Grafana","uri":"https://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/"},{"categories":null,"content":"","keywords":null,"title":"abstract","uri":"https://rmoff.net/tag/abstract/"},{"categories":null,"content":"","keywords":null,"title":"abstract","uri":"https://rmoff.net/categories/abstract/"},{"categories":["conferences","speaking","user groups","abstract"],"content":"Here‚Äôs a collection of useful articles that I‚Äôve found over the years that give good advice on writing a good abstract, mistakes to avoid, etc:\nAdam Machanic - Capturing Attention: Writing Great Session Descriptions Gwen Shapira - Concrete Advice for Abstract Writers Kellyn Pot‚ÄôVin-Gorman - Abstracts, Reviews and Conferences, Oh My! Russ Unger - Conference Proposals that Don‚Äôt Suck Martin Widlake - Tips on Submitting an Abstract to Conference Bridget Kromhout - give actionable takeaways (post photo courtesy of Calum MacAulay on https://unsplash.","keywords":null,"title":"Collection of Articles on How to Write a Good Conference Abstract","uri":"https://rmoff.net/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/"},{"categories":null,"content":"","keywords":null,"title":"user groups","uri":"https://rmoff.net/tag/user-groups/"},{"categories":null,"content":"","keywords":null,"title":"user groups","uri":"https://rmoff.net/categories/user-groups/"},{"categories":null,"content":"","keywords":null,"title":"dft","uri":"https://rmoff.net/tag/dft/"},{"categories":null,"content":"","keywords":null,"title":"dft","uri":"https://rmoff.net/categories/dft/"},{"categories":null,"content":"","keywords":null,"title":"dplyr","uri":"https://rmoff.net/tag/dplyr/"},{"categories":null,"content":"","keywords":null,"title":"dplyr","uri":"https://rmoff.net/categories/dplyr/"},{"categories":null,"content":"","keywords":null,"title":"elastic","uri":"https://rmoff.net/tag/elastic/"},{"categories":null,"content":"","keywords":null,"title":"elastic","uri":"https://rmoff.net/categories/elastic/"},{"categories":null,"content":"","keywords":null,"title":"lubridate","uri":"https://rmoff.net/tag/lubridate/"},{"categories":null,"content":"","keywords":null,"title":"lubridate","uri":"https://rmoff.net/categories/lubridate/"},{"categories":null,"content":"","keywords":null,"title":"R","uri":"https://rmoff.net/tag/r/"},{"categories":null,"content":"","keywords":null,"title":"R","uri":"https://rmoff.net/categories/r/"},{"categories":["R","dft","kibana","elasticsearch","dplyr","lubridate","wrangling","elastic"],"content":"Kibana is a tool from Elastic that makes analysis of data held in Elasticsearch really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a blog post last year, taking a CSV file and loading it into Elasticsearch via Logstash.","keywords":null,"title":"Using R to Denormalise Data for Analysis in Kibana","uri":"https://rmoff.net/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/"},{"categories":null,"content":"","keywords":null,"title":"wrangling","uri":"https://rmoff.net/tag/wrangling/"},{"categories":null,"content":"","keywords":null,"title":"wrangling","uri":"https://rmoff.net/categories/wrangling/"},{"categories":["obiee"],"content":"Two vulns for OBIEE in the latest critical patch update (CPU): http://www.oracle.com/technetwork/security-advisory/cpuapr2016v3-2985753.html?elq_mid=45463\u0026sh=91225181314122121267715271910\u0026cmid=WWMK10067711MPP001C140\nPatches is bundle patch .160419:\n12.2.1: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026sourceId=2102148.1\u0026patchId=22734181 11.1.1.9: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026sourceId=2102148.1\u0026patchId=22393988 11.1.1.7: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026sourceId=2102148.1\u0026patchId=22225110 Note that April 2016 is the last regular patchset for 11.1.1.7, ref: https://support.oracle.com/epmos/faces/DocumentDisplay?id=2102148.1#mozTocId410847. If you‚Äôre still on it, or earlier, time to upgrade!\n(Photo credit: https://unsplash.com/@jenlittlebirdie)","keywords":null,"title":"OBIEE security patches, and FINAL 11.1.1.7 patchset release","uri":"https://rmoff.net/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/"},{"categories":["elasticsearch","goldengate","Apache Kafka","logstash","oracle"],"content":"Recently added to the oracledi project over at java.net is an adaptor enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently over at the Elastic blog.\nElasticsearch is a ‚Äòdocument store‚Äô widely used for both search and analytics. It‚Äôs something I‚Äôve written a lot about (here and here for archives), as well as spoken about - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with.","keywords":null,"title":"Streaming Data through Oracle GoldenGate to Elasticsearch","uri":"https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/"},{"categories":["apache kafka","Apache Kafka","logstash","elastic","elasticsearch","kibana","elastic v5","zookeeper"],"content":"I‚Äôve recently been playing around with the ELK stack (now officially known as the Elastic stack) collecting data from an IRC channel with Elastic‚Äôs Logstash, storing it in Elasticsearch and analysing it with Kibana. But, this isn‚Äôt an ‚ÄúELK‚Äù post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.\nAs I wrote about last year, Apache Kafka provides a handy way to build flexible ‚Äúpipelines‚Äù.","keywords":null,"title":"Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example","uri":"https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/"},{"categories":null,"content":"","keywords":null,"title":"elastic v5","uri":"https://rmoff.net/categories/elastic-v5/"},{"categories":null,"content":"","keywords":null,"title":"zookeeper","uri":"https://rmoff.net/categories/zookeeper/"},{"categories":null,"content":"","keywords":null,"title":"beer","uri":"https://rmoff.net/tag/beer/"},{"categories":null,"content":"","keywords":null,"title":"beer","uri":"https://rmoff.net/categories/beer/"},{"categories":null,"content":"","keywords":null,"title":"devon","uri":"https://rmoff.net/tag/devon/"},{"categories":null,"content":"","keywords":null,"title":"devon","uri":"https://rmoff.net/categories/devon/"},{"categories":null,"content":"","keywords":null,"title":"dorset","uri":"https://rmoff.net/tag/dorset/"},{"categories":null,"content":"","keywords":null,"title":"dorset","uri":"https://rmoff.net/categories/dorset/"},{"categories":["lardy cake","dorset","devon","gyle 59","palmers","beer","fryup","fullenglish"],"content":"On a family holiday in South Devon last week I had some good food experiences. Top of the pile was a Lardy Cake, a delicacy new to me but which I‚Äôll be sure to be searching out again. It reminded me of an Eccles cake, but bigger and lardier!\nI got mine from Vinnicombes on the High Street in Sidmouth.\nJust down the coast from Sidmouth is a village called Beer.","keywords":null,"title":"Food Pr0n 02 - Devon \u0026 Dorset","uri":"https://rmoff.net/2016/04/11/food-pr0n-02-devon-dorset/"},{"categories":null,"content":"","keywords":null,"title":"gyle 59","uri":"https://rmoff.net/tag/gyle-59/"},{"categories":null,"content":"","keywords":null,"title":"gyle 59","uri":"https://rmoff.net/categories/gyle-59/"},{"categories":null,"content":"","keywords":null,"title":"lardy cake","uri":"https://rmoff.net/tag/lardy-cake/"},{"categories":null,"content":"","keywords":null,"title":"lardy cake","uri":"https://rmoff.net/categories/lardy-cake/"},{"categories":null,"content":"","keywords":null,"title":"palmers","uri":"https://rmoff.net/tag/palmers/"},{"categories":null,"content":"","keywords":null,"title":"palmers","uri":"https://rmoff.net/categories/palmers/"},{"categories":["kibana","timelion","quandl","topbeat"],"content":"Timelion was released in November 2015 and with the 4.4.2 release of Kibana is available as a native visualisation once installed. It adds some powerful capabilities to Kibana as an timeseries analysis tool, using its own data manipulation language.\nInstalling Timelion is a piece of cake:\n./bin/kibana plugin -i kibana/timelion After restarting Kibana, you‚Äôll see it as an option from the application picker\nThere‚Äôs a bit of a learning curve with Timelion, but it‚Äôs worth it.","keywords":null,"title":"Experiments with Kibana Timelion","uri":"https://rmoff.net/2016/03/29/experiments-with-kibana-timelion/"},{"categories":null,"content":"","keywords":null,"title":"quandl","uri":"https://rmoff.net/tag/quandl/"},{"categories":null,"content":"","keywords":null,"title":"quandl","uri":"https://rmoff.net/categories/quandl/"},{"categories":null,"content":"","keywords":null,"title":"topbeat","uri":"https://rmoff.net/tag/topbeat/"},{"categories":null,"content":"","keywords":null,"title":"topbeat","uri":"https://rmoff.net/categories/topbeat/"},{"categories":["obiee","jdbc","jisql","logical sql"],"content":"OBIEE supports JDBC as a connection protocol, using the driver available on all installations of OBIEE, bijdbc.jar. This makes connecting to OBIEE from custom or third-party applications very easy. Once connected, you issue ‚ÄúLogical SQL‚Äù against the ‚Äútables‚Äù of the Presentation Layer. An example of logical SQL is:\nSELECT \"Time\".\"T05 Per Name Year\" saw_0 FROM \"A - Sample Sales\" To find more Logical SQL simply inspect your nqquery.log (obis-query.log in 12c), or Usage Tracking.","keywords":null,"title":"Connecting to OBIEE via JDBC - with jisql","uri":"https://rmoff.net/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/"},{"categories":null,"content":"","keywords":null,"title":"jdbc","uri":"https://rmoff.net/tag/jdbc/"},{"categories":null,"content":"","keywords":null,"title":"jisql","uri":"https://rmoff.net/tag/jisql/"},{"categories":null,"content":"","keywords":null,"title":"jisql","uri":"https://rmoff.net/categories/jisql/"},{"categories":null,"content":"","keywords":null,"title":"logical sql","uri":"https://rmoff.net/tag/logical-sql/"},{"categories":null,"content":"","keywords":null,"title":"logical sql","uri":"https://rmoff.net/categories/logical-sql/"},{"categories":null,"content":"","keywords":null,"title":"irc","uri":"https://rmoff.net/tag/irc/"},{"categories":null,"content":"","keywords":null,"title":"irc","uri":"https://rmoff.net/categories/irc/"},{"categories":["logstash","kibana","elasticsearch","irc","obihackers"],"content":"OK, maybe that‚Äôs not entirely true. But my read-only client, certainly.\nI was perusing the Logstash input plugins recently when I noticed that there was one for IRC. Being a fan of IRC and a regular on the #obihackers channel, I thought this could be fun and yet another great example of how easy the Elastic stack is to work with.\nInstallation is a piece of cake:\nwget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip wget https://download.","keywords":null,"title":"My latest IRC client : Kibana","uri":"https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/"},{"categories":null,"content":"","keywords":null,"title":"obihackers","uri":"https://rmoff.net/tag/obihackers/"},{"categories":null,"content":"","keywords":null,"title":"obihackers","uri":"https://rmoff.net/categories/obihackers/"},{"categories":null,"content":"","keywords":null,"title":"burger","uri":"https://rmoff.net/tag/burger/"},{"categories":null,"content":"","keywords":null,"title":"burger","uri":"https://rmoff.net/categories/burger/"},{"categories":null,"content":"","keywords":null,"title":"cubano","uri":"https://rmoff.net/tag/cubano/"},{"categories":null,"content":"","keywords":null,"title":"cubano","uri":"https://rmoff.net/categories/cubano/"},{"categories":null,"content":"","keywords":null,"title":"food","uri":"https://rmoff.net/tag/food/"},{"categories":null,"content":"","keywords":null,"title":"food","uri":"https://rmoff.net/categories/food/"},{"categories":["food","fullenglish","hawksmoor","burger","indian","black pudding","suet pudding","cubano"],"content":"One of the perks of my job is that I get to travel to some pretty nice places (hi, San Francisco, Bergen, √Öland Islands), and get to eat some pretty good food too. If you‚Äôre looking for some techie content, then move along and go and read about Kafka, but if you enjoy food pr0n then stay put.\nI was working for a client in the centre of Manchester this week, staying as usual at a Premier Inn.","keywords":null,"title":"Food Pr0n - 01","uri":"https://rmoff.net/2016/03/19/food-pr0n-01/"},{"categories":null,"content":"","keywords":null,"title":"hawksmoor","uri":"https://rmoff.net/tag/hawksmoor/"},{"categories":null,"content":"","keywords":null,"title":"hawksmoor","uri":"https://rmoff.net/categories/hawksmoor/"},{"categories":null,"content":"","keywords":null,"title":"indian","uri":"https://rmoff.net/tag/indian/"},{"categories":null,"content":"","keywords":null,"title":"indian","uri":"https://rmoff.net/categories/indian/"},{"categories":null,"content":"","keywords":null,"title":"suet pudding","uri":"https://rmoff.net/tag/suet-pudding/"},{"categories":null,"content":"","keywords":null,"title":"suet pudding","uri":"https://rmoff.net/categories/suet-pudding/"},{"categories":null,"content":"","keywords":null,"title":"environment variables","uri":"https://rmoff.net/tag/environment-variables/"},{"categories":null,"content":"","keywords":null,"title":"environment variables","uri":"https://rmoff.net/categories/environment-variables/"},{"categories":null,"content":"","keywords":null,"title":"installation","uri":"https://rmoff.net/tag/installation/"},{"categories":null,"content":"","keywords":null,"title":"installation","uri":"https://rmoff.net/categories/installation/"},{"categories":null,"content":"","keywords":null,"title":"jdk","uri":"https://rmoff.net/tag/jdk/"},{"categories":null,"content":"","keywords":null,"title":"jps-06514","uri":"https://rmoff.net/tag/jps-06514/"},{"categories":null,"content":"","keywords":null,"title":"jps-06514","uri":"https://rmoff.net/categories/jps-06514/"},{"categories":null,"content":"","keywords":null,"title":"keystore","uri":"https://rmoff.net/tag/keystore/"},{"categories":null,"content":"","keywords":null,"title":"keystore","uri":"https://rmoff.net/categories/keystore/"},{"categories":["obiee","installation","jps-06514","jdk","environment variables","wls","keystore"],"content":"I got this lovely failure during a fresh install of OBIEE 11.1.1.9. I emphasise that it was during the install because there‚Äôs other causes for this error on an existing system to do with corrupted credential stores etc ‚Äì not the case here.\nThe install had copied in the binaries and was in the process of building the domain. During the early stages of this where it starts configuring and restarting the AdminServer it failed, with the AdminServer.","keywords":null,"title":"OBIEE 11.1.1.9 installation - JPS-06514: Opening of file based keystore failed","uri":"https://rmoff.net/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/"},{"categories":null,"content":"","keywords":null,"title":"wls","uri":"https://rmoff.net/tag/wls/"},{"categories":null,"content":"","keywords":null,"title":"wls","uri":"https://rmoff.net/categories/wls/"},{"categories":["logstash","Apache Kafka","goldengate","avro","elasticsearch"],"content":"The Oracle by Example (ObE) here demonstrating how to use Goldengate to replicate transactions big data targets such as HDFS is written for the BigDataLite 4.2.1, and for me didn‚Äôt work on the current latest version, 4.4.0.\nThe OBE (and similar Hands On Lab PDF) assume the presence of pmov.prm and pmov.properties in /u01/ogg/dirprm/. On BDL 4.4 there‚Äôs only the extract to from Oracle configuration, emov.\nFortunately it‚Äôs still possible to run this setup out of the box in BDL 4.","keywords":null,"title":"Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4","uri":"https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/"},{"categories":null,"content":"I‚Äôve always defaulted to Slideshare for hosting slides from presentations that I‚Äôve given, but it‚Äôs become more and more crap-infested. The UI is messy, and the UX sucks - for example, I want to download a slide deck, I most definitely 100% am not interested in ‚Äúclipping‚Äù it‚Ä¶even if you ask me every. damn. time:\nLooking around it seems the other popular option is Speakerdeck. The UI is clean and simple, and I as both a user and uploader I feel like I‚Äôm there to read and share slides rather than be monetised as an eyeball on the site.","keywords":null,"title":"Presentation Slides‚Ä¶ bye-bye Slideshare, hello Speakerdeck","uri":"https://rmoff.net/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/"},{"categories":null,"content":"#obihackers There‚Äôs a #obihackers IRC channel on freenode, where a dozen or so of us have hung out for several years now. Chat is usually OBIEE, Oracle, ODI, and general geek out.\nBear in mind this is the equivalent of us hanging out in a bar; if you wanna shoot the shit with a geeky question about OBIEE go ahead, but if you‚Äôve come to get help with your homework without even buying a round, you‚Äôll probably get short shrift‚Ä¶ ;-)","keywords":null,"title":"obihackers IRC channel","uri":"https://rmoff.net/2016/03/03/obihackers-irc-channel/"},{"categories":null,"content":"","keywords":null,"title":"awk","uri":"https://rmoff.net/tag/awk/"},{"categories":null,"content":"","keywords":null,"title":"awk","uri":"https://rmoff.net/categories/awk/"},{"categories":null,"content":"","keywords":null,"title":"du","uri":"https://rmoff.net/tag/du/"},{"categories":null,"content":"","keywords":null,"title":"du","uri":"https://rmoff.net/categories/du/"},{"categories":null,"content":"","keywords":null,"title":"metrics","uri":"https://rmoff.net/tag/metrics/"},{"categories":null,"content":"","keywords":null,"title":"metrics","uri":"https://rmoff.net/categories/metrics/"},{"categories":null,"content":"","keywords":null,"title":"sed","uri":"https://rmoff.net/tag/sed/"},{"categories":null,"content":"","keywords":null,"title":"sed","uri":"https://rmoff.net/categories/sed/"},{"categories":["influxdb","metrics","bash","awk","sed","du","curl","grafana"],"content":"InfluxDB is a great time series database, that‚Äôs recently been rebranded as part of the ‚ÄúTICK‚Äù stack, including data collectors, visualisation, and ETL/Alerting. I‚Äôve yet to really look at the other components, but InfluxDB alone works just great with my favourite visualisation/analysis tool for time series metrics, Grafana.\nGetting data into InfluxDB is easy, with many tools supporting the native InfluxDB line input protocol, and those that don‚Äôt often supporting the carbon protocol (from Graphite), which InfluxDB also supports (along with others).","keywords":null,"title":"Streaming data to InfluxDB from any bash command","uri":"https://rmoff.net/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/"},{"categories":["food","fullenglish"],"content":"Thanks to the power of twitter, I can look back on all the many and varied Full English breakfasts that I‚Äôve (mostly) enjoyed:\nüëâüèª https://twitter.com/search?q=rmoff%20%23fullenglish\u0026src=typd\nWhat makes a good Full English?\nGood ingredients, cooked well. Nothing worse than a limp pink sausage, as it were. Sausage, standard pork or Cumberland at most. Definitely no daft apricot and guava bean with a hint of foie gras nonsense. Must be cooked right well, crispy skin, almost burnt.","keywords":null,"title":"What makes a good Full English?","uri":"https://rmoff.net/2016/02/26/what-makes-a-good-full-english/"},{"categories":null,"content":"","keywords":null,"title":"dms","uri":"https://rmoff.net/tag/dms/"},{"categories":null,"content":"","keywords":null,"title":"dms","uri":"https://rmoff.net/categories/dms/"},{"categories":null,"content":"","keywords":null,"title":"jmanage","uri":"https://rmoff.net/tag/jmanage/"},{"categories":null,"content":"","keywords":null,"title":"jmanage","uri":"https://rmoff.net/categories/jmanage/"},{"categories":null,"content":"","keywords":null,"title":"jmx","uri":"https://rmoff.net/tag/jmx/"},{"categories":["obiee","dms","metrics","jmanage","jmx"],"content":"It struck me today when I was writing my most recent blog over at Rittman Mead that I‚Äôve been playing with visualising OBIEE metrics for years now.\nBack in 2009 I wrote about using something called JManage to pull metrics out of OBIEE 10g via JMX:\nStill with OBIEE 10g in 2011, I was using rrdtool and some horrible-looking tcl hacking to get the metrics out through jmx :\n2014 brought with it DMS and my first forays with Graphite for storing \u0026 visualising data:","keywords":null,"title":"Visualising OBIEE DMS Metrics over the years","uri":"https://rmoff.net/2016/02/26/visualising-obiee-dms-metrics-over-the-years/"},{"categories":["obiee"],"content":"I will now be blogging mostly over at the venerable blog of my employer, Rittman Mead.\nYou can see my first posting here: Web Services in BI Publisher 11g.\nDon‚Äôt entirely exclude this rnm1978 blog from your feeds, as I may still post more esoteric and random tidbits here.","keywords":null,"title":"Blogging","uri":"https://rmoff.net/2011/11/28/blogging/"},{"categories":null,"content":"","keywords":null,"title":"dbms_application_info","uri":"https://rmoff.net/categories/dbms_application_info/"},{"categories":null,"content":"","keywords":null,"title":"dbms_session","uri":"https://rmoff.net/categories/dbms_session/"},{"categories":["dbms_application_info","dbms_session","monitoring","obiee","performance","systemsmanagement"],"content":"This article has been superseded by a newer version:¬†Instrumenting OBIEE Database Connections For Improved Performance Diagnostics (Previously on this blog: 1, 2, 3‚Ä¶)\nSummary Instrument your code. Stop guessing. Make your DBA happy. Make your life as a BI Admin easier.\nThe Problem OBIEE will typically connect to the database using a generic application account. (Hopefully, you‚Äôll have isolated it to an account used only for this purpose - if you haven‚Äôt, you should.","keywords":null,"title":"Instrumenting OBIEE - the final chapter","uri":"https://rmoff.net/2011/10/10/instrumenting-obiee-the-final-chapter/"},{"categories":null,"content":"","keywords":null,"title":"performance","uri":"https://rmoff.net/categories/performance/"},{"categories":null,"content":"","keywords":null,"title":"systemsmanagement","uri":"https://rmoff.net/categories/systemsmanagement/"},{"categories":null,"content":"","keywords":null,"title":"inventory","uri":"https://rmoff.net/categories/inventory/"},{"categories":["inventory","obiee-11g","oui"],"content":"It‚Äôs not my fault really.\nWhen running an installation, presented with the option of\n(a) do a bunch of stuff and wait to continue the install later or (b) tick a box and continue now it‚Äôs a better man that I who would opt for option (a).\nWhen I recently installed OBIEE 11g, I was prompted to get a script run as root to set up the inventory, or tick ‚ÄúContinue Installation with local inventory‚Äù to continue with the install.","keywords":null,"title":"OBI 11g : UPGAST-00055: error reading the Oracle Universal Installer inventory","uri":"https://rmoff.net/2011/10/05/obi-11g-upgast-00055-error-reading-the-oracle-universal-installer-inventory/"},{"categories":null,"content":"","keywords":null,"title":"obiee-11g","uri":"https://rmoff.net/categories/obiee-11g/"},{"categories":null,"content":"","keywords":null,"title":"oui","uri":"https://rmoff.net/categories/oui/"},{"categories":["windows","xe-11gr2"],"content":"Short note to record this, as Google drew no hits on it.\nWindows XP machine with existing Oracle 11.1 client installation, all working fine.\nInstalled Oracle 11.2 XE, and started getting these errors: [sourcecode] C:\\Windows\\System32\u003etnsping DBNAME\nTNS Ping Utility for 32-bit Windows: Version 11.2.0.2.0 - Production on 26-SEP-2011 11:01:11\nCopyright (c) 1997, 2010, Oracle. All rights reserved.\nUsed parameter files: C:\\app\\userid\\product\\11.1.0\\client_1\\network\\admin\\sqlnet.ora\nUsed TNSNAMES adapter to resolve the alias Message 3513 not found; product=NETWORK; facility=TNS OK (20 msec) [/sourcecode]","keywords":null,"title":"Oracle - tnsping - Message 3513 not found;  product=NETWORK; facility=TNS","uri":"https://rmoff.net/2011/09/26/oracle-tnsping-message-3513-not-found-productnetwork-facilitytns/"},{"categories":["wordpress"],"content":"I noticed in Ed Stevens‚Äô blog posting here that some sourcecode he‚Äôd posted had certain lines highlighted.\nWordpress provides the sourcecode tag for marking up sourcecode in blog posts. For example:\ncd /some/random/folder ls -l # do not run this next line!\nis much better presented as: [sourcecode language=‚Äúbash‚Äù] cd /some/random/folder ls -l # do not run this next line! [/sourcecode] by wrapping it in [sourcecode] tags\nI‚Äôve known about the language=‚Äòxx‚Äô attribute that you can use with the tag, but Ed‚Äôs posting prompted me to check on the syntax and it turns out there a few tweaks one can use.","keywords":null,"title":"Sourcecode markup tweaks in Wordpress","uri":"https://rmoff.net/2011/09/26/sourcecode-markup-tweaks-in-wordpress/"},{"categories":null,"content":"","keywords":null,"title":"windows","uri":"https://rmoff.net/categories/windows/"},{"categories":null,"content":"","keywords":null,"title":"xe-11gr2","uri":"https://rmoff.net/categories/xe-11gr2/"},{"categories":["obiee"],"content":" If you only read one blog post this month, read James Morle‚Äôs eloquent attack on the term ‚ÄúBest Practice‚Äù. I‚Äôm very excited to be joining RittmanMead next month! I‚Äôm looking forward to working with some of the industry‚Äôs most respected experts. ","keywords":null,"title":"Friday miscellany","uri":"https://rmoff.net/2011/09/16/friday-miscellany/"},{"categories":null,"content":"","keywords":null,"title":"dbms_stats","uri":"https://rmoff.net/categories/dbms_stats/"},{"categories":["dbms_stats","oracle"],"content":"In Oracle 11g, the DBMS_STATS procedure GATHER_SCHEMA_STATS takes a parameter ‚Äòoptions‚Äô which defines the scope of the objects processed by the procedure call, as well as the action. It can be either GATHER or LIST (gather the stats, or list out the objects to be touched, respectively), and AUTO, STALE or EMPTY (defining the object selection to process).\nGATHER on its own will gather stats on all objects in the schema GATHER EMPTY / LIST EMPTY is self-explanatory - objects with no statistics.","keywords":null,"title":"DBMS_STATS - GATHER AUTO","uri":"https://rmoff.net/2011/09/13/dbms_stats-gather-auto/"},{"categories":null,"content":"","keywords":null,"title":"linux","uri":"https://rmoff.net/categories/linux/"},{"categories":["fedora","linux"],"content":"When using preupgrade to upgrade an existing Fedora 14 installation to Fedora 15, the following two errors were encountered:\nFailed to fetch release info No groups available in any repository The box sits on a network behind a proxy out to the web.\nThe resolution was to make sure that environment variables http_proxy and https_proxy are set: [sourcecode language=‚Äúbash‚Äù] export http_proxy=http://user:password@proxyserver:port export https_proxy=http://user:password@proxyserver:port [/sourcecode] Make sure you do this from the user from which you run preupgrade.","keywords":null,"title":"Using preupgrade to upgrade Fedora 14 to Fedora 15 - proxy errors","uri":"https://rmoff.net/2011/09/12/using-preupgrade-to-upgrade-fedora-14-to-fedora-15-proxy-errors/"},{"categories":null,"content":"","keywords":null,"title":"excel","uri":"https://rmoff.net/categories/excel/"},{"categories":null,"content":"","keywords":null,"title":"hack","uri":"https://rmoff.net/categories/hack/"},{"categories":["excel","hack","visualisation"],"content":"Excel may send chills down the spine of us when we hear users talking about its [ab]use, but it has its place in the toolset. For my money, it is a very good tool for knocking out graphs which look decent. Of course, rrdtool is my geek tool of choice for dynamic long-term graphing, but when doing scratch PoC work, I normally fall back to Excel.\nOne thing which has frustrated me over time is, well, time, and Excel‚Äôs handling thereof.","keywords":null,"title":"Labelling Time axes in Excel","uri":"https://rmoff.net/2011/09/08/labelling-time-axes-in-excel/"},{"categories":null,"content":"","keywords":null,"title":"visualisation","uri":"https://rmoff.net/categories/visualisation/"},{"categories":["performance","quotation"],"content":"Here‚Äôs a quotation that I‚Äôve just read and wanted to share. It is all part of a BAAG approach to troubleshooting problems, performance in particular.\nFrom Greg Rahn (web|twitter) on oracle-l:\nThere are always exceptions, but exceptions can be justified and supported with data. Just beware of the the silver bullet syndrome‚Ä¶\nThe unfortunate part [‚Ä¶] is that rarely anyone goes back and does the root cause analysis. It tends to fall into the bucket of ‚Äúproblem‚Ä¶solved‚Äù.","keywords":null,"title":"A quotation to print out and stick on your wall","uri":"https://rmoff.net/2011/08/18/a-quotation-to-print-out-and-stick-on-your-wall/"},{"categories":null,"content":"","keywords":null,"title":"quotation","uri":"https://rmoff.net/categories/quotation/"},{"categories":null,"content":"","keywords":null,"title":"bi","uri":"https://rmoff.net/categories/bi/"},{"categories":["bi","obiee"],"content":"Blogging from Oracle itself about OBIEE has always been a bit sparse, certainly in comparison to that which there is for core RDBMS.\nIt‚Äôs good to see a new blog emerge in the last couple of months from OBI Product Assurance, including some nice ‚Äôn spicy detailed config/tuning info.\nFind it here: http://blogs.oracle.com/pa/.\nThere‚Äôs a couple more OBI blogs from Oracle, but both are fairly stale:\nImplementing Oracle BI \u0026 EPM Solutions - Rob Reynolds OBIEE Ramblings Oracle BI Foundation (thanks Daan for pointing this one out) ","keywords":null,"title":"New blog from Oracle - OBI Product Assurance","uri":"https://rmoff.net/2011/08/15/new-blog-from-oracle-obi-product-assurance/"},{"categories":["hp","itanium","obiee"],"content":"OK, a bit tired on a Monday morning, and so a bit sarcastic.\nI‚Äôve not really fallen off my chair, but I am shocked. I honestly didn‚Äôt think it would happen.\nOracle have finally released OBI 11g for HP-UX Itanium:\nIn other news, patchset 10.1.3.4.2 for OBI 10g was released today, I wonder if/when we‚Äôll get an HP-UX Itanium version? The download page has it conspicuous by its absence even from ‚ÄúComing Soon‚Äù: ","keywords":null,"title":"Did you hear that thunk? That was me falling off my chair in shock","uri":"https://rmoff.net/2011/08/08/did-you-hear-that-thunk-that-was-me-falling-off-my-chair-in-shock/"},{"categories":["obiee","oracle"],"content":"Have you defined CLIENT_ID in your OBIEE RPD yet? You really ought to.\nAs well as helping track down users of troublesome queries, it also tags dump files with the OBIEE user of an offending query should the worst occur: For details, see:\nIdentify your OBIEE users by setting Client ID in Oracle connection Instrumenting OBIEE for tracing Oracle DB calls ","keywords":null,"title":"Have you defined CLIENT_ID in OBIEE yet?","uri":"https://rmoff.net/2011/08/08/have-you-defined-client_id-in-obiee-yet/"},{"categories":null,"content":"","keywords":null,"title":"hp","uri":"https://rmoff.net/categories/hp/"},{"categories":null,"content":"","keywords":null,"title":"itanium","uri":"https://rmoff.net/categories/itanium/"},{"categories":["obiee"],"content":"A new version of OBI 10g (remember that?) has just been released, the Oracle twitter machine announced: Along with presumably a bunch of bugfixes, the release notes list new functionality in catalog manager: Download 10.1.3.4.2 from here","keywords":null,"title":"OBIEE 10.1.3.4.2 released","uri":"https://rmoff.net/2011/08/08/obiee-10.1.3.4.2-released/"},{"categories":null,"content":"","keywords":null,"title":"bug","uri":"https://rmoff.net/categories/bug/"},{"categories":null,"content":"","keywords":null,"title":"odi","uri":"https://rmoff.net/categories/odi/"},{"categories":["odi"],"content":"Trying to connect to a repository in ODI using OCI. Target database is Oracle 11.1.0.7. Throws this error: [sourcecode] com.sunopsis.sql.l: Oracle Data Integrator Timeout: connection with URL jdbc:oracle:oci8:@ODIPRD and user ODI_USER. at com.sunopsis.sql.SnpsConnection.a(SnpsConnection.java) at com.sunopsis.sql.SnpsConnection.t(SnpsConnection.java) at com.sunopsis.sql.SnpsConnection.connect(SnpsConnection.java) at com.sunopsis.tools.connection.DwgRepositoryConnectionsCreator.a(DwgRepositoryConnectionsCreator.java) at com.sunopsis.tools.connection.DwgRepositoryConnectionsCreator.a(DwgRepositoryConnectionsCreator.java) at com.sunopsis.graphical.l.oi.a(oi.java) [‚Ä¶] [/sourcecode]\nNormally this error would be caused by a misconfigured Oracle client. For example, a missing or incorrect tnsnames.ora entry. I validated these and got a successful response using tnsping.","keywords":null,"title":"ODI 10g connectivity problem with OCI","uri":"https://rmoff.net/2011/08/04/odi-10g-connectivity-problem-with-oci/"},{"categories":null,"content":"","keywords":null,"title":"security","uri":"https://rmoff.net/categories/security/"},{"categories":["bug","obiee","security"],"content":"July‚Äôs Critical Patch Update from Oracle includes CVE-2011-2241, which affects OBIEE versions 10.1.3.4.1 and 11.1.1.3. No details of the exploit other than it ‚Äúallows remote attackers to affect availability via unknown vectors related to Analytics Server.‚Äù\nIt is categorised with a CVSS score of 5 (on a scale of 10), with no impact on Authentication, Confidentiality, or Integrity, and ‚ÄúPartial+‚Äù impact on Availability. So to a security-unqualified layman (me), it sounds like someone could remotely crash your NQSServer process, but not do any more damage than that.","keywords":null,"title":"Security issue on OBIEE 10.1.3.4.1, 11.1.1.3","uri":"https://rmoff.net/2011/08/04/security-issue-on-obiee-10.1.3.4.1-11.1.1.3/"},{"categories":null,"content":"","keywords":null,"title":"nqcmd","uri":"https://rmoff.net/categories/nqcmd/"},{"categories":["documentation","hack","nqcmd","obiee"],"content":"I noticed on Nico‚Äôs wiki (which is amazing by the way, it has so much information in it) a bunch of additional parameters for nqcmd other than those which are displayed in the default helptext (nqcmd -h).\nThese are the additional ones: [sourcecode] -b -w\u003c# wait seconds\u003e -c\u003c# cancel interval seconds\u003e -n\u003c# number of loops\u003e -r\u003c# number of requests per shared session\u003e -t\u003c# number of threads\u003e -T (a flag to turn on time statistics) -SmartDiff (a flag to enable SmartDiff tags in output) -P -impersonate -runas [/sourcecode]","keywords":null,"title":"Undocumented nqcmd parameters","uri":"https://rmoff.net/2011/07/13/undocumented-nqcmd-parameters/"},{"categories":["etl","oracle","performance","plan-management","sql-plan-baseline"],"content":"Here‚Äôs a scenario that‚Äôll be depressingly familiar to most reading this: after ages of running fine, and no changes to the code, a query suddenly starts running for magnitudes longer than it used to.\nIn this instance it was an ETL step which used to take c.1 hour, and was now at 5 hours and counting. Since it still hadn‚Äôt finished, and the gods had conspired to bring down Grid too (unrelated), I generated a SQL Monitor report to see what was happening: [sourcecode language=‚Äúsql‚Äù] select DBMS_SQLTUNE.","keywords":null,"title":"Oracle 11g - How to force a sql_id to use a plan_hash_value using SQL Baselines","uri":"https://rmoff.net/2011/06/28/oracle-11g-how-to-force-a-sql_id-to-use-a-plan_hash_value-using-sql-baselines/"},{"categories":null,"content":"","keywords":null,"title":"plan-management","uri":"https://rmoff.net/categories/plan-management/"},{"categories":null,"content":"","keywords":null,"title":"sql-plan-baseline","uri":"https://rmoff.net/categories/sql-plan-baseline/"},{"categories":null,"content":"","keywords":null,"title":"copy_table_stats","uri":"https://rmoff.net/categories/copy_table_stats/"},{"categories":null,"content":"","keywords":null,"title":"dwh","uri":"https://rmoff.net/categories/dwh/"},{"categories":["copy_table_stats","dbms_stats","dwh","oracle","statistics"],"content":"There is a well-documented problem relating to DBMS_STATS.COPY_TABLE_STATS between partitions where high/low values of the partitioning key column were just copied verbatim from the source partition. This particular problem has now been patched (see 8318020.8). For background, see Doug Burns‚Äô blog and his excellent paper which covers the whole topic of statistics on partitioned tables.\nThis post Maintaining statistics on large partitioned tables on the Oracle Optimizer blog details what the dbms_stats.","keywords":null,"title":"Global statistics high/low values when using DBMS_STATS.COPY_TABLE_STATS","uri":"https://rmoff.net/2011/06/15/global-statistics-high/low-values-when-using-dbms_stats.copy_table_stats/"},{"categories":null,"content":"","keywords":null,"title":"statistics","uri":"https://rmoff.net/categories/statistics/"},{"categories":["dbms_stats","dwh","oracle","statistics"],"content":"Chucking a stick in the spokes of your carefully-tested ETL/BI ‚Ä¶ My opinion is that automated stats gathering for non-system objects should be disabled on Oracle Data Warehouses across all environments.\nAll it does it cover up poor design or implementation which has omitted to consider statistics management. Once you get into the realms of millions or billions of rows of data, the automated housekeeping may well not have time to stat all of your tables on each run.","keywords":null,"title":"Data Warehousing and Statistics in Oracle 11g - Automatic Optimizer Statistics Collection","uri":"https://rmoff.net/2011/05/26/data-warehousing-and-statistics-in-oracle-11g-automatic-optimizer-statistics-collection/"},{"categories":["bi","dwh","obiee","oracle","performance","rant","rrdtool"],"content":"Just because something produces the correct numbers on the report, it doesn‚Äôt mean you can stop there.\nHow you are producing those numbers matters, and matters a lot if you have an interest in the long-term health of your system and its ability to scale.\nOBIEE is the case in point here, but the principle applies to any architecture with \u003e1 tiers or components.\nLet me start with a rhetorical question.","keywords":null,"title":"OBIEE performance - get your database sweating","uri":"https://rmoff.net/2011/05/19/obiee-performance-get-your-database-sweating/"},{"categories":null,"content":"","keywords":null,"title":"rant","uri":"https://rmoff.net/categories/rant/"},{"categories":null,"content":"","keywords":null,"title":"rrdtool","uri":"https://rmoff.net/categories/rrdtool/"},{"categories":["dwh","exadata","hp","oracle"],"content":"Chris Mellor at The Register posted an interesting article a couple of days ago, entitled HP and Violin build Oracle Exadata killer. The slidedeck has been removed from HP‚Äôs FTP site, but a bit of Google magic throws up a couple of mirror copies.\nIt‚Äôs an entertaining read (‚ÄúDo a Proof of Concept! 94% win rate!! We can and do win against Exadata!!‚Äù), and a nice illustration of the FUD techniques that companies use in marketing their products against others.","keywords":null,"title":"Entertaining Exadata FUD from HP","uri":"https://rmoff.net/2011/04/11/entertaining-exadata-fud-from-hp/"},{"categories":null,"content":"","keywords":null,"title":"exadata","uri":"https://rmoff.net/categories/exadata/"},{"categories":null,"content":"","keywords":null,"title":"kindle","uri":"https://rmoff.net/categories/kindle/"},{"categories":["documentation","kindle","oracle"],"content":"Whilst perusing the Oracle database documentation, I noticed something which caught my eye:\nAs well as reading the documentation online as HTML or downloading as PDF for viewing on your computer etc, you can also download it in formats (Mobi and ePub) designed for eReaders such as the Kindle and iPad (the latter obviously isn‚Äôt ‚Äújust‚Äù an eReader). For information on format support, there‚Äôs a handy table on Wikipedia.\nIt looks like the availability of mobi/epub files isn‚Äôt universal.","keywords":null,"title":"Oracle documentation - available on Kindle and iPad","uri":"https://rmoff.net/2011/04/07/oracle-documentation-available-on-kindle-and-ipad/"},{"categories":["oracle","windows","xe","xe-11gr2"],"content":"Oracle XE 11gR2 beta has just been released, some details here and download here. It‚Äôs not a great deal of use for sandboxing DWH-specific stuff, given this list of excluded functionality (and this is by no means everything that‚Äôs not included):\nBitmapped index, bitmapped join index, and bitmap plan conversions Oracle Partitioning Parallel Data Pump Export/Import Parallel query/DML Parallel Statement Queuing (source)\nHowever, it‚Äôs always interesting to have to hand for trying out other things.","keywords":null,"title":"Oracle XE 11gR2 installation - ‚ÄúOracleXEService should not be installed already‚Äù","uri":"https://rmoff.net/2011/04/04/oracle-xe-11gr2-installation-oraclexeservice-should-not-be-installed-already/"},{"categories":null,"content":"","keywords":null,"title":"xe","uri":"https://rmoff.net/categories/xe/"},{"categories":null,"content":"","keywords":null,"title":"awr","uri":"https://rmoff.net/categories/awr/"},{"categories":["awr","io","oracle","performance","visualisation"],"content":"This post expands on one I made last year here about sampling frequency (of I/O throughput, but it‚Äôs a generic concept). The background to this is my analysis of the performance and capacity of our data warehouse on Oracle 11g.\nBefore I get too boring, here‚Äôs the fun bit:\nPork Pies per Hour (PP/h) Jim wants to enter a championship pork-pie eating competition. He‚Äôs timed himself practising and over the course of an hour he eats four pork pies.","keywords":null,"title":"Getting good quality I/O throughput data","uri":"https://rmoff.net/2011/03/11/getting-good-quality-i/o-throughput-data/"},{"categories":null,"content":"","keywords":null,"title":"io","uri":"https://rmoff.net/categories/io/"},{"categories":["io","oracle","rrdtool","unix"],"content":"I wrote last year about Graphing I/O data using gnuplot and Oracle V$SYSSTAT, using a script from Kevin Closson in his article How To Produce Raw, Spreadsheet-Ready Physical I/O Data With PL/SQL. Good For Exadata, Good For Traditional Storage. Here I‚Äôve got a simple comparison of the data recorded through this script (in essence, Oracle‚Äôs V$SYSSTAT), and directly on the OS through HP‚Äôs MeasureWare. It‚Äôs graphed out with my new favourite tool, rrdtool:","keywords":null,"title":"Comparing methods for recording I/O - V$SYSSTAT vs HP Measureware","uri":"https://rmoff.net/2011/03/09/comparing-methods-for-recording-i/o-vsysstat-vs-hp-measureware/"},{"categories":null,"content":"","keywords":null,"title":"unix","uri":"https://rmoff.net/categories/unix/"},{"categories":null,"content":"","keywords":null,"title":"mbeans","uri":"https://rmoff.net/categories/mbeans/"},{"categories":["bi","bug","jmx","mbeans","obiee","systemsmanagement"],"content":"Over the last few months I‚Äôve been doing a lot of exploring of OBIEE Systems Management data, covered in a mini-series of blog posts, Collecting OBIEE systems management data.\nThere are a vast number of counters exposed, ranging from the very interesting (Active Sessions, Cache Hits, etc) to the less so (Total Query Piggybacks, although for some seriously hardcore performance tuning even this may be of interest).\nThis short blog post is about a couple of counters which I‚Äôve been monitoring but which looks to not be entirely reliable.","keywords":null,"title":"OBIEE Systems Management - dodgy counter behaviour","uri":"https://rmoff.net/2011/03/08/obiee-systems-management-dodgy-counter-behaviour/"},{"categories":["jmx","unix","visualisation"],"content":"I‚Äôve added two new toys to my geek arsenal today. First is one with which I‚Äôve dabbled before, but struggled to master. The second is a revelation to me and which I discovered courtesy of twitter.\nrrdtool rrdtool is a data collection and graphing tool which I‚Äôve been aware of for a while. I wanted to use it when I wrote about Collecting OBIEE systems management data with JMX, but couldn‚Äôt get it to work.","keywords":null,"title":"Shiny new geek toys ‚Äì rrdtool and screen","uri":"https://rmoff.net/2011/03/01/shiny-new-geek-toys--rrdtool-and-screen/"},{"categories":["hack","ldap","obiee","udml"],"content":"A chap called Kevin posted a comment on a previous posting of mine asking\ndid you ever come across anything that could be used to change the LDAP server settings from a command line (admintool.exe, UDML, or otherwise)?\nI did a quick play around with some UDML and it appears that you can.\nSet up the initial LDAP server definition in the RPD First I added a dummy LDAP server to samplesales.","keywords":null,"title":"Changing LDAP settings in an OBIEE RPD with UDML","uri":"https://rmoff.net/2011/02/23/changing-ldap-settings-in-an-obiee-rpd-with-udml/"},{"categories":null,"content":"","keywords":null,"title":"ldap","uri":"https://rmoff.net/categories/ldap/"},{"categories":null,"content":"","keywords":null,"title":"udml","uri":"https://rmoff.net/categories/udml/"},{"categories":["log","monitoring","obiee","oracle","performance","usagetracking"],"content":"Cary Millsap recently published a paper ‚ÄúMastering Performance with Extended SQL Trace‚Äù describing how to use Oracle trace to assist with troubleshooting the performance of database queries. As with all of Cary Millsap‚Äôs papers it is superbly written, presenting very detailed information in a clear and understandable way. (and yes I do have a DBA crush ;-)) It discusses how you can automate the tracing of specific sessions on the database, and requiring the application to be appropriately instrumented.","keywords":null,"title":"Instrumenting OBIEE for tracing Oracle DB calls","uri":"https://rmoff.net/2011/02/02/instrumenting-obiee-for-tracing-oracle-db-calls/"},{"categories":null,"content":"","keywords":null,"title":"log","uri":"https://rmoff.net/categories/log/"},{"categories":null,"content":"","keywords":null,"title":"usagetracking","uri":"https://rmoff.net/categories/usagetracking/"},{"categories":["bug","dwh","mviews","oracle"],"content":"I‚Äôve been doing some work recently that involved the use of Materialised Views on Oracle 11g (11.1.0.7), particularly around PCT refresh. There are some things that are not clear from the documentation, or are actually bugs so far as I‚Äôm concerned, and I‚Äôve detailed these below.\nIn this example I was working on part of a DWH with c.2 millions rows aggregated up daily. One of the things that I spent a long time trying to get to work was Partition Truncation when using PCT refresh.","keywords":null,"title":"Materialised Views - PCT Partition Truncation","uri":"https://rmoff.net/2011/01/08/materialised-views-pct-partition-truncation/"},{"categories":null,"content":"","keywords":null,"title":"mviews","uri":"https://rmoff.net/categories/mviews/"},{"categories":["dwh","oracle"],"content":"Poking around on My Oracle Support today, I found a link to a white paper dated November 2010, titled ‚ÄúBest Practices for a Data Warehouse on Oracle Database 11g‚Äù. It‚Äôs new to me and I‚Äôve not noticed a blog post announcing it, so I thought I‚Äôd share it here. It‚Äôs by Maria Colgan, who has posted in the past on both the Inside the Oracle Optimizer blog and The Data Warehouse Insider blog.","keywords":null,"title":"Oracle Whitepaper - ‚ÄúBest Practices for a Data Warehouse on Oracle Database 11g‚Äù","uri":"https://rmoff.net/2011/01/05/oracle-whitepaper-best-practices-for-a-data-warehouse-on-oracle-database-11g/"},{"categories":["etl","oracle","statistics"],"content":"This is a series of posts where I hope to humbly plug some gaps in the information available (or which has escaped my google-fu) regarding statistics management in Oracle 11g specific to Data Warehousing.\nIncremental Global Statistics is new functionality in Oracle 11g (and 10.2.0.4?) and is explained in depth in several places including:\nOracle¬Æ Database Performance Tuning Guide - Statistics on Partitioned Objects Greg Rahn - Oracle 11g: Incremental Global Statistics On Partitioned Tables Inside the Oracle Optimiser - Maintaining statistics on large partitioned tables Amit Poddar - One Pass Distinct Sampling (ppt - slides 52 onwards are most relevant) In essence, Oracle maintains information about each partition when statistics is gathered on the partition, and it uses this to work out the global statistics - without having to scan the whole table.","keywords":null,"title":"Data Warehousing and Statistics in Oracle 11g - incremental global statistics","uri":"https://rmoff.net/2010/12/30/data-warehousing-and-statistics-in-oracle-11g-incremental-global-statistics/"},{"categories":["hack","jmx","monitoring","oas","obiee","unix"],"content":"Introduction This is the third part of three detailed articles making up a mini-series about OBIEE monitoring. It demonstrates how to capture OBIEE performance information, and optionally graph it out and serve it through an auto-updating webpage.\nThis final article describes how to bolt on to OAS a simple web page hosting the graphs that you created in part 2, plotting data from OBIEE collected in part 1.\nThe webpage This is just an old-school basic HTML page, with a meta-refresh tag (which note that Chrome doesn‚Äôt work with) and img tags:","keywords":null,"title":"Adding OBIEE monitoring graphs into OAS","uri":"https://rmoff.net/2010/12/06/adding-obiee-monitoring-graphs-into-oas/"},{"categories":["gnuplot","jmx","monitoring","obiee","unix","visualisation"],"content":"Introduction This is the second part of three detailed articles making up a mini-series about OBIEE monitoring. It demonstrates how to capture OBIEE performance information, and optionally graph it out and serve it through an auto-updating webpage.\nThis article takes data that part one showed you how to collect into a tab-separated file that looks something like this:\n[sourcecode] 2010-11-29-14:48:18 1 0 11 0 3 2 1 676 340 0 53 1 0 41 0 3 0 2010-11-29-14:49:18 1 0 11 0 3 2 1 676 0 0 0 1 0 0 0 3 0 2010-11-29-14:50:18 2 0 16 1 4 3 1 679 0 0 0 1 0 0 0 4 0 2010-11-29-14:51:18 2 2 19 1 4 3 1 679 32 0 53 1 0 58 0 4 0 2010-11-29-14:52:18 2 1 19 1 4 3 4 682 0 0 0 1 0 0 0 4 0 2010-11-29-14:53:18 2 1 19 1 4 3 4 682 0 0 0 1 0 0 0 4 0 2010-11-29-14:54:18 2 0 19 1 4 3 1 682 0 0 0 1 0 0 0 4 0 [/sourcecode]","keywords":null,"title":"Charting OBIEE performance data with gnuplot","uri":"https://rmoff.net/2010/12/06/charting-obiee-performance-data-with-gnuplot/"},{"categories":["jmx","mbeans","monitoring","obiee","systemsmanagement"],"content":"Introduction This is the first part of three detailed articles making up a mini-series about OBIEE monitoring. It demonstrates how to capture OBIEE performance information, and optionally graph it out and serve it through an auto-updating webpage.\nFor some background on OBIEE‚Äôs Systems Management component, along with JMX and MBeans, see here and here. The following assumes you know your mbeans from coffee beans and jmx from a bmx.\nThe metric collection is built around the jmxsh tool.","keywords":null,"title":"Collecting OBIEE systems management data with jmx","uri":"https://rmoff.net/2010/12/06/collecting-obiee-systems-management-data-with-jmx/"},{"categories":null,"content":"","keywords":null,"title":"gnuplot","uri":"https://rmoff.net/categories/gnuplot/"},{"categories":null,"content":"","keywords":null,"title":"oas","uri":"https://rmoff.net/categories/oas/"},{"categories":["hack","jmx","mbeans","monitoring","obiee","systemsmanagement"],"content":"Those of you who read my blog regularly may have noticed I have a slight obsession with the OBIEE systems management capability which is exposed through JMX. Venkat has blogged this week about JMX in OBI11g, and it‚Äôs clearly a technology worth understanding properly. I‚Äôve recently been tinkering with how to make use of it for monitoring purposes, most recently using JConsole and discussed here. What follows is an extension of this idea, cobbled together with a bit of shell scripting, awk, gnuplot, and sticky backed plastic.","keywords":null,"title":"OBIEE monitoring","uri":"https://rmoff.net/2010/12/06/obiee-monitoring/"},{"categories":null,"content":"","keywords":null,"title":"javahost","uri":"https://rmoff.net/categories/javahost/"},{"categories":["javahost","obiee","sawserver"],"content":"Hot on the heels of one problem, another has just reared its head.\nUsers started reporting an error with reports that included charts: [sourcecode] Chart server does not appear to be responding in a timely fashion. It may be under heavy load or unavailable. [/sourcecode]\nSet up is a OBIEE 10.1.3.4.1 two-server deployment with BI/PS/Javahost clustered and loadbalanced throughout.\nDiagnostics Javahost was running, and listening, on both servers: [sourcecode] $ps -ef|grep javahost obieeadm 14076 1 0 Nov 25 ?","keywords":null,"title":"OBIEE 10g - javahost hang","uri":"https://rmoff.net/2010/12/03/obiee-10g-javahost-hang/"},{"categories":["obiee","unix"],"content":"They say about travelling that it‚Äôs the journey and not the destination, and the same is true with this problem we hit during a deployment to Production.\nWe were deploying a new OBIEE 10g implementation, with authentication provided by Microsoft Active Directory (AD) through the LDAP functionality in OBIEE. As a side note, it‚Äôs a rather nice way to do authentication, although maybe I‚Äôm biased coming from our previous implementation which used EBS integrated authentication and was a bugger to set up and work with.","keywords":null,"title":"Troubleshooting OBIEE - LDAP (ADSI) authentication","uri":"https://rmoff.net/2010/12/02/troubleshooting-obiee-ldap-adsi-authentication/"},{"categories":["jmx","monitoring","obiee","systemsmanagement"],"content":"Folk from Yorkshire are tight, so the stereotype goes. So here‚Äôs a cheap-ass way to monitor OBIEE 10g using nothing but the OBIEE built-in systemsmanagement component, the jmx agent, and jconsole (which is part of the standard Java distribution):\nFrom here you can also export to CSV the various counters, and then store history, plot it out with gnuplot or Excel, etc.\nIf anyone‚Äôs interested let me know and I‚Äôll document a bit more about how I did this, but it‚Äôs basically building on previous work I‚Äôve documented around jmx and OBIEE.","keywords":null,"title":"A Poor Man‚Äôs OBIEE EM/BI Management Pack","uri":"https://rmoff.net/2010/11/04/a-poor-mans-obiee-em/bi-management-pack/"},{"categories":["etl","odi","performance"],"content":"I‚Äôve been involved with some performance work around an ODI DWH load batch. The batch comprises well over 1000 tasks in ODI, and whilst the Operator console is not a bad interface, it‚Äôs not very easy to spot the areas consuming the most runtime.\nHere‚Äôs a set of SQL statements to run against the ODI work repository tables to help you methodically find the steps of most interest for tuning efforts.","keywords":null,"title":"Analysing ODI batch performance","uri":"https://rmoff.net/2010/11/03/analysing-odi-batch-performance/"},{"categories":["support","thoughts"],"content":"I heard this on Thinking Allowed and thought how applicable it was to the attitudes that you can sometimes encounter in both systems development, and the support of production systems:\n‚ÄúEach uneventful day that passes reinforces a steadily growing false sense of confidence that everything is all right ‚Äì that I, we, my group must be OK because the way we did things today resulted in no adverse consequences.‚Äù\nby Scott Snook (Senior Lecturer in the Organizational Behavior unit at Harvard Business School )","keywords":null,"title":"Does this summarise your system development \u0026 support ethos?","uri":"https://rmoff.net/2010/10/27/does-this-summarise-your-system-development-amp-support-ethos/"},{"categories":null,"content":"","keywords":null,"title":"support","uri":"https://rmoff.net/categories/support/"},{"categories":null,"content":"","keywords":null,"title":"thoughts","uri":"https://rmoff.net/categories/thoughts/"},{"categories":["obiee","performance"],"content":"Jeff McQuigg has posted two presentations that he gave at Openworld 2010 on his website here: http://greatobi.wordpress.com/2010/10/26/oow-presos/\nThey‚Äôre full of real content and well worth a read. There‚Äôs excellent levels of detail and plenty to think about if you‚Äôre involved in OBI or DW development projects.","keywords":null,"title":"Two excellent OBI presentations from Jeff McQuigg","uri":"https://rmoff.net/2010/10/27/two-excellent-obi-presentations-from-jeff-mcquigg/"},{"categories":["io","oracle","unix"],"content":"Continuing in the beard-scratching theme of Unix related posts (previously - awk), here‚Äôs a way to graph out the I/O profile of your Oracle database via the Oracle metrics in gv$sysstat, and gnuplot. This is only the system I/O as observed by Oracle, so for belts \u0026 braces (or to placate a cynical sysadmin ;-)) you may want to cross-reference it with something like sar.\nFirst, a pretty picture of what you can get:","keywords":null,"title":"Graphing I/O data using gnuplot and Oracle V$SYSSTAT","uri":"https://rmoff.net/2010/10/26/graphing-i/o-data-using-gnuplot-and-oracle-vsysstat/"},{"categories":["obiee"],"content":"More of a unix thing than DW/BI this post, but I have a beard so am semi-qualified‚Ä¶.\nThe requirement was to improve the performance of some ODI processing that as part of its work was taking one huge input file, and splitting it into chunks based on content in the file. To add some (minor) spice the file was fixed width with no deliminators, so the easy awk answers that I found on google weren‚Äôt applicable.","keywords":null,"title":"awk - split a fixed width file into separate files named on content","uri":"https://rmoff.net/2010/10/19/awk-split-a-fixed-width-file-into-separate-files-named-on-content/"},{"categories":null,"content":"","keywords":null,"title":"informatica","uri":"https://rmoff.net/categories/informatica/"},{"categories":null,"content":"","keywords":null,"title":"obia","uri":"https://rmoff.net/categories/obia/"},{"categories":["informatica","obia","rant","support"],"content":"Last month I wrote about a problem that Informatica as part of OBIA was causing us, wherein an expired database account would bring Oracle down by virtue of multiple connections from Informatica.\nI raised an SR with Oracle (under OBIA support), who after some back-and-forth with Informatica, were told:\nThis is not a bug. That the two error messages coming back from Oracle are handled differently is the result of a design decision and as such not a product fault.","keywords":null,"title":"When is a bug not a bug? When it‚Äôs a ‚Äúdesign decision‚Äù","uri":"https://rmoff.net/2010/10/18/when-is-a-bug-not-a-bug-when-its-a-design-decision/"},{"categories":["bi","visualisation"],"content":"Jeff Atwood of Coding Horror fame observed:\nThis is sage advice (if a little crude :-)) to bear in mind when building reports for users.","keywords":null,"title":"A good maxim to bear in mind when designing reports","uri":"https://rmoff.net/2010/09/23/a-good-maxim-to-bear-in-mind-when-designing-reports/"},{"categories":["metalink","silly","support"],"content":"As Twitter learnt yesterday, you should always sanitise user input. I was amused to see My Oracle Support doing so‚Ä¶.recursively :)\nThe apostrophe in ‚Äúdoesn‚Äôt‚Äù got escaped once, and then again, and then again, and then again, and then again ‚Ä¶‚Ä¶","keywords":null,"title":"Better safe than sorry‚Ä¶sanitising DB input","uri":"https://rmoff.net/2010/09/22/better-safe-than-sorry...sanitising-db-input/"},{"categories":null,"content":"","keywords":null,"title":"metalink","uri":"https://rmoff.net/categories/metalink/"},{"categories":null,"content":"","keywords":null,"title":"silly","uri":"https://rmoff.net/categories/silly/"},{"categories":null,"content":"","keywords":null,"title":"svn","uri":"https://rmoff.net/categories/svn/"},{"categories":["svn"],"content":"Here‚Äôs one in the series of stupid things I‚Äôve done but which Google has thrown no answers, so I post it here to help out fellow idiots.\nToday‚Äôs episode involves our SCM tool, TortoiseSVN. I‚Äôd been happily using it for over a year, when suddenly I couldn‚Äôt commit any more. I could browse and checkout to my heart‚Äôs content, but when I tried to commit, boom:\nCommit failed (details follow): Authorization failed","keywords":null,"title":"TortoiseSVN doesn‚Äôt prompt for authentication","uri":"https://rmoff.net/2010/09/21/tortoisesvn-doesnt-prompt-for-authentication/"},{"categories":["io","oracle"],"content":"This query, based on AWR snapshots on sys.wrh$_sysstat, includes in its metrics the I/O read throughput for a given snapshot duration.\nHowever it‚Äôs important to realise the huge limitation to this figure - it‚Äôs an average. It completely shoots you in the foot if you‚Äôre looking at capacity requirements.\nConsider this real-life example extracted from the above query:\n[sourcecode] Timestamp Total Read MBPS =========================================== 14-SEP-10 05.15.12.660 113.748 14-SEP-10 06.00.40.953 202.250 14-SEP-10 06.","keywords":null,"title":"The danger of averages - Measuring I/O throughput","uri":"https://rmoff.net/2010/09/14/the-danger-of-averages-measuring-i/o-throughput/"},{"categories":["documentation","obiee","oracle","rant"],"content":"I‚Äôm a geek. I like understanding things in their absolute entirety. It frustrates me to have to make presumptions or assumptions about something. I like to get down ‚Äôn dirty and find out what makes things tick.\nSo that necessitates reading manuals. And unfortunately, Oracle don‚Äôt make that easy all the time.\nSome of the manuals are not very good, and others are difficult to find. Given the complexity of the OBIEE stack and proliferation of terminology and product names, sorting the wheat from the chaff can be a headache.","keywords":null,"title":"RTFM? But where TF is the FM?  \u003e\u003e Offline searchable OBIEE 11g documentation","uri":"https://rmoff.net/2010/09/13/rtfm-but-where-tf-is-the-fm-gtgt-offline-searchable-obiee-11g-documentation/"},{"categories":["oracle","performance","resource-manager","unix"],"content":"Introduction We‚Äôre in the process of implemention Resource Manager (RM) on our Oracle 11gR1 Data Warehouse. We‚Äôve currently got one DW application live, but have several more imminent. We identified RM as a suitable way of - as the name would suggest - managing the resources on the server.\nIn the first instance we‚Äôre looking at simply protecting CPU for, and from, future applications. At some point it would be interesting to use some of the more granular and precise functions to demote long-running queries, have nighttime/daytime plans, etc.","keywords":null,"title":"A fair bite of the CPU pie? Monitoring \u0026 Testing Oracle Resource Manager","uri":"https://rmoff.net/2010/09/10/a-fair-bite-of-the-cpu-pie-monitoring-amp-testing-oracle-resource-manager/"},{"categories":null,"content":"","keywords":null,"title":"resource-manager","uri":"https://rmoff.net/categories/resource-manager/"},{"categories":["bug","informatica","obia","ora-28001","oracle","security"],"content":"This problem, which in essence is bad behaviour from Informatica bringing down Oracle, is a good illustration of unintended consequences of an apparently innocuous security setting. Per our company‚Äôs security standards, database passwords expire every 90 days. When this happens users are prompted to change their password before they can continue logging into Oracle. This applies to application user IDs too. It appears that Informatica 8.6.1 HF6 (part of OBIA 7.","keywords":null,"title":"Misbehaving Informatica kills Oracle","uri":"https://rmoff.net/2010/09/02/misbehaving-informatica-kills-oracle/"},{"categories":null,"content":"","keywords":null,"title":"ora-28001","uri":"https://rmoff.net/categories/ora-28001/"},{"categories":["bi-publisher","quartz"],"content":"A very short blog post to break the drought, but I didn‚Äôt hit any google results for this error so thought it might be useful to record it.\nIn BI Publisher 10.1.3.4, trying to install the Scheduler (Quartz) schema, I got this error:\n[sourcecode]Schema installation failed while creating tables. Schema may already exist. Please remove the existing schema or choose another database and try again.[/sourcecode]\nTo me, the error text is a bit unhelpful.","keywords":null,"title":"BI Publisher - error creating Quartz tables","uri":"https://rmoff.net/2010/08/25/bi-publisher-error-creating-quartz-tables/"},{"categories":null,"content":"","keywords":null,"title":"bi-publisher","uri":"https://rmoff.net/categories/bi-publisher/"},{"categories":null,"content":"","keywords":null,"title":"quartz","uri":"https://rmoff.net/categories/quartz/"},{"categories":["obiee","performance","sawserver"],"content":"@alexgorbachev tweeted me recently after picking up my presentation on Performance Testing and OBIEE. His question got me thinking, and as ever the answer ‚ÄúIt Depends‚Äù is appropriate here :-)\nWhy is the measurement being done? Without knowing the context of the work Alex is doing, how to measure depends on whether the measurement needs to be of: -\nThe actual response times that the users are getting, or The response times that the system is currently capable of delivering This may sound like splitting hairs or beard-scratching irrelevance, but it‚Äôs not.","keywords":null,"title":"Measuring real user response times for OBIEE","uri":"https://rmoff.net/2010/06/14/measuring-real-user-response-times-for-obiee/"},{"categories":["hack","nqcmd","obiee","unix"],"content":"Here are a couple of little unix scripts that I wrote whilst developing my performance testing OBIEE method.\nThey‚Äôre nothing particularly special, but may save you the couple of minutes it‚Äôd take to write them :)\nNote that some of this data is available from Usage Tracking and where it is I‚Äôd recommend getting it from there, databases generally being easier to reliably and repeatably query than a transient log file.","keywords":null,"title":"Scripts to extract information from OBIEE NQQuery.log","uri":"https://rmoff.net/2010/06/11/scripts-to-extract-information-from-obiee-nqquery.log/"},{"categories":["obiee","obiee-11g"],"content":"OBIEE 11g is going to be officially launched on 7th July (this year!) in London: Launch Event: Introducing Oracle Business Intelligence Enterprise Edition 11g\nh/t to Twitter and blogs (including an Oracle-branded one) this morning. Good news travels fast! :-)\nLooks like John Minkjan won the bet, did he know something we didn‚Äôt? ;-) (although is ‚ÄúLaunch‚Äù the same as ‚ÄúGA‚Äù?)","keywords":null,"title":"OBIEE 11g launch date - 7th July 2010","uri":"https://rmoff.net/2010/06/03/obiee-11g-launch-date-7th-july-2010/"},{"categories":null,"content":"","keywords":null,"title":"biforum","uri":"https://rmoff.net/categories/biforum/"},{"categories":["biforum","obiee","performance","presentation"],"content":"Here‚Äôs my presentation ‚ÄúPerformance Testing and OBIEE‚Äù that I gave at the RittmanMead BI Forum 2010: Performance Testing and OBIEE.pptx\nIt‚Äôs a Powerpoint 2007 file (pptx) for which you may need the Microsoft PowerPoint Viewer 2007. I‚Äôve included copious notes on each slide which hopefully cover the gist of what I talked about when I was delivering it. There are also a handful of funky animations which is why I‚Äôve left it in pptx and not exported to PDF or other format (sorry Open Office users).","keywords":null,"title":"Performance Testing and OBIEE","uri":"https://rmoff.net/2010/05/24/performance-testing-and-obiee/"},{"categories":null,"content":"","keywords":null,"title":"presentation","uri":"https://rmoff.net/categories/presentation/"},{"categories":["biforum","presentation"],"content":"I delivered my first presentation today, at the RittmanMead BI Forum.I was really nervous in the hours and minutes leading up to it, but once I got up there and started talking I actually quite enjoyed it. If you were in the audience, I‚Äôd love some feedback in the comments section below, particularly any ‚Äúconstructive criticism‚Äù. I obviously didn‚Äôt make too much of a mess of it, as I was awarded ‚Äúbest speaker‚Äù of the event, which was a great honour.","keywords":null,"title":"My first presentation - afterthoughts","uri":"https://rmoff.net/2010/05/23/my-first-presentation-afterthoughts/"},{"categories":["obiee","presentation","weblogic"],"content":"I‚Äôve just returned from the RittmanMead 2010 BI Forum which this year was at the Seattle Hotel in Brighton Marina.\nThe event was limited to 50 attendees, and I think this was a good number. The event was explicitly pitched at a very technical expert level, and the audience very much represented this. Whereas at a larger conference you may find the odd manager wandering around pretending to be technical ;-) this event was most definitely not pitched at such types.","keywords":null,"title":"RittmanMead BI Forum 2010","uri":"https://rmoff.net/2010/05/21/rittmanmead-bi-forum-2010/"},{"categories":null,"content":"","keywords":null,"title":"weblogic","uri":"https://rmoff.net/categories/weblogic/"},{"categories":["obia","obiee","security"],"content":"Troubleshooting EBS-BI integrated authentication can be a tiresome activity, so here‚Äôs a shortcut that might help. If you suspect the problem lies with EBS then you can leave OBIEE out of the equation.\nLogin to EBS\nUse FireBug or Fiddler2 to inspect web traffic as follows:\nClick the BI link from EBS\nShould be first a request to EBS server, which returns 302 and redirects to http://\u003cbi server\u003e:\u003cport\u003e/analytics/saw.dll?Dashboard\u0026acf=101507310\nRecord the value of acf (eg 101507310)","keywords":null,"title":"Validating EBS-BI authentication, without BI","uri":"https://rmoff.net/2010/05/17/validating-ebs-bi-authentication-without-bi/"},{"categories":null,"content":"","keywords":null,"title":"dac","uri":"https://rmoff.net/categories/dac/"},{"categories":null,"content":"","keywords":null,"title":"ora-01017","uri":"https://rmoff.net/categories/ora-01017/"},{"categories":["dac","ora-01017","oracle"],"content":"What‚Äôs going on here? The username/password is definitely valid, proved by the sqlplus connection.\nConfiguring DAC in OBIA 7.9.5.1:\n[sourcecode] What can I do for you?\n1 - Enter repository connection information 2 - Test repository connection 3 - Enter email account information 4 - Send test email 5 - Save changes 6 - Exit\nPlease make your selection: 1\nThese are your connection type choices:\n1 - MSSQL 2 - DB2 3 - Oracle (OCI8) 4 - Oracle (Thin) 5 - Keep current ( Oracle (Thin) )","keywords":null,"title":"What am I missing here??? ORA-01017: invalid username/password; logon denied","uri":"https://rmoff.net/2010/05/06/what-am-i-missing-here-ora-01017-invalid-username/password-logon-denied/"},{"categories":null,"content":"","keywords":null,"title":"ora-00845","uri":"https://rmoff.net/categories/ora-00845/"},{"categories":["ora-00845","oracle"],"content":"This is a note-to-self really. When playing around with Oracle and something‚Äôs not working - RTFAL: Read The Flippin Alert Log!\nAfter resizing a VM I was getting this problem: [sourcecode] [oracle@RNMVM01 ~]$ sqlplus / as sysdba\nSQL*Plus: Release 11.2.0.1.0 Production on Sat Apr 24 17:44:44 2010\nCopyright (c) 1982, 2009, Oracle. All rights reserved.\nConnected to an idle instance.\nSQL\u003e startup nomount ORA-00845: MEMORY_TARGET not supported on this system SQL\u003e [/sourcecode]","keywords":null,"title":"RTFAL!","uri":"https://rmoff.net/2010/04/24/rtfal/"},{"categories":null,"content":"","keywords":null,"title":"em","uri":"https://rmoff.net/categories/em/"},{"categories":null,"content":"","keywords":null,"title":"opera","uri":"https://rmoff.net/categories/opera/"},{"categories":["em","opera"],"content":"A faithful FireFox user for many users, I‚Äôve been having a rather delightful rekindling of my old love for Opera recently. Back in the days, when I was a beardless bairn, I even paid for Opera I liked it so much. Then Firefox came along and Opera got dropped by the wayside like a teenage crush.\nWell Opera 10.5 was released recently and I‚Äôm liking it. I miss my Firefox extensions too much to switch entirely, but damn, Opera is FAST!","keywords":null,"title":"Opera + Oracle EM = true love","uri":"https://rmoff.net/2010/04/22/opera--oracle-em-true-love/"},{"categories":["obiee","presentation"],"content":"I‚Äôm doing my first ever conference presentation next month at the 2010 Rittman Mead BI Forum.\nMy presentation is called Performance Testing OBIEE, which is something I‚Äôve spent a lot of time working on over the last few months. I think the challenge is going to be distilling it all into a session that‚Äôs not going to overwhelm everyone or bore them to death! Well, actually, the challenge is going to be the presenting.","keywords":null,"title":"My first presentation - help!","uri":"https://rmoff.net/2010/04/21/my-first-presentation-help/"},{"categories":["obiee","obiee-11g"],"content":"Spotted this when trawling through My Oracle Support. It‚Äôs pretty common knowledge anyway amongst people already familiar with hacking around with OBIEE, but worth recording for people coming along to it new.\nDoc ID 1068266.1 states:\nUDML is not supported in OBI 10g.\nin 11g, XUDML (the Oracle BI Server XML API) will be fully supported and documented.","keywords":null,"title":"OBIEE 11g tidbit - XUDML support","uri":"https://rmoff.net/2010/03/18/obiee-11g-tidbit-xudml-support/"},{"categories":["oracle","performance"],"content":"I‚Äôve been playing around with SQL Tuning Sets, and was trying to clear up my mess.\nTo list all the tuning sets: [sourcecode language=‚Äúsql‚Äù] SET WRAP OFF SET LINE 140 COL NAME FOR A15 COL DESCRIPTION FOR A50 WRAPPED\nselect name,created,last_modified,statement_count,description from DBA_SQLSET [/sourcecode]\n[sourcecode] NAME CREATED LAST_MODI STATEMENT_COUNT DESCRIPTION ‚Äî‚Äî‚Äî‚Äî‚Äî ‚Äî‚Äî‚Äî ‚Äî‚Äî‚Äî ‚Äî‚Äî‚Äî‚Äî‚Äî ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- sts_test_02 09-MAR-10 09-MAR-10 1 Test run 1 sts_test_01 12-FEB-10 12-FEB-10 1 an old STS test test test [/sourcecode]","keywords":null,"title":"ORA-13757: ‚ÄúSQL Tuning Set‚Äù ‚Äústring‚Äù owned by user ‚Äústring‚Äù is active.","uri":"https://rmoff.net/2010/03/09/ora-13757-sql-tuning-set-string-owned-by-user-string-is-active./"},{"categories":null,"content":"","keywords":null,"title":"cluster","uri":"https://rmoff.net/categories/cluster/"},{"categories":null,"content":"","keywords":null,"title":"load-balancing","uri":"https://rmoff.net/categories/load-balancing/"},{"categories":["jmx","obiee","security","systemsmanagement"],"content":"JMX OBIEE‚Äôs Systems Management functionality exposes performance counters and the application‚Äôs configuration options through Java MBeans and optionally a protocol called JMX.\nIt‚Äôs extremely useful, and is documented pretty widely :\nJConsole / JMX JConsole / JMX ‚Äì followup Oracle BI Management / Systems Management MBeans PerfMon OBIEE MBeans and OC4J OBIEE performance monitoring and alerting with jManage In this article I‚Äôm going to discuss the use of JMX to access these counters remotely, and a possible security issue that‚Äôs present in the BI Management Pack manual.","keywords":null,"title":"Securing OBIEE Systems Management JMX for remote access","uri":"https://rmoff.net/2010/03/05/securing-obiee-systems-management-jmx-for-remote-access/"},{"categories":["cluster","load-balancing","obiee","sawserver","support"],"content":"We hit a very interesting problem in our Production environment recently. We‚Äôd made no changes for a long time to the configuration, but all of a sudden users were on the phone complaining. They could login to BI from EBS but after logging in the next link they clicked took them to the OBIEE ‚ÄúYou are not logged in‚Äù screen.\nOur users login to EBS R12 and then using EBS authentication log in to OBIEE (10.","keywords":null,"title":"Who‚Äôs been at the cookie jar? EBS-BI authentication and Load Balancers","uri":"https://rmoff.net/2010/03/05/whos-been-at-the-cookie-jar-ebs-bi-authentication-and-load-balancers/"},{"categories":["obia","performance"],"content":"A new document has been published by Oracle, discussing ways of improving performance for OBIA 7.9.6 and 7.9.6.1. Its primary focus is around improving ETL performance. There‚Äôs some very interesting content including hardware sizing recommendations, and I‚Äôd strongly recommend anyone working with OBIA reads it.\nIt‚Äôs called ‚ÄúOracle Business Intelligence Applications Version 7.9.6 Performance Recommendations‚Äù and is available on My Oracle Support through Doc ID 870314.1","keywords":null,"title":"OBIA 7.9.6 Performance Recommendations","uri":"https://rmoff.net/2010/03/02/obia-7.9.6-performance-recommendations/"},{"categories":["oracle","rant"],"content":"After it was ripped down last week, Chris Warticki‚Äôs blog is back online, albeit with the last posting redirecting visitors to a new location on Oracle Communities.\nMaybe I‚Äôm getting too old for this s##t, but I‚Äôm yet to really get a handle on how Oracle want to interact with real people on the ground. Oracle Communities is a fairly new site that I‚Äôve not explored so much because of no OBIEE area.","keywords":null,"title":"Oracle Support Blog back online, kinda.","uri":"https://rmoff.net/2010/02/23/oracle-support-blog-back-online-kinda./"},{"categories":["obiee"],"content":"A sad little passing last week, of the Oracle Support Blog and related tweets by Chris Warticki.\nLast week Chris posted this comment on twitter\n‚ÄúSo, what to do if you‚Äôre the ‚Äúonline customer presence‚Äù and your own leadership wants to censor your posts and comments?‚Äù\nfollowed by this terse blog posting:\nSupport Blog: No longer available\nBy chris.warticki on February 12, 2010 4:08 PM Please use My Oracle Support Communities instead","keywords":null,"title":"Oracle Support blog no more","uri":"https://rmoff.net/2010/02/15/oracle-support-blog-no-more/"},{"categories":["obiee"],"content":"I stumbled across this blog posting recently, and am reposting the link here as I‚Äôve seen no mention of it elsewhere. This surprised me as with Exadata (and most new technologies) any snippet of news or tech insight gets blogged and tweeted multiple times over.\nAny insight into Exadata is interesting as unlike all the other Oracle software which can be downloaded and poked \u0026 prodded, this is a physical box so we rely on blog postings and marketing (with a BS filter) to understand more about it.","keywords":null,"title":"Exadata V2 POC numbers","uri":"https://rmoff.net/2010/02/10/exadata-v2-poc-numbers/"},{"categories":["bi","visualisation"],"content":"In the following list, which two mind-mapping programs are rated best?\nNow look at the actual numbers, and answer again Different answer?\nI can‚Äôt be the only one in this frantic world whose eyes are drawn to the pictures instead of words and leap to conclusions. It‚Äôs only because I use FreeMind and was surprised it scored so low ‚Ä¶. and then realised it hadn‚Äôt. Looks like the HTML rendering isn‚Äôt the same here (FF3.","keywords":null,"title":"Illustrating data","uri":"https://rmoff.net/2010/02/08/illustrating-data/"},{"categories":["performance"],"content":"There is a LOT written about performance. And this post is now adding to it. Some of it‚Äôs excellent, some of it less so. But a lot of it starts from a point so far down the process that unless you know the first bit, you‚Äôre going to raring off and end up chasing your tail or p###ing in the wind‚Ä¶. (pardon my french). Without a well structured approach that you understand and always follow you‚Äôll hit on solutions by luck only.","keywords":null,"title":"Brilliant performance articles by Cary Millsap","uri":"https://rmoff.net/2010/01/29/brilliant-performance-articles-by-cary-millsap/"},{"categories":["obiee","oracle","support"],"content":"You get a call from your friendly DBA. He says the production database is up the spout, and it‚Äôs ‚Äúthat bee eye thingumy causing it‚Äù. What do you do now? All you‚Äôve got to go on is a program name in the Oracle session tables of ‚Äúnqsserver@MYSERVER (TNS V1-V3)‚Äù and the SQL the DBA sent you that if you‚Äôre lucky will look as presentable as this: The username against the SQL is the generic User ID that you had created for connections to the database from OBIEE.","keywords":null,"title":"Identify your OBIEE users by setting Client ID in Oracle connection","uri":"https://rmoff.net/2010/01/26/identify-your-obiee-users-by-setting-client-id-in-oracle-connection/"},{"categories":null,"content":"","keywords":null,"title":"config","uri":"https://rmoff.net/categories/config/"},{"categories":["config","obiee","security","unix","windows"],"content":"This error caught me out today. I was building a Linux VM to do some work on, and for the life of me couldn‚Äôt get the OBIEE Admin Tool to connect to the BI Server on the VM.\nThe error I got when trying to define a DSN on the Windows box was:\n[nQSError: 12008] Unable to connect to port 9703 on machine 10.3.105.132 [nQSError: 12010] Communication error connecting to remote end point: address = 10.","keywords":null,"title":"How to resolve ‚Äú[nQSError: 12002] Socket communication error at call=: (Number=-1) Unknown‚Äù","uri":"https://rmoff.net/2010/01/22/how-to-resolve-nqserror-12002-socket-communication-error-at-call-number-1-unknown/"},{"categories":null,"content":"","keywords":null,"title":"apache","uri":"https://rmoff.net/categories/apache/"},{"categories":["apache","oas","security"],"content":"Oracle Application Server (OAS) is the Web and Application server typically deployed with OBIEE. There are several settings which by default may be viewed as security weaknesses. Whether realistically a target or not, it‚Äôs good practice to always be considering security and lock down your servers as much as reasonably possible. I adopt the default stance of having to find a reason to leave something less secure, rather than justify why it needs doing.","keywords":null,"title":"Hardening OAS","uri":"https://rmoff.net/2010/01/21/hardening-oas/"},{"categories":["obiee","oel"],"content":"Quick post as the snow‚Äôs coming down and I wanna go home ‚Ä¶\nI‚Äôve been working on building a VM based on OEL5.4 and OBIEE 10.1.3.4.1. After installing XE 10.2 I tried to fire my RPD up, but hit this:\n/usr/lib/oracle/xe/app/oracle/product/10.2.0/server/lib/libnnz10.so: cannot restore segment prot after reloc: Permission denied [nQSError: 46029] Failed to load the DLL /app/oracle/product/obiee/server/Bin/libnqsdbgatewayoci10g.so. Check if ‚ÄòOracle OCI 10G‚Äô database client is installed.\nIf you trace the ‚Äòstack‚Äô back you find that it parses down to this nub of an error:","keywords":null,"title":"libnnz10.so: cannot restore segment prot after reloc: Permission denied","uri":"https://rmoff.net/2009/12/18/libnnz10.so-cannot-restore-segment-prot-after-reloc-permission-denied/"},{"categories":null,"content":"","keywords":null,"title":"oel","uri":"https://rmoff.net/categories/oel/"},{"categories":["obiee"],"content":"Mucho kudos to Borkur Steingrimsson for getting the OBIEE admin tool working on Unix!","keywords":null,"title":"Running the OBIEE admin tool on Unix","uri":"https://rmoff.net/2009/12/14/running-the-obiee-admin-tool-on-unix/"},{"categories":null,"content":"","keywords":null,"title":"caf","uri":"https://rmoff.net/categories/caf/"},{"categories":["caf","obiee"],"content":"Very interesting post by Kevin McGinley about CAF here:¬†CAF = Migration Utility? Use Caution!\nIt articulates better than I ever could reasons against using CAF particularly in a production environment.\nSince the tool came out I‚Äôd been struggling to get my head around it, convinced I was missing something. I still don‚Äôt profess to understand it properly, but Kevin‚Äôs article reassures me that I shouldn‚Äôt be losing too much sleep over it, especially given that it‚Äôs unsupported and won‚Äôt work with 11g.","keywords":null,"title":"CAF","uri":"https://rmoff.net/2009/12/11/caf/"},{"categories":null,"content":"","keywords":null,"title":"sawping","uri":"https://rmoff.net/categories/sawping/"},{"categories":["obiee","sawping","sawserver"],"content":"Short but sweet this one - a way of troubleshooting connectivity problems between analytics (the Presentation Services Plug-in, either j2ee servlet or ISAPI, a.k.a. SAWBridge) and sawserver (Presentation Services).\nFor a recap on the services \u0026 flow please see the first few paragraphs of this post.\nProblems in connectivity between analytics and sawserver normally manifest themselves through this error message:\n500 Internal Server Error Servlet error: An exception occurred. The current application deployment descriptors do not allow for including it in this response.","keywords":null,"title":"Troubleshooting Presentation Services / analytics connectivity","uri":"https://rmoff.net/2009/12/09/troubleshooting-presentation-services-/-analytics-connectivity/"},{"categories":["obiee"],"content":"This was my first UKOUG TEBS, in fact my first conference I‚Äôd ever attended! I was quite unsure what to expect, but three days later I can safely say it was invaluable.\nThe variety of presentations and expertise being shared was impressive, and it was great to hear people sharing and discussing their ideas and opinions around the subjects I work with each day.\nWorking in isolation is not a good idea, one can develop a blinkered or bunker mentality.","keywords":null,"title":"UKOUG TEBS 2009","uri":"https://rmoff.net/2009/12/03/ukoug-tebs-2009/"},{"categories":["obia","silly"],"content":"Why did this make me think of the OBIA upgrade documentation?? ;-)\nvia I think this summarizes everything..\n[Update December 18, 2013: The lady in the picture is Kathy Sierra]","keywords":null,"title":"I think this summarises everything.","uri":"https://rmoff.net/2009/11/27/i-think-this-summarises-everything./"},{"categories":["oas","obiee","obiee-11g","weblogic"],"content":"Oracle have published an interesting doc 968223.1, entitled ‚ÄúEnterprise Deployment of Oracle BI EE on OC4J and App Servers‚Äù.\nIt details the differences between OC4J and OAS which is useful for the current versions of OBIEE. It then also gives a useful heads-up ‚Äì that WebLogic becomes the App server of choice in the next version of OBIEE\nAll of this changes in OBI EE 11g where several projects will become absolutely dependent upon an App Server.","keywords":null,"title":"OBIEE application servers, now and future","uri":"https://rmoff.net/2009/11/25/obiee-application-servers-now-and-future/"},{"categories":["obiee"],"content":"(Sorry for the lame title, but it gives me an excuse to put this picture in :) )\nI was really pleased by the response I got from my posting about The state of OBIEE on the web, knowing that it‚Äôs not just me goes a long way to keeping my blood pressure down.\nThe OTN forum is still an unbalanced mix of tosh (is this guy for real?) with the occasional insightful and interesting post or idea such as this really neat one from Joe Bertram about using multiple presentation services on top of the same RPD to give different interfaces to different end devices.","keywords":null,"title":"SoOotW and sweep","uri":"https://rmoff.net/2009/11/25/soootw-and-sweep/"},{"categories":["config","sawserver"],"content":"Whilst installing OBIA 7.9.6.1 I hit this problem when firing up Presentation Services (sawserver):\nError loading security privilege /system/privs/catalog/ChangePermissionsPrivilege.\nA quick search on the forums threw up two posts suggesting a corrupted WebCat.\nSince I‚Äôd got this webcat fresh out of the box I was puzzled how it could be corrupted.\nI did a bit more tinkering (including nosying around in the sawserver log), before realising it was indeed corrupt, and that I was indeed a muppet.","keywords":null,"title":"Resolved: sawserver : Error loading security privilege /system/privs/catalog/ChangePermissionsPrivilege","uri":"https://rmoff.net/2009/11/17/resolved-sawserver-error-loading-security-privilege-/system/privs/catalog/changepermissionsprivilege/"},{"categories":["cluster","obiee","performance","unix"],"content":"A very interesting new PDF from Sun on deploying OBIEE has been published, with discussions on architecture, performance and best practice.\nThis Sun BluePrints article describes an enterprise deployment architecture for Oracle Business Intelligence Enterprise Edition using Sun servers running the Solaris Operating System and Sun Storage 7000 Unified Storage systems. Designed to empower employees in organizations in any industry‚Äîfrom customer service, shipping, and finance to manufacturing, human resources, and more‚Äîto become potential decision makers, the architecture brings fault tolerance, security, resiliency, and performance to enterprise deployments.","keywords":null,"title":"Deploying Oracle Business Intelligence Enterprise Edition on Sun Systems","uri":"https://rmoff.net/2009/11/12/deploying-oracle-business-intelligence-enterprise-edition-on-sun-systems/"},{"categories":["oracle","rant","support"],"content":"Metalink was retired this weekend and made way for the new My Oracle Support system. It didn‚Äôt go as smoothly as it could have done.\nThis post is going to be a bit of a rambling rant.\nUltimately people, including me, don‚Äôt like their cheese being moved (not unless there‚Äôs a really runny piece of Camembert at the end of it). That makes it a bit more difficult to discuss because some of people‚Äôs complaints will just be geeks being stubborn (and boy, can geeks be stubborn).","keywords":null,"title":"#Fail: My Oracle Support","uri":"https://rmoff.net/2009/11/11/#fail-my-oracle-support/"},{"categories":["load-balancing","oas","obiee","sawserver","unix"],"content":"Introduction Whilst the BI Cluster Controller takes care nicely of clustering and failover for BI Server (nqsserver), we have to do more to ensure further resilience of the stack.\nA diagram I come back to again and again when working out configuration or connectivity problems is the one on P16 of the Deployment Guide. With this you can work out most issues for yourself through simple reasoning. Print it out, pin it to your wall, and read it!","keywords":null,"title":"OBIEE clustering - specifying multiple Presentation Services from Presentation Services Plug-in","uri":"https://rmoff.net/2009/11/06/obiee-clustering-specifying-multiple-presentation-services-from-presentation-services-plug-in/"},{"categories":["google","obiee","oracle"],"content":"Want to find all PDFs from Oracle about OBIEE?\nThere‚Äôs some interesting ones that this turned up on Oracle‚Äôs public FTP. In particular:\nBI EE Environmental and Multi-User Development Migration Processes Securing Oracle BI Enterprise Edition (There‚Äôs also PowerPoint files on there including this one but Google doesn‚Äôt seem to index them)\nYou can use Google‚Äôs Advanced Search page to build similar queries:\nYou can also use Google Alerts so that any time a new entry matching your search criteria is added in Google‚Äôs index you get notified of it either by email or on an RSS feed:","keywords":null,"title":"Advanced Googling for OBIEE information","uri":"https://rmoff.net/2009/11/03/advanced-googling-for-obiee-information/"},{"categories":["caf","catalogmanager","obiee"],"content":"Christian Screen has done a nice video explaining the CAF installation, and has promised a deep-dive followup which I‚Äôm looking forward to. Click here for the article","keywords":null,"title":"CAF installation video","uri":"https://rmoff.net/2009/11/03/caf-installation-video/"},{"categories":null,"content":"","keywords":null,"title":"catalogmanager","uri":"https://rmoff.net/categories/catalogmanager/"},{"categories":null,"content":"","keywords":null,"title":"google","uri":"https://rmoff.net/categories/google/"},{"categories":["timemanagement"],"content":"A brilliant posting here from Jonathan Lewis on the subject of Experts. He in turn is quoting Chen Shapira: ‚ÄúDBAs are under a lot of pressure not to be experts.‚Äù. Read the sentence again, as it took me a minute to figure out. He‚Äôs writing in the context of an Oracle DBA but I think it‚Äôs equally applicable to those working with an looking after installations of OBIEE et al.","keywords":null,"title":"Experts","uri":"https://rmoff.net/2009/11/02/experts/"},{"categories":null,"content":"","keywords":null,"title":"timemanagement","uri":"https://rmoff.net/categories/timemanagement/"},{"categories":["obiee","rant"],"content":"An advance footnote I‚Äôll start this by saying why I think things are how they are, and then I‚Äôll get to the meat of my article.\nOBIEE in its current incarnation (v10.1.3) is a mature product. All the big bugs have been caught and fixed. All the known quirks are well documented. All the missing features are known. All the clever workarounds have been found. All the neat little hacks have been explored.","keywords":null,"title":"The state of OBIEE on the web","uri":"https://rmoff.net/2009/10/30/the-state-of-obiee-on-the-web/"},{"categories":["silly"],"content":"I honestly don‚Äôt dare click the ‚ÄúDid you mean‚Äù link ;-)","keywords":null,"title":"The mind boggles‚Ä¶","uri":"https://rmoff.net/2009/10/26/the-mind-boggles.../"},{"categories":["obiee","oracle"],"content":"A frequent question on the OTN OBIEE forum is how to fix this error:\n[nQSError: 17001] Oracle Error code: 12154, message: ORA-12154: TNS:could not resolve the connect identifier specified at OCI call OCIServerAttach. [nQSError: 17014] Could not connect to Oracle database.\nThe error is simply OBIEE reporting that it tried to connect from the BI Server to an Oracle database and the Oracle client returned an error. Distilling it down gives us this error:","keywords":null,"title":"Troubleshooting OBIEE and ORA-12154: TNS:could not resolve the connect identifier","uri":"https://rmoff.net/2009/10/22/troubleshooting-obiee-and-ora-12154-tnscould-not-resolve-the-connect-identifier/"},{"categories":["bug","obiee","security"],"content":"October‚Äôs Oracle Critical Patch Update Advisory has been released. There are two vulnerabilities (CVE-2009-1999, CVE-2009-1990) listed under Oracle Application Server for ‚ÄúComponent‚Äù Business Intelligence Enterprise Edition and one (CVE-2009-3407) for ‚Äúcomponent‚Äù Portal.\nCVE-2009-1999 is OBIEE and ‚ÄúFixed in all supported versions. No patch provided in this Critical Patch Update.‚Äù. CVE-2009-3407 looks like only OAS (not OBIEE), up to versions 10.1.2.3 and 10.1.4.2. CVE-2009-1990 is OBIEE and is the main vuln of interest.","keywords":null,"title":"Critical Patch Update - OBIEE vuln CVE-2009-1990","uri":"https://rmoff.net/2009/10/21/critical-patch-update-obiee-vuln-cve-2009-1990/"},{"categories":null,"content":"","keywords":null,"title":"otn","uri":"https://rmoff.net/categories/otn/"},{"categories":["otn"],"content":"w00t :-D\nYesterday I got my ‚ÄúPro‚Äù medal for 500 points on OTN Forums\nI‚Äôd been intending to post a grumpy rant about OTN recently, but maybe I‚Äôll postpone that for a few days now ;)","keywords":null,"title":"OTN forum - ‚ÄúPro‚Äù","uri":"https://rmoff.net/2009/10/21/otn-forum-pro/"},{"categories":["obiee"],"content":"Two new blogs of note :\nShiv Bharti‚Äôs Blog Debashis‚Äôs OBIEE Blog Both are well worth a read \u0026 following.","keywords":null,"title":"New OBIEE blogs","uri":"https://rmoff.net/2009/10/19/new-obiee-blogs/"},{"categories":["obiee"],"content":"Watch out if you are using init blocks in your RPD. We hit a bug (#9019374) recently that caused BI Server (10.1.3.4) to hang.\nThe init block in question should have returned a date to update a repository variable, but because of badly-written SQL and abnormal data in the source table actually returned a null value. BI Server evidently didn‚Äôt like this null being inserted somewhere where it shouldn‚Äôt have and understandably logged :","keywords":null,"title":"BI Server hung - nQSError 14054 / 15001 / 23005","uri":"https://rmoff.net/2009/10/16/bi-server-hung-nqserror-14054-/-15001-/-23005/"},{"categories":["bug","oas","obiee","security"],"content":"The Critical Patch Update Pre-Release Announcement for October has been published. The pre-release is advance notice of the affected software prior to release of the quarterly Critical Patch Update. It is published on the Thursday prior to the patch releases (which was postponed by a week because of OOW).\nIt looks like if you‚Äôre running OBIEE 10.1.3.4.0 or 10.1.3.4.1 through OAS 10.1.2.3.0/10.1.3.4.0/10.1.3.5.0 then you should check back next Tuesday 20th for details.","keywords":null,"title":"Heads up - Critical Patch Update affecting OBIEE","uri":"https://rmoff.net/2009/10/16/heads-up-critical-patch-update-affecting-obiee/"},{"categories":["obiee","performance"],"content":"A new OBIEE benchmark has been published by Oracle. It‚Äôs on the same hardware as August‚Äôs benchmark - Sun T5440s. Anyone would think that Oracle like Sun ;-)\nDetails here\nNumbers added to my collation post here","keywords":null,"title":"New OBIEE benchmark - 50,000 users","uri":"https://rmoff.net/2009/10/12/new-obiee-benchmark-50000-users/"},{"categories":["informatica","obia"],"content":"The latest point release of Oracle Business Intelligence Applications, 7.9.6.1, has been released and is available for download from here (direct link to download).\nThe version.txt reports the version as:\nBuild: 7.9.6.1.100609.2038 Release Version: Oracle Business Intelligence Applications 7.9.6.1 Package: 100609.2038\nNo updated documentation library yet through, so can‚Äôt nosey through the release notes. The docs that come with the download are labelled 7.9.6 and dated April 09 so don‚Äôt look like they‚Äôve been updated either.","keywords":null,"title":"OBIA 7.9.6.1 released","uri":"https://rmoff.net/2009/10/12/obia-7.9.6.1-released/"},{"categories":["obiee","systemsmanagement","usagetracking"],"content":"OBIEE comes with a very useful usage tracking feature. For information about it and how to set it up see these links:\nhttp://obiee101.blogspot.com/2008/08/obiee-setting-up-usage-tracking.html http://www.oracle.com/technology/obe/obe_bi/bi_ee_1013/usage_tracking/usage_tracking.htm http://108obiee.blogspot.com/2009/07/obiee-usage-tracking-setup-and-cloning.html Usage Tracking captures the logical SQL of queries in a column called QUERY_TEXT in the table S_NQ_ACCT. However, out of the box this column is defined as 1k (1024 bytes) long. In some situations this will limit its usefulness because the text will be truncated if necessary when it‚Äôs inserted.","keywords":null,"title":"Usage Tracking - only half the story ‚Ä¶","uri":"https://rmoff.net/2009/10/06/usage-tracking-only-half-the-story-.../"},{"categories":null,"content":"","keywords":null,"title":"loadrunner","uri":"https://rmoff.net/categories/loadrunner/"},{"categories":["loadrunner","obiee","performance"],"content":"This is a supplemental post to this one describing how to set up a VUser in LoadRunner to test OBIEE. It‚Äôs various notes that I made during the development but which aren‚Äôt directly part of the step-by-step tutorial. They‚Äôre not necessarily vital for recording scripts, but observations and explanations that should be helpful when working with LoadRunner and OBIEE.\nValidation using sawserver logs It‚Äôs no use running a load test if the load you think you‚Äôre applying isn‚Äôt actually being applied.","keywords":null,"title":"OBIEE and HP Performance Center (a.k.a. LoadRunner) - Notes","uri":"https://rmoff.net/2009/10/01/obiee-and-hp-performance-center-a.k.a.-loadrunner-notes/"},{"categories":["loadrunner","obiee","performance"],"content":"My two earlier posts (here and here) detail the difficulties I had with LoadRunner (now called HP Performance Center). After a bit of a break along with encouragement from knowing that it must be possible because it‚Äôs how Oracle generates their OBIEE benchmarks I‚Äôve now got something working. I also got a useful doc from Oracle support which outlines pretty much what I‚Äôve done here too.\nIn essence what you do - and what the Metalink document 496417.","keywords":null,"title":"Performance testing OBIEE using HP Performance Center (a.k.a. LoadRunner)","uri":"https://rmoff.net/2009/10/01/performance-testing-obiee-using-hp-performance-center-a.k.a.-loadrunner/"},{"categories":["oracle","performance"],"content":"A new blog from¬†James Morle, who I don‚Äôt know but from other bloggers sounds well respected, and describes himself thus:\nSince it‚Äôs been nearly ten years since I wrote my book, Scaling Oracle8i, I thought it was about time that I started writing again. I thought I would start with the new-fangled blogging thing, and see where it takes me. Here goes.\nHe‚Äôs got a really interesting post on ‚Äúred flags‚Äù to look for in diagnosing performance problems in Oracle:¬†Spotting the Red Flags (Part 1 of n).","keywords":null,"title":"James Morle : Spotting the Red Flags (Part 1 of n)","uri":"https://rmoff.net/2009/09/25/james-morle-spotting-the-red-flags-part-1-of-n/"},{"categories":["oracle"],"content":"SQL Developer v2.1 Early Adopter was released yesterday.\nDownload it here\nNew features list\nHat-tip and hat-tip","keywords":null,"title":"SQL Developer v2.1 Early Adopter released","uri":"https://rmoff.net/2009/09/25/sql-developer-v2.1-early-adopter-released/"},{"categories":["oracle"],"content":"Bit of an odd one this. Oracle 11g database, a user‚Äôs password has expired. But when I try to change it, I can‚Äôt: [sourcecode language=‚Äúbash‚Äù] $sqlplus MYUSER/oldPW@oraDBServer\nSQL*Plus: Release 10.2.0.1.0 - Production on Wed Sep 23 07:57:41 2009\nCopyright (c) 1982, 2005, Oracle. All rights reserved.\nERROR: ORA-28001: the password has expired\nChanging password for MYUSER New password: Retype new password: ERROR: ORA-01017: invalid username/password; logon denied\nPassword unchanged [/sourcecode]","keywords":null,"title":"Changing password on Oracle 11g from 10g clients (ORA-28001 -\u003e ORA-01017)","uri":"https://rmoff.net/2009/09/23/changing-password-on-oracle-11g-from-10g-clients-ora-28001-gt-ora-01017/"},{"categories":["obiee","performance"],"content":"(Updated 12th Oct 09)\nHere‚Äôs a list of the OBIEE benchmark documents published by Oracle:\nBenchmarkDateSource document1 - IBM System x3755Sep-07PDF2 - HP DL380 G4Sep-07PDF3 - Sun T2000Sep-07PDF4 - Sun SPARC Enterprise T5440Aug-09PDF5 - Sun SPARC Enterprise T5440Oct-09PDF Collecting the numbers into one table gives this: Based on the details in the documents I think these were all against OBIA‚Äôs Service Analytics schema \u0026 dashboards/reports.\nInteresting to note the side-by-side comparison in benchmark 3 (Sun T2000) of two servers, in one case both running BI and Presentation Services and in the other having the two components separate.","keywords":null,"title":"Collated OBIEE benchmarks","uri":"https://rmoff.net/2009/09/18/collated-obiee-benchmarks/"},{"categories":["obiee","performance"],"content":"Here‚Äôs a list of the OBIEE benchmark documents published by Oracle:\n28,000 User Benchmark on Sun SPARC Enterprise T5440 Server running Solaris 10 [Aug 09]\n10,000 User Benchmark on Sun T2000 [Sept 07]\n5,800 User Benchmark on HP DL380 G4 [Sept 07]\n4,000 User Benchmark on an IBM System x3755 Server running Red Hat Enterprise Linux","keywords":null,"title":"OBIEE benchmarks","uri":"https://rmoff.net/2009/09/17/obiee-benchmarks/"},{"categories":["cluster","load-balancing","obiee","performance","sawserver","unix"],"content":"Production cluster is 2x BI Server and 2x Presentation Services, with a BIG-IP F5 load balancer on the front.\nSymptoms Users started reporting slow login times to BI. Our monitoring tool (Openview) reported that ‚ÄúBIServer01 may be down. Failed to contact it using ping.‚Äù. BIServer01 cannot be reached by ping or ssh from Windows network.\nDiagnostics nqsserver and nqsclustercontroller on BIServer01 was logging these repeated errors:\n[nQSError: 12002] Socket communication error at call=send: (Number=9) Bad file number","keywords":null,"title":"OBIEE cluster controller failover in action","uri":"https://rmoff.net/2009/09/15/obiee-cluster-controller-failover-in-action/"},{"categories":null,"content":"","keywords":null,"title":"firefox","uri":"https://rmoff.net/categories/firefox/"},{"categories":["firefox","obiee"],"content":"A new version of the web browser Opera was released recently. Several years ago I used Opera and may even have paid for it IIRC. Then Firefox came along, and the ‚Äúit‚Äôs not IE‚Äù excuse was lost for Opera. Not that I mind IE too much nowadays but at the time it was atrocious. Since then I‚Äôve revisited Opera each time a new release has come out, but nothing has impressed me enough to ditch Firefox (and nowadays Google Chrome).","keywords":null,"title":"OBIEE on Opera 10 / IE 7 / FF 3.5 / Chrome 4","uri":"https://rmoff.net/2009/09/10/obiee-on-opera-10-/-ie-7-/-ff-3.5-/-chrome-4/"},{"categories":null,"content":"","keywords":null,"title":"admintool","uri":"https://rmoff.net/categories/admintool/"},{"categories":["admintool","hack"],"content":"Bringing together in one place all of the script syntax that I‚Äôve found so far for using with OBIEE‚Äôs AdminTool.exe /command\nDetails and examples on usage in the following blogs (where I compiled the commands from):\nVenkat Erik Eckhardt (translated from Czech, original here) @lex Kumar Kambam DON‚ÄôT TRY THIS AT HOME!\nI would only recommend this for read-only purposes such as generating the metadata dictionary or consistency check.\n* OpenOnline DSN [user [password]] - Opens the online repository.","keywords":null,"title":"Syntax for AdminTool.exe command line script","uri":"https://rmoff.net/2009/09/09/syntax-for-admintool.exe-command-line-script/"},{"categories":["admintool","hack","obiee","windows"],"content":"There‚Äôs an undocumented feature in AdminTool.exe that you can use the /command switch with a text file containing scripted commands to make changes to an RPD file (or create a new one).\nIt‚Äôs undocumented and UNSUPPORTED so be careful using it.\nSome good details in these blog posts, especially Erik‚Äôs which has a good list of syntax.\nVenkat Erik Eckhardt (translated from Czech, original here) @lex Kumar Kambam I‚Äôm intrigued to know how the original posters figured out the commands available, if it‚Äôs undocumented‚Ä¶ :)","keywords":null,"title":"AdminTool.exe /command","uri":"https://rmoff.net/2009/09/08/admintool.exe-/command/"},{"categories":["metalink"],"content":"Kudos to the My Oracle Support blog for taking the time to respond to my to my comment about searching for Metalink 3 SRs throwing an error.\nIn essence, if you previously used Metalink 3 you must use https://support.oracle.com. If you use https://metalink.oracle.com/ then you‚Äôll hit the problems I did.\nML3 SR‚Äôs are not supported on https://metalink.oracle.com/CSP/ui/index.html. This front end is used for support on legacy server technology, middleware including BEA, and EBusinessSuite","keywords":null,"title":"Metalink 3 followup","uri":"https://rmoff.net/2009/09/04/metalink-3-followup/"},{"categories":["obiee","rss"],"content":"Another way of keeping up with what‚Äôs going on in the obiee world, add this RSS feed of del.icio.us obiee tags to your reader. It may be less ‚Äúcurrent‚Äù (because people might discover and bookmark ‚Äòold‚Äô pages), but it‚Äôs another tool in the armoury :)\nIf you want an aggregated RSS feed of OBIEE / Oracle related blog postings try this one.","keywords":null,"title":"RSS feeds for OBIEE, including del.icio.us obiee tags","uri":"https://rmoff.net/2009/09/04/rss-feeds-for-obiee-including-del.icio.us-obiee-tags/"},{"categories":["caf","catalogmanager","obiee","windows"],"content":"Following the Oracle CAF tutorial here, I got to Cloning Answers Requests section and then got stuck. I‚Äôd set up my environment exactly the same as in the tutorial, down to the same paths etc. After firing up the CAF to clone requests from the SampleSales catalog: I clicked on Next and got the error ‚ÄúException occurred when while initializing repository!!!‚Äù\nBy playing around with the passwords and path names I determined that both RPD files existed and that CAF could load them enough to validate the passwords.","keywords":null,"title":"CAF troubles","uri":"https://rmoff.net/2009/09/03/caf-troubles/"},{"categories":["obiee"],"content":"This weekend just gone Metalink3 went to the digital dustbin. In principle this is a Good Thing, as multiple support websites for a single company is confusing and frustrating.\nMetalink is now ‚ÄúMy Oracle Support‚Äù and is a flash-based whizz-bang affair. Everyone has different tastes, but there‚Äôs a lot to be said for plain HTML for ease and speed of access. But then people probably grumbled to the Wright Brothers that there was nothing wrong with land-transport at the time‚Ä¶","keywords":null,"title":"Metalink 3 RIP","uri":"https://rmoff.net/2009/09/02/metalink-3-rip/"},{"categories":["obiee","sawserver","unix"],"content":"Introduction In this article I plan to get samplesales and paint repositories hosted on a single server, using one BI Server instance and two Presentation Services instances. This is on both Unix (OEL 4) and Windows, and both OC4J (OBIEE‚Äôs ‚Äúbasic installation‚Äù option) and OAS (‚ÄúAdvanced Installation‚Äù).\nBoth samplesales and paint are shipped with 10.1.3.4 of OBIEE, you‚Äôll find them in $OracleBI/OracleBI/server/Sample. This article assumes you‚Äôve got the RPD of each into $OracleBI/OracleBI/server/Repository and unpacked the web cats for each into $OracleBIdata/web/catalog.","keywords":null,"title":"Multiple RPDs on one server - Part 1 - the BI Server","uri":"https://rmoff.net/2009/08/25/multiple-rpds-on-one-server-part-1-the-bi-server/"},{"categories":["hack","obiee","sawserver"],"content":"Introduction In this article I plan to get sample and paint repositories hosted on a single server, using one BI Server instance and two Presentation Services instances. This is on both Unix (OEL 4) and Windows, and both OC4J (OBIEE‚Äôs ‚Äúbasic installation‚Äù option) and OAS (‚ÄúAdvanced Installation‚Äù).\nMake sure you‚Äôve read and followed part 1 - BI Server first.\nRemember that multiple Presentation Services instances on a machine is UNSUPPORTED BY ORACLE!","keywords":null,"title":"Multiple RPDs on one server - Part 2 - Presentation Services","uri":"https://rmoff.net/2009/08/25/multiple-rpds-on-one-server-part-2-presentation-services/"},{"categories":["obiee","oracle","support"],"content":"For some reason Oracle haven‚Äôt put out a 10.x version of the error \u0026 message codes reference guide for OBIEE, but the previous version for Siebel Analytics is still useful:\nPDF version: http://download.oracle.com/otndocs/products/bi/bi-ee/docs/784/AnyMsg.pdf HTML version: http://download.oracle.com/docs/cd/E12103_01/books/AnyMsg/AnyMsg_Messages.html#wp1007961 ","keywords":null,"title":"OBIEE error/message code reference","uri":"https://rmoff.net/2009/08/24/obiee-error/message-code-reference/"},{"categories":["silly"],"content":"xkcd.com is one of my favourite comics on the web. It strikes just the right balance of geeky witty humour without being too smart-ass.\nI liked this recent one a lot :-)\nA couple of other favourites:\nhttp://xkcd.com/149/ http://xkcd.com/327/ http://xkcd.com/123/ ","keywords":null,"title":"Tech Support Cheat Sheet [xkcd.com]","uri":"https://rmoff.net/2009/08/24/tech-support-cheat-sheet-xkcd.com/"},{"categories":["loadrunner","obiee","performance","sawserver"],"content":"UPDATED: See a HOWTO for OBIEE and LoadRunner here: /2009/10/01/performance-testing-obiee-using-hp-performance-center-a.k.a.-loadrunner/\nThis is following on from my first post about OBIEE and LoadRunner, in which I failed dismally to get a simple session replaying.\nIn a nutshell where I‚Äôd got to was using the ‚ÄúWeb (Click and Script)‚Äù function which worked fine for logging in but when running a report resulted in an error on the rendered page. Digging around showed the error was from the javascript of the OBIEE front end.","keywords":null,"title":"OBIEE and Load Runner - part 2","uri":"https://rmoff.net/2009/08/21/obiee-and-load-runner-part-2/"},{"categories":["obiee","sqlserver","unix","windows"],"content":"A question that pops up on the OBIEE OTN forum quite often is how to use non-Oracle databases like MS SQL Server when the OBIEE server is running on a non-Windows OS such as Linux.\nThe answer in a nutshell is that since version 10.1.3.3.1 OBIEE has been bundled with ODBC drivers for unix/linux from a company called DataDirect. See the release notes here for more information and installation instructions (as well as a list of support databases).","keywords":null,"title":"Querying SQL Server from OBIEE running on Unix","uri":"https://rmoff.net/2009/08/21/querying-sql-server-from-obiee-running-on-unix/"},{"categories":null,"content":"","keywords":null,"title":"sqlserver","uri":"https://rmoff.net/categories/sqlserver/"},{"categories":["metalink","silly","support"],"content":"A follow up to my previous post about Metalink‚Äôs ‚ÄúDo you mean‚Äù feature, this one made me laugh: I shall miss this kind of thing when Metalink3 merges into My Oracle Support‚Ä¶.\nMeep meep!","keywords":null,"title":"Do you mean (pt II)","uri":"https://rmoff.net/2009/08/20/do-you-mean-pt-ii/"},{"categories":["config","log","sawserver"],"content":"As well as tinkering with the sawserver (Presentation Services) logging level and format, we can specific which bits of the log we‚Äôre interested in. This is useful for two reasons:\nWe can enable detailed logging for a specific area, without impacting performance as much as detailed logging throughout would cause By only logging in detail the area of interest we can more easily read the log output and not have to wade through pages of irrelevant information Chapter 9 (‚ÄúUsing the Oracle BI Presentation Services Logging Facility‚Äù) of the Presentation Services Administration Guide details the log configuration.","keywords":null,"title":"Logging specific types of sawserver activity","uri":"https://rmoff.net/2009/08/20/logging-specific-types-of-sawserver-activity/"},{"categories":["loadrunner","obiee","performance"],"content":"UPDATED: See a HOWTO for OBIEE and LoadRunner here\nIntroduction LoadRunner is a tool from HP (bought from Mercury) that can be used to simulate user activity. It supports a whole host of protocols but for OBIEE I‚Äôm obviously using the Web one.\nThere are two flavours, ‚ÄúWeb (Click and Script)‚Äù and ‚ÄúWeb (HTTP/HTML)‚Äù. The latter simply shoves HTTP requests at the server, whereas ‚ÄúClick and Script‚Äù simulates mouse and keyboard entry and thus is more appropriate for this user-based application.","keywords":null,"title":"OBIEE and Load Runner - part 1","uri":"https://rmoff.net/2009/08/19/obiee-and-load-runner-part-1/"},{"categories":["config","hack","log","sawserver"],"content":"I posted a while ago about the sawserver (Presentation Services) log configuration file. Today I‚Äôm doing some work digging around why sawserver‚Äôs throwing an error and so increased the log detail. This parameter is really helpful to use:\nfmtName=‚Äúshort‚Äù\nConsider in these two screenshots, the first is with the default log format and shows about six entries. The second is short log format and is about ten times as much data.","keywords":null,"title":"sawserver log - short format","uri":"https://rmoff.net/2009/08/19/sawserver-log-short-format/"},{"categories":["otn"],"content":"I have a couple of OTN forums bookmarked, and found that generally every few hours I get signed out and end up viewing them as Guest. I then have to click on sign in, enter password, etc.\n(Signing out this frequently is ridiculous, IMHO).\nI found that instead of using the direct URL of a forum, eg: http://forums.oracle.com/forums/forum.jspa?forumID=378 if I use this form: http://forums.oracle.com/forums/adfAuthentication?success_url=/forum.jspa?forumID=378 then I end up signed in more often, and if I‚Äôve been signed out then it goes straight to the login page and then through to the forum.","keywords":null,"title":"OTN forums - different URL to get prompted to login less often","uri":"https://rmoff.net/2009/08/18/otn-forums-different-url-to-get-prompted-to-login-less-often/"},{"categories":["apache","cluster","dac","obia","obiee","sawserver","unix"],"content":"Here‚Äôs a set of scripts that I use on our servers as a quick way to check if the various BI components are up and running.\nBecause we split the stack across servers, there are different scripts called in combination. On our dev boxes we have everything and so the script calls all three sub-scripts, whereas on Production each server will run one of:\nBI Server Presentation Server \u0026 OAS Informatica \u0026 DAC The scripts source another script called process_check.","keywords":null,"title":"Unix script to report on OBIEE and OBIA processes state","uri":"https://rmoff.net/2009/08/14/unix-script-to-report-on-obiee-and-obia-processes-state/"},{"categories":["bug","dac","obia","oracle"],"content":"We‚Äôre upgrading from OBIA 7.9.5 (Financials - GL) to OBIA 7.9.6. Our reasons are for support (7.9.5 does not support Oracle 11g) and minor functionality additions.\nOur architecture is: HP-UX 64 bit Itanium (11.31), Oracle 11g (11.1.0.7), separate ETL server, 4x OBIEE servers (2x BI, 2xPS). We have no customisations in the ETL except something for budgets, which is superseded in 7.9.6.\nThis post is a semi-formed articulation of my frustrations encountered during an initial run through of the upgrade in a sandbox.","keywords":null,"title":"OBIA upgrade 7.9.5 to 7.9.6 - first thoughts","uri":"https://rmoff.net/2009/08/13/obia-upgrade-7.9.5-to-7.9.6-first-thoughts/"},{"categories":["obia"],"content":"I keep hitting this error when setting up OBIA. I suppose it‚Äôs what it says on the tin, but Googling it didn‚Äôt match so I‚Äôm posting this so next time I hit it I remember :-)\nRepository Error ([REP_51821] Failed to connect from Integration Service (pmserver) to repository Oracle_BI_DW_Base running in exclusive mode.)\nThe cause is the Repository Service having OperatingMode set to Exclusive. This is necessary for some of the setup operations like restoring the pre-built Repository, but if you forget to switch it back the Integration Service will suddenly stop working.","keywords":null,"title":"Repository Error ([REP_51821] Failed to connect from Integration Service (pmserver) to repository Oracle_BI_DW_Base running in exclusive mode.)","uri":"https://rmoff.net/2009/08/10/repository-error-rep_51821-failed-to-connect-from-integration-service-pmserver-to-repository-oracle_bi_dw_base-running-in-exclusive-mode./"},{"categories":["oas"],"content":"I successfully installed OAS 10.1.3.3 and patched to 10.1.3.4. http://localhost:7777 gave the OAS welcome page, but going to http://localhost:7777/em gave 404 Not Found.\nIn [OASHome]/j2ee/home/config/servers.xml search for ascontrol, you should get:\nchange the start attribute to true\nRestart OAS ([OAShome]/opmn/bin/opmnctl restartproc) and Enterprise Manager should now be available","keywords":null,"title":"Clean install of OAS - Enterprise Manager not available","uri":"https://rmoff.net/2009/08/06/clean-install-of-oas-enterprise-manager-not-available/"},{"categories":["google"],"content":"There‚Äôs been a bit of hype about Bing recently, so I thought I‚Äôd try it out in trying to get to the bottom of this question on the OBIEE forum.\nThe question was around the nqschangepassword utility and the error it‚Äôs reporting: nQSError: 46090 The odbc.ini file could not found or could not be accessed.\nI did a google for the error to see what other issues could cause the error.","keywords":null,"title":"Google vs Bing","uri":"https://rmoff.net/2009/08/05/google-vs-bing/"},{"categories":["obia","oracle","rant"],"content":"I‚Äôm starting on an upgrade from OBIA 7.9.5 to 7.9.6 and wading through the two main docs:\n7.9.6 Upgrade guide 7.9.6 Installation guide It would be nice if Oracle could come up with some less confusing terminology. It seems that not only is the whole product of OBIA referred to as OBIA (see @lex‚Äôs posting for a good explanation), but that the sub-components which are not-OBIEE-or-DAC-or-Informatica is also OBIA, c.f. page 6-1 of the Upgrade guide ‚Äú[‚Ä¶]upgrade your Oracle BI Applications environment to the current version.","keywords":null,"title":"OBIA grumble","uri":"https://rmoff.net/2009/08/04/obia-grumble/"},{"categories":["obia","obiee"],"content":"Very good post by @lex giving a nice clear explanation of what OBIA (Oracle Business Intelligence Applications) is\nhttp://siebel-essentials.blogspot.com/2009/06/can-you-describe-oracle-bi-applications.html\nThis should be made a sticky on the OBIA forum in my opinion.\nIt‚Äôs clear from postings on the forum that an awful lot of people don‚Äôt understand what OBIA is or how it sits with OBIEE. I even attended a course last week in which the Oracle trainer stated that OBIEE included ETL and DW schemas, and stuck to this when challenged.","keywords":null,"title":"What is OBIA‚Ä¶","uri":"https://rmoff.net/2009/07/30/what-is-obia.../"},{"categories":["jmanage","jmx","monitoring","obiee","performance","systemsmanagement"],"content":"OBIEE‚Äôs Systems Management component exposes configuration and performance data through Java MBeans. As discussed in other posts these can be be accessed through several different ways:\nJConsole (see also here) oc4j Windows PerfMon (although I guess this isn‚Äôt actually using MBeans/JMX?) saw.dll?perfmon BI Management Pack Since it‚Äôs a standard java technology being used we can in theory use anything that is designed for monitoring mbeans via jmx. Doing some Googling I discovered jManage.","keywords":null,"title":"OBIEE performance monitoring and alerting with jManage","uri":"https://rmoff.net/2009/07/29/obiee-performance-monitoring-and-alerting-with-jmanage/"},{"categories":["oas","obiee","sawserver"],"content":"If, for some reason, you need to check what web application server is in use for Presentation Services (as this chap needed to), you can use an add-in for FireFox called HttpFox to inspect the HTTP headers.\n1. Install HttpFox (and obviously Firefox if you don‚Äôt have it already!)\n2. Open the HttpFox window (Tools -\u003e HttpFox -\u003e Toggle HttpFox)\n3. Click the Start button in the HttpFox window\n4. Navigate to your OBIEE home page","keywords":null,"title":"How to find out what web application server is in use","uri":"https://rmoff.net/2009/07/28/how-to-find-out-what-web-application-server-is-in-use/"},{"categories":["timemanagement"],"content":"I found this post very interesting: Paul Graham : Maker‚Äôs Schedule, Manager‚Äôs Schedule (originally found here)\nIt was one of those mini-revelations when I found something that greatly resonated and explained an inexplicable frustration I find in the workplace sometimes.\nI wonder how applicable it can be to a large corporation though, rather than a start-up where it‚Äôs given that people are allowed to be bolshy with their meetings? ;-)","keywords":null,"title":"Maker‚Äôs Schedule, Manager‚Äôs Schedule","uri":"https://rmoff.net/2009/07/28/makers-schedule-managers-schedule/"},{"categories":null,"content":"","keywords":null,"title":"ora-00922","uri":"https://rmoff.net/categories/ora-00922/"},{"categories":["ora-00922","oracle"],"content":"We routinely change Oracle passwords as part of security best-practice, I keep hitting this and keep forgetting why! :-)\n[sourcecode language=‚Äòsql‚Äô] ALTER USER DAC_REPO IDENTIFIED BY 1KoBe3RH REPLACE YlR94tqp [/sourcecode]\nError report: SQL Error: ORA-00922: missing or invalid option 00922. 00000 - ‚Äúmissing or invalid option‚Äù *Cause: *Action:\nSomeone better qualified than me can explain why but I suspect it‚Äôs the leading number in the new password. Quoting the passwords then works fine:","keywords":null,"title":"ORA-00922: missing or invalid option","uri":"https://rmoff.net/2009/07/27/ora-00922-missing-or-invalid-option/"},{"categories":["metalink","silly","support"],"content":"One of my little gripes with Metalink is its purporting to be helpful when it‚Äôs blatantly not. Here‚Äôs one: Now which is more likely, on Metalink 3; that I‚Äôm searching for sawserver (integral component to OBIEE), or sqlserver?!","keywords":null,"title":"Metalink 3 - Do You Mean ‚Ä¶ ?","uri":"https://rmoff.net/2009/07/24/metalink-3-do-you-mean-.../"},{"categories":["hack","obiee","performance","windows"],"content":"Yet another way to access the BI Management data discussed here - through Windows‚Äô PerfMon tool.\nThis will only work for installations where your OBIEE server is running on Windows. You should be able to run PerfMon locally or remotely. Standard practise would be not to run it locally on a Production machine :-)\nTo run PerfMon go to Start-\u003eRun and enter perfmon, or navigate Start -\u003e Settings -\u003e Control Panel -\u003e Administrative Tools -\u003e Performance","keywords":null,"title":"OBIEE Windows PerfMon counters","uri":"https://rmoff.net/2009/07/24/obiee-windows-perfmon-counters/"},{"categories":["dac","etl","obiee"],"content":"Mark Rittman has an excellent article about querying the DAC repository database tables, including a downloadable RPD file. Being new to working with RPDs I thought it would be good practise to explore this as well as hopefully get some useful information about our current ETL deployment.\nI downloaded the RPD to c:\\OracleBI\\server\\Repository and opened it up in the Admin tool (Administrator/Administrator).\nFirst off I changed the connection pool to point to my DAC repository database, having setup a TNS entry for it first.","keywords":null,"title":"Mark Rittman‚Äôs OBIEE repository for DAC","uri":"https://rmoff.net/2009/07/23/mark-rittmans-obiee-repository-for-dac/"},{"categories":["hack","obiee","sawserver","services","windows"],"content":"Our main servers are Unix and I‚Äôm as happy as a pig in muck at the command line, so when I‚Äôm working on Windows (where I‚Äôve got a test OBIEE install) I like to stick with the CLI where possible.\nPSService is one of those tools that I instinctively reach for without realising it. Combined with Launchy, it‚Äôs even better.\nSimply put, you can control windows services from the command line.","keywords":null,"title":"psservice - Windows command line goodness!","uri":"https://rmoff.net/2009/07/23/psservice-windows-command-line-goodness/"},{"categories":["bug","obiee","sawserver"],"content":"By a strange co-incidence after following this thread on OTN forums about a BI crash and struggling to understand the actual problem, I think I‚Äôve encountered it myself!\nI‚Äôve got a test install of OBIEE running on my Windows XP laptop, and whilst building a report in Answers got this:\n](http://2.bp.blogspot.com/_RCx_EVJpczQ/SmiDrEorbbI/AAAAAAAAGcg/OPGwTLXXg8k/s1600/crash1.png)[![](/images/rnm1978/crash2.png) was:\nszAppName : sawserver.exe szAppVer : 10.1.3.4 szModName : kernel32.dll szModVer : 5.1.2600.3119 offset : 000097a3\nGoing to the sawserver log at c:\\OracleBIData\\web\\log\\sawlog0.","keywords":null,"title":"sawserver charts crash","uri":"https://rmoff.net/2009/07/23/sawserver-charts-crash/"},{"categories":["config","hack","log","sawserver"],"content":"The configuration of how Presentation Services (sawserver) does its logging is in the file web/config/logconfig.xml (same directory as instanceconfig.xml).\nIt‚Äôs all nice and XML‚Äôd:\nLogging Detail\nChange the numerical values in the FilterRecord entries to alter the detail level of the logging. Lower means less detail, higher means more.\nBe aware that your log files can grow very rapidly if you set the logging too high, and unless you‚Äôre troubleshooting then leave them at the defaults.","keywords":null,"title":"sawserver logging configuration - logconfig.xml","uri":"https://rmoff.net/2009/07/23/sawserver-logging-configuration-logconfig.xml/"},{"categories":null,"content":"","keywords":null,"title":"services","uri":"https://rmoff.net/categories/services/"},{"categories":["jmx","mbeans","performance"],"content":"Part of looking at the various gubbins inside OBIEE led me to realise that the Oracle BI Management application drives quite a few things. It exposes MBeans (Management Beans, a java term), accessible through jmx. In the installation of OBIEE this component is referred to as ‚ÄúSystems Management‚Äù.\nThe MBeans give us real-time performance information, along with access to all the configuration options that are normally done through config files (instanceconfig.","keywords":null,"title":"Oracle BI Management / Systems Management MBeans","uri":"https://rmoff.net/2009/07/22/oracle-bi-management-/-systems-management-mbeans/"},{"categories":["jmx","obiee","performance","sawserver"],"content":"A few points to add to my previous posting on JConsole:\nAs well as performance data, you have access to configuration data. Be aware that it is read-write! So whilst it might be a nice alternative to digging around for your instanceconfig.xml etc, you should be careful If you have your BI Server and Presentation Services deployed on separate servers then you will only get MBeans for the relevant service: If you want to view the values of the BI Server MBeans and your Presentation Services server is not on the same box then you have to use JConsole/JMX, as Performance Monitor will not have access to the values: ](http://2.","keywords":null,"title":"JConsole / JMX - followup","uri":"https://rmoff.net/2009/07/21/jconsole-/-jmx-followup/"},{"categories":["hack","mbeans","nqcmd","obiee","performance","sawping","sawserver","unix"],"content":"As a kid I loved the idea of lego where you can disassemble and reassemble something from the ground up. As soon as I got my hands on a computer it was the same. You can have your Acorn Archimedes with its games, where do I find the sprites and sound files behind it? Likewise Microsoft Word, let me at the VBA underneath to hack it around and see what else it can do.","keywords":null,"title":"OBIEE admin tools \u0026 hacks","uri":"https://rmoff.net/2009/07/21/obiee-admin-tools-amp-hacks/"},{"categories":["oracle","rant","silly"],"content":"I read and post a bit on the OBIEE and OBIA OTN forums. The noise ratio isn‚Äôt too bad, but a few things really get my goat:\nNot responding to answers!\nIf I‚Äôve gone out of my way to help, or try to help, at least have the courtesy to acknowledge it, and ideally mark as Helpful or Correct as appropriate.\nEven a simple ‚Äúthanks.‚Äù would do. It‚Äôs just good manners.","keywords":null,"title":"OTN forum rant","uri":"https://rmoff.net/2009/07/21/otn-forum-rant/"},{"categories":["jmx","monitoring","obiee","performance","unix"],"content":"[edit] See this post too [/edit] On an OBIEE server run [sourcecode language=‚Äúbash‚Äù] nohup obiee/systemsmanagement/runagent.sh \u0026 [/sourcecode] and then run jconsole (make sure you‚Äôve set the DISPLAY first if you‚Äôre running it from UNIX). NB: if you don‚Äôt have jconsole in your path you can search for it: [sourcecode language=‚Äúbash‚Äù] $whereis jconsole jconsole: /opt/java1.5/bin/jconsole /opt/java6/bin/jconsole\u003c/span\u003e [/sourcecode] You should find it under your java/bin directory\nYou should get this kind of connection dialog: Click connect, and the console will launch.","keywords":null,"title":"JConsole / JMX","uri":"https://rmoff.net/2009/07/16/jconsole-/-jmx/"},{"categories":["obiee"],"content":"I‚Äôm working on a scripted load test for OBIEE using nqcmd to run reports multiple times. I hit this interesting issue.\nCut and pasting the logical SQL that Presentation Services sends to BI Server from Manage Sessions -\u003e Statement, I kept getting this error when I ran it through nqcmd:\n[10058][State: S1000] [NQODBC] [SQL_STATE: S1000] [nQSError: 10058] A general error has occurred.\n[nQSError: 27005] Unresolved column: ‚ÄúNatural Account (COA)‚Äù.‚ÄúAccount Parent1 Code‚Äù.","keywords":null,"title":"nqcmd and [nQSError: 27005] Unresolved column","uri":"https://rmoff.net/2009/05/28/nqcmd-and-nqserror-27005-unresolved-column/"},{"categories":["apache","oas","obiee"],"content":"It‚Äôs possible to change the error pages served up by OAS/Apache by using the ErrorDocument directive. This is widely documented.\nHowever, to get this to take effect in an oc4j application (such as analytics) you need to change mod_oc4j.conf too.\n(I found this out from this post here)\nTake backups of httpd.conf and mod_oc4j.conf, and then edit them as follows:\nIn httpd.conf add:\nErrorDocument 500 /500.html\nwhere /500.html is a relative path to your custom document","keywords":null,"title":"Custom HTTP error page in OBIEE / OAS","uri":"https://rmoff.net/2009/05/18/custom-http-error-page-in-obiee-/-oas/"},{"categories":["obia","obiee"],"content":"New releases this week - 10.1.3.4.1 of OBIEE, and 7.9.6 of OBIA.\nAfter a bit of scrabbling around found:\n‚ÄúThe 10.1.3.4.1 release of the Oracle Business Intelligence Enterprise Edition introduces no new features.‚Äù\nThough there are some new bits and pieces for Publisher\nFor 7.9.6 OBIA, there‚Äôs no New Features document :(\nBy piecing together the ‚ÄúWhat‚Äôs New in This Release‚Äù for each document you can build up a picture (eg in the Upgrade Guide there‚Äôs reference to changes for the doc relating to a new version of Informatica), but it would be nice to have it all in one place:","keywords":null,"title":"New releases","uri":"https://rmoff.net/2009/04/28/new-releases/"},{"categories":["apache","load-balancing","oas","obiee"],"content":"We‚Äôve got a setup of two OAS/Presentation Services boxes and two BI Server boxes, with load balancing/failover throughout.\nThe Load Balancing of the web requests is being done through a separate bit of kit, an F5 BIG-IP load balancer. This directs the requests at the two OAS servers.\nThe problem we have is that by default OAS serves HTTP on port 7777, but the F5 is using port 80. A request for our load balanced URL: http://bi.","keywords":null,"title":"OBIEE and F5 BIG-IP","uri":"https://rmoff.net/2009/04/15/obiee-and-f5-big-ip/"},{"categories":["google","otn"],"content":"Why does Google more often than not return the korean (kr) version of OTN forums for a match, but not english?\nIf I search Google for ‚ÄúSend notification:‚Äù ‚Äúoracle.ons.Notification‚Äù the top hit is for kr.forums.oracle.com/forums/thread.jspa?threadID=650662\nIf you strip the kr from the URL you get this, which makes more sense given the locality when doing the search in Google.\nOnly mildly irritating, but odd behaviour nonetheless","keywords":null,"title":"Google and Korean OTN forums","uri":"https://rmoff.net/2009/04/02/google-and-korean-otn-forums/"},{"categories":["oas"],"content":"I noticed that the j2ee server.log file was filling up with these entries:\noracle.ons.Notification@afba5d\n09/04/02 10:15:12.207 Send notification:\noracle.ons.Notification@17ca0f5\n09/04/02 10:15:42.217 Send notification:\noracle.ons.Notification@1a28842\n09/04/02 10:16:12.227 Send notification:\noracle.ons.Notification@5144d5\n09/04/02 10:16:42.237 Send notification:\noracle.ons.Notification@19078ed\n09/04/02 10:17:12.247 Send notification:\noracle.ons.Notification@fcc268\n09/04/02 10:17:42.257 Send notification:\noracle.ons.Notification@16df388\nA quick google turned up this page in which bug 7132128 (‚ÄúOC4J EMITS ‚ÄúSEND NOTIFICATION‚Äù MESSAGES THAT FILL SERVER.LOG‚Äù) is identified.\nThe server.log is currently only 6MB and given the size of the server it will be a while before it causes us problems, but it‚Äôs worth being aware of.","keywords":null,"title":"OAS bug 7132128 - Send notification: oracle.ons.Notification","uri":"https://rmoff.net/2009/04/02/oas-bug-7132128-send-notification-oracle.ons.notification/"},{"categories":["apache","oas"],"content":"A very minor irritation, but an irritation nonetheless, is when I go to Application Server Control in OAS I have to login twice.\nReading around I found this is an Apache feature, and is actually designed behaviour.\nFor reasons I‚Äôve not explored our servers have several different hostnames which resolve to the same IP, e.g.:\nmyserver\nmyserver-app\nmyserver-data\nWhen you request a page from Apache using a hostname other than that configured as ServerName in Apache‚Äôs httpd.","keywords":null,"title":"OAS makes you log in twice","uri":"https://rmoff.net/2009/04/02/oas-makes-you-log-in-twice/"},{"categories":["obiee","sawserver","unix"],"content":"(See here and here for history)\nI edited the shell script which is eventually called by run-saw.sh to start the sawserver, (OracleBI)/setup/sawserver.sh, to use trus:\nComment out the final line:\n$SASAWSERVER\nand insert as a new line:\ntusc -fepan -o /tmp/sawserver_tusc.out $SASAWSERVER\nThe output of trus ended with this:\nopen(\"/app/oracle/product/10.2.0/lib/libstd_v2.so.1\", O_RDONLY|0x800, 0) ‚Ä¶‚Ä¶‚Ä¶ ERR#2 ENOENT\nopen(\"/app/oracle/product/obiee/server/Bin64/libstd_v2.so.1\", O_RDONLY|0x800, 0) . ERR#2 ENOENT\nopen(\"/app/oracle/product/obiee/web/bin64/libstd_v2.so.1\", O_RDONLY|0x800, 0) ‚Ä¶. ERR#2 ENOENT\nopen(\"/app/oracle/product/obiee/odbc/lib64/libstd_v2.so.1\", O_RDONLY|0x800, 0) ‚Ä¶ ERR#2 ENOENT","keywords":null,"title":"sawserver won‚Äôt start up - resolved","uri":"https://rmoff.net/2009/04/01/sawserver-wont-start-up-resolved/"},{"categories":["unix"],"content":"In investigating the problems with sawserver I was pointed towards a tool called tusc (which appears to be an HP version of truss).\nYou can use it to invoke a program, and get out a bunch of debug information including system calls.\nYou run it like this:\n$tusc -fep /app/oracle/product/obiee/web/bin64/sawserver64\nAs a beginner when it comes to hardcore *nix I can only look at this and take pot shots at what‚Äôs going on, but with Google by my side I‚Äôm interested in the last lines of the output:","keywords":null,"title":"Troubleshooting an HPUX program","uri":"https://rmoff.net/2009/04/01/troubleshooting-an-hpux-program/"},{"categories":["bi-publisher","cluster","quartz"],"content":"Follow on from setting up Publisher in a clustered environment, I‚Äôve found a nasty little bug in the scheduling element of Publisher, Quartz.\nLooking at the oc4j log file /opmn/logs/default_group~home~default_group~1.log I can see OC4J starting up, and then a whole load of repeated messages:\n09/03/30 11:28:43 Oracle Containers for J2EE 10g (10.1.3.3.0) initialized\n- ClusterManager: detected 1 failed or restarted instances.\n- ClusterManager: Scanning for instance ‚Äúmyserver.fqdn.company.net1238408921404‚Äù‚Äôs failed in-progress jobs.","keywords":null,"title":"Bug in Clustered Publisher Scheduler - ClusterManager: detected 1 failed or restarted instances","uri":"https://rmoff.net/2009/03/30/bug-in-clustered-publisher-scheduler-clustermanager-detected-1-failed-or-restarted-instances/"},{"categories":["obiee","sawserver","unix"],"content":"We‚Äôre getting this error in the Presentation Services plug-in [analytics].\nLog file: /j2ee/home/application-deployments/analytics/home_default_group_1/application.log\n09/03/30 13:16:38.75 analytics: Servlet error\njava.net.ConnectException: Connection refused (errno:239)\nat java.net.PlainSocketImpl.socketConnect(Native Method)\nat java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)\nat java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)\nat java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)\nat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)\nat java.net.Socket.connect(Socket.java:517)\nat java.net.Socket.connect(Socket.java:467)\nat java.net.Socket.(Socket.java:364)\nat java.net.Socket.(Socket.java:178)\nat com.siebel.analytics.web.sawconnect.ConnectionPoolSocketFactoryImpl.createSocket(ConnectionPoolSocketFactoryImpl.java:63)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:585)\nat com.siebel.analytics.web.sawconnect.ConnectionPoolSocketFactoryImpl.createSocket(ConnectionPoolSocketFactoryImpl.java:70)\nat com.siebel.analytics.web.sawconnect.ConnectionPool.createNewConnection(ConnectionPool.java:314)\nat com.siebel.analytics.web.sawconnect.ConnectionPool.getConnection(ConnectionPool.java:133)\nat com.siebel.analytics.web.SAWBridge.processRequest(SAWBridge.java:299)\nat com.siebel.analytics.web.SAWBridge.doGet(SAWBridge.java:325)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:743)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:856)\nat com.evermind[Oracle Containers for J2EE 10g (10.","keywords":null,"title":"sawserver won‚Äôt start (analytics: Servlet error java.net.ConnectException: Connection refused (errno:239))","uri":"https://rmoff.net/2009/03/30/sawserver-wont-start-analytics-servlet-error-java.net.connectexception-connection-refused-errno239/"},{"categories":["odi","oui","unix"],"content":"I‚Äôm installing ODI agent on our database server using OUI. I selected the ‚ÄúServer‚Äù option at install time to get the Agent only, but looking in oracledi/bin odiparams.sh is missing:\n$ls -l *.sh\n-rwxrwxrwx 1 odiadm dba 685 Nov 21 15:58 agent.sh\n-rwxrwxrwx 1 odiadm dba 908 Nov 21 15:58 agentscheduler.sh\n-rwxrwxrwx 1 odiadm dba 707 Nov 21 15:58 agentstop.sh\n-rwxrwxrwx 1 odiadm dba 941 Nov 21 15:58 agentweb.sh\n-rwxrwxrwx 1 odiadm dba 724 Nov 21 15:58 jython.","keywords":null,"title":"ODI Server install - missing odiparams.sh file","uri":"https://rmoff.net/2009/03/27/odi-server-install-missing-odiparams.sh-file/"},{"categories":["unix"],"content":"If you work with a file in Windows and Unix at some point you might end up with windows line feed characters in your Unix file. It‚Äôll look like this:\none line of text ^M\nnext line ^M\nand next line with more ^M\nTo remove the ^M character, load the file into vi on unix and enter as a line command the following:\n:1,$s/^M//\nbut instead of typing ^M do Ctrl-V Ctrl-M to get the charaters","keywords":null,"title":"Remove windows line feed characters in vi","uri":"https://rmoff.net/2009/03/27/remove-windows-line-feed-characters-in-vi/"},{"categories":["dac","informatica","obia","oracle"],"content":"We‚Äôre getting problems with an instance of Informatica / out-of-the-box OBIA on a new set of servers. When we run the execution plan we get this error soon after starting:\nMAPPING\u003e DBG_21075 Connecting to database [TNSENTRY], user [MYUSER]\nMAPPING\u003e CMN_1761 Timestamp Event: [Tue Mar 24 18:56:33 2009]\nMAPPING\u003e CMN_1022 Database driver error‚Ä¶\nCMN_1022 [\nDatabase driver error‚Ä¶\nFunction Name : Logon\nORA-12537: TNS:connection closed\nDatabase driver error‚Ä¶\nFunction Name : Connect","keywords":null,"title":"ORA-12537 / ORA-12518 [Informatica DAC error CMN_1022]","uri":"https://rmoff.net/2009/03/25/ora-12537-/-ora-12518-informatica-dac-error-cmn_1022/"},{"categories":["bi-publisher","cluster","obiee","quartz"],"content":"The Oracle BI Publisher Enterprise Cluster Deployment doc which I just found through Metalink highlighted a couple of points:\n- Report repository should be shared\n- The scheduler should be configured for a cluster\nReport Repository\nThrough Admin\u003eSystem Maintenance\u003eReport Repository I changed the path from the default, /xmlp/XMLP to a NFS mount data/shared/xmlp and restarted the xmlpserver application in OAS. On coming back up Publisher complained because all its config files (in xmlp/Admin), had disappeared.","keywords":null,"title":"Clustering Publisher - Scheduler and Report Repository","uri":"https://rmoff.net/2009/03/24/clustering-publisher-scheduler-and-report-repository/"},{"categories":["firefox"],"content":"\nDelicious Bookmarks FireGestures FoxyProxy Greasemonkey HttpFox Screengrab User Agent Switcher ","keywords":null,"title":"Firefox add-ins - ones I find useful","uri":"https://rmoff.net/2009/03/24/firefox-add-ins-ones-i-find-useful/"},{"categories":["metalink","oracle","support"],"content":"I‚Äôm learning about a lot of the Oracle BI stack through reading manuals and trial-and-error.\nOne thing I‚Äôve realised is that Metalink holds a whole heap of useful information.\nFor example, simply searching on ‚Äúpublisher cluster‚Äù throws up these two very pertinent docs:\nOBIEE Clustered Installation with BI Publisher (Doc ID 744515.1) BI Publisher does not accept cluster jdbc connection strings (Doc ID 559795.1) The first one is a publically available PDF, the second one is the answer to the problem I spent more time than I needed to scratching my head over yesterday.","keywords":null,"title":"Metalink Metalink Metalink","uri":"https://rmoff.net/2009/03/24/metalink-metalink-metalink/"},{"categories":["bi-publisher","jdbc","obiee"],"content":"In setting the scheduler in Publisher I discovered a useful difference in jdbc drivers.\nOur repository is on Oracle 11g.\nAccording to the manual oracle.jdbc.driver.OracleDriver should be used, but previous installations have used oracle.bi.jdbc.AnaJdbcDriver so I tried this too.\nIn experimenting with both I found you get more useful feedback from the second one. Here‚Äôs the same problem reported by both drivers:\n¬∑ Exception [TOPLINK-4002] (Oracle TopLink - 11g Release 1 (11.","keywords":null,"title":"Which jdbc driver to use","uri":"https://rmoff.net/2009/03/24/which-jdbc-driver-to-use/"},{"categories":["bi-publisher","unix"],"content":"Following my previous work on configuring Publisher, I wanted to note down where the changes were written to.\nThe -mname syntax of the unix find command comes in handy here:\nfind /app/oracle/product/obiee -mtime -1\nShows me all files under the specified path which were modified in the last 1 day\nand helpfully throws up:\n/app/oracle/product/obiee/xmlp/XMLP/Admin/DataSource/datasources.xml","keywords":null,"title":"Finding config files in unix","uri":"https://rmoff.net/2009/03/23/finding-config-files-in-unix/"},{"categories":["bi-publisher","cluster","obiee"],"content":"I‚Äôm setting up a clustered OBIEE 10.1.3.4 production environment. There are four servers; two BI Server + Cluster Controller + Scheduler and two OAS + Presentation Services + Publisher. Clustering of BI is configured, now I‚Äôm setting up the other bits. Today is Publisher.\nOn publisher instance A connections to the BI Servers directly work fine:\njdbc:oraclebi://serverA.fqdn.company.net:9703/ jdbc:oraclebi://serverB.fqdn.company.net:9703/\nboth work individually as Connection Strings (with database driver class of oracle.bi.jdbc.AnaJdbcDriver) - verified with ‚ÄúTest Connection‚Äù button.","keywords":null,"title":"OBIEE Publisher - configuring connection to clustered BI Server","uri":"https://rmoff.net/2009/03/23/obiee-publisher-configuring-connection-to-clustered-bi-server/"},{"categories":null,"content":"As I explore the current world of data \u0026 analytics engineering, one of the things I‚Äôm trying to grok is the idea of a Metric Layer (described by Drew Banin\nin this keynote: https://www.youtube.com/watch?v=MdSMSbQxnO0 and Benn Stancil here https://benn.substack.com/p/metrics-layer).\nIt strikes me as conceptually similar to what #OBIEE did in its logical layer with measures. Admittedly it was almost always exposed through the tool‚Äôs front end itself, but there was an ODBC and JDBC (https://rmoff.","keywords":null,"title":"","uri":"https://rmoff.net/1/01/01/"},{"categories":null,"content":"","keywords":null,"title":"/proc/meminfo","uri":"https://rmoff.net/tag/proc/meminfo/"},{"categories":null,"content":"","keywords":null,"title":"/proc/meminfo","uri":"https://rmoff.net/categories/proc/meminfo/"},{"categories":null,"content":"","keywords":null,"title":"5k run/walk","uri":"https://rmoff.net/categories/5k-run/walk/"},{"categories":null,"content":"whoami Robin is a Sr. Principal Advisor, Streaming Data Technologies at Confluent.\nHe has been speaking at conferences since 2009 including QCon, Devoxx, Strata, Kafka Summit, and √òredev.\nYou can find his talks online, subscribe to his YouTube channel, and read his blog. Outside of work, Robin enjoys running, drinking good beer, and eating fried breakfasts‚Äîalthough generally not at the same time.\nüëâüèª Bio \u0026 speaker photos Speaking experience Speaker since 2009 at conferences including QCon, Devoxx, O‚ÄôReilly Strata, NDC, USENIX LISA, Kafka Summit, √òredev, O‚ÄôReilly SACon, Oracle OpenWorld, JavaZone, Big Data LDN, UKOUG, Oracle CODE, PGConf, etc plus numerous meetups.","keywords":null,"title":"About Me","uri":"https://rmoff.net/about-me/"},{"categories":null,"content":"See talks","keywords":null,"title":"Talks","uri":"https://rmoff.net/presentations/"},{"categories":null,"content":"You can find all my recent talks on Notist at https://talks.rmoff.net/. Older ones are on Speaker Deck.\nI have a variety of talks, around several different aspects of Apache Kafka and related technologies.\nKafka 101 / introductory talk : ‚ÄúKafka as a Platform: the Ecosystem from the Ground Up‚Äù Introduction to ksqlDB and stream processing principles and semantics - with lots of live demos. ‚ÄúAn introduction to ksqlDB‚Äù Live demo showing use of Kafka Connect and ksqlDB: ‚ÄúApache Kafka in Action : Let‚Äôs Build a Streaming Data Pipeline!","keywords":null,"title":"Talks","uri":"https://rmoff.net/talks/"}]