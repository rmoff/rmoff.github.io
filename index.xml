<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>rmoff&#39;s random ramblings </title>
		<link>https://rmoff.github.io/</link>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<author>Robin Moffatt</author>
		<rights>Copyright (c) 2018</rights>
		<updated>2018-12-17 23:00:21 &#43;0000 UTC</updated>
		
		<item>
			<title>Moving from Ghost to Hugo</title>
			<link>https://rmoff.github.io/2018/12/17/moving-from-ghost-to-hugo/</link>
			<pubDate>Mon, 17 Dec 2018 23:00:21 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/17/moving-from-ghost-to-hugo/</guid>
			<description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_why&#34;&gt;Why?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;ve been blogging for quite a few years now, starting on Blogger, soon onto &lt;a href=&#34;https://rnm1978.wordpress.com/&#34;&gt;WordPress&lt;/a&gt;, and then to Ghost a couple of years ago. Blogger was fairly lame, WP yucky, but I really do like Ghost. It&amp;#8217;s simple and powerful and was &lt;em&gt;perfect&lt;/em&gt; for my needs. My needs being, an outlet for technical content that respected formatting, worked with a markup language (Markdown), and didn&amp;#8217;t &lt;em&gt;f**k things up&lt;/em&gt; in the way that WP often would in its WYSIWYG handling of content.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I ran Ghost self-hosted on AWS EC2, and this was the start of this particular story. Whilst 20 years ago, even 10 years ago, I&amp;#8217;d quite happily spend an evening immersed in a hack project to get something working, times are different now. With a wife and two kids—and a fair bit of travel for work—the time I have at home I don&amp;#8217;t want to be spending on &lt;em&gt;getting shit to work&lt;/em&gt;. And therein lies the beauty of PaaS. Never mind cloud-bollocks or what the acronyms stand for—in essence you write your blog, and someone else worries about all the rest of it. SSL certificate expired? Not my problem. Ghost needs upgrading? Not my problem. Whereas if you self-host…&lt;em&gt;totally&lt;/em&gt; your problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In addition, paying for my AWS costs out of my pocket started to grate just a little. A few bucks here and there kinda add up over the course of a couple of years. Nothing major, but enough to make me pause and look around. The final itch I had was Markdown. Now &lt;a href=&#34;https://rmoff.net/tags/markdown/&#34;&gt;I do like Markdown&lt;/a&gt;, but a little less than I used to—and that&amp;#8217;s because I found Asciidoc. Asciidoc finishes what Markdown starts. All the slightly limited functionality in Markdown once you get beyond &lt;code&gt;&lt;strong&gt;bold&lt;/strong&gt;&lt;/code&gt; and &lt;code&gt;&lt;em&gt;italic&lt;/em&gt;&lt;/code&gt;, asciidoc swoops in and gives you in spades. And as much as I liked Ghost, it doesn&amp;#8217;t support Asciidoc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_enter_hugo&#34;&gt;Enter Hugo&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hugo is one of the breed of blog platforms that works by generating static HTML, which makes it simple (fewer moving parts, no database backend, etc) and nice and fast. Because it just generates HTML you can host the contents on &lt;a href=&#34;https://pages.github.com/&#34;&gt;Github Pages&lt;/a&gt; (bye bye AWS hosting costs), and because it is just static HTML one can worry less about falling behind in versions. After all, Github are doing the actual hosting and can worry about securing the server; my only bit is the static HTML, and who cares if it&amp;#8217;s generated by an older version of code?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_getting_set_up&#34;&gt;Getting set up&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To get it set up locally on your Mac:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;brew install hugo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There&amp;#8217;s a ton of quickstart tutorials out there—I picked the first off Google and went with that :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;ve got my blog source on a &lt;a href=&#34;https://github.com/rmoff/rmoff-blog&#34;&gt;github repo&lt;/a&gt;, and then the output from Hugo on another - my &lt;a href=&#34;https://github.com/rmoff/rmoff.github.io&#34;&gt;github pages repo&lt;/a&gt;. Using &lt;a href=&#34;https://github.com/rmoff/rmoff-blog/blob/master/deploy.sh&#34;&gt;a script I got from somewhere&lt;/a&gt; (apologies I didn&amp;#8217;t keep the source link) I can automagically build and deploy the changes made locally up to both repos, and thus publish new content.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So far I&amp;#8217;m doing everything in text editor (VS Code). All I&amp;#8217;ve missed so far compared to Ghost is that blog tags, slugs etc are entered manually in the header of the post itself, which seems a bit tedious and error prone:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span style=&#34;&#34;&gt;+++&lt;/span&gt;
author = &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Robin Moffatt&amp;#34;&lt;/span&gt;
categories = [&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;oracle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;cdc&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;debezium&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;goldengate&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;xstream&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;logminer&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;flashback&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;licence&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ksql&amp;#34;&lt;/span&gt;]
date = 2018-12-12T09:49:04Z
description = &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
draft = &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;false&lt;/span&gt;
image = &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;/images/2018/12/IMG_7464.jpg&amp;#34;&lt;/span&gt;
slug = &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;streaming-data-from-oracle-into-kafka-december-2018&amp;#34;&lt;/span&gt;
tag = [&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;oracle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;cdc&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;debezium&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;goldengate&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;xstream&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;logminer&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;flashback&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;licence&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ksql&amp;#34;&lt;/span&gt;]
title = &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Streaming data from Oracle into Kafka (December 2018)&amp;#34;&lt;/span&gt;
&lt;span style=&#34;&#34;&gt;+++&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check out the nice &lt;a href=&#34;https://gohugo.io/content-management/syntax-highlighting/&#34;&gt;syntax highlighting&lt;/a&gt; there - using the &lt;code&gt;highlight&lt;/code&gt; &lt;a href=&#34;https://gohugo.io/content-management/shortcodes/&#34;&gt;shortcode&lt;/a&gt;. You can use standard code markup from asciidoc but so far I&amp;#8217;ve not found out how to get it to do the syntax highlighting on it too:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;echo &#39;This is a bash statement in asciidoc code markup&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;This is a bash statement in Hugo syntax highlighting markup&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_migrating_from_ghost&#34;&gt;Migrating from Ghost&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This was pretty easy:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Export from Ghost to get a json file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convert it with &lt;a href=&#34;https://github.com/jbarone/ghostToHugo&#34; class=&#34;bare&#34;&gt;https://github.com/jbarone/ghostToHugo&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./ghostToHugo -p ~/git/rmoff-blog/ --dateformat &#34;2006-01-02 15:04:05&#34; ~/Downloads/rmoffs-random-ramblings.ghost.2018-12-15.json&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;code&gt;content&lt;/code&gt; folder from Ghost and move the images folder to &lt;code&gt;static&lt;/code&gt; in your Hugo deployment. For me this was sufficient for all the paths to match up and all existing posts to retain their images&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ghost uses &lt;code&gt;tag&lt;/code&gt; for its tag URLs (e.g. &lt;code&gt;&lt;a href=&#34;https://rmoff.net/tag/goldengate/&#34; class=&#34;bare&#34;&gt;https://rmoff.net/tag/goldengate/&lt;/a&gt;&lt;/code&gt;), whereas Hugo uses &lt;code&gt;tags&lt;/code&gt;. You can change your config to match this (&lt;code&gt;tag: tags&lt;/code&gt; &amp;#8594; &lt;code&gt;tag: tag&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;taxonomies:
  category: categories
  tag: tag&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You then need to change each article&amp;#8217;s header to use &lt;code&gt;tag&lt;/code&gt; instead of &lt;code&gt;tags&lt;/code&gt; - a quick &lt;code&gt;sed&lt;/code&gt; will do this (another reason why markup is so powerful; you don&amp;#8217;t lose metadata and you can bulk-process files easily)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sed -i &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;.bak&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/^tags =/tag =/g&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_themes&#34;&gt;Themes&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There&amp;#8217;s a ton of themes available; I&amp;#8217;m using the rather nice &lt;a href=&#34;https://story.xaprb.com&#34;&gt;&lt;strong&gt;Story&lt;/strong&gt;&lt;/a&gt; theme from Baron Schwarz. Installing themes is a simple matter of adding it as a submodule into your existing Hugo folder&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git submodule add https://github.com/xaprb/story.git themes/story&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and then setting the theme name in your &lt;code&gt;config.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Different themes have different properties; this one reserves H1 and H2 for its own use, so you need to make sure your articles use H3 onwards. Again, easily fixed with &lt;code&gt;sed&lt;/code&gt; (all mine started at H2):
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sed -i &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;.bak&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/^##/###/g&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If I get round to it I might try and customise it myself to make the body just a bit wider; a lot of my articles include code samples and it&amp;#8217;d be useful to see a bit more of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can configure the top menu items using the &lt;code&gt;menu&lt;/code&gt; configuration in &lt;code&gt;config.yaml&lt;/code&gt; (mine is public &lt;a href=&#34;https://github.com/rmoff/rmoff-blog/blob/master/config.yaml#L39&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_wanna_see_under_the_covers&#34;&gt;Wanna See Under the Covers?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you want to see how this is all set up, the code is public.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rmoff/rmoff-blog/master/content/post/moving-my-blog-to-hugo.adoc&#34;&gt;This article&amp;#8217;s source (asciidoc)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rmoff/rmoff-blog/blob/master/config.yaml&#34;&gt;Hugo config&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rmoff/rmoff-blog/blob/master/deploy.sh&#34;&gt;Deploy script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/rmoff/rmoff-blog/&#34;&gt;Blog repo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_comments&#34;&gt;Comments&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I used Disqus for comments on my Ghost blog, but got very few really, and those I did half were asking for help and would be better posted on StackOverflow or elsewhere. I &lt;a href=&#34;https://rmoff.net/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/&#34;&gt;wrote an article recently&lt;/a&gt; in which I invited comments and discussion in the comments facility; I got zero there, and a good half dozen on Twitter, LinkedIn, and email.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;My conclusion is that comments are not really that useful nowadays; the social networks have pretty much replaced them—and so I&amp;#8217;ve not bothered to migrate them over here. I &lt;em&gt;think&lt;/em&gt; that they do actually work out of the box with Hugo and Disqus, but the theme I&amp;#8217;m using doesn&amp;#8217;t support them and that doesn&amp;#8217;t bother me. It just makes for a cleaner site.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
		</item>
		
		<item>
			<title>Pull new version of multiple Docker images</title>
			<link>https://rmoff.github.io/post/docker-pull-new-version/</link>
			<pubDate>Mon, 17 Dec 2018 17:44:02 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/post/docker-pull-new-version/</guid>
			<description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Tiny little snippet this one. Given a list of images:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker images|grep confluent
confluentinc/cp-enterprise-kafka                &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.0.0               d0c5528d7f99        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        600MB
confluentinc/cp-kafka                           &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.0.0               373a4e31e02e        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        558MB
confluentinc/cp-zookeeper                       &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.0.0               3cab14034c43        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        558MB
confluentinc/cp-ksql-server                     &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.0.0               691bc3c1991f        &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt; months ago        493MB
confluentinc/cp-ksql-cli                        &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.0.0               e521f3e787d6        &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt; months ago        488MB
…&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now there&amp;#8217;s a new version available, and you want to pull down all the latest ones for it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker images|grep &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;^confluentinc&amp;#34;&lt;/span&gt;|awk &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;|xargs -Ifoo docker pull foo:5.1.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Magic!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker images|grep confluent|grep &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.1
confluentinc/cp-ksql-server                     &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.1.0               db9d3eaf4624        &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; days ago          503MB
confluentinc/cp-ksql-cli                        &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.1.0               d8aab59c51d2        &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; days ago          499MB
confluentinc/ksql-examples                      &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.1.0               61b943a67fa8        &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; days ago          491MB
confluentinc/cp-enterprise-kafka                &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.1.0               9a27f2588978        &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; days ago          619MB
…&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
		</item>
		
		<item>
			<title>Docker Tips and Tricks with KSQL and Kafka</title>
			<link>https://rmoff.github.io/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</link>
			<pubDate>Sat, 15 Dec 2018 22:00:55 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</guid>
			<description>

&lt;p&gt;A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad…how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.&lt;/p&gt;

&lt;p&gt;So, here&amp;rsquo;s a collection of tricks I use with Docker and Docker Compose that might be useful, particularly for those working with Apache Kafka and Confluent Platform.&lt;/p&gt;

&lt;h3 id=&#34;wait-for-an-http-endpoint-to-be-available&#34;&gt;Wait for an HTTP endpoint to be available&lt;/h3&gt;

&lt;p&gt;Often a container will be &amp;lsquo;up&amp;rsquo; before it&amp;rsquo;s &lt;em&gt;actually&lt;/em&gt; up. So Docker Compose&amp;rsquo;s &lt;code&gt;depends_on&lt;/code&gt; dependencies don&amp;rsquo;t do everything we need here. For a service that exposes an HTTP endpoint (e.g. Kafka Connect, KSQL Server, etc) you can use this bash snippet to force a script to wait before continuing execution of something that requires the service to actually be ready and available:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; -e &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;\n\n⏳ Waiting for KSQL to be available before launching CLI\n&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#19177c&#34;&gt;$$&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;curl -s -o /dev/null -w %&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;http_code&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; http://ksql-server:8088/&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; -eq &lt;span style=&#34;color:#666&#34;&gt;000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;do&lt;/span&gt; 
  &lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; -e &lt;span style=&#34;color:#19177c&#34;&gt;$$&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;date&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;KSQL Server HTTP state: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#19177c&#34;&gt;$$&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;curl -s -o /dev/null -w %&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;http_code&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; http://ksql-server:8088/&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34; (waiting for 200)&amp;#34;&lt;/span&gt;
  sleep &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;done&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here this is assuming that KSQL Server is running on a host accessible as &lt;code&gt;ksql-server&lt;/code&gt; and on port &lt;code&gt;8088&lt;/code&gt;. You can use this trick if, for example, you only want to launch the KSQL CLI once the server is available:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker-compose &lt;span style=&#34;color:#008000&#34;&gt;exec&lt;/span&gt; ksql-cli bash -c &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;echo -e &amp;#34;\n\n⏳ Waiting for KSQL to be available before launching CLI\n&amp;#34;; while [ $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) -eq 000 ] ; do echo -e $(date) &amp;#34;KSQL Server HTTP state: &amp;#34; $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) &amp;#34; (waiting for 200)&amp;#34; ; sleep 5 ; done; ksql http://ksql-server:8088&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can also build it into a Docker Compose file as shown below.&lt;/p&gt;

&lt;h3 id=&#34;wait-for-a-particular-message-in-a-container-s-log&#34;&gt;Wait for a particular message in a container&amp;rsquo;s log&lt;/h3&gt;

&lt;p&gt;For the same reason as above—waiting for a service to be ready—you can use this trick built around &lt;code&gt;grep&lt;/code&gt; and bash&amp;rsquo;s &lt;a href=&#34;http://tldp.org/LDP/abs/html/process-sub.html&#34;&gt;&lt;em&gt;process substitution&lt;/em&gt;&lt;/a&gt;, which will make the script wait until the given phrase is found in the logs from Docker Compose:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#19177c&#34;&gt;CONNECT_HOST&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;connect-debezium
&lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; -e &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;\n--\n\nWaiting for Kafka Connect to start on &lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;$CONNECT_HOST&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt; … ⏳&amp;#34;&lt;/span&gt;
grep -q &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Kafka Connect started&amp;#34;&lt;/span&gt; &amp;lt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;docker-compose logs -f &lt;span style=&#34;color:#19177c&#34;&gt;$CONNECT_HOST&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;run-custom-code-before-launching-a-container-s-program&#34;&gt;Run custom code before launching a container&amp;rsquo;s program&lt;/h3&gt;

&lt;p&gt;Maybe you want to download a dependency, or move some files around, or do something. You could build a new &lt;code&gt;Dockerfile&lt;/code&gt;, but often you want to overlay on an existing standard image a slight tweak for a demo. With Docker Compose this is easy enough to do. First you need to figure out what command the container is going to run when it launches, which will either be through &lt;code&gt;Entrypoint&lt;/code&gt; or &lt;code&gt;Cmd&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker inspect --format&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{{.Config.Entrypoint}}&amp;#39;&lt;/span&gt; confluentinc/cp-ksql-server:5.0.1
&lt;span style=&#34;color:#666&#34;&gt;[]&lt;/span&gt;

$ docker inspect --format&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{{.Config.Cmd}}&amp;#39;&lt;/span&gt; confluentinc/cp-ksql-server:5.0.1
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;/etc/confluent/docker/run&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the above example it&amp;rsquo;s &lt;code&gt;/etc/confluent/docker/run&lt;/code&gt;. So now build this into your Docker Compose:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;ksql-server:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;confluentinc/cp-ksql-server:&lt;span style=&#34;color:#666&#34;&gt;5.0&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;depends_on:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kafka&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;environment:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;KSQL_BOOTSTRAP_SERVERS:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;kafka:&lt;span style=&#34;color:#666&#34;&gt;29092&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;KSQL_LISTENERS:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;http://&lt;span style=&#34;color:#666&#34;&gt;0.0&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;0.0&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;8088&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/bin/bash&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-c&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      mkdir -p /data/maxmind
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      cd /data/maxmind
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      curl https://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz | tar xz 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      /etc/confluent/docker/run &lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the additional bits will run, and &lt;em&gt;then&lt;/em&gt; the container&amp;rsquo;s intended process will actually run.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;- |&lt;/code&gt; is some YAML magic; we&amp;rsquo;re passing in three arguments to &lt;code&gt;command&lt;/code&gt;, and the &lt;code&gt;|&lt;/code&gt; tells YAML that the following lines are all part of the same entry. It makes for a much neater and easier to read file than trying to put everything into a single line as is sometimes done with &lt;code&gt;command&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;deploy-a-kafka-connect-connector-automatically&#34;&gt;Deploy a Kafka Connect connector automatically&lt;/h3&gt;

&lt;p&gt;In the above example, we run some code &lt;em&gt;before&lt;/em&gt; the container&amp;rsquo;s payload (the KSQL Server) starts because of a dependency on it. In the next example we&amp;rsquo;ll do it the other way around; launch the service and wait for it to start, and then run some more code. This is the pattern we need for deploying a Kafka Connect connector.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;kafka-connect:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;confluentinc/cp-kafka-connect:&lt;span style=&#34;color:#666&#34;&gt;5.0&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;environment:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;CONNECT_REST_PORT:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;18083&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;CONNECT_REST_ADVERTISED_HOST_NAME:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka-connect&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;[…]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;$PWD/scripts:/scripts&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;bash&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-c&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      /etc/confluent/docker/run &amp;amp; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      echo &amp;#34;Waiting for Kafka Connect to start listening on $$CONNECT_REST_ADVERTISED_HOST_NAME ⏳&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      while [ $$(curl -s -o /dev/null -w %{http_code} http://$$CONNECT_REST_ADVERTISED_HOST_NAME:$$CONNECT_REST_PORT/connectors) -eq 000 ] ; do 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;        echo -e $$(date) &amp;#34; Kafka Connect listener HTTP state: &amp;#34; $$(curl -s -o /dev/null -w %{http_code} http://$$CONNECT_REST_ADVERTISED_HOST_NAME:$$CONNECT_REST_PORT/connectors) &amp;#34; (waiting for 200)&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;        sleep 5 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      done
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      nc -vz $$CONNECT_REST_ADVERTISED_HOST_NAME $$CONNECT_REST_PORT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      echo -e &amp;#34;\n--\n+&amp;gt; Creating Kafka Connect Elasticsearch sink&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      /scripts/create-es-sink.sh 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      sleep infinity&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the command section, &lt;code&gt;$&lt;/code&gt; are replaced with &lt;code&gt;$$&lt;/code&gt; to avoid the error &lt;code&gt;Invalid interpolation format for &amp;quot;command&amp;quot; option&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sleep infinity&lt;/code&gt; is necessary, because we&amp;rsquo;ve sent the &lt;code&gt;/etc/confluent/docker/run&lt;/code&gt; process to a background thread (&lt;code&gt;&amp;amp;&lt;/code&gt;) and so the container will exit if the main &lt;code&gt;command&lt;/code&gt; finishes.&lt;/li&gt;
&lt;li&gt;The actual script to configure the connector is a &lt;code&gt;curl&lt;/code&gt; call in &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/ksql-atm-fraud-detection/scripts/create-es-sink.sh&#34;&gt;a separate file&lt;/a&gt;. You &lt;em&gt;could&lt;/em&gt; build this into the Docker Compose but it feels a bit yucky.&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You could combine both this and the technique above if you wanted to install a custom connector plugin before launching Kafka Connect, e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent-hub install --no-prompt confluentinc/kafka-connect-gcs:5.0.0 
/etc/confluent/docker/run
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;execute-a-ksql-script-through-ksql-cli&#34;&gt;Execute a KSQL script through ksql-cli&lt;/h3&gt;

&lt;p&gt;This Docker Compose snippet will run KSQL CLI and pass in a KSQL script for execution to it. The manual &lt;code&gt;EXIT&lt;/code&gt; is required because of a &lt;a href=&#34;https://github.com/confluentinc/ksql/issues/1327&#34;&gt;NPE bug&lt;/a&gt;. The advantage of this method vs running KSQL Server headless with a queries file passed to it is that you can still interact with KSQL this way, but can pre-build the environment to a certain state.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;ksql-cli:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;confluentinc/cp-ksql-cli:&lt;span style=&#34;color:#666&#34;&gt;5.0&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;depends_on:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ksql-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;$PWD/ksql-scripts/:/data/scripts/&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;entrypoint:&lt;span style=&#34;color:#bbb&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;/bin/bash&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-c&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      echo -e &amp;#34;\n\n⏳ Waiting for KSQL to be available before launching CLI\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      while [ $$(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) -eq 000 ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      do 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;        echo -e $$(date) &amp;#34;KSQL Server HTTP state: &amp;#34; $$(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) &amp;#34; (waiting for 200)&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;        sleep 5
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      done
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      echo -e &amp;#34;\n\n-&amp;gt; Running KSQL commands\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      cat /data/scripts/my-ksql-script.sql &amp;lt;(echo &amp;#39;EXIT&amp;#39;)| ksql http://ksql-server:8088
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      echo -e &amp;#34;\n\n-&amp;gt; Sleeping…\n&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:block;width:100%;background-color:#e5e5e5&#34;&gt;&lt;span style=&#34;color:#ba2121;font-style:italic&#34;&gt;      sleep infinity&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code&gt;sleep infinity&lt;/code&gt; is required, otherwise the container will exit since all of the defined &lt;code&gt;entrypoint&lt;/code&gt; will have executed.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Streaming data from Oracle into Kafka (December 2018)</title>
			<link>https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/</link>
			<pubDate>Wed, 12 Dec 2018 09:49:04 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/</guid>
			<description>

&lt;p&gt;&lt;em&gt;This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018. For a more detailed background to why and how at a broader level for all databases (not just Oracle) see &lt;a href=&#34;http://cnfl.io/kafka-cdc&#34;&gt;this blog&lt;/a&gt; and &lt;a href=&#34;https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;these slides&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;what-techniques-tools-are-there&#34;&gt;What techniques &amp;amp; tools are there?&lt;/h3&gt;

&lt;p&gt;As of December 2018, this is what the line-up looks like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query-based CDC&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/source_config_options.html&#34;&gt;JDBC Connector&lt;/a&gt; for Kafka Connect, polls the database for new or changed data based on an incrementing ID column and/or update timestamp&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Log-based CDC&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Oracle GoldenGate for Big Data&lt;/strong&gt; (license &lt;a href=&#34;https://www.oracle.com/assets/technology-price-list-070617.pdf&#34;&gt;$20k per CPU&lt;/a&gt;). Supports three &amp;ldquo;handlers&amp;rdquo;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-handler.htm#GADBD449&#34;&gt;Kafka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-81730248-AC12-438E-AF82-48C7002178EC&#34;&gt;Kafka Connect&lt;/a&gt; (runs in the OGG runtime, not a Connect worker. It doesn&amp;rsquo;t support the full Connect API, including Single Message Transforms.)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/goldengate/bd123210/gg-bd/GADBD/using-kafka-rest-proxy-handler.htm&#34;&gt;Kafka REST Proxy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Oracle XStream&lt;/strong&gt;  (requires &lt;strong&gt;Oracle GoldenGate&lt;/strong&gt; license &lt;a href=&#34;https://www.oracle.com/assets/technology-price-list-070617.pdf&#34;&gt;$17.5k per CPU&lt;/a&gt;).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Built on top of LogMiner. Oracle&amp;rsquo;s API for third-party applications wanting to stream events from the database.&lt;/li&gt;
&lt;li&gt;Currently beta implementation by &lt;a href=&#34;https://debezium.io/docs/connectors/oracle/&#34;&gt;&lt;strong&gt;Debezium&lt;/strong&gt;&lt;/a&gt; (0.9) with Kafka Connect&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Oracle Log Miner&lt;/strong&gt; No special license required (even available in Oracle XE).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/browse/DBZ-20&#34;&gt;Being&lt;/a&gt; &lt;a href=&#34;https://issues.jboss.org/browse/DBZ-137?_sscc=t&#34;&gt;considered&lt;/a&gt; by Debezium, and also implemented by &lt;a href=&#34;https://github.com/erdemcer/kafka-connect-oracle&#34;&gt;community connector here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Available commercially from Attunity, SQData, HVR, StreamSets, Striim etc&lt;/li&gt;
&lt;li&gt;DBVisit Replicate is no longer developed.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Triggers&lt;/strong&gt; to capture changes made to a table, write details of those changes to another database table, ingest that table into Kafka (e.g. with JDBC connector).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flashback&lt;/strong&gt; to show all changes to a given table between two points in time. &lt;a href=&#34;https://blog.pythian.com/streaming-oracle-kafka-stories-message-bus-stop/&#34;&gt;Implemented as a PoC by Stewart Bryson and Björn Rost&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-do-they-look-like-in-action&#34;&gt;What do they look like in action?&lt;/h3&gt;

&lt;p&gt;I did a recent talk at UK Oracle User Group TECH18 conference, presenting my talk &amp;ldquo;&lt;a href=&#34;https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;No More Silos: Integrating Databases and Apache Kafka&lt;/a&gt;&amp;rdquo;. As part of this I did a live demo showing the difference between using the JDBC Connector (query-based CDC) and the new Debezium/XStream option (log-based CDC). Here I&amp;rsquo;ll try and replicate the discussion and examples. You can also see previous articles that I&amp;rsquo;ve written showing &lt;a href=&#34;https://rmoff.net/tag/goldengate/&#34;&gt;GoldenGate in action&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find all of the code on the &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/no-more-silos-oracle.adoc&#34;&gt;demo-scene&lt;/a&gt; repository, runnable through Docker and Docker Compose. Simply clone the repo, and then run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd docker-compose
./scripts/setup.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The setup script does all of the rest, including bringing up Confluent Platform, and configuring the connectors. &lt;em&gt;You do have to &lt;a href=&#34;https://github.com/oracle/docker-images/blob/master/OracleDatabase/SingleInstance/README.md&#34;&gt;build the Oracle database docker image&lt;/a&gt; first&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&#34;setup&#34;&gt;Setup&lt;/h4&gt;

&lt;p&gt;Some notes on setup of each option:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;JDBC connector

&lt;ul&gt;
&lt;li&gt;The main thing you need here is the Oracle JDBC driver in the correct folder for the Kafka Connect JDBC connector.&lt;/li&gt;
&lt;li&gt;In the Docker Compose I use a pass-through volume (&lt;code&gt;db-leach&lt;/code&gt;) mounted from the database container to &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/docker-compose.yml#L77-L83&#34;&gt;copy the JDBC driver directly from the database container onto the Kafka Connect container&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You also need to make sure that the source table has an incrementing ID column and/or update timestamp column that can be used to identify changed rows. Without that you can only do a bulk load of the data each time.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Debezium connector

&lt;ul&gt;
&lt;li&gt;Requires a bunch of libraries (instant client and others), &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/docker-compose.yml#L146-L165&#34;&gt;copied from the database container using the same pass-through volume as above&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;This requires config work on the database, covered by the &lt;a href=&#34;https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle&#34;&gt;Debezium docs&lt;/a&gt; and done by the Docker script &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-setup-scripts/01_xstreams-setup.sh&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Each table needs to be configured (script &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-startup-scripts/01_create_customers.sh#L36-L37&#34;&gt;here&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;I hit problems with the Capture stopping with permission errors so &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-startup-scripts/03_restart_capture.sh&#34;&gt;automated its restart&lt;/a&gt; (hacky, I know)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The actual config of the two connectors is done in separate calls to Kafka Connect&amp;rsquo;s REST API (&lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/create-ora-source-jdbc.sh&#34;&gt;JDBC&lt;/a&gt; / &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/create-ora-source-debezium-xstream.sh&#34;&gt;Debezium&lt;/a&gt;). I run separate instances of Kafka Connect (in distributed mode, single node) just to keep troubleshooting simple, but in theory they could be in the same worker.&lt;/p&gt;

&lt;p&gt;The invocation of the above REST configuration scripts is managed by the master &lt;code&gt;setup.sh&lt;/code&gt; script, with &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/setup.sh#L14-L18&#34;&gt;some logic in it&lt;/a&gt; to wait until Kafka Connect is available before launching the config.&lt;/p&gt;

&lt;p&gt;You can validate that each connector is running by querying the REST API for the two Kafka Connect worker instances:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -s &amp;quot;http://localhost:18083/connectors&amp;quot;| jq &#39;.[]&#39;| xargs -I{connector_name} curl -s &amp;quot;http://localhost:18083/connectors/&amp;quot;{connector_name}&amp;quot;/status&amp;quot;| jq -c -M &#39;[.name,.connector.state,.tasks[].state]|join(&amp;quot;:|:&amp;quot;)&#39;| column -s : -t| sed &#39;s/\&amp;quot;//g&#39;| sort
ora-source-jdbc  |  RUNNING  |  RUNNING


$ curl -s &amp;quot;http://localhost:8083/connectors&amp;quot;| jq &#39;.[]&#39;| xargs -I{connector_name} curl -s &amp;quot;http://localhost:8083/connectors/&amp;quot;{connector_name}&amp;quot;/status&amp;quot;| jq -c -M &#39;[.name,.connector.state,.tasks[].state]|join(&amp;quot;:|:&amp;quot;)&#39;| column -s : -t| sed &#39;s/\&amp;quot;//g&#39;| sort
ora-source-debezium-xstream  |  RUNNING  |  RUNNING
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;initial-data-load&#34;&gt;Initial data load&lt;/h4&gt;

&lt;p&gt;In Oracle, check the source data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;COL FIRST_NAME FOR A15
COL LAST_NAME FOR A15
COL ID FOR 999
COL CLUB_STATUS FOR A12
COL UPDATE_TS FOR A29
SET LINESIZE 200
SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS, UPDATE_TS FROM CUSTOMERS;

  ID FIRST_NAME      LAST_NAME       CLUB_STATUS  UPDATE_TS
---- --------------- --------------- ------------ -----------------------------
  1 Rica            Blaisdell       bronze       11-DEC-18 05.16.00.000000 PM
  2 Ruthie          Brockherst      platinum     11-DEC-18 05.16.00.000000 PM
  3 Mariejeanne     Cocci           bronze       11-DEC-18 05.16.00.000000 PM
  4 Hashim          Rumke           platinum     11-DEC-18 05.16.00.000000 PM
  5 Hansiain        Coda            platinum     11-DEC-18 05.16.00.000000 PM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s see what&amp;rsquo;s in Kafka. I&amp;rsquo;m using KSQL here to inspect the data; you could use other Kafka console consumers if you&amp;rsquo;d rather.&lt;/p&gt;

&lt;p&gt;Launch KSQL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose exec ksql-cli ksql http://ksql-server:8088
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inspect the topics on the Kafka cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; LIST TOPICS;

Kafka Topic               | Registered | Partitions | Partition Replicas | Consumers | ConsumerGroups
-------------------------------------------------------------------------------------------------------
asgard.DEBEZIUM.CUSTOMERS | false      | 1          | 1                  | 0         | 0
ora-CUSTOMERS-jdbc        | false      | 1          | 1                  | 0         | 0
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The two topics listed are for the same table (&lt;code&gt;CUSTOMERS&lt;/code&gt;) from the Debezium and JDBC connectors respectively.&lt;/p&gt;

&lt;p&gt;Dump the contents:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Debezium/XStreams:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; PRINT &#39;asgard.DEBEZIUM.CUSTOMERS&#39; FROM BEGINNING;
Format:AVRO
12/11/18 5:16:40 PM UTC, , {&amp;quot;before&amp;quot;: null, &amp;quot;after&amp;quot;: {&amp;quot;ID&amp;quot;: 1, &amp;quot;FIRST_NAME&amp;quot;: &amp;quot;Rica&amp;quot;, &amp;quot;LAST_NAME&amp;quot;: &amp;quot;Blaisdell&amp;quot;, &amp;quot;EMAIL&amp;quot;: &amp;quot;rblaisdell0@rambler.ru&amp;quot;, &amp;quot;GENDER&amp;quot;: &amp;quot;Female&amp;quot;, &amp;quot;CLUB_STATUS&amp;quot;: &amp;quot;bronze&amp;quot;, &amp;quot;COMMENTS&amp;quot;: &amp;quot;Universal optimal hierarchy&amp;quot;, &amp;quot;CREATE_TS&amp;quot;: 1544548560283613, &amp;quot;UPDATE_TS&amp;quot;: 1544548560000000}, &amp;quot;source&amp;quot;: {&amp;quot;version&amp;quot;: &amp;quot;0.9.0.Alpha2&amp;quot;, &amp;quot;connector&amp;quot;: &amp;quot;oracle&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;asgard&amp;quot;, &amp;quot;ts_ms&amp;quot;: 1544548595164, &amp;quot;txId&amp;quot;: null, &amp;quot;scn&amp;quot;: 3014605, &amp;quot;snapshot&amp;quot;: true}, &amp;quot;op&amp;quot;: &amp;quot;r&amp;quot;, &amp;quot;ts_ms&amp;quot;: 1544548595189, &amp;quot;messagetopic&amp;quot;: &amp;quot;asgard.DEBEZIUM.CUSTOMERS&amp;quot;, &amp;quot;messagesource&amp;quot;: &amp;quot;Debezium CDC from Oracle on asgard&amp;quot;}
…
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;JDBC connector&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; PRINT &#39;ora-CUSTOMERS-jdbc&#39; FROM BEGINNING;
Format:AVRO
12/11/18 5:16:55 PM UTC, null, {&amp;quot;ID&amp;quot;: 1, &amp;quot;FIRST_NAME&amp;quot;: &amp;quot;Rica&amp;quot;, &amp;quot;LAST_NAME&amp;quot;: &amp;quot;Blaisdell&amp;quot;, &amp;quot;EMAIL&amp;quot;: &amp;quot;rblaisdell0@rambler.ru&amp;quot;, &amp;quot;GENDER&amp;quot;: &amp;quot;Female&amp;quot;, &amp;quot;CLUB_STATUS&amp;quot;: &amp;quot;bronze&amp;quot;, &amp;quot;COMMENTS&amp;quot;: &amp;quot;Universal optimal hierarchy&amp;quot;, &amp;quot;CREATE_TS&amp;quot;: 1544548560283, &amp;quot;UPDATE_TS&amp;quot;: 1544548560000, &amp;quot;messagetopic&amp;quot;: &amp;quot;ora-CUSTOMERS-jdbc&amp;quot;, &amp;quot;messagesource&amp;quot;: &amp;quot;JDBC Source Connector from Oracle on asgard&amp;quot;}
…
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each has the full contents of the source table (5 records, only first is shown above). We can actually use KSQL to easily query the topic directly if we want. First we declare each topic as the source for a stream:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SET &#39;auto.offset.reset&#39; = &#39;earliest&#39;;
CREATE STREAM CUSTOMERS_STREAM_DBZ_SRC WITH (KAFKA_TOPIC=&#39;asgard.DEBEZIUM.CUSTOMERS&#39;, VALUE_FORMAT=&#39;AVRO&#39;);
CREATE STREAM CUSTOMERS_STREAM_JDBC_SRC WITH (KAFKA_TOPIC=&#39;ora-CUSTOMERS-jdbc&#39;, VALUE_FORMAT=&#39;AVRO&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then query the JDBC-sourced Kafka topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC LIMIT 5;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the one from Debezium:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT AFTER-&amp;gt;ID AS ID, AFTER-&amp;gt;FIRST_NAME AS FIRST_NAME, AFTER-&amp;gt;LAST_NAME AS LAST_NAME, AFTER-&amp;gt;CLUB_STATUS AS CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
3 | Mariejeanne | Cocci | bronze
4 | Hashim | Rumke | platinum
5 | Hansiain | Coda | platinum
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I&amp;rsquo;m accessing nested attributes of the &lt;code&gt;AFTER&lt;/code&gt; object here using the &lt;code&gt;-&amp;gt;&lt;/code&gt; operator.&lt;/p&gt;

&lt;p&gt;The schema for both topics come from the Schema Registry, in which Kafka Connect automatically stores the schema for the data coming from Oracle and serialises the data into Avro. The great thing about this is in a consuming application, such as KSQL, the schema is already available and doesn&amp;rsquo;t have to be manually entered.&lt;/p&gt;

&lt;h4 id=&#34;insert&#34;&gt;INSERT&lt;/h4&gt;

&lt;p&gt;Insert a row in the Oracle database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; SET AUTOCOMMIT ON;
SQL&amp;gt;
SQL&amp;gt; INSERT INTO CUSTOMERS (FIRST_NAME,LAST_NAME,CLUB_STATUS) VALUES (&#39;Rick&#39;,&#39;Astley&#39;,&#39;Bronze&#39;);

1 row created.

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Straight away in the Kafka topics you&amp;rsquo;ll see a new row (in fact, if you have left the above &lt;code&gt;SELECT&lt;/code&gt; running you won&amp;rsquo;t need to rerun this, it&amp;rsquo;ll show the new row already):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;JDBC&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
42 | Rick | Astley | Bronze
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Debezium/XStream&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT AFTER-&amp;gt;ID AS ID, AFTER-&amp;gt;FIRST_NAME AS FIRST_NAME, AFTER-&amp;gt;LAST_NAME AS LAST_NAME, AFTER-&amp;gt;CLUB_STATUS AS CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
3 | Mariejeanne | Cocci | bronze
4 | Hashim | Rumke | platinum
5 | Hansiain | Coda | platinum
42 | Rick | Astley | Bronze
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So far, so same. Each captures an insert. Debezium from XStream and the database&amp;rsquo;s redo log, JDBC by polling the database for any rows with a newer &lt;code&gt;UPDATE_TS&lt;/code&gt; or higher &lt;code&gt;ID&lt;/code&gt; than the previous request.&lt;/p&gt;

&lt;h4 id=&#34;update&#34;&gt;UPDATE&lt;/h4&gt;

&lt;p&gt;This is where things get interesting. Let&amp;rsquo;s update the row in Oracle that we just created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; UPDATE CUSTOMERS SET CLUB_STATUS = &#39;Platinum&#39; where ID=42;

1 row updated.

Commit complete.
SQL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now check out the data in Kafka.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;JDBC is as before; the changed data row is available to us:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
42 | Rick | Astley | Bronze
42 | Rick | Astley | Platinum
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Debezium/XStream now comes into its own. As well as the new row of data, we can see what it was previously, through the &lt;code&gt;BEFORE&lt;/code&gt; nested object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT OP, AFTER-&amp;gt;ID, BEFORE-&amp;gt;CLUB_STATUS, AFTER-&amp;gt;CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
r | 1 | null | bronze
r | 2 | null | platinum
r | 3 | null | bronze
r | 4 | null | platinum
r | 5 | null | platinum
c | 42 | null | Bronze
u | 42 | Bronze | Platinum
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m just showing the before/after &lt;code&gt;CLUB_STATUS&lt;/code&gt; but all the other fields are also available. There&amp;rsquo;s also metadata about the change, including the type of operation in the &lt;code&gt;OP&lt;/code&gt; field (&lt;code&gt;r&lt;/code&gt;=read, i.e the initial snapshot, &lt;code&gt;c&lt;/code&gt;=create, &lt;code&gt;u&lt;/code&gt;=update)&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at the full payload of each message sent to Kafka:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;before&amp;quot;: {
    &amp;quot;ID&amp;quot;: 42,
    &amp;quot;FIRST_NAME&amp;quot;: &amp;quot;Rick&amp;quot;,
    &amp;quot;LAST_NAME&amp;quot;: &amp;quot;Astley&amp;quot;,
    &amp;quot;EMAIL&amp;quot;: null,
    &amp;quot;GENDER&amp;quot;: null,
    &amp;quot;CLUB_STATUS&amp;quot;: &amp;quot;Bronze&amp;quot;,
    &amp;quot;COMMENTS&amp;quot;: null,
    &amp;quot;CREATE_TS&amp;quot;: 1544000706681769,
    &amp;quot;UPDATE_TS&amp;quot;: 1544000706000000
  },
  &amp;quot;after&amp;quot;: {
    &amp;quot;ID&amp;quot;: 42,
    &amp;quot;FIRST_NAME&amp;quot;: &amp;quot;Rick&amp;quot;,
    &amp;quot;LAST_NAME&amp;quot;: &amp;quot;Astley&amp;quot;,
    &amp;quot;EMAIL&amp;quot;: null,
    &amp;quot;GENDER&amp;quot;: null,
    &amp;quot;CLUB_STATUS&amp;quot;: &amp;quot;Platinum&amp;quot;,
    &amp;quot;COMMENTS&amp;quot;: null,
    &amp;quot;CREATE_TS&amp;quot;: 1544000706681769,
    &amp;quot;UPDATE_TS&amp;quot;: 1544000742000000
  },
  &amp;quot;source&amp;quot;: {
    &amp;quot;version&amp;quot;: &amp;quot;0.9.0.Alpha2&amp;quot;,
    &amp;quot;connector&amp;quot;: &amp;quot;oracle&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;asgard&amp;quot;,
    &amp;quot;ts_ms&amp;quot;: 1544000742000,
    &amp;quot;txId&amp;quot;: &amp;quot;6.26.734&amp;quot;,
    &amp;quot;scn&amp;quot;: 2796831,
    &amp;quot;snapshot&amp;quot;: false
  },
  &amp;quot;op&amp;quot;: &amp;quot;u&amp;quot;,
  &amp;quot;ts_ms&amp;quot;: 1544000745823,
  &amp;quot;messagetopic&amp;quot;: &amp;quot;asgard.DEBEZIUM.CUSTOMERS&amp;quot;,
  &amp;quot;messagesource&amp;quot;: &amp;quot;Debezium CDC from Oracle on asgard&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So each time a change is made in the database, you get a full before/after snapshot of the record, plus a bunch of other metadata. This is great for applications processing inbound changes that need to know not just that something changed (&lt;em&gt;here&amp;rsquo;s the new record&lt;/em&gt;) but also exactly &lt;em&gt;what&lt;/em&gt; changed (before/after payloads) as well as &lt;em&gt;how&lt;/em&gt; (insert/update/etc.)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;delete&#34;&gt;DELETE&lt;/h4&gt;

&lt;p&gt;Delete a record from the source system&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; DELETE FROM CUSTOMERS WHERE ID=42;

1 row deleted.

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now check out the data in Kafka.&lt;/p&gt;

&lt;p&gt;JDBC is unchanged; it&amp;rsquo;s not captured any change to the source table. If you think about it, this is perfectly reasonable. How you query a database for a row that doesn&amp;rsquo;t exist?
Debezium/XStream, on the other hand, reports the data change precisely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT OP, AFTER-&amp;gt;ID, BEFORE-&amp;gt;CLUB_STATUS, AFTER-&amp;gt;CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
r | 1 | null | bronze
r | 2 | null | platinum
r | 3 | null | bronze
r | 4 | null | platinum
r | 5 | null | platinum
c | 42 | null | Bronze
u | 42 | Bronze | Platinum
d | null | Platinum | null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the &lt;code&gt;d&lt;/code&gt; record on the last row. This has captured the &lt;code&gt;DELETE&lt;/code&gt; operation perfectly. The &lt;code&gt;null&lt;/code&gt; in the right-most column is the current value for &lt;code&gt;AFTER-&amp;gt;CLUB_STATUS&lt;/code&gt;, and since the record is deleted, it has no value. We can see this even more clearly if we look at the raw payload for the whole record:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;before&amp;quot;: {
    &amp;quot;ID&amp;quot;: 42,
    &amp;quot;FIRST_NAME&amp;quot;: &amp;quot;Rick&amp;quot;,
    &amp;quot;LAST_NAME&amp;quot;: &amp;quot;Astley&amp;quot;,
    &amp;quot;EMAIL&amp;quot;: null,
    &amp;quot;GENDER&amp;quot;: null,
    &amp;quot;CLUB_STATUS&amp;quot;: &amp;quot;Platinum&amp;quot;,
    &amp;quot;COMMENTS&amp;quot;: null,
    &amp;quot;CREATE_TS&amp;quot;: 1544562543660463,
    &amp;quot;UPDATE_TS&amp;quot;: 1544562791000000
  },
  &amp;quot;after&amp;quot;: null,
  &amp;quot;source&amp;quot;: {
    &amp;quot;version&amp;quot;: &amp;quot;0.9.0.Alpha2&amp;quot;,
    &amp;quot;connector&amp;quot;: &amp;quot;oracle&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;asgard&amp;quot;,
    &amp;quot;ts_ms&amp;quot;: 1544563479000,
    &amp;quot;txId&amp;quot;: &amp;quot;9.32.712&amp;quot;,
    &amp;quot;scn&amp;quot;: 3042804,
    &amp;quot;snapshot&amp;quot;: true
  },
  &amp;quot;op&amp;quot;: &amp;quot;d&amp;quot;,
  &amp;quot;ts_ms&amp;quot;: 1544563482682,
  &amp;quot;messagetopic&amp;quot;: &amp;quot;asgard.DEBEZIUM.CUSTOMERS&amp;quot;,
  &amp;quot;messagesource&amp;quot;: &amp;quot;Debezium CDC from Oracle on asgard&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The full record that has been deleted is present in the &lt;code&gt;BEFORE&lt;/code&gt; object, but &lt;code&gt;AFTER&lt;/code&gt; is null—it&amp;rsquo;s been deleted, it no longer exists. It is an ex-record.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Bonus KSQL :&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re working with data in a Kafka topic. As it happens, KSQL is kinda useful for interogating that data, but at the end of the day it&amp;rsquo;s still just a Kafka topic. We can use KSQL to also help monitor the lag between the event in the source system (&lt;code&gt;source-&amp;gt;ms_ms&lt;/code&gt; as provided by Debezium) and the time recorded on the Kafka broker (the Kafka message timestamp, exposed in &lt;code&gt;ROWTIME&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT TIMESTAMPTOSTRING(ROWTIME, &#39;yyyy-MM-dd HH:mm:ss Z&#39;), \
&amp;gt;         OP, \
&amp;gt;         ROWTIME - SOURCE-&amp;gt;TS_MS AS LAG_MS \
&amp;gt; FROM CUSTOMERS_STREAM_DBZ_SRC;
2018-12-11 17:16:40 +0000 | r | 5829
2018-12-11 17:16:40 +0000 | r | 5806
2018-12-11 17:16:40 +0000 | r | 5802
2018-12-11 17:16:41 +0000 | r | 5805
2018-12-11 17:16:41 +0000 | r | 5805
2018-12-11 21:09:07 +0000 | c | 4104
2018-12-11 21:13:51 +0000 | u | 40734
2018-12-11 21:28:10 +0000 | d | 211438
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some of these lag times are pretty high; &lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1018&#34;&gt;DBZ-1018 Oracle connector is laggy&lt;/a&gt; is a JIRA currently tracking it.&lt;/p&gt;

&lt;p&gt;You can get the same data out of the JDBC connector, based on the &lt;code&gt;UPDATE_TS&lt;/code&gt; of the record itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT TIMESTAMPTOSTRING(ROWTIME, &#39;yyyy-MM-dd HH:mm:ss Z&#39;), \
&amp;gt;          ROWTIME - UPDATE_TS AS LAG_MS \
&amp;gt; FROM CUSTOMERS_STREAM_JDBC_SRC;
2018-12-11 17:16:55 +0000 | 55612
2018-12-11 17:16:55 +0000 | 55613
2018-12-11 17:16:55 +0000 | 55614
2018-12-11 17:16:55 +0000 | 55615
2018-12-11 17:16:55 +0000 | 55615
2018-12-11 21:09:04 +0000 | 1330
2018-12-11 21:13:12 +0000 | 1384
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll note here no available &lt;code&gt;OP&lt;/code&gt; information, and no row for the corresponding &lt;code&gt;DELETE&lt;/code&gt; action in the source database.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h3&gt;

&lt;p&gt;When you&amp;rsquo;re bringing data into Kafka, you need to remember the bigger picture. Dumping it into a topic alone is not enough. Well, it is, but your wider community of developers won&amp;rsquo;t thank you.&lt;/p&gt;

&lt;p&gt;You want to ensure that the schema of the source data is preserved, and that you&amp;rsquo;re using a serialisation method for the data that is suitable. Doing this means that developers can use the data without being tightly coupled to the producer of the data to understand how to use it.&lt;/p&gt;

&lt;p&gt;However you do this, it should be in a way that integrates with the broader Kafka and Confluent Platform ecosystem. One option is the Schema Registry and Avro. &lt;strong&gt;If you&amp;rsquo;re using Kafka Connect then this is available by default&lt;/strong&gt;, since you just select the Avro converter when you set up Kafka Connect.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;overview-of-the-pros-and-cons-of-each-technique&#34;&gt;Overview of the Pros and Cons of each technique&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Some of these are objective, others subjective. Others may indeed be plain false ;-) Discussion, comments, and corrections in the comment function below welcomed!&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query-based CDC&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Pros:

&lt;ul&gt;
&lt;li&gt;Easy to set up&lt;/li&gt;
&lt;li&gt;Minimal privileges required&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Cons:

&lt;ul&gt;
&lt;li&gt;No capture of DELETEs&lt;/li&gt;
&lt;li&gt;No capture of before-state in an UPDATE&lt;/li&gt;
&lt;li&gt;No guarantee that &lt;em&gt;all&lt;/em&gt; events are captured; only the state at the time of polling&lt;/li&gt;
&lt;li&gt;Increased load on the source DB due to polling (and/or unacceptable latency in capturing the events if polling interval too high)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Another con one could add to query-based CDC is that it needs support by the model (update column); I&amp;#39;ve blogged about here: &lt;a href=&#34;https://t.co/DDYV62DIVF&#34;&gt;https://t.co/DDYV62DIVF&lt;/a&gt;. Log-based CDC also can give you additional metadata like TX ids, causing queries (for some DBs) etc.&lt;/p&gt;&amp;mdash; Gunnar Morling (@gunnarmorling) &lt;a href=&#34;https://twitter.com/gunnarmorling/status/1073323155370987521?ref_src=twsrc%5Etfw&#34;&gt;December 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Log-based CDC&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oracle GoldenGate for Big Data (&amp;ldquo;OGGBD&amp;rdquo;)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Low latency&lt;/li&gt;
&lt;li&gt;Minimal impact on the source&lt;/li&gt;
&lt;li&gt;Captures every event&lt;/li&gt;
&lt;li&gt;Capture before/after state in UPDATEs&lt;/li&gt;
&lt;li&gt;Captures DELETEs include prior state&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Requires sysadmin privileges to install&lt;/li&gt;
&lt;li&gt;Relatively complex to set up (compared to JDBC connector)&lt;/li&gt;
&lt;li&gt;License cost&lt;/li&gt;
&lt;li&gt;Kafka Connect support is not fully compliant with the Kafka Connect API which may matter if you want to use things like custom converters, Single Message Transform, and so on.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Oracle XStream&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Marginally cheaper than OGGBD&lt;/li&gt;
&lt;li&gt;Debezium is open source and is under active development&lt;/li&gt;
&lt;li&gt;Low latency&lt;/li&gt;
&lt;li&gt;Minimal impact on the source&lt;/li&gt;
&lt;li&gt;Captures every event&lt;/li&gt;
&lt;li&gt;Capture before/after state in UPDATEs&lt;/li&gt;
&lt;li&gt;Captures DELETEs include prior state&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Requires sysadmin privileges to install&lt;/li&gt;
&lt;li&gt;Relatively complex to set up (compared to JDBC connector)&lt;/li&gt;
&lt;li&gt;License cost&lt;/li&gt;
&lt;li&gt;Some issues with current Debezium implementation:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1022&#34;&gt;DBZ-1022 org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for field insertion, found: null&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1019&#34;&gt;DBZ-1019 java.lang.IllegalArgumentException: timeout value is negative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1018&#34;&gt;DBZ-1018 Oracle connector is laggy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1014&#34;&gt;DBZ-1014 Oracle connector requires libaio to be installed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1013&#34;&gt;DBZ-1013 Include Instant Client in Docker build for Oracle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://issues.jboss.org/projects/DBZ/issues/DBZ-1012&#34;&gt;DBZ-1012 Debezium doesn&amp;rsquo;t report absence of Instant Client&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Oracle Log Miner&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No additional license cost&lt;/li&gt;
&lt;li&gt;Full access to events?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cons&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Relatively complex to set up (compared to JDBC connector)&lt;/li&gt;
&lt;li&gt;Requires code to parse events. How infallible is this?&lt;/li&gt;
&lt;li&gt;Could be inefficient if only capturing events from a small proportion of the DB activity (has to scan all REDO log still). Is this also a problem with XStream?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Triggers&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No licence cost&lt;/li&gt;
&lt;li&gt;Entirely customisable&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Completely bespoke code to develop and maintain.&lt;/li&gt;
&lt;li&gt;Tightly coupled to source application&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flashback&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pro + Con:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Requires EE licence—but this is something users are more likely to have already than OGG/OGGBD&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unknown:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What granularity of data can be retrieved?&lt;/li&gt;
&lt;li&gt;Impact on the DB from polling?&lt;/li&gt;
&lt;li&gt;Unclear how much bespoke coding this would require per integration?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;There are 2 flashback features:&lt;br&gt;Flashback transaction query shows transactions from redo, similar to log miner.&lt;br&gt;Flashback version query shows previous versions, from undo, within undo_retention. Allowed in all editions. I think this is the one you mention.&lt;/p&gt;&amp;mdash; Franck Pachot (@FranckPachot) &lt;a href=&#34;https://twitter.com/FranckPachot/status/1073323013750317056?ref_src=twsrc%5Etfw&#34;&gt;December 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oracle GoldenGate for Big Data&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oracle.com/middleware/data-integration/goldengate/big-data/&#34;&gt;https://www.oracle.com/middleware/data-integration/goldengate/big-data/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmoff.net/tag/goldengate/&#34;&gt;https://rmoff.net/tag/goldengate/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/&#34;&gt;https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/connector/oracle-goldengate/&#34;&gt;https://www.confluent.io/connector/oracle-goldengate/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Oracle XStream / Debezium&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en/database/oracle/oracle-database/18/xstrm/index.html&#34;&gt;https://docs.oracle.com/en/database/oracle/oracle-database/18/xstrm/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle&#34;&gt;https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LogMiner&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en/database/oracle/oracle-database/18/sutil/oracle-logminer-utility.html#GUID-2EAA593B-DC09-4D30-87EB-34819FC68B3D&#34;&gt;https://docs.oracle.com/en/database/oracle/oracle-database/18/sutil/oracle-logminer-utility.html#GUID-2EAA593B-DC09-4D30-87EB-34819FC68B3D&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;try-it-out&#34;&gt;Try it out!&lt;/h3&gt;

&lt;p&gt;You can find all of the code used in this article &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/&#34;&gt;on github here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;feedback&#34;&gt;Feedback?&lt;/h3&gt;

&lt;p&gt;Some of these are objective, others subjective. Others may indeed be plain false ;-) Discussion, comments, and corrections in the comment function below welcomed!&lt;/p&gt;

&lt;p&gt;For &lt;em&gt;help&lt;/em&gt; in getting this working, the best place to head is the Confluent Community:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href=&#34;https://groups.google.com/forum/#!forum/confluent-platform&#34;&gt;https://groups.google.com/forum/#!forum/confluent-platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Slack group: &lt;a href=&#34;https://slackpass.io/confluentcommunity&#34;&gt;https://slackpass.io/confluentcommunity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s also a good Debezium community:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Mailing list: &lt;a href=&#34;https://groups.google.com/forum/#!forum/debezium&#34;&gt;https://groups.google.com/forum/#!forum/debezium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gitter: &lt;a href=&#34;https://gitter.im/debezium/user&#34;&gt;https://gitter.im/debezium/user&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;updates-comments&#34;&gt;Updates &amp;amp; Comments&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Adam Leszczyński has built a (non-Kafka Connect) source: &lt;a href=&#34;https://www.bersler.com/blog/openlogreplicator-first-log-based-open-source-oracle-to-kafka-replication/&#34;&gt;https://www.bersler.com/blog/openlogreplicator-first-log-based-open-source-oracle-to-kafka-replication/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;From Tanel Poder:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;1) Materialized view logs&lt;br&gt;&lt;br&gt;2) JDBC table change notifications (limited use as it only stores a few rowids at fine grained before switching to table level notifications)&lt;br&gt;&lt;br&gt;Btw i thought attunity had a binary reader?&lt;/p&gt;&amp;mdash; Tanel Poder (@TanelPoder) &lt;a href=&#34;https://twitter.com/TanelPoder/status/1073335673455894531?ref_src=twsrc%5Etfw&#34;&gt;December 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Tools I Use: iPad Pro</title>
			<link>https://rmoff.github.io/2018/12/11/tools-i-use-ipad-pro/</link>
			<pubDate>Tue, 11 Dec 2018 15:12:15 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/11/tools-i-use-ipad-pro/</guid>
			<description>

&lt;p&gt;I&amp;rsquo;ve written recently about &lt;a href=&#34;https://rmoff.net/2018/12/10/so-how-do-you-make-those-cool-diagrams/&#34;&gt;how I create the diagrams in my blog posts and talks&lt;/a&gt;, and from discussions around that, a couple of people were interested more broadly in how I use my iPad Pro. So, on the basis that if two people are interested maybe others are (and if no-one else is, I have a copy-and-paste answer to give to those two people) here we go.&lt;/p&gt;

&lt;p&gt;### Kit&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;iPad Pro 10.5&amp;rdquo; (2018)

&lt;ul&gt;
&lt;li&gt;256GB model&lt;/li&gt;
&lt;li&gt;Apple Pencil&lt;/li&gt;
&lt;li&gt;Apple Keyboard&lt;/li&gt;
&lt;li&gt;iPad wallet/protector&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.co.uk/gp/product/B073X5BML2&#34;&gt;Matte screen protector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;I travel quite a lot for work, so want something with a decent battery life for stuff like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;emails&lt;/li&gt;
&lt;li&gt;twitters&lt;/li&gt;
&lt;li&gt;presentations (minimum: reviewing, bonus: preparing &amp;amp; editing)&lt;/li&gt;
&lt;li&gt;watching films, listening to music&lt;/li&gt;
&lt;li&gt;writing and reviewing blog posts and articles&lt;/li&gt;
&lt;li&gt;photos - importing from camera, light editing/effects&lt;/li&gt;
&lt;li&gt;reading articles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is always going to be in addition to my laptop, as I usually need stuff like Docker for building my demos and blogs. But, it&amp;rsquo;s nice not always needing to get the laptop out, nor have to worry quite so much where the next power socket is going to come from.&lt;/p&gt;

&lt;h3 id=&#34;the-keyboard&#34;&gt;The keyboard&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Previously I had an iPad mini with the fantastic &lt;a href=&#34;https://images-na.ssl-images-amazon.com/images/I/41KJMg6MWGL.jpg&#34;&gt;4mm slim Anker keyboard&lt;/a&gt;, which sadly I broke and is no longer in production (the &lt;a href=&#34;https://www.amazon.co.uk/gp/product/B00PIMRCFG&#34;&gt;new version&lt;/a&gt; was not nearly as good). I replaced it with the Logitech keyboard which was big and clunky and made the iPad anything but Mini. So, perhaps I was biased in choosing a keyboard for my iPad Pro.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For the iPad Pro I weighed up between the Apple keyboard and the Logitech one. The tl;dr of online reviews seemed to be that the Logitech one had all the bells and whistles (backlight, etc), whilst the Apple one—being Apple—was less featured but got the job of being a keyboard done and got it done very well.&lt;/p&gt;

&lt;p&gt;I went with the Apple keyboard and haven&amp;rsquo;t regretted it once. It&amp;rsquo;s lovely to type on, integrates perfectly with the iPad (when connected, switches straight over to it, when not, falls back to the iOS soft keyboard). When folded away the keyboard acts as a case/screen protector itself. I wouldn&amp;rsquo;t fancy the iPad&amp;rsquo;s chances being dropped—there&amp;rsquo;s no corner buffers to save the screen, for example—but (until I do it, probably) this is a good trade off for bulk vs function.&lt;/p&gt;

&lt;p&gt;My only gripe with the keyboard is that there is an utterly pointless (for me) Keyboard key in the bottom left, whose only purpose is to throw up an emoji keyboard when I hit it by mistake&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/12/emoji-keyboard.png.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-pencil&#34;&gt;The pencil&lt;/h3&gt;

&lt;p&gt;I used a bog standard stylus with my iPad Mini previously, and so from that my expectations for the Apple Pencil were pretty low. I thought I&amp;rsquo;d be able to write on the screen but only at a low resolution, kind of fun but ultimately useless. The Apple Pencil is anything but useless. It&amp;rsquo;s amazingly high resolution, responsive, and a pleasure to use. It integrates with Notes for note taking (duh!), and you can pretty credibly leave your notebook behind and use this instead (or pair it with the keyboard for written notes and the pencil for sketching diagrams etc).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/12/pencil-notes.JPG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It also integrates with Keynote which is pretty nifty for sketching diagrams directly into slide decks.
A colleague recommended putting a matte screen protector on the iPad to make it nicer to draw on, and I&amp;rsquo;d second this advice. It makes it feel more &amp;lsquo;papery&amp;rsquo; and less like a glass screen.&lt;/p&gt;

&lt;p&gt;Drawing apps: &lt;strong&gt;Paper&lt;/strong&gt;, &lt;strong&gt;Notes&lt;/strong&gt;, &lt;strong&gt;Grafio 3&lt;/strong&gt;, &lt;strong&gt;Adobe Draw&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;writing-articles&#34;&gt;Writing articles&lt;/h3&gt;

&lt;p&gt;The Google Docs iPad app is fairly good for offline reviewing of articles. The review functionality (suggested edits &amp;amp; comments) is pretty clunky but passable if you have to. Given the choice, I&amp;rsquo;ll still tend to use my Mac for this.&lt;/p&gt;

&lt;p&gt;For writing articles from scratch (like this one) I use &lt;strong&gt;Working Copy&lt;/strong&gt; and asciidoc or markdown. It integrates with git (which I use for my own version control of docs) and supports previewing and editing of both languages (plus a ton of others). &lt;strong&gt;Textastic&lt;/strong&gt; is also a good option, and previously I&amp;rsquo;ve used &lt;strong&gt;Editorial&lt;/strong&gt; a lot (although IIRC that doesn&amp;rsquo;t support asciidoc).&lt;/p&gt;

&lt;h3 id=&#34;films-and-music&#34;&gt;Films and Music&lt;/h3&gt;

&lt;p&gt;I use Plex for taking content from my home server with me on the move. You have to pay a subscription for offline use, but it works great. For example, you set how many unwatched episodes of a series you want at any time, and it tracks when you&amp;rsquo;ve watched something and downloads the next one ready for you. With the 256GB iPad, I&amp;rsquo;m not anywhere near filling it yet but it&amp;rsquo;s useful if space is tight.&lt;/p&gt;

&lt;h3 id=&#34;photos&#34;&gt;Photos&lt;/h3&gt;

&lt;p&gt;The Apple Photos app is fine for importing and organising photos, and then I use Snapseed for editing and applying effects. I am very much an amateur here but it works for me :)&lt;/p&gt;

&lt;h3 id=&#34;email&#34;&gt;Email&lt;/h3&gt;

&lt;p&gt;Gmail app, works fine offline too.&lt;/p&gt;

&lt;h3 id=&#34;presentations&#34;&gt;Presentations&lt;/h3&gt;

&lt;p&gt;I am Keynote through-and-through, and the iPad is great for that. With &amp;ldquo;handoff&amp;rdquo; I can even have a presentation open on my Mac, modify a slide, open my iPad and modify the same slide (perhaps using the Apple Pencil to mark it up in some way) and then continue on my Mac.
The Keynote app keeps getting better and better, and I can quite happily put together a set of slides using the iPad Pro alone. Here I think the keyboard makes the real difference. Without it, it becomes more of a &amp;ldquo;because I can&amp;rdquo; than a &amp;ldquo;because why wouldn&amp;rsquo;t I&amp;rdquo; task to use the iPad.
I&amp;rsquo;ve yet present from the iPad but do have the connector for the time when the opportunity presents itself (i.e. I don&amp;rsquo;t need to do a live demo and I pluck up the courage ;-) )&lt;/p&gt;

&lt;h3 id=&#34;twitters&#34;&gt;Twitters&lt;/h3&gt;

&lt;p&gt;Tweetdeck, same as on my Mac and iPhone. Keeps track of where I&amp;rsquo;ve read up to in my timeline across devices. Doesn&amp;rsquo;t pollute my timeline with promoted crap, tweets from people I don&amp;rsquo;t follow, or out-of-order tweets.&lt;/p&gt;

&lt;p&gt;I also use Buffer to schedule posts, for two reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stuff that I&amp;rsquo;ve read and want to share&lt;/li&gt;
&lt;li&gt;Scheduling tweets to promote my talks at a conference (&amp;ldquo;starting in an hour in room 42…&amp;rdquo; etc)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;reading-articles&#34;&gt;Reading articles&lt;/h3&gt;

&lt;p&gt;Long-time Pocket user. Well, collector of unread articles anyway. Not very good at actually reading them. But they&amp;rsquo;re all there, offline and ready for me when I do actually get to reading some.&lt;/p&gt;

&lt;h3 id=&#34;questions&#34;&gt;Questions?&lt;/h3&gt;

&lt;p&gt;Leave a comment below, or &lt;a href=&#34;https://twitter.com/rmoff/&#34;&gt;tweet me&lt;/a&gt; and I&amp;rsquo;ll update the article :)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>So how DO you make those cool diagrams?</title>
			<link>https://rmoff.github.io/2018/12/10/so-how-do-you-make-those-cool-diagrams/</link>
			<pubDate>Mon, 10 Dec 2018 12:38:18 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/10/so-how-do-you-make-those-cool-diagrams/</guid>
			<description>

&lt;p&gt;I &lt;a href=&#34;https://www.confluent.io/blog/author/robin/&#34;&gt;write&lt;/a&gt; and &lt;a href=&#34;http://rmoff.net/presentations/&#34;&gt;speak&lt;/a&gt; lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How do you make those cool diagrams?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So here&amp;rsquo;s a short, and longer, answer!&lt;/p&gt;

&lt;p&gt;### tl;dr&lt;/p&gt;

&lt;p&gt;An iOS app called &lt;a href=&#34;https://paper.bywetransfer.com/&#34;&gt;Paper, from a company called FiftyThree&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;so-how-do-you-make-those-cool-diagrams&#34;&gt;So, how DO you make those cool diagrams?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Disclaimer: This is a style that I have copied straight from my esteemed colleagues at Confluent, including Neha Narkhede and Ben Stopford, as well as others including Martin Kleppmann.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;equipment&#34;&gt;Equipment&lt;/h4&gt;

&lt;p&gt;I use an iPad Pro (10.5&amp;rdquo;), with Apple Pencil, and &lt;a href=&#34;https://paper.bywetransfer.com/&#34;&gt;Paper from FiftyThree/WeTransfer&lt;/a&gt;. I also have a &lt;a href=&#34;https://www.amazon.co.uk/gp/product/B073X5BML2&#34;&gt;matte screen protector&lt;/a&gt; on the iPad which makes it feel nicer to draw on (less &amp;ldquo;skiddy&amp;rdquo;). I previously used an iPad mini with a cheapy stylus which did the job but was no use for detailed diagrams (and certainly not for actual writing)&lt;/p&gt;

&lt;h4 id=&#34;process&#34;&gt;Process&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Sketch out the diagram in Paper, leaving space for text.
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/12/diagrams.png&#34; alt=&#34;&#34; /&gt;
I rarely try to do actual handwriting in Paper, because (a) my handwriting is pretty dreadful and (b) re-editing that is a massive PITA—there are enough nice handwriting fonts to make it fairly pointless.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;From Paper, export the image as a PNG and airdrop it to my Mac (or just open in Keynote on the iPad Pro)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On the Mac, use Keynote to overlay text markup up on the diagram. I use &lt;strong&gt;Indie Flower&lt;/strong&gt; font.
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/12/keynote-1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rinse and repeat, going back to Paper as required to make more space for text, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On Keynote I&amp;rsquo;ll sometimes create white boxes to overlay and hide parts of the diagram if I just want to show a particular component.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Paper is not vector-based, and editing existing diagrams gets pretty clunky.&lt;/p&gt;

&lt;h4 id=&#34;other-tools&#34;&gt;Other tools&lt;/h4&gt;

&lt;p&gt;Since getting the iPad Pro with Pencil, I&amp;rsquo;ve tried:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Apple Notes&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Pretty good, but not been able to get the same style as Paper with things like washes/smudges&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keynote&lt;/strong&gt; itself has support for drawing, similar to that provided by Apple Notes. Useful for things like hand-drawn annotations on slides (e.g. circling points of interest) but as with Notes, not been able to fully reproduce the Paper style.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grafio 3&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Nice for precise diagrams, but not &amp;lsquo;hand drawn&amp;rsquo; style&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adobe Draw&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;I got pretty excited when I saw this as it&amp;rsquo;s apparently vector-based, but with hand-drawing support. That said, I&amp;rsquo;m either missing something or it is very primitive when it comes to support for actually building diagrams, editing content, etc. Seems more like a children&amp;rsquo;s drawing program. What am I missing?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Get mtr working on the Mac</title>
			<link>https://rmoff.github.io/2018/12/08/get-mtr-working-on-the-mac/</link>
			<pubDate>Sat, 08 Dec 2018 12:45:40 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/08/get-mtr-working-on-the-mac/</guid>
			<description>

&lt;h3 id=&#34;install&#34;&gt;Install&lt;/h3&gt;

&lt;p&gt;Not sure why the &lt;code&gt;brew&lt;/code&gt; doesn&amp;rsquo;t work as it used to, but here&amp;rsquo;s how to get it working:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install mtr
sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr /usr/local/bin/mtr
sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr-packet /usr/local/bin/mtr-packet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;(If you don&amp;rsquo;t do the two symbolic links (&lt;code&gt;ln&lt;/code&gt;) you&amp;rsquo;ll get &lt;code&gt;mtr: command not found&lt;/code&gt; or &lt;code&gt;mtr: Failure to start mtr-packet: Invalid argument&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;run&#34;&gt;Run&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;sudo mtr google.com
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect CLI tricks</title>
			<link>https://rmoff.github.io/2018/12/03/kafka-connect-cli-tricks/</link>
			<pubDate>Mon, 03 Dec 2018 14:50:45 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/12/03/kafka-connect-cli-tricks/</guid>
			<description>

&lt;p&gt;I do lots of work with Kafka Connect, almost entirely in &lt;a href=&#34;https://docs.confluent.io/current/connect/concepts.html#distributed-workers&#34;&gt;Distributed mode&lt;/a&gt;—even just with 1 node -&amp;gt; makes scaling out much easier when/if needed. Because I&amp;rsquo;m using Distributed mode, I use the &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;Kafka Connect REST API&lt;/a&gt; to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m showing the commands split with a line continuation character (&lt;code&gt;\&lt;/code&gt;) but you can of course run them on a single line. You might also choose to get fancy and set the Connect host and port as environment variables etc, but I leave that as an exercise for the reader :)&lt;/p&gt;

&lt;script id=&#34;asciicast-jYF38DsTB4PQbdoittTz1QARn&#34; src=&#34;https://asciinema.org/a/jYF38DsTB4PQbdoittTz1QARn.js&#34; async&gt;&lt;/script&gt;

&lt;h3 id=&#34;list-all-running-connectors&#34;&gt;List all running connectors:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s &amp;quot;http://localhost:8083/connectors&amp;quot;| \
jq &#39;.[]&#39;| \
xargs -I{connector_name} curl -s &amp;quot;http://localhost:8083/connectors/&amp;quot;{connector_name}&amp;quot;/status&amp;quot;| jq -c -M &#39;[.name,.connector.state,.tasks[].state]|join(&amp;quot;:|:&amp;quot;)&#39;| \
column -s : -t| \
sed &#39;s/\&amp;quot;//g&#39;| \
sort
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;selectively-delete-a-connector&#34;&gt;Selectively delete a connector&lt;/h3&gt;

&lt;p&gt;(h/t to &lt;a href=&#34;https://twitter.com/madewithtea/status/1068467440202514432&#34;&gt;@madewithtea&lt;/a&gt; for the idea of using &lt;code&gt;peco&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s &amp;quot;http://localhost:8083/connectors&amp;quot;| \
jq &#39;.[]&#39;| \
peco | \
xargs -I{connector_name} curl -s -XDELETE &amp;quot;http://loc
alhost:8083/connectors/&amp;quot;{connector_name}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;delete-all-connectors&#34;&gt;Delete all connectors&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;CAUTION&lt;/strong&gt; with this one, as it will delete all your connectors!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -s &amp;quot;http://localhost:8083/connectors&amp;quot;| \
jq &#39;.[]&#39;| \
xargs -I{connector_name} curl -s -XDELETE &amp;quot;http://localhost:8083/connectors/&amp;quot;{connector_name}
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Logging in as root on Oracle Database Docker image</title>
			<link>https://rmoff.github.io/2018/11/30/logging-in-as-root-on-oracle-database-docker-image/</link>
			<pubDate>Fri, 30 Nov 2018 12:13:41 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/11/30/logging-in-as-root-on-oracle-database-docker-image/</guid>
			<description>&lt;p&gt;tl;dr:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker exec --interactive \
            --tty \
            --user root \
            --workdir / \
            oracle-container-name bash
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Using Oracle&amp;rsquo;s &lt;a href=&#34;https://github.com/oracle/docker-images/blob/master/OracleDatabase/SingleInstance/README.md&#34;&gt;Docker database image&lt;/a&gt; I wanted to install some additional apps, without modifying the &lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Connect to the container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker exec --interactive --tty docker-compose_oracle_1_479e7fa05ab5 bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No sudo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@a37d6e99353b ~]$ sudo whoami
bash: sudo: command not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Googled, found the the &lt;code&gt;--user&lt;/code&gt; flag for Docker, tried that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker exec --interactive --tty --user root docker-compose_oracle_1_479e7fa05ab5 bash
OCI runtime exec failed: exec failed: container_linux.go:348: starting container process caused &amp;quot;chdir to cwd (\&amp;quot;/home/oracle\&amp;quot;) set in config.json failed: permission denied&amp;quot;: unknown
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Evidently, the Docker image tries to change directory to the Oracle home folder which Docker&amp;rsquo;s not happy doing as another user (even though it&amp;rsquo;s &lt;code&gt;root&lt;/code&gt;?).&lt;/p&gt;

&lt;p&gt;Googled some more, found the &lt;code&gt;--workdir&lt;/code&gt; flag to override the &lt;code&gt;WORKDIR&lt;/code&gt; setting of &lt;a href=&#34;https://github.com/oracle/docker-images/blob/master/OracleDatabase/SingleInstance/dockerfiles/12.2.0.1/Dockerfile#L105&#34;&gt;the Dockerfile from which the image is built&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker exec --interactive --tty --user root --workdir / docker-compose_oracle_1_479e7fa05ab5 bash
bash-4.2# whoami
root
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>ERROR: Invalid interpolation format for &#34;command&#34; option in service…</title>
			<link>https://rmoff.github.io/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/</link>
			<pubDate>Tue, 20 Nov 2018 17:47:54 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/</guid>
			<description>&lt;p&gt;Doing some funky Docker Compose stuff, including:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;…
kafkacat:
  image: confluentinc/cp-kafkacat:latest
  depends_on:
    - kafka
  command: 
    - bash 
    - -c 
    - |
      echo &amp;quot;Waiting for Kafka ⏳&amp;quot;
      cub kafka-ready -b kafka:29092 1 300 &amp;amp;&amp;amp; 
      while [ 1 -eq 1 ]
        do awk &#39;{print $0;system(&amp;quot;sleep 0.5&amp;quot;);}&#39; /data/data.json | \
            kafkacat -b kafka:29092 -P -t purchases
        done
  volumes: 
    - $PWD:/data
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oh noes! Error!&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ERROR: Invalid interpolation format for &amp;quot;command&amp;quot; option in service &amp;quot;kafkacat&amp;quot;: &amp;quot;echo &#39;Waiting for Kafka&#39;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The cause? &lt;code&gt;$&lt;/code&gt; in the embedded &lt;code&gt;command&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;…
do awk &#39;{print $0;system(&amp;quot;sleep 0.5&amp;quot;);}&#39; /data/data.json | \
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fix? Double it up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafkacat:
  image: confluentinc/cp-kafkacat:latest
  depends_on:
    - kafka
  command: 
    - bash 
    - -c 
    - |
      echo &amp;quot;Waiting for Kafka ⏳&amp;quot;
      cub kafka-ready -b kafka:29092 1 300 &amp;amp;&amp;amp; 
      while [ 1 -eq 1 ]
        do awk &#39;{print $$0;system(&amp;quot;sleep 0.5&amp;quot;);}&#39; /data/data.json | \
            kafkacat -b kafka:29092 -P -t purchases
        done
  volumes: 
    - $PWD:/data
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Flatten CDC records in KSQL</title>
			<link>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</link>
			<pubDate>Thu, 11 Oct 2018 15:13:59 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</guid>
			<description>

&lt;h3 id=&#34;the-problem-nested-messages-in-kafka&#34;&gt;The problem - nested messages in Kafka&lt;/h3&gt;

&lt;p&gt;Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;SCN&amp;quot;: 12206116841348,
  &amp;quot;SEG_OWNER&amp;quot;: &amp;quot;KFKUSER&amp;quot;,
  &amp;quot;TABLE_NAME&amp;quot;: &amp;quot;CDCTAB2&amp;quot;,
  &amp;quot;TIMESTAMP&amp;quot;: 1539162785000,
  &amp;quot;SQL_REDO&amp;quot;: &amp;quot;insert into \&amp;quot;KFKUSER\&amp;quot;.\&amp;quot;CDCTAB2\&amp;quot;(\&amp;quot;ID\&amp;quot;,\&amp;quot;CITY\&amp;quot;,\&amp;quot;NATIONALITY\&amp;quot;) values (634789,&#39;AHMEDABAD&#39;,&#39;INDIA&#39;)&amp;quot;,
  &amp;quot;OPERATION&amp;quot;: &amp;quot;INSERT&amp;quot;,
  &amp;quot;data&amp;quot;: {
    &amp;quot;value&amp;quot;: {
      &amp;quot;ID&amp;quot;: 634789,
      &amp;quot;CITY&amp;quot;: {
        &amp;quot;string&amp;quot;: &amp;quot;AHMEDABAD&amp;quot;
      },
      &amp;quot;NATIONALITY&amp;quot;: {
        &amp;quot;string&amp;quot;: &amp;quot;INDIA&amp;quot;
      }
    }
  },
  &amp;quot;before&amp;quot;: null
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the &amp;lsquo;payload&amp;rsquo; is nested under &lt;code&gt;data&lt;/code&gt;-&amp;gt;&lt;code&gt;value&lt;/code&gt;. A user on the &lt;a href=&#34;https://groups.google.com/forum/#!forum/confluent-platform&#34;&gt;Confluent Community mailing list&lt;/a&gt; recently posted &lt;a href=&#34;https://groups.google.com/d/msg/confluent-platform/vWle1i3TibI/a9sgDWzAAgAJ&#34;&gt;a question&lt;/a&gt; in which they wanted to use the Kafka Connect JDBC Sink connector to stream this data to a target database. The problem was that the field they wanted to use as the primary key (&lt;code&gt;ID&lt;/code&gt;) was nested (as you can see above). This caused the error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Caused by: org.apache.kafka.connect.errors.ConnectException: PK mode for table &#39;CDCTAB2&#39; is RECORD_VALUE with configured PK fields [ID], but record value schema does not contain field: ID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The key(!) thing here is &lt;code&gt;record value schema does not contain field ID&lt;/code&gt;. The connector cannot find the field &lt;code&gt;ID&lt;/code&gt; in the source message—because it&amp;rsquo;s nested.&lt;/p&gt;

&lt;h3 id=&#34;the-solution-ksql-to-flatten-messages&#34;&gt;The solution - KSQL to flatten messages&lt;/h3&gt;

&lt;p&gt;You can use &lt;a href=&#34;https://www.confluent.io/ksql&#34;&gt;KSQL&lt;/a&gt; to transform every record on the source topic as it arrives, writing to a new topic and using &lt;em&gt;that&lt;/em&gt; topic as the source for the JDBC Sink.&lt;/p&gt;

&lt;p&gt;In KSQL, register the topic in KSQL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE STREAM CDC_SOURCE_DATA \
    (SCN BIGINT, TABLE_NAME VARCHAR, OPERATION VARCHAR, \
     DATA STRUCT&amp;lt;VALUE STRUCT&amp;lt;\
       ID BIGINT, \
       CITY STRUCT&amp;lt;string VARCHAR&amp;gt;, \
       NATIONALITY STRUCT&amp;lt;string VARCHAR&amp;gt;&amp;gt;&amp;gt;\
    ) \
    WITH (KAFKA_TOPIC=&#39;asif_test2&#39;, VALUE_FORMAT=&#39;JSON&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verify the schema—observe that it is nested&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; describe CDC_SOURCE_DATA;

Name                 : CDC_SOURCE_DATA
 Field      | Type
-------------------------------------------------------------------------------------------------------------------------------
 ROWTIME    | BIGINT           (system)
 ROWKEY     | VARCHAR(STRING)  (system)
 SCN        | BIGINT
 TABLE_NAME | VARCHAR(STRING)
 OPERATION  | VARCHAR(STRING)
 DATA       | STRUCT&amp;lt;VALUE STRUCT&amp;lt;ID BIGINT, CITY STRUCT&amp;lt;STRING VARCHAR(STRING)&amp;gt;, NATIONALITY STRUCT&amp;lt;STRING VARCHAR(STRING)&amp;gt;&amp;gt;&amp;gt;
-------------------------------------------------------------------------------------------------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &amp;lt;Stream,Table&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a flattened topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE STREAM FLATTENED_DATA AS \
  SELECT SCN, TABLE_NAME, OPERATION, \
          DATA-&amp;gt;VALUE-&amp;gt;ID AS ID, \
          DATA-&amp;gt;VALUE-&amp;gt;CITY-&amp;gt;string AS CITY, \
          DATA-&amp;gt;VALUE-&amp;gt;NATIONALITY-&amp;gt;string AS NATIONALITY \
  FROM CDC_SOURCE_DATA;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verify the new schema - note there are no nested fields&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; DESCRIBE FLATTENED_DATA;

Name                 : FLATTENED_DATA
 Field       | Type
-----------------------------------------
 ROWTIME     | BIGINT           (system)
 ROWKEY      | VARCHAR(STRING)  (system)
 SCN         | BIGINT
 TABLE_NAME  | VARCHAR(STRING)
 OPERATION   | VARCHAR(STRING)
 ID          | BIGINT
 CITY        | VARCHAR(STRING)
 NATIONALITY | VARCHAR(STRING)
-----------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &amp;lt;Stream,Table&amp;gt;;
ksql&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Query the new topic, e.g. using kafkacat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafkacat -b kafka:29092 -t FLATTENED_DATA -C

{&amp;quot;SCN&amp;quot;:12206116841348,&amp;quot;TABLE_NAME&amp;quot;:&amp;quot;CDCTAB2&amp;quot;,&amp;quot;OPERATION&amp;quot;:&amp;quot;INSERT&amp;quot;,&amp;quot;ID&amp;quot;:634789,&amp;quot;CITY&amp;quot;:&amp;quot;AHMEDABAD&amp;quot;,&amp;quot;NATIONALITY&amp;quot;:&amp;quot;INDIA&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can use the target topic (&lt;code&gt;FLATTENED_DATA&lt;/code&gt;) as the source for the JDBC sink, with &lt;code&gt;ID&lt;/code&gt; and other columns exposed as top-level elements.&lt;/p&gt;

&lt;p&gt;KSQL is a continuous query language and so every new event on the source topic will automatically be processed and written to the target topic.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Streaming geopoint data from Kafka to Elasticsearch</title>
			<link>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</link>
			<pubDate>Fri, 05 Oct 2018 15:22:51 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</guid>
			<description>&lt;p&gt;Using the &lt;a href=&#34;https://www.confluent.io/connector/kafka-connect-elasticsearch/&#34;&gt;Elasticsearch Kafka Connect connector&lt;/a&gt; to stream events from a Kafka topic to Elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -X &amp;quot;POST&amp;quot; &amp;quot;http://kafka-connect:8083/connectors/&amp;quot; \
     -H &amp;quot;Content-Type: application/json&amp;quot; \
     -d &#39;{
  &amp;quot;name&amp;quot;: &amp;quot;es_sink_ATM_POSSIBLE_FRAUD&amp;quot;,
  &amp;quot;config&amp;quot;: {
    &amp;quot;topics&amp;quot;: &amp;quot;ATM_POSSIBLE_FRAUD&amp;quot;,
    &amp;quot;key.converter&amp;quot;: &amp;quot;org.apache.kafka.connect.storage.StringConverter&amp;quot;,
    &amp;quot;value.converter&amp;quot;: &amp;quot;org.apache.kafka.connect.json.JsonConverter&amp;quot;,
    &amp;quot;value.converter.schemas.enable&amp;quot;: false,
    &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&amp;quot;,
    &amp;quot;key.ignore&amp;quot;: &amp;quot;true&amp;quot;,
    &amp;quot;schema.ignore&amp;quot;: &amp;quot;true&amp;quot;,
    &amp;quot;type.name&amp;quot;: &amp;quot;type.name=kafkaconnect&amp;quot;,
    &amp;quot;topic.index.map&amp;quot;: &amp;quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&amp;quot;,
    &amp;quot;connection.url&amp;quot;: &amp;quot;http://elasticsearch:9200&amp;quot;
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPUT &amp;quot;http://elasticsearch:9200/_template/kafkaconnect/&amp;quot; -H &#39;Content-Type: application/json&#39; -d&#39;
{
  &amp;quot;index_patterns&amp;quot;: &amp;quot;*&amp;quot;,
  &amp;quot;settings&amp;quot;: {
    &amp;quot;number_of_shards&amp;quot;: 1,
    &amp;quot;number_of_replicas&amp;quot;: 0
  },
  &amp;quot;mappings&amp;quot;: {
    &amp;quot;_default_&amp;quot;: {
      &amp;quot;dynamic_templates&amp;quot;: [
        {
          &amp;quot;dates&amp;quot;: {
            &amp;quot;match&amp;quot;: &amp;quot;*TIMESTAMP&amp;quot;,
            &amp;quot;mapping&amp;quot;: {
              &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot;
            }
          }
        },
        {
          &amp;quot;geopoint&amp;quot;: {
            &amp;quot;match&amp;quot;: &amp;quot;*LOCATION&amp;quot;,
            &amp;quot;mapping&amp;quot;: {
              &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot;
            }
          }
        },
        {
          &amp;quot;geopoint2&amp;quot;: {
            &amp;quot;match&amp;quot;: &amp;quot;location&amp;quot;,
            &amp;quot;mapping&amp;quot;: {
              &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot;
            }
          }
        },
        {
          &amp;quot;non_analysed_string_template&amp;quot;: {
            &amp;quot;match&amp;quot;: &amp;quot;account_id, atm, transaction_id&amp;quot;,
            &amp;quot;match_mapping_type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;mapping&amp;quot;: {
              &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;
            }
          }
        }
      ]
    }
  }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sample JSON message from Kafka: (pretty-printed)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ACCOUNT_ID&amp;quot;: &amp;quot;a898&amp;quot;,
  &amp;quot;TXN1_TIMESTAMP&amp;quot;: 1538733229153,
  &amp;quot;TXN2_TIMESTAMP&amp;quot;: 1538733200285,
  &amp;quot;TXN1_LOCATION&amp;quot;: {
    &amp;quot;LON&amp;quot;: -122.4026113,
    &amp;quot;LAT&amp;quot;: 37.7911278
  },
  &amp;quot;TXN2_LOCATION&amp;quot;: {
    &amp;quot;LON&amp;quot;: -121.4943199,
    &amp;quot;LAT&amp;quot;: 38.5320738
  },
  &amp;quot;TXN1_AMOUNT&amp;quot;: 400,
  &amp;quot;TXN2_AMOUNT&amp;quot;: 50,
  &amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;: 114.42848872962888,
  &amp;quot;MS_DIFFERENCE&amp;quot;: -28868
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the case of all columns is uppercase which causes problems trying to stream this to Elasticsearch. Kafka Connect worker log shows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WARN Failed to execute batch 5560 of 19 records with attempt 2/6, will attempt retry after 111 ms. Failure reason: Bulk request failed: [{&amp;quot;type&amp;quot;:&amp;quot;mapper_parsing_exception&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;failed to parse&amp;quot;,&amp;quot;caused_by&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;parse_exception&amp;quot;,&amp;quot;reason&amp;quot;:&amp;quot;field must be either [lat], [lon] or [geohash]&amp;quot;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Taking the JSON message from the Kafka topic and manually sending it to Elasticsearch replicates the probem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPOST &amp;quot;http://elasticsearch:9200/atm_possible_fraud/kafkaconnect&amp;quot; -H &#39;Content-Type: application/json&#39; -d&#39;
{&amp;quot;ACCOUNT_ID&amp;quot;:&amp;quot;a898&amp;quot;,&amp;quot;TXN1_TIMESTAMP&amp;quot;:1538733229153,&amp;quot;TXN2_TIMESTAMP&amp;quot;:1538733200285,&amp;quot;TXN1_LOCATION&amp;quot;:{&amp;quot;LON&amp;quot;:-122.4026113,&amp;quot;LAT&amp;quot;:37.7911278},&amp;quot;TXN2_LOCATION&amp;quot;:{&amp;quot;LON&amp;quot;:-121.4943199,&amp;quot;LAT&amp;quot;:38.5320738},&amp;quot;TXN1_AMOUNT&amp;quot;:400,&amp;quot;TXN2_AMOUNT&amp;quot;:50,&amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;:114.42848872962888,&amp;quot;MS_DIFFERENCE&amp;quot;:-28868}&#39;

{
&amp;quot;error&amp;quot;: {
    &amp;quot;root_cause&amp;quot;: [
    {
        &amp;quot;type&amp;quot;: &amp;quot;parse_exception&amp;quot;,
        &amp;quot;reason&amp;quot;: &amp;quot;field must be either [lat], [lon] or [geohash]&amp;quot;
    }
    ],
    &amp;quot;type&amp;quot;: &amp;quot;mapper_parsing_exception&amp;quot;,
    &amp;quot;reason&amp;quot;: &amp;quot;failed to parse&amp;quot;,
    &amp;quot;caused_by&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;parse_exception&amp;quot;,
    &amp;quot;reason&amp;quot;: &amp;quot;field must be either [lat], [lon] or [geohash]&amp;quot;
    }
},
&amp;quot;status&amp;quot;: 400
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So Elasticsearch is sensitive to the &lt;em&gt;case&lt;/em&gt; of the &lt;code&gt;lat&lt;/code&gt;, &lt;code&gt;lon&lt;/code&gt; columns. The fix (&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html[per&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html[per&lt;/a&gt; the examples here]) is to force the column names to lower case, or concatenate the lat/long into a single string):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPOST &amp;quot;http://elasticsearch:9200/atm_possible_fraud/kafkaconnect&amp;quot; -H &#39;Content-Type: application/json&#39; -d&#39;
{&amp;quot;ACCOUNT_ID&amp;quot;:&amp;quot;a898&amp;quot;,
&amp;quot;TXN1_TIMESTAMP&amp;quot;:1538733229153,
&amp;quot;TXN2_TIMESTAMP&amp;quot;:1538733200285,
&amp;quot;TXN1_LOCATION&amp;quot;:&amp;quot;37.7911278,-122.4026113&amp;quot;,
&amp;quot;TXN2_LOCATION&amp;quot;:{&amp;quot;lon&amp;quot;:-121.4943199,&amp;quot;lat&amp;quot;:38.5320738},
&amp;quot;TXN1_AMOUNT&amp;quot;:400,
&amp;quot;TXN2_AMOUNT&amp;quot;:50,
&amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;:114.42848872962888,
&amp;quot;MS_DIFFERENCE&amp;quot;:-28868}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To implement this, I used KSQL to wrangle the data from a &lt;code&gt;STRUCT&lt;/code&gt; (with uppercase names) to a lat/long string:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE STREAM ATM_POSSIBLE_FRAUD_02 AS \
SELECT CAST(X.location-&amp;gt;lat AS STRING) + &#39;,&#39; + CAST(X.location-&amp;gt;lon AS STRING) AS TXN1_LOCATION
[...]
FROM   ATM_TXNS X 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the JSON messages look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;ACCOUNT_ID&amp;quot;: &amp;quot;a182&amp;quot;,
  &amp;quot;TXN1_TIMESTAMP&amp;quot;: 1538735677504,
  &amp;quot;TXN2_TIMESTAMP&amp;quot;: 1538735677528,
  &amp;quot;TXN1_LOCATION&amp;quot;: &amp;quot;37.8002247,-122.2160293&amp;quot;,
  &amp;quot;TXN2_LOCATION&amp;quot;: &amp;quot;37.764931,-122.4232384&amp;quot;,
  &amp;quot;TXN1_AMOUNT&amp;quot;: 400,
  &amp;quot;TXN2_AMOUNT&amp;quot;: 20,
  &amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;: 18.628023818908343,
  &amp;quot;MS_DIFFERENCE&amp;quot;: 24
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the Kafka Connect -&amp;gt; Elasticsearch pipeline works just great. Here&amp;rsquo;s the resulting Elasticsearch index and sample document:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;atm_possible_fraud&amp;quot;: {
    &amp;quot;aliases&amp;quot;: {},
    &amp;quot;mappings&amp;quot;: {
[...]
      &amp;quot;type.name=kafkaconnect&amp;quot;: {
[...]
        &amp;quot;properties&amp;quot;: {
          &amp;quot;ACCOUNT_ID&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
            &amp;quot;fields&amp;quot;: {
              &amp;quot;keyword&amp;quot;: {
                &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
                &amp;quot;ignore_above&amp;quot;: 256
              }
            }
          },
          &amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;float&amp;quot;
          },
          &amp;quot;MS_DIFFERENCE&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;long&amp;quot;
          },
          &amp;quot;TXN1_AMOUNT&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;long&amp;quot;
          },
          &amp;quot;TXN1_LOCATION&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot;
          },
          &amp;quot;TXN1_TIMESTAMP&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot;
          },
          &amp;quot;TXN2_AMOUNT&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;long&amp;quot;
          },
          &amp;quot;TXN2_LOCATION&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot;
          },
          &amp;quot;TXN2_TIMESTAMP&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot;
          }
        }
      }
    },
    &amp;quot;settings&amp;quot;: {
      &amp;quot;index&amp;quot;: {
        &amp;quot;creation_date&amp;quot;: &amp;quot;1538735883573&amp;quot;,
        &amp;quot;number_of_shards&amp;quot;: &amp;quot;1&amp;quot;,
        &amp;quot;number_of_replicas&amp;quot;: &amp;quot;0&amp;quot;,
        &amp;quot;uuid&amp;quot;: &amp;quot;ppXU3hFvS-CU9kKlFaK-NA&amp;quot;,
        &amp;quot;version&amp;quot;: {
          &amp;quot;created&amp;quot;: &amp;quot;6040299&amp;quot;
        },
        &amp;quot;provided_name&amp;quot;: &amp;quot;atm_possible_fraud&amp;quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  {
    &amp;quot;_index&amp;quot;: &amp;quot;atm_possible_fraud&amp;quot;,
    &amp;quot;_type&amp;quot;: &amp;quot;type.name=kafkaconnect&amp;quot;,
    &amp;quot;_id&amp;quot;: &amp;quot;ATM_POSSIBLE_FRAUD2+0+7742&amp;quot;,
    &amp;quot;_score&amp;quot;: 1,
    &amp;quot;_source&amp;quot;: {
      &amp;quot;TXN2_TIMESTAMP&amp;quot;: 1538735677573,
      &amp;quot;TXN2_AMOUNT&amp;quot;: 300,
      &amp;quot;ACCOUNT_ID&amp;quot;: &amp;quot;a874&amp;quot;,
      &amp;quot;TXN1_TIMESTAMP&amp;quot;: 1538735677515,
      &amp;quot;MS_DIFFERENCE&amp;quot;: 58,
      &amp;quot;TXN1_AMOUNT&amp;quot;: 300,
      &amp;quot;DISTANCE_BETWEEN_TXNS&amp;quot;: 57.33495049372549,
      &amp;quot;TXN1_LOCATION&amp;quot;: &amp;quot;37.7923185,-122.3940464&amp;quot;,
      &amp;quot;TXN2_LOCATION&amp;quot;: &amp;quot;37.3540655,-122.0512763&amp;quot;
    }
  }
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Exploring JMX with jmxterm</title>
			<link>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</link>
			<pubDate>Wed, 19 Sep 2018 08:11:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</guid>
			<description>&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/jiaqi/jmxterm/&#34;&gt;jmxterm repository&lt;/a&gt; / Download jmxterm from &lt;a href=&#34;http://wiki.cyclopsgroup.org/jmxterm/&#34;&gt;http://wiki.cyclopsgroup.org/jmxterm/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Launch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java -jar ~/Downloads/jmxterm-1.0.0-uber.jar --url localhost:30002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can pass the jmx host/port directly, or use the &lt;code&gt;open&lt;/code&gt; command once jmxterm launches.&lt;/p&gt;

&lt;p&gt;Once connected, use &lt;code&gt;domains&lt;/code&gt; to list available domains&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;domains
#following domains are available
JMImplementation
com.sun.management
io.confluent.ksql.metrics
io.confluent.rest
java.lang
java.nio
java.util.logging
kafka.admin.client
kafka.consumer
kafka.producer
kafka.streams
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Switch to a particular domain:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;domain io.confluent.ksql.metrics
#domain is set to io.confluent.ksql.metrics
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List the available MBeans in a the selected domain (you can also run this without choosing a &lt;code&gt;domain&lt;/code&gt; first, to see every MBean, but it&amp;rsquo;s a long list):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;beans
#domain = io.confluent.ksql.metrics:
io.confluent.ksql.metrics:id=_confluent-ksql-confluent_rmoff_01query_CSAS_GOOD_RATINGS_0-11,key=ratings,type=consumer-metrics
io.confluent.ksql.metrics:id=_confluent-ksql-confluent_rmoff_01query_CSAS_GOOD_RATINGS_0-6904389f-5901-4b89-b331-53a6933d8ce0-StreamThread-4-producer,key=good_ratings,type=producer-metrics
io.confluent.ksql.metrics:type=kafka-metrics-count
io.confluent.ksql.metrics:type=ksql-engine-query-stats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Switch to a particular bean of interest:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;bean io.confluent.ksql.metrics:type=ksql-engine-query-stats
#bean is set to io.confluent.ksql.metrics:type=ksql-engine-query-stats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List the available attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;info
#mbean = io.confluent.ksql.metrics:type=ksql-engine-query-stats
#class name = org.apache.kafka.common.metrics.JmxReporter$KafkaMbean
# attributes
%0   - bytes-consumed-total (double, r)
%1   - error-rate (double, r)
%2   - messages-consumed-avg (double, r)
%3   - messages-consumed-max (double, r)
%4   - messages-consumed-min (double, r)
%5   - messages-consumed-per-sec (double, r)
%6   - messages-consumed-total (double, r)
%7   - messages-produced-per-sec (double, r)
%8   - num-active-queries (double, r)
%9   - num-idle-queries (double, r)
%10  - num-persistent-queries (double, r)
#there&#39;s no operations
#there&#39;s no notifications
$&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Read the value of an attribute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$&amp;gt;get messages-consumed-total
#mbean = io.confluent.ksql.metrics:type=ksql-engine-query-stats:
messages-consumed-total = 251329.0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that jmxterm support tab-completion for all the commands, which makes it much easier to use.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Accessing Kafka Docker containers&#39; JMX from host</title>
			<link>https://rmoff.github.io/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</link>
			<pubDate>Mon, 17 Sep 2018 15:29:48 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</guid>
			<description>&lt;p&gt;To help future Googlers… with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KSQL_JMX_HOSTNAME&lt;/code&gt; - the hostname/IP of the &lt;em&gt;host&lt;/em&gt; machine. This is used by the JMX client to connect back into JMX, so must be accessible from the &lt;em&gt;host machine running the JMX client&lt;/em&gt;. If you&amp;rsquo;re just running your JMX client locally on the Docker host, you can set this to &lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KSQL_JMX_PORT&lt;/code&gt; - a port on which you want to access the metrics. Make sure you expose this port through Docker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you don&amp;rsquo;t set &lt;code&gt;KSQL_JMX_HOSTNAME&lt;/code&gt; then the Docker launch script uses the &lt;em&gt;host details of the container&lt;/em&gt;, which results in connectivity problems.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&#34;http://wiki.cyclopsgroup.org/jmxterm/&#34;&gt;jmxterm&lt;/a&gt; you&amp;rsquo;ll get errors like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar ~/Downloads/jmxterm-1.0.0-uber.jar
Welcome to JMX terminal. Type &amp;quot;help&amp;quot; for available commands.
$&amp;gt;open localhost:18088
#RuntimeIOException: Runtime IO exception: Connection refused to host: 192.168.144.4; nested exception is:
        java.net.ConnectException: Operation timed out (Connection timed out)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -jar ~/Downloads/jmxterm-1.0.0-uber.jar
Welcome to JMX terminal. Type &amp;quot;help&amp;quot; for available commands.
$&amp;gt;open localhost:18088
#RuntimeIOException: Runtime IO exception: Failed to retrieve RMIServer stub: javax.naming.CommunicationException [Root exception is java.rmi.ConnectIOException: error during JRMP connection establishment; nested exception is:
        java.io.EOFException]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For JConsole it&amp;rsquo;ll just hang/timeout, or appear to work but disconnected.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Sending multiline messages to Kafka</title>
			<link>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</link>
			<pubDate>Tue, 04 Sep 2018 08:26:51 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</guid>
			<description>&lt;p&gt;(&lt;a href=&#34;https://stackoverflow.com/questions/52151816/push-multiple-line-text-as-one-message-in-a-kafka-topic/52162998#52162998&#34;&gt;SO answer repost&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;You can use &lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;&lt;code&gt;kafkacat&lt;/code&gt;&lt;/a&gt; to send messages to Kafka that include line breaks. To do this, use its &lt;code&gt;-D&lt;/code&gt; operator to specify a custom message delimiter (in this example &lt;code&gt;/&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafkacat -b kafka:29092 \
        -t test_topic_01 \
        -D/ \
        -P &amp;lt;&amp;lt;EOF
this is a string message 
with a line break/this is 
another message with two 
line breaks!
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Note that the delimiter &lt;strong&gt;must&lt;/strong&gt; be a single byte - multi-byte chars will end up getting included in the resulting message &lt;a href=&#34;https://github.com/edenhill/kafkacat/issues/140&#34;&gt;See issue #140&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Resulting messages, inspected also using kafkacat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b kafka:29092 -C \
         -f &#39;\nKey (%K bytes): %k\t\nValue (%S bytes): %s\n\Partition: %p\tOffset: %o\n--\n&#39; \
         -t test_topic_01

Key (-1 bytes):
Value (43 bytes): this is a string message
with a line break
Partition: 0    Offset: 0
--

Key (-1 bytes):
Value (48 bytes): this is
another message with two
line breaks!

Partition: 0    Offset: 1
--
% Reached end of topic test_topic_01 [0] at offset 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inspecting using &lt;code&gt;kafka-console-consumer&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafka-console-consumer \
    --bootstrap-server kafka:29092 \
    --topic test_topic_01 \
    --from-beginning

this is a string message
with a line break
this is
another message with two
line breaks!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;em&gt;thus illustrating why &lt;code&gt;kafkacat&lt;/code&gt; is nicer to work with than &lt;code&gt;kafka-console-consumer&lt;/code&gt; because of its optional verbosity :)&lt;/em&gt; )&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;Read more about kafkacat here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Window Timestamps in KSQL / Integration with Elasticsearch</title>
			<link>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</link>
			<pubDate>Mon, 03 Sep 2018 16:16:30 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</guid>
			<description>&lt;p&gt;KSQL provides the ability to create windowed aggregations. For example,
count the number of messages in a 1 minute window, grouped by a
particular column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.sql&#34;&gt;CREATE TABLE RATINGS_BY_CLUB_STATUS AS \
SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \
FROM RATINGS_WITH_CUSTOMER_DATA \
     WINDOW TUMBLING (SIZE 1 MINUTES) \
GROUP BY CLUB_STATUS;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How KSQL, and Kafka Streams, stores the window timestamp associated with
an aggregate, has recently changed. &lt;a href=&#34;https://github.com/confluentinc/ksql/issues/1497&#34;&gt;See #1497 for
details&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whereas previously the &lt;em&gt;Kafka message timestamp&lt;/em&gt; (accessible through the
KSQL &lt;code&gt;ROWTIME&lt;/code&gt; system column) stored the start of the window for which
the aggregate had been calculated, this changed in July 2018 to instead
be the timestamp of the latest message to update that aggregate value.
This was in Apache Kafka 2.0 and Confluent Platform 5.0, and back-ported
to previous versions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; DESCRIBE RATINGS_BY_CLUB_STATUS;

Name                 : RATINGS_BY_CLUB_STATUS
 Field        | Type
------------------------------------------
 ROWTIME      | BIGINT           (system)
 ROWKEY       | VARCHAR(STRING)  (system)
 CLUB_STATUS  | VARCHAR(STRING)
 RATING_COUNT | BIGINT
------------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &amp;lt;Stream,Table&amp;gt;;

ksql&amp;gt; SELECT * FROM RATINGS_BY_CLUB_STATUS LIMIT 5;
1535994657217 | platinum : Window{start=1535994600000 end=-} | platinum | 14
1535994718988 | platinum : Window{start=1535994660000 end=-} | platinum | 26
1535994776177 | platinum : Window{start=1535994720000 end=-} | platinum | 23
1535994827952 | platinum : Window{start=1535994780000 end=-} | platinum | 14
1535994658145 | bronze : Window{start=1535994600000 end=-} | bronze | 12
Limit Reached
Query terminated
ksql&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s useful to be able to access the start time of a windowed aggregate,
particularly for analytical uses. If KSQL is being used to build
aggregates for analysis and reporting, the window for which an aggregate
is required to give it any context. Otherwise it&amp;rsquo;s just a number!&lt;/p&gt;

&lt;p&gt;An example of using the window timestamp is in streaming KSQL aggregates
into Elasticsearch for visualisation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/09/ksqlsoe08.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are &lt;a href=&#34;https://github.com/confluentinc/ksql/issues/1674&#34;&gt;plans&lt;/a&gt; to create a function in KSQL that will expose the window start timestamp again.&lt;/p&gt;

&lt;p&gt;To get it to work with Elasticsearch, in Kafka Connect use the SMT as
before to pull the message timestamp out into a field&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;transforms&amp;quot;: &amp;quot;ExtractTimestamp&amp;quot;,
&amp;quot;transforms.ExtractTimestamp.type&amp;quot;: &amp;quot;org.apache.kafka.connect.transforms.InsertField$Value&amp;quot;,
&amp;quot;transforms.ExtractTimestamp.timestamp.field&amp;quot; : &amp;quot;TS&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then make sure you set &lt;code&gt;&amp;quot;key.ignore&amp;quot;: &amp;quot;false&amp;quot;&lt;/code&gt;. This will then make
Kafka Connect use the Kafka message key (which is the grouped-by
field(s) plus the window start + end timestamp) as the Elasticsearch
document id. The effect of this is that you&amp;rsquo;ll end up with one document
per aggregation in Elasticsearch, updated in place.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/09/kib-es-keys01.png&#34; alt=&#34;kib es keys01&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/09/kib-es-keys02.png&#34; alt=&#34;kib es keys02&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The timestamp value will not be on the beginning of the window but it
will be within it - and you can use Kibana&amp;rsquo;s visualisation which will
display it rounded:&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Converting from AsciiDoc to MS Word</title>
			<link>https://rmoff.github.io/2018/08/22/converting-from-asciidoc-to-ms-word/</link>
			<pubDate>Wed, 22 Aug 2018 18:50:53 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/08/22/converting-from-asciidoc-to-ms-word/</guid>
			<description>&lt;p&gt;Short and sweet this one. I&amp;rsquo;ve written in the past how &lt;a href=&#34;https://rmoff.net/2017/09/12/what-is-markdown-and-why-is-it-awesome/&#34;&gt;I love Markdown&lt;/a&gt; but I&amp;rsquo;ve actually moved on from that and now firmly throw my hat in the &lt;a href=&#34;http://www.methods.co.nz/asciidoc/&#34;&gt;AsciiDoc&lt;/a&gt; ring. I&amp;rsquo;ll write another post another time explaining why in more detail, but in short it&amp;rsquo;s just more powerful whilst still simple and readable without compilation.&lt;/p&gt;

&lt;p&gt;So anyway, I use AsciiDoc (ADOC) for all my technical (and often non-technical) writing now, and from there usually dump it out to HTML which I can share with people as needed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;asciidoctor --backend html5 -a data-uri my_input_file.adoc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;code&gt;-a data-uri&lt;/code&gt; embeds any images as part of the HTML file, for easier sharing)&lt;/p&gt;

&lt;p&gt;But today I needed to generate a MS Word (DOCX) file, and found a neat combination of tools to do this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INPUT_ADOC=my_input_file.adoc
asciidoctor --backend docbook --out-file - $INPUT_ADOC|pandoc --from docbook --to docx --output $INPUT_ADOC.docx
# On the Mac, this will open the generated file in MS Word
open $INPUT_ADOC.docx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ref:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://asciidoctor.org/&#34;&gt;Asciidoctor&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;On the mac: &lt;code&gt;brew install asciidoctor&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pandoc.org/&#34;&gt;Pandoc&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;On the mac: &lt;code&gt;brew install pandoc&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=joaompinto.asciidoctor-vscode&#34;&gt;AsciiDoc extension for VS Code&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;VSCode is my new favourite editor (but I still ❤️ emacs for org-mode)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Where I&#39;m speaking in the rest of 2018</title>
			<link>https://rmoff.github.io/2018/08/21/where-im-speaking-in-the-rest-of-2018/</link>
			<pubDate>Tue, 21 Aug 2018 21:01:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/08/21/where-im-speaking-in-the-rest-of-2018/</guid>
			<description>

&lt;p&gt;There&amp;rsquo;s lots going on in the next few months :-)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m particularly excited to be speaking at several notable conferences for the first time, including JavaZone, USENIX LISA, and Devoxx.&lt;/p&gt;

&lt;p&gt;As always, if you&amp;rsquo;re nearby then hope to see you there, and let me know if you want to meet for a coffee or beer!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;september&#34;&gt;September&lt;/h3&gt;

&lt;h4 id=&#34;madrid-spain&#34;&gt;🇪🇸 Madrid, Spain&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;6th Sept: &lt;a href=&#34;https://www.meetup.com/apachekafkamadrid/events/251264347/&#34;&gt;Madrid Kafka Meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;oslo-norway&#34;&gt;🇳🇴 Oslo, Norway&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;10th Sept: &lt;a href=&#34;https://www.meetup.com/Oslo-Kafka/events/&#34;&gt;Oslo Kafka Meetup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;11th Sept: &lt;a href=&#34;https://2018.javazone.no/program/73fa52a7-661c-47a0-b4f3-55aaf5b10f6b&#34;&gt;JavaZone&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;antwerp-brussels-belgium&#34;&gt;🇧🇪 Antwerp/Brussels, Belgium&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;25th Sept: &lt;a href=&#34;https://www.meetup.com/Brussels-Apache-Kafka-Meetup-by-Confluent/events/253855467/&#34;&gt;Brussels Kafka Meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;barcelona-spain&#34;&gt;🇪🇸 Barcelona, Spain&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;26th Sept: &lt;a href=&#34;https://www.meetup.com/Barcelona-Kafka-Meetup/events/254252957/&#34;&gt;Barcelona Kafka Meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;october&#34;&gt;October&lt;/h3&gt;

&lt;h4 id=&#34;leeds-uk&#34;&gt;🇬🇧 Leeds, UK&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;4th Oct: &lt;a href=&#34;https://www.meetup.com/Leeds-JVMThing/events/&#34;&gt;The JVM Thing meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;nashville-tn-usa&#34;&gt;🇺🇸 Nashville (TN), USA&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;31st Oct: &lt;a href=&#34;https://www.usenix.org/conference/lisa18/conference-program&#34;&gt;LISA18&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;november&#34;&gt;November&lt;/h3&gt;

&lt;h4 id=&#34;münich-germany&#34;&gt;🇩🇪 Münich, Germany&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;7th Nov: &lt;a href=&#34;https://jax.de/&#34;&gt;W-JAX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;antwerp-belgium&#34;&gt;🇧🇪 Antwerp, Belgium&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;13th Nov: &lt;a href=&#34;https://devoxx.be/&#34;&gt;Devoxx Belgium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;krakow-poland&#34;&gt;🇵🇱 Krakow, Poland&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;26th Nov: &lt;a href=&#34;http://coredump.events/2018/&#34;&gt;CoreDump&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;december&#34;&gt;December&lt;/h3&gt;

&lt;h4 id=&#34;liverpool-uk&#34;&gt;🇬🇧 Liverpool, UK&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;4th Dec: &lt;a href=&#34;https://www.ukougconferences.org.uk/ukoug/frontend/reg/tAgendaWebsite.csp?pageID=306&amp;amp;eventID=2&amp;amp;language=1&amp;amp;mainFramePage=dailyagenda.csp&amp;amp;mode=&#34;&gt;UKOUG TECH 18&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;frankfurt-germany&#34;&gt;🇩🇪 Frankfurt, Germany&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;10th Dec: &lt;a href=&#34;https://www.meetup.com/Frankfurt-Apache-Kafka-Meetup-by-Confluent/events/256599175/&#34;&gt;Apache Kafka Meetup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;11th Dec: &lt;a href=&#34;https://www.ittage.informatik-aktuell.de/&#34;&gt;IT Days&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Listeners - Explained</title>
			<link>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</link>
			<pubDate>Thu, 02 Aug 2018 19:38:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</guid>
			<description>

&lt;p&gt;This question comes up on StackOverflow and such places a &lt;strong&gt;lot&lt;/strong&gt;, so here&amp;rsquo;s something to try and help.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; : You need to set &lt;code&gt;advertised.listeners&lt;/code&gt; (or &lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address–and if that&amp;rsquo;s not reachable then problems ensue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/docker01.png&#34; alt=&#34;images/docker01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this post I&amp;rsquo;ll talk about &lt;em&gt;why&lt;/em&gt; this is necessary, and then show &lt;em&gt;how&lt;/em&gt; to do it, based on a couple of scenarios - Docker, and AWS.&lt;/p&gt;

&lt;h3 id=&#34;is-anyone-listening&#34;&gt;Is anyone listening?&lt;/h3&gt;

&lt;p&gt;Kafka is a distributed system. Data is read from &amp;amp; written to the &lt;em&gt;Leader&lt;/em&gt; for a given partition, which could be on any of the brokers in a cluster. When a client (producer/consumer) starts, it will request metadata about which broker is the leader for a partition—and it can do this from &lt;em&gt;any&lt;/em&gt; broker. The metadata returned will include the endpoints available for the Leader broker for that partition, and the client will then use those endpoints to connect to the broker to read/write data as required.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s these endpoints that cause people trouble. On a &lt;em&gt;single machine, running &amp;lsquo;bare metal&amp;rsquo;&lt;/em&gt; (no VMs, no Docker), everything might be the hostname (or just &lt;em&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/em&gt;) and it&amp;rsquo;s easy. But once you move into more complex networking setups, and multiple nodes, you have to pay more attention to it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume you have more than one network. This could be things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker internal network(s) plus host machine&lt;/li&gt;
&lt;li&gt;Brokers in the cloud (eg. AWS EC2), and on-premises machines locally (or even in another cloud)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You need to tell Kafka how the brokers can reach each other, but also make sure that external clients (producers/consumers) can reach the broker they need to.&lt;/p&gt;

&lt;p&gt;The key thing is that when you run a client, &lt;strong&gt;the broker you pass to it is &lt;em&gt;just where it&amp;rsquo;s going to go and get the metadata about brokers in the cluster from&lt;/em&gt;&lt;/strong&gt;. The actual host &amp;amp; IP that it will connect to for reading/writing data is based on &lt;strong&gt;&lt;em&gt;the data that the broker passes back in that initial connection&lt;/em&gt;&lt;/strong&gt;—even if it&amp;rsquo;s just a single node and the broker returned is the same as the one connected to.&lt;/p&gt;

&lt;p&gt;For configuring this correctly, you need to understand that Kafka brokers can have multiple &lt;em&gt;listeners&lt;/em&gt;. A listener is a combination of&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Host/IP&lt;/li&gt;
&lt;li&gt;Port&lt;/li&gt;
&lt;li&gt;Protocol&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s check out some config. Often the protocol is used for the listener name too, but here let&amp;rsquo;s make it nice and clear by using abstract names for the listeners:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT
KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m using the Docker config names—the equivalents if you&amp;rsquo;re configuring &lt;code&gt;server.properties&lt;/code&gt; directly (e.g. on AWS etc) are shown indented in the following list&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KAFKA_LISTENERS&lt;/code&gt; is a comma-separated list of listeners, and the host/ip and port to which Kafka binds to on which to listen. For more complex networking this might be an IP address associated with a given network interface on a machine. The default is 0.0.0.0, which means listening on all interfaces.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;listeners&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; is a comma-separated list of listeners with their the host/ip and port. This is the metadata that&amp;rsquo;s passed back to clients.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;advertised.listeners&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KAFKA_LISTENER_SECURITY_PROTOCOL_MAP&lt;/code&gt; defines key/value pairs for the security protocol to use, per listener name.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;listener.security.protocol.map&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Kafka brokers communicate between themselves&lt;/em&gt;, usually on the internal network (e.g. Docker network, AWS VPC, etc). To define which listener to use, specify &lt;code&gt;KAFKA_INTER_BROKER_LISTENER_NAME&lt;/code&gt; (&lt;code&gt;inter.broker.listener.name&lt;/code&gt;). The host/IP used must be accessible from the broker machine to others.&lt;/p&gt;

&lt;p&gt;Kafka &lt;em&gt;clients&lt;/em&gt; may well not be local to the broker&amp;rsquo;s network, and this is where the additional listeners come in.&lt;/p&gt;

&lt;p&gt;Each listener will, when connected to, report back the address on which it can be reached. &lt;em&gt;The address on which you reach a broker depends on the network used&lt;/em&gt;. If you&amp;rsquo;re connecting to the broker from an internal network it&amp;rsquo;s going to be a different host/IP than when connecting externally.&lt;/p&gt;

&lt;p&gt;When connecting to a broker, the listener that will be returned to the client will be the listener to which you connected (based on the port).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kafkacat&lt;/code&gt; is a useful tool for exploring this. Using &lt;code&gt;-L&lt;/code&gt; you can see the metadata for the listener to which you connected.
Based on the same listener config as above (&lt;code&gt;LISTENER_BOB&lt;/code&gt; / &lt;code&gt;LISTENER_FRED&lt;/code&gt;), check out the respective entries for &lt;strong&gt;&lt;code&gt;broker 0 at&lt;/code&gt;&lt;/strong&gt;: -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Connecting on port 9092 (which we map as &lt;code&gt;LISTENER_FRED&lt;/code&gt;), the broker&amp;rsquo;s address is given back as &lt;code&gt;localhost&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b kafka0:9092 \
           -L
Metadata for all topics (from broker -1: kafka0:9092/bootstrap):
1 brokers:
  broker 0 at localhost:9092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connecting on port 29092 (which we map as &lt;code&gt;LISTENER_BOB&lt;/code&gt;), the broker&amp;rsquo;s address is given back as &lt;code&gt;kafka0&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b kafka0:29092 \
           -L
Metadata for all topics (from broker 0: kafka0:29092/0):
1 brokers:
  broker 0 at kafka0:29092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can also use &lt;code&gt;tcpdump&lt;/code&gt; to examine the traffic from a client connecting to the broker, and spot the hostname that&amp;rsquo;s returned from the broker.&lt;/p&gt;

&lt;h3 id=&#34;why-can-i-connect-to-the-broker-but-the-client-still-fails&#34;&gt;Why can I connect to the broker, but the client still fails?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;tl;dr&lt;/em&gt; Even if you can make the initial connection to the broker, the address returned in the metadata may still be for a hostname that is not accessible from your client.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s walk this through step by step.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;We&amp;rsquo;ve got a broker on AWS. We want to send a message to it from our laptop. We know the external hostname for the EC2 instance (&lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt;). We&amp;rsquo;ve created the necessary entry in the security group to open the broker&amp;rsquo;s port to our inbound traffic. We do smart things like checking that our local machine can connect to the port on the AWS instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nc -vz ec2-54-191-84-122.us-west-2.compute.amazonaws.com 9092
found 0 associations
found 1 connections:
    1:  flags=82&amp;lt;CONNECTED,PREFERRED&amp;gt;
  outif utun5
  src 172.27.230.23 port 53352
  dst 54.191.84.122 port 9092
  rank info not available
  TCP aux info available

Connection to ec2-54-191-84-122.us-west-2.compute.amazonaws.com port 9092 [tcp/XmlIpcRegSvc] succeeded!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Things are looking good! We run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;test&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now…what happens next?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Our laptop resolves &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; successfully (to the IP address 54.191.84.122), and connects to the AWS machine on port 9092&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The broker receives the inbound connection on port 9092. &lt;em&gt;It returns the metadata to the client, with the hostname &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt;&lt;/em&gt; because this is the host name of the broker and the default value for &lt;code&gt;listeners&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The client the tries to send data to the broker using the metadata it was given. Since &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt; is not resolvable from the internet, it fails.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;test&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Puzzled, we try the same thing from the broker machine itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;foo&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&amp;gt;&amp;gt;
$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginning
foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works fine! That&amp;rsquo;s because we are connecting to port 9092, which is configured as the &lt;em&gt;internal&lt;/em&gt; listener, and thus reports back its hostname as &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt; which &lt;em&gt;is&lt;/em&gt; resolvable from the broker machine (since it&amp;rsquo;s its own hostname!)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We can make life even easier by using &lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;&lt;code&gt;kafkacat&lt;/code&gt;&lt;/a&gt;. Using the &lt;code&gt;-L&lt;/code&gt; flag we can see the metadata returned by the broker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -L
Metadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):
1 brokers:
  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clear as day, the &lt;em&gt;internal&lt;/em&gt; hostname is returned. This also makes this seemingly-confusing error make a lot more sense—connecting to one hostname, getting a lookup error on another:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -C -t test
% ERROR: Local: Host resolution failure: ip-172-31-18-160.us-west-2.compute.internal:9092/0: Failed to resolve &#39;ip-172-31-18-160.us-west-2.compute.internal:9092&#39;: nodename nor servname provided, or not known
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we&amp;rsquo;re using &lt;code&gt;kafkacat&lt;/code&gt; in producer mode (&lt;code&gt;-C&lt;/code&gt;) from our local machine to try and read from the topic. As before, because we&amp;rsquo;re getting the &lt;em&gt;internal&lt;/em&gt; listener hostname back from the broker in the metadata, the client cannot resolve that hostname to read/write from.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;i-saw-a-stackoverflow-answer-suggesting-to-just-update-my-hosts-file-isn-t-that-easier&#34;&gt;I saw a StackOverflow answer suggesting to just update my hosts file…isn&amp;rsquo;t that easier?&lt;/h3&gt;

&lt;p&gt;This is nothing more than a hack to workaround a mis-configuration, instead of actually fixing it.&lt;/p&gt;

&lt;p&gt;If the broker is reporting back a hostname to which the client cannot connect, then hardcoding the hostname/IP combo into the local &lt;code&gt;/etc/hosts&lt;/code&gt; may seem a nice fix. But this is a very brittle and manual solution. What happens when the IP changes, when you move hosts and forget to take the little hack with you, when other people want to do the same?&lt;/p&gt;

&lt;p&gt;Much better is to understand and actually fix the &lt;code&gt;advertised.listeners&lt;/code&gt; setting for your network.&lt;/p&gt;

&lt;h3 id=&#34;howto-connecting-to-kafka-on-docker&#34;&gt;HOWTO: Connecting to Kafka on Docker&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/docker01.png&#34; alt=&#34;images/docker01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Run within Docker, you will need to configure two listeners for Kafka:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Communication &lt;em&gt;within the Docker network&lt;/em&gt;. This could be inter-broker communication (i.e. between brokers), and between other components running in Docker such as Kafka Connect, or third-party clients or producers.&lt;/p&gt;

&lt;p&gt;For these comms, we need to use &lt;em&gt;the hostname of the Docker container(s)&lt;/em&gt;. Each Docker container on the same Docker network will use the hostname of the Kafka broker container to reach it&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Non-Docker network traffic. This could be clients running local on the Docker host machine, for example. The assumption is that they will connect on &lt;code&gt;localhost&lt;/code&gt;, to a port exposed from the Docker container.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the docker-compose snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka0:
    image: &amp;quot;confluentinc/cp-enterprise-kafka:5.0.0-rc3&amp;quot;
    ports:
    - &#39;9092:9092&#39;
    depends_on:
    - zookeeper
    environment:
    KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT
    […]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Clients &lt;em&gt;within&lt;/em&gt; the Docker network connect using listener &amp;ldquo;BOB&amp;rdquo;, with port 29092 and hostname &lt;code&gt;kafka0&lt;/code&gt;. In doing so, they get back the hostname &lt;code&gt;kafka0&lt;/code&gt; to which to connect. Each docker container will resolve &lt;code&gt;kafka0&lt;/code&gt; using Docker&amp;rsquo;s internal network, and be able to reach the broker.&lt;/li&gt;
&lt;li&gt;Clients &lt;em&gt;external&lt;/em&gt; to the Docker network connect using listener &amp;ldquo;FRED&amp;rdquo;, with port 9092 and hostname &lt;code&gt;localhost&lt;/code&gt;. Port 9092 is exposed by the Docker container and so available to connect to. When clients connect, they are given the hostname &lt;code&gt;localhost&lt;/code&gt; for the broker&amp;rsquo;s metadata, and so connect to this when reading/writing data.&lt;/li&gt;
&lt;li&gt;The above configuration would &lt;em&gt;not&lt;/em&gt; handle the scenario in which a client external to Docker &lt;em&gt;and&lt;/em&gt; external to the host machine wants to connect. This is because neither &lt;code&gt;kafka0&lt;/code&gt; (the internal Docker hostname) &lt;em&gt;or&lt;/em&gt; &lt;code&gt;localhost&lt;/code&gt; (the loopback address for the Docker host machine) would be resolvable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;howto-connecting-to-kafka-on-aws-iaas&#34;&gt;HOWTO: Connecting to Kafka on AWS/IaaS&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m naming AWS because it&amp;rsquo;s what the majority of people use, but this applies to any IaaS/Cloud solution.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Exactly the same concepts apply here as with Docker. The main difference is that whilst with Docker the external connections may well be just on localhost (as above), with Cloud-hosted Kafka (such as on AWS) the external connection will be from a machine not local to to the broker and which will need to be able to connect to the broker.&lt;/p&gt;

&lt;p&gt;A further complication is that whilst Docker networks are heavily segregated from the host&amp;rsquo;s, on IaaS often the &lt;em&gt;external&lt;/em&gt; hostname is resolvable &lt;em&gt;internally&lt;/em&gt;, making it hit and miss when you may actually encounter these problems.&lt;/p&gt;

&lt;p&gt;There are two approaches, depending on whether the external address through which you&amp;rsquo;re going to connect to the broker is also resolvable locally to all of the brokers on the network (e.g VPC).&lt;/p&gt;

&lt;h4 id=&#34;option-1-external-address-is-resolvable-locally&#34;&gt;Option 1 - external address IS resolvable locally&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/aws01-1.png&#34; alt=&#34;images/aws01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can get by with one listener here. The existing listener, called &lt;code&gt;PLAINTEXT&lt;/code&gt;, just needs overriding to set the advertised hostname (i.e. the one that is passed to inbound clients)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now connections both internally and externally will use &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; for connecting. Because &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; can be resolved both locally and externally, things work fine.&lt;/p&gt;

&lt;h4 id=&#34;option-2-external-address-is-not-resolvable-locally&#34;&gt;Option 2 - external address is NOT resolvable locally&lt;/h4&gt;

&lt;p&gt;You will need to configure two listeners for Kafka:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Communication &lt;em&gt;within the AWS network (VPC)&lt;/em&gt;. This could be inter-broker communication (i.e. between brokers), and between other components running in the VPC such as Kafka Connect, or third-party clients or producers.&lt;/p&gt;

&lt;p&gt;For these comms, we need to use &lt;em&gt;the internal IP of the EC2 machine&lt;/em&gt; (or hostname, if DNS is configured).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;External AWS traffic. This could be testing connectivity from a laptop, or simply from machines not hosted in Amazon. In both cases, the external IP of the instance needs to be used (or hostname, if DNS is configured).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/aws02.png&#34; alt=&#34;images/aws02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092
listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
advertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092
inter.broker.listener.name=INTERNAL
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;exploring-listeners-with-docker&#34;&gt;Exploring listeners with Docker&lt;/h3&gt;

&lt;p&gt;Take a look at &lt;a href=&#34;https://github.com/rmoff/kafka-listeners&#34;&gt;https://github.com/rmoff/kafka-listeners&lt;/a&gt;. This includes a docker-compose to bring up a Zookeeper instance along with Kafka broker configured with several listeners.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;BOB&lt;/code&gt; (port 29092) for internal traffic on the Docker network&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:29092 \
                    -L
Metadata for all topics (from broker 0: kafka0:29092/0):
1 brokers:
  broker 0 at kafka0:29092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;FRED&lt;/code&gt; (port 9092) for traffic from the Docker-host machine (&lt;code&gt;localhost&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:9092 \
                    -L
Metadata for all topics (from broker -1: kafka0:9092/bootstrap):
1 brokers:
  broker 0 at localhost:9092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;ALICE&lt;/code&gt; (port 29094) for traffic from outside, reaching the Docker host on the DNS name &lt;code&gt;never-gonna-give-you-up&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:29094 \
                    -L
Metadata for all topics (from broker -1: kafka0:29094/bootstrap):
1 brokers:
  broker 0 at never-gonna-give-you-up:29094
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;redux&#34;&gt;Redux&lt;/h3&gt;

&lt;p&gt;I recently referenced this post in &lt;a href=&#34;https://stackoverflow.com/questions/52438822/docker-kafka-dockerized-python-application/52440056#52440056&#34;&gt;a StackOverflow answer I gave&lt;/a&gt;, and re-articulated the solution. If you&amp;rsquo;re still not quite following, check it out and maybe second time around I&amp;rsquo;ll have explained it better:)&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/#brokerconfigs&#34;&gt;https://kafka.apache.org/documentation/#brokerconfigs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/KIP-2+-+Refactor+brokers+to+allow+listening+on+multiple+ports+and+IPs&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-2+-+Refactor+brokers+to+allow+listening+on+multiple+ports+and+IPs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/Multiple+Listeners+for+Kafka+Brokers&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Multiple+Listeners+for+Kafka+Brokers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners&#34;&gt;https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;still-not-sure&#34;&gt;Still not sure?&lt;/h3&gt;

&lt;p&gt;Check out the &lt;a href=&#34;http://cnfl.io/slack&#34;&gt;Confluent Community Slack group&lt;/a&gt; or &lt;a href=&#34;https://lists.apache.org/list.html?users@kafka.apache.org&#34;&gt;Apache Kafka user mailing list&lt;/a&gt; for more help.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Listeners - Explained</title>
			<link>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</link>
			<pubDate>Thu, 02 Aug 2018 19:38:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</guid>
			<description>

&lt;p&gt;This question comes up on StackOverflow and such places a &lt;strong&gt;lot&lt;/strong&gt;, so here&amp;rsquo;s something to try and help.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; : You need to set &lt;code&gt;advertised.listeners&lt;/code&gt; (or &lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address–and if that&amp;rsquo;s not reachable then problems ensue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/docker01.png&#34; alt=&#34;images/docker01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this post I&amp;rsquo;ll talk about &lt;em&gt;why&lt;/em&gt; this is necessary, and then show &lt;em&gt;how&lt;/em&gt; to do it, based on a couple of scenarios - Docker, and AWS.&lt;/p&gt;

&lt;h4 id=&#34;is-anyone-listening&#34;&gt;Is anyone listening?&lt;/h4&gt;

&lt;p&gt;Kafka is a distributed system. Data is read from &amp;amp; written to the &lt;em&gt;Leader&lt;/em&gt; for a given partition, which could be on any of the brokers in a cluster. When a client (producer/consumer) starts, it will request metadata about which broker is the leader for a partition—and it can do this from &lt;em&gt;any&lt;/em&gt; broker. The metadata returned will include the endpoints available for the Leader broker for that partition, and the client will then use those endpoints to connect to the broker to read/write data as required.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s these endpoints that cause people trouble. On a &lt;em&gt;single machine, running &amp;lsquo;bare metal&amp;rsquo;&lt;/em&gt; (no VMs, no Docker), everything might be the hostname (or just &lt;em&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/em&gt;) and it&amp;rsquo;s easy. But once you move into more complex networking setups, and multiple nodes, you have to pay more attention to it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume you have more than one network. This could be things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker internal network(s) plus host machine&lt;/li&gt;
&lt;li&gt;Brokers in the cloud (eg. AWS EC2), and on-premises machines locally (or even in another cloud)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You need to tell Kafka how the brokers can reach each other, but also make sure that external clients (producers/consumers) can reach the broker they need to.&lt;/p&gt;

&lt;p&gt;The key thing is that when you run a client, &lt;strong&gt;the broker you pass to it is &lt;em&gt;just where it&amp;rsquo;s going to go and get the metadata about brokers in the cluster from&lt;/em&gt;&lt;/strong&gt;. The actual host &amp;amp; IP that it will connect to for reading/writing data is based on &lt;strong&gt;&lt;em&gt;the data that the broker passes back in that initial connection&lt;/em&gt;&lt;/strong&gt;—even if it&amp;rsquo;s just a single node and the broker returned is the same as the one connected to.&lt;/p&gt;

&lt;p&gt;For configuring this correctly, you need to understand that Kafka brokers can have multiple &lt;em&gt;listeners&lt;/em&gt;. A listener is a combination of&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Host/IP&lt;/li&gt;
&lt;li&gt;Port&lt;/li&gt;
&lt;li&gt;Protocol&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s check out some config. Often the protocol is used for the listener name too, but here let&amp;rsquo;s make it nice and clear by using abstract names for the listeners:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT
KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m using the Docker config names—the equivalents if you&amp;rsquo;re configuring &lt;code&gt;server.properties&lt;/code&gt; directly (e.g. on AWS etc) are shown indented in the following list&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;KAFKA_LISTENERS&lt;/code&gt; is a comma-separated list of listeners, and the host/ip and port to which Kafka binds to on which to listen. For more complex networking this might be an IP address associated with a given network interface on a machine. The default is 0.0.0.0, which means listening on all interfaces.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;listeners&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; is a comma-separated list of listeners with their the host/ip and port. This is the metadata that&amp;rsquo;s passed back to clients.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;advertised.listeners&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KAFKA_LISTENER_SECURITY_PROTOCOL_MAP&lt;/code&gt; defines key/value pairs for the security protocol to use, per listener name.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;listener.security.protocol.map&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Kafka brokers communicate between themselves&lt;/em&gt;, usually on the internal network (e.g. Docker network, AWS VPC, etc). To define which listener to use, specify &lt;code&gt;KAFKA_INTER_BROKER_LISTENER_NAME&lt;/code&gt; (&lt;code&gt;inter.broker.listener.name&lt;/code&gt;). The host/IP used must be accessible from the broker machine to others.&lt;/p&gt;

&lt;p&gt;Kafka &lt;em&gt;clients&lt;/em&gt; may well not be local to the broker&amp;rsquo;s network, and this is where the additional listeners come in.&lt;/p&gt;

&lt;p&gt;Each listener will, when connected to, report back the address on which it can be reached. &lt;em&gt;The address on which you reach a broker depends on the network used&lt;/em&gt;. If you&amp;rsquo;re connecting to the broker from an internal network it&amp;rsquo;s going to be a different host/IP than when connecting externally.&lt;/p&gt;

&lt;p&gt;When connecting to a broker, the listener that will be returned to the client will be the listener to which you connected (based on the port).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kafkacat&lt;/code&gt; is a useful tool for exploring this. Using &lt;code&gt;-L&lt;/code&gt; you can see the metadata for the listener to which you connected.
Based on the same listener config as above (&lt;code&gt;LISTENER_BOB&lt;/code&gt; / &lt;code&gt;LISTENER_FRED&lt;/code&gt;), check out the respective entries for &lt;strong&gt;&lt;code&gt;broker 0 at&lt;/code&gt;&lt;/strong&gt;: -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Connecting on port 9092 (which we map as &lt;code&gt;LISTENER_FRED&lt;/code&gt;), the broker&amp;rsquo;s address is given back as &lt;code&gt;localhost&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b kafka0:9092 \
           -L
Metadata for all topics (from broker -1: kafka0:9092/bootstrap):
1 brokers:
  broker 0 at localhost:9092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Connecting on port 29092 (which we map as &lt;code&gt;LISTENER_BOB&lt;/code&gt;), the broker&amp;rsquo;s address is given back as &lt;code&gt;kafka0&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b kafka0:29092 \
           -L
Metadata for all topics (from broker 0: kafka0:29092/0):
1 brokers:
  broker 0 at kafka0:29092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can also use &lt;code&gt;tcpdump&lt;/code&gt; to examine the traffic from a client connecting to the broker, and spot the hostname that&amp;rsquo;s returned from the broker.&lt;/p&gt;

&lt;h4 id=&#34;why-can-i-connect-to-the-broker-but-the-client-still-fails&#34;&gt;Why can I connect to the broker, but the client still fails?&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;tl;dr&lt;/em&gt; Even if you can make the initial connection to the broker, the address returned in the metadata may still be for a hostname that is not accessible from your client.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s walk this through step by step.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;We&amp;rsquo;ve got a broker on AWS. We want to send a message to it from our laptop. We know the external hostname for the EC2 instance (&lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt;). We&amp;rsquo;ve created the necessary entry in the security group to open the broker&amp;rsquo;s port to our inbound traffic. We do smart things like checking that our local machine can connect to the port on the AWS instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nc -vz ec2-54-191-84-122.us-west-2.compute.amazonaws.com 9092
found 0 associations
found 1 connections:
    1:  flags=82&amp;lt;CONNECTED,PREFERRED&amp;gt;
  outif utun5
  src 172.27.230.23 port 53352
  dst 54.191.84.122 port 9092
  rank info not available
  TCP aux info available

Connection to ec2-54-191-84-122.us-west-2.compute.amazonaws.com port 9092 [tcp/XmlIpcRegSvc] succeeded!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Things are looking good! We run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;test&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now…what happens next?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Our laptop resolves &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; successfully (to the IP address 54.191.84.122), and connects to the AWS machine on port 9092&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The broker receives the inbound connection on port 9092. &lt;em&gt;It returns the metadata to the client, with the hostname &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt;&lt;/em&gt; because this is the host name of the broker and the default value for &lt;code&gt;listeners&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The client the tries to send data to the broker using the metadata it was given. Since &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt; is not resolvable from the internet, it fails.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;test&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Puzzled, we try the same thing from the broker machine itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;foo&amp;quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test
&amp;gt;&amp;gt;
$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginning
foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works fine! That&amp;rsquo;s because we are connecting to port 9092, which is configured as the &lt;em&gt;internal&lt;/em&gt; listener, and thus reports back its hostname as &lt;code&gt;ip-172-31-18-160.us-west-2.compute.internal&lt;/code&gt; which &lt;em&gt;is&lt;/em&gt; resolvable from the broker machine (since it&amp;rsquo;s its own hostname!)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We can make life even easier by using &lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;&lt;code&gt;kafkacat&lt;/code&gt;&lt;/a&gt;. Using the &lt;code&gt;-L&lt;/code&gt; flag we can see the metadata returned by the broker:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -L
Metadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):
1 brokers:
  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Clear as day, the &lt;em&gt;internal&lt;/em&gt; hostname is returned. This also makes this seemingly-confusing error make a lot more sense—connecting to one hostname, getting a lookup error on another:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -C -t test
% ERROR: Local: Host resolution failure: ip-172-31-18-160.us-west-2.compute.internal:9092/0: Failed to resolve &#39;ip-172-31-18-160.us-west-2.compute.internal:9092&#39;: nodename nor servname provided, or not known
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we&amp;rsquo;re using &lt;code&gt;kafkacat&lt;/code&gt; in producer mode (&lt;code&gt;-C&lt;/code&gt;) from our local machine to try and read from the topic. As before, because we&amp;rsquo;re getting the &lt;em&gt;internal&lt;/em&gt; listener hostname back from the broker in the metadata, the client cannot resolve that hostname to read/write from.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;i-saw-a-stackoverflow-answer-suggesting-to-just-update-my-hosts-file-isn-t-that-easier&#34;&gt;I saw a StackOverflow answer suggesting to just update my hosts file…isn&amp;rsquo;t that easier?&lt;/h4&gt;

&lt;p&gt;This is nothing more than a hack to workaround a mis-configuration, instead of actually fixing it.&lt;/p&gt;

&lt;p&gt;If the broker is reporting back a hostname to which the client cannot connect, then hardcoding the hostname/IP combo into the local &lt;code&gt;/etc/hosts&lt;/code&gt; may seem a nice fix. But this is a very brittle and manual solution. What happens when the IP changes, when you move hosts and forget to take the little hack with you, when other people want to do the same?&lt;/p&gt;

&lt;p&gt;Much better is to understand and actually fix the &lt;code&gt;advertised.listeners&lt;/code&gt; setting for your network.&lt;/p&gt;

&lt;h4 id=&#34;howto-connecting-to-kafka-on-docker&#34;&gt;HOWTO: Connecting to Kafka on Docker&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/docker01.png&#34; alt=&#34;images/docker01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Run within Docker, you will need to configure two listeners for Kafka:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Communication &lt;em&gt;within the Docker network&lt;/em&gt;. This could be inter-broker communication (i.e. between brokers), and between other components running in Docker such as Kafka Connect, or third-party clients or producers.&lt;/p&gt;

&lt;p&gt;For these comms, we need to use &lt;em&gt;the hostname of the Docker container(s)&lt;/em&gt;. Each Docker container on the same Docker network will use the hostname of the Kafka broker container to reach it&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Non-Docker network traffic. This could be clients running local on the Docker host machine, for example. The assumption is that they will connect on &lt;code&gt;localhost&lt;/code&gt;, to a port exposed from the Docker container.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the docker-compose snippet:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka0:
    image: &amp;quot;confluentinc/cp-enterprise-kafka:5.0.0-rc3&amp;quot;
    ports:
    - &#39;9092:9092&#39;
    depends_on:
    - zookeeper
    environment:
    KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT
    […]
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;Clients &lt;em&gt;within&lt;/em&gt; the Docker network connect using listener &amp;ldquo;BOB&amp;rdquo;, with port 29092 and hostname &lt;code&gt;kafka0&lt;/code&gt;. In doing so, they get back the hostname &lt;code&gt;kafka0&lt;/code&gt; to which to connect. Each docker container will resolve &lt;code&gt;kafka0&lt;/code&gt; using Docker&amp;rsquo;s internal network, and be able to reach the broker.&lt;/li&gt;
&lt;li&gt;Clients &lt;em&gt;external&lt;/em&gt; to the Docker network connect using listener &amp;ldquo;FRED&amp;rdquo;, with port 9092 and hostname &lt;code&gt;localhost&lt;/code&gt;. Port 9092 is exposed by the Docker container and so available to connect to. When clients connect, they are given the hostname &lt;code&gt;localhost&lt;/code&gt; for the broker&amp;rsquo;s metadata, and so connect to this when reading/writing data.&lt;/li&gt;
&lt;li&gt;The above configuration would &lt;em&gt;not&lt;/em&gt; handle the scenario in which a client external to Docker &lt;em&gt;and&lt;/em&gt; external to the host machine wants to connect. This is because neither &lt;code&gt;kafka0&lt;/code&gt; (the internal Docker hostname) &lt;em&gt;or&lt;/em&gt; &lt;code&gt;localhost&lt;/code&gt; (the loopback address for the Docker host machine) would be resolvable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;howto-connecting-to-kafka-on-aws-iaas&#34;&gt;HOWTO: Connecting to Kafka on AWS/IaaS&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m naming AWS because it&amp;rsquo;s what the majority of people use, but this applies to any IaaS/Cloud solution.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Exactly the same concepts apply here as with Docker. The main difference is that whilst with Docker the external connections may well be just on localhost (as above), with Cloud-hosted Kafka (such as on AWS) the external connection will be from a machine not local to to the broker and which will need to be able to connect to the broker.&lt;/p&gt;

&lt;p&gt;A further complication is that whilst Docker networks are heavily segregated from the host&amp;rsquo;s, on IaaS often the &lt;em&gt;external&lt;/em&gt; hostname is resolvable &lt;em&gt;internally&lt;/em&gt;, making it hit and miss when you may actually encounter these problems.&lt;/p&gt;

&lt;p&gt;There are two approaches, depending on whether the external address through which you&amp;rsquo;re going to connect to the broker is also resolvable locally to all of the brokers on the network (e.g VPC).&lt;/p&gt;

&lt;h5 id=&#34;option-1-external-address-is-resolvable-locally&#34;&gt;Option 1 - external address IS resolvable locally&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/aws01-1.png&#34; alt=&#34;images/aws01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can get by with one listener here. The existing listener, called &lt;code&gt;PLAINTEXT&lt;/code&gt;, just needs overriding to set the advertised hostname (i.e. the one that is passed to inbound clients)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now connections both internally and externally will use &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; for connecting. Because &lt;code&gt;ec2-54-191-84-122.us-west-2.compute.amazonaws.com&lt;/code&gt; can be resolved both locally and externally, things work fine.&lt;/p&gt;

&lt;h5 id=&#34;option-2-external-address-is-not-resolvable-locally&#34;&gt;Option 2 - external address is NOT resolvable locally&lt;/h5&gt;

&lt;p&gt;You will need to configure two listeners for Kafka:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Communication &lt;em&gt;within the AWS network (VPC)&lt;/em&gt;. This could be inter-broker communication (i.e. between brokers), and between other components running in the VPC such as Kafka Connect, or third-party clients or producers.&lt;/p&gt;

&lt;p&gt;For these comms, we need to use &lt;em&gt;the internal IP of the EC2 machine&lt;/em&gt; (or hostname, if DNS is configured).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;External AWS traffic. This could be testing connectivity from a laptop, or simply from machines not hosted in Amazon. In both cases, the external IP of the instance needs to be used (or hostname, if DNS is configured).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/08/aws02.png&#34; alt=&#34;images/aws02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092
listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
advertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092
inter.broker.listener.name=INTERNAL
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;exploring-listeners-with-docker&#34;&gt;Exploring listeners with Docker&lt;/h4&gt;

&lt;p&gt;Take a look at &lt;a href=&#34;https://github.com/rmoff/kafka-listeners&#34;&gt;https://github.com/rmoff/kafka-listeners&lt;/a&gt;. This includes a docker-compose to bring up a Zookeeper instance along with Kafka broker configured with several listeners.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;BOB&lt;/code&gt; (port 29092) for internal traffic on the Docker network&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:29092 \
                    -L
Metadata for all topics (from broker 0: kafka0:29092/0):
1 brokers:
  broker 0 at kafka0:29092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;FRED&lt;/code&gt; (port 9092) for traffic from the Docker-host machine (&lt;code&gt;localhost&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:9092 \
                    -L
Metadata for all topics (from broker -1: kafka0:9092/bootstrap):
1 brokers:
  broker 0 at localhost:9092
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Listener &lt;code&gt;ALICE&lt;/code&gt; (port 29094) for traffic from outside, reaching the Docker host on the DNS name &lt;code&gt;never-gonna-give-you-up&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -t --network kafka-listeners_default \
            confluentinc/cp-kafkacat \
            kafkacat -b kafka0:29094 \
                    -L
Metadata for all topics (from broker -1: kafka0:29094/bootstrap):
1 brokers:
  broker 0 at never-gonna-give-you-up:29094
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;redux&#34;&gt;Redux&lt;/h4&gt;

&lt;p&gt;I recently referenced this post in &lt;a href=&#34;https://stackoverflow.com/questions/52438822/docker-kafka-dockerized-python-application/52440056#52440056&#34;&gt;a StackOverflow answer I gave&lt;/a&gt;, and re-articulated the solution. If you&amp;rsquo;re still not quite following, check it out and maybe second time around I&amp;rsquo;ll have explained it better:)&lt;/p&gt;

&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/#brokerconfigs&#34;&gt;https://kafka.apache.org/documentation/#brokerconfigs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/KIP-2+-+Refactor+brokers+to+allow+listening+on+multiple+ports+and+IPs&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-2+-+Refactor+brokers+to+allow+listening+on+multiple+ports+and+IPs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/Multiple+Listeners+for+Kafka+Brokers&#34;&gt;https://cwiki.apache.org/confluence/display/KAFKA/Multiple+Listeners+for+Kafka+Brokers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners&#34;&gt;https://stackoverflow.com/questions/42998859/kafka-server-configuration-listeners-vs-advertised-listeners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;still-not-sure&#34;&gt;Still not sure?&lt;/h4&gt;

&lt;p&gt;Check out the &lt;a href=&#34;http://cnfl.io/slack&#34;&gt;Confluent Community Slack group&lt;/a&gt; or &lt;a href=&#34;https://lists.apache.org/list.html?users@kafka.apache.org&#34;&gt;Apache Kafka user mailing list&lt;/a&gt; for more help.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Syntax highlighting code for presentation slides</title>
			<link>https://rmoff.github.io/2018/06/20/syntax-highlighting-code-for-presentation-slides/</link>
			<pubDate>Wed, 20 Jun 2018 18:32:10 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/06/20/syntax-highlighting-code-for-presentation-slides/</guid>
			<description>

&lt;p&gt;So you&amp;rsquo;ve got a code sample you want to share in a presentation, but whilst it looks beautiful in your text-editor with syntax highlighting, it&amp;rsquo;s fugly in Keynote? You could screenshot it and paste the image into your slide, but you just know that you&amp;rsquo;ll want to change that code, and end up re-snapshotting it…what a PITA.&lt;/p&gt;

&lt;p&gt;Better to have a nicely syntax-highlighted code snippet that you can paste as formatted text into Keynote and amend from there as needed. Here&amp;rsquo;s how.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://pygments.org/&#34;&gt;Pygments&lt;/a&gt; is a library that does syntax highlighting for you, and you can access it through the &lt;code&gt;pygmentize&lt;/code&gt; script. Install:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install Pygments
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can pass it a file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pygmentize -l json /tmp/foo.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and get some nice output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-20_17-25-43.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s &lt;a href=&#34;https://help.farbox.com/pygments.html&#34;&gt;different styles&lt;/a&gt; to chose from:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pygmentize -l json -f terminal256 -O style=emacs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-20_17-26-37.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just a tip - black background looks l33t, but sucks for being able to read from a projector. Use iTerm&amp;rsquo;s multiple profiles option to set up a white background for whenever you&amp;rsquo;re projecting your screen or copying formatting text like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-20_17-28-31.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;getting-it-into-keynote-or-pptx-if-you-must&#34;&gt;Getting it into Keynote (or PPTX, if you must)&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re using iTerm you can select the text and then use the &lt;strong&gt;Copy with Styles&lt;/strong&gt; option from the Edit menu (press Alt to access this option), and paste the results directly into Keynote with the formatting preserved.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-20_17-29-25.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From here you can change the font, size, line spacing etc etc - much easier than having to re-do the screengrab each time.&lt;/p&gt;

&lt;p&gt;If you also want to format your JSON with linebreaks, run it through &lt;code&gt;jq&lt;/code&gt; first:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /tmp/foo.json | jq &#39;.&#39; | pygmentize -l json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-20_20-09-22.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;output-to-html&#34;&gt;Output to HTML&lt;/h3&gt;

&lt;p&gt;As well as writing the highlighted code to the terminal, you can output to things like HTML, for nice embedding in…blogs!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat /tmp/foo.json | jq &#39;.&#39; | pygmentize -l json -f html -O noclasses
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;highlight&amp;quot; style=&amp;quot;background: #f8f8f8&amp;quot;&amp;gt;&amp;lt;pre style=&amp;quot;line-height: 125%&amp;quot;&amp;gt;&amp;lt;span&amp;gt;&amp;lt;/span&amp;gt;{
  &amp;lt;span style=&amp;quot;color: #008000; font-weight: bold&amp;quot;&amp;gt;&amp;amp;quot;id&amp;amp;quot;&amp;lt;/span&amp;gt;: &amp;lt;span style=&amp;quot;color: #666666&amp;quot;&amp;gt;2&amp;lt;/span&amp;gt;,
  &amp;lt;span style=&amp;quot;color: #008000; font-weight: bold&amp;quot;&amp;gt;&amp;amp;quot;first_name&amp;amp;quot;&amp;lt;/span&amp;gt;: &amp;lt;span style=&amp;quot;color: #BA2121&amp;quot;&amp;gt;&amp;amp;quot;Merilyn&amp;amp;quot;&amp;lt;/span&amp;gt;,
  &amp;lt;span style=&amp;quot;color: #008000; font-weight: bold&amp;quot;&amp;gt;&amp;amp;quot;last_name&amp;amp;quot;&amp;lt;/span&amp;gt;: &amp;lt;span style=&amp;quot;color: #BA2121&amp;quot;&amp;gt;&amp;amp;quot;Doughartie&amp;amp;quot;&amp;lt;/span&amp;gt;
}
&amp;lt;/pre&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which looks like:&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #f8f8f8&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;{
  &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span style=&#34;color: #666666&#34;&gt;2&lt;/span&gt;,
  &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;&amp;quot;first_name&amp;quot;&lt;/span&gt;: &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;Merilyn&amp;quot;&lt;/span&gt;,
  &lt;span style=&#34;color: #008000; font-weight: bold&#34;&gt;&amp;quot;last_name&amp;quot;&lt;/span&gt;: &lt;span style=&#34;color: #BA2121&#34;&gt;&amp;quot;Doughartie&amp;quot;&lt;/span&gt;
}
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There&amp;rsquo;s other &lt;a href=&#34;http://pygments.org/docs/formatters/&#34;&gt;formatters&lt;/a&gt;, including generating images, which is quite nifty.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/json.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For images, run &lt;code&gt;pip install PILLOW&lt;/code&gt; for the required library first.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;support-for-sql-and-more&#34;&gt;Support for SQL, and more!&lt;/h3&gt;

&lt;p&gt;Pygments supports &lt;a href=&#34;http://pygments.org/languages/&#34;&gt;multiple languages&lt;/a&gt; too, and can do things like enforce upper-case for SQL. If you don&amp;rsquo;t pass a file to &lt;code&gt;pygmentize&lt;/code&gt; then it&amp;rsquo;ll read from &lt;code&gt;stdin&lt;/code&gt; - press Ctrl-D to end input and see the result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pygmentize -l sql -F keywordcase:case=upper
&lt;/code&gt;&lt;/pre&gt;

&lt;script src=&#34;https://asciinema.org/a/xH1Wy06DDMtKw0wUbhUsgFSAJ.js&#34; id=&#34;asciicast-xH1Wy06DDMtKw0wUbhUsgFSAJ&#34; async&gt;&lt;/script&gt;
</description>
		</item>
		
		<item>
			<title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
			<link>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
			<pubDate>Sun, 17 Jun 2018 11:35:20 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
			<description>&lt;p&gt;In &lt;a href=&#34;http://cnfl.io/syslogs-filtering&#34;&gt;this article&lt;/a&gt; I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/&#34;&gt;enrich streams&lt;/a&gt;. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.&lt;/p&gt;

&lt;p&gt;The data came from my home &lt;a href=&#34;https://www.ubnt.com/&#34;&gt;Ubiquiti&lt;/a&gt; router, and took two forms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A stream of network events from the router, sent over &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-part-1-filtering&#34;&gt;syslog&lt;/a&gt; to Apache Kafka&lt;/li&gt;
&lt;li&gt;Device information that the router stores in an internal MongoDB database, streamed into Kafka using &lt;a href=&#34;https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/&#34;&gt;Debezium and Kafka Connect&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/&#34;&gt;KSQL I denormalised the two sets of data&lt;/a&gt;, enriching each network event with full details of the associated device, based on the device&amp;rsquo;s ID stored in each network event. The device information came from MongoDB, a copy of which was in a Kafka topic along with any changes that subsequently occurred in the MongoDB data (courtesy of CDC and Debezium).&lt;/p&gt;

&lt;p&gt;From the Kibana dashboard I built, I noticed an interesting pattern. Around 29th April, there is a huge peak in activity relative to the rest of the time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_01.png&#34; alt=&#34;ubnt_analyse_01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wonder what&amp;rsquo;s causing this? Let&amp;rsquo;s drill into the data a bit more. Looking closely at the device type and Access Point, we can see it&amp;rsquo;s the &amp;ldquo;Attic&amp;rdquo; AP, and &amp;ldquo;Espressi&amp;rdquo; device types&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_02.png&#34; alt=&#34;images/ubnt_analyse_02.png&#34; /&gt;
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_03.png&#34; alt=&#34;images/ubnt_analyse_03.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using Kibana&amp;rsquo;s filtering to isolate the data to just these two facets, it&amp;rsquo;s clear that it&amp;rsquo;s just a particular device that&amp;rsquo;s so busy&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_04.png&#34; alt=&#34;images/ubnt_analyse_04.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Attic lights plug&amp;rdquo; is, as the name suggests, a wifi plug that I have controlling the lights in the attic of my house. But why&amp;rsquo;s it got so much activity, compared to usual? Let&amp;rsquo;s look at the times again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_05.png&#34; alt=&#34;images/ubnt_analyse_05.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Roughly 10:00 on the 28th April, through to c.18:00 on the 30th April. The missing data here necessary for the automatic correlation is the howls of complaint from my kids when Netflix wouldn&amp;rsquo;t work this weekend—my home internet connection was down!&lt;/p&gt;

&lt;p&gt;So from the looks of it, this wifi plug is not at all happy when it can&amp;rsquo;t &amp;lsquo;phone home&amp;rsquo;, and so tries reconnecting to the AP again…and again…and again!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_07.png&#34; alt=&#34;images/ubnt_analyse_07.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The frequency works out at just under one connection attempt per minute, as can be seen from this graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/ubnt_analyse_06.png&#34; alt=&#34;images/ubnt_analyse_06.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So yay for dataviz and analytics. But—what can we do with this new-found knowledge? Watching a dashboard to look for this happening again is not so useful. Since we know what the pattern is, perhaps we can encode this into an application, and have an automatic alert tell me when it looks like my broadband&amp;rsquo;s gone offline?&lt;/p&gt;

&lt;p&gt;The pattern we want to catch is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A particular device (&amp;ldquo;Attic lights plug&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;Connecting to an Access Point&lt;/li&gt;
&lt;li&gt;At a rate of once per minute—or to not make it too sensitive, let&amp;rsquo;s say more than twice in a five minute period&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Traditionally, doing this kind of realtime detection against a stream of inbound data would require some serious coding beyond the scope of many. Even seasoned programmers might not be familiar with the latest stream processing libraries. But what almost all programmers, developers, and even hand-crafted artisanal data engineers are familiar with is SQL! Let&amp;rsquo;s see what the above English statement of the pattern looks like in KSQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ksql&amp;gt; SELECT USER_DEVICE_NAME, COUNT(*) AS AP_CONNECT_COUNT \
FROM UBNT_AP_USER_DEVICE_CONNECTS WINDOW TUMBLING (SIZE 5 MINUTES) \
WHERE USER_DEVICE_NAME=&#39;Attic lights plug&#39; \
GROUP BY USER_DEVICE_NAME \
HAVING COUNT(*)&amp;gt;2;

Attic lights plug | 7
Attic lights plug | 8
Attic lights plug | 7
Attic lights plug | 7
Attic lights plug | 5
Attic lights plug | 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can persist this as a Kafka topic, so that any new instances of this condition being met (i.e. a signal that my internet might be down!), are written to a Kafka topic that I can use to drive an alert:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE OFFLINE_WARNING_SIGNAL AS \
SELECT USER_DEVICE_NAME, COUNT(*) AS AP_CONNECT_COUNT \
FROM UBNT_AP_USER_DEVICE_CONNECTS WINDOW TUMBLING (SIZE 5 MINUTES) \
WHERE USER_DEVICE_NAME=&#39;Attic lights plug&#39; \
GROUP BY USER_DEVICE_NAME \
HAVING COUNT(*)&amp;gt;2;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now I have a Kafka topic (&lt;code&gt;OFFLINE_WARNING_SIGNAL&lt;/code&gt;) that I can do something like hook up to a Python driven alert as &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-with-apache-kafka-and-ksql-part-2-event-driven-alerting-with-slack/&#34;&gt;illustrated here&lt;/a&gt;. All this driven with a simple SQL expression, in effect giving us a full-blown stream processing application!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/slack_notify_01.jpg&#34; alt=&#34;iOS Slack Alert&#34; /&gt;&lt;/p&gt;

&lt;p&gt;How cool is that: expressing patterns of interest in data, and building it into a real-time stream processing application, all using SQL!&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Compare and apply a diff / patch recursively</title>
			<link>https://rmoff.github.io/2018/06/07/compare-and-apply-a-diff-patch-recursively/</link>
			<pubDate>Thu, 07 Jun 2018 14:35:36 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/06/07/compare-and-apply-a-diff-patch-recursively/</guid>
			<description>&lt;p&gt;Hacky way to keep config files in sync when there&amp;rsquo;s a new version of some software.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Caveat : probably completely wrong, may not pick up config entries added in the new version, etc. But, &lt;em&gt;works for me right here right now&lt;/em&gt; ;-)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s say we have two folders:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent-4.1.0
confluent-4.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same structures, different versions. 4.1.0 was set up with our local config in &lt;code&gt;./etc&lt;/code&gt;, that we want to preserve. We can use &lt;code&gt;diff&lt;/code&gt; to easily see what&amp;rsquo;s changed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;diff -r confluent-4.1.0/etc confluent-4.1.0/etc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this only tells us what changed. Nicer is to automagically apply it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s run this from the folder in which we&amp;rsquo;re going to apply the changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd confluent-4.1.1/etc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the &lt;code&gt;diff&lt;/code&gt;, and write the results to a file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;diff -ur . ../../confluent-4.1.0/etc &amp;gt; 4.1.1.patch 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now apply it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;patch -p0 &amp;lt; 4.1.1.patch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, you took a backup first before you did that, just in case something broke… right?&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect and Oracle data types</title>
			<link>https://rmoff.github.io/2018/05/21/kafka-connect-and-oracle-data-types/</link>
			<pubDate>Mon, 21 May 2018 08:59:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/05/21/kafka-connect-and-oracle-data-types/</guid>
			<description>

&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/current/connect/connect-jdbc/docs/source_connector.html&#34;&gt;Kafka Connect JDBC Connector&lt;/a&gt; by default does not cope so well with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NUMBER&lt;/code&gt; columns with no defined precision/scale. You may end up with apparent junk (&lt;code&gt;bytes&lt;/code&gt;) in the output, or just errors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TIMESTAMP WITH LOCAL TIME ZONE&lt;/code&gt;. Throws &lt;code&gt;JDBC type -102 not currently supported&lt;/code&gt; warning in the log.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Read more about &lt;code&gt;NUMBER&lt;/code&gt; data type in the &lt;a href=&#34;https://docs.oracle.com/database/121/SQLRF/sql_elements001.htm#SQLRF002220&#34;&gt;Oracle docs&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr-how-do-i-make-it-work&#34;&gt;tl;dr : How do I make it work?&lt;/h3&gt;

&lt;p&gt;There are several options:&lt;/p&gt;

&lt;h4 id=&#34;new-in-confluent-platform-4-1-1-numeric-mapping&#34;&gt;New in Confluent Platform 4.1.1 : &lt;code&gt;numeric.mapping&lt;/code&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;In the connector configuration, set &lt;code&gt;&amp;quot;numeric.mapping&amp;quot;:&amp;quot;best_fit&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;New in Confluent Platform 4.1.1 (&lt;a href=&#34;https://docs.confluent.io/current/connect/connect-jdbc/docs/source_config_options.html#database&#34;&gt;Doc&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;avoid-the-problem-in-the-first-place&#34;&gt;Avoid the problem in the first place&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Change the DDL of the source object. For example:

&lt;ul&gt;
&lt;li&gt;refine the &lt;code&gt;NUMBER&lt;/code&gt; &amp;rsquo;s precision and scale&lt;/li&gt;
&lt;li&gt;Use a &lt;code&gt;TIMESTAMP&lt;/code&gt; type that is supported&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;cast-the-datatypes-in-the-query&#34;&gt;CAST the datatypes in the &lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Pull from the object directly, and use &lt;code&gt;query&lt;/code&gt; in the JDBC connector (instead of &lt;code&gt;table.whitelist&lt;/code&gt;)—and cast the columns appropriately:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
  -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
  -d &#39;{
        &amp;quot;name&amp;quot;: &amp;quot;jdbc_source_oracle_soe_logon_07&amp;quot;,
        &amp;quot;config&amp;quot;: {
                &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.jdbc.JdbcSourceConnector&amp;quot;,
                &amp;quot;connection.url&amp;quot;: &amp;quot;jdbc:oracle:thin:soe/soe@localhost:1521/ORCLPDB1&amp;quot;,
                &amp;quot;mode&amp;quot;: &amp;quot;incrementing&amp;quot;,
                &amp;quot;query&amp;quot;: &amp;quot;SELECT CAST(LOGON_ID AS NUMERIC(8,0)) AS LOGON_ID, CAST(CUSTOMER_ID AS NUMERIC(18,0)) AS CUSTOMER_ID, LOGON_DATE FROM LOGON&amp;quot;,
                &amp;quot;poll.interval.ms&amp;quot;: &amp;quot;1000&amp;quot;,
                &amp;quot;incrementing.column.name&amp;quot;:&amp;quot;LOGON_ID&amp;quot;,
                &amp;quot;topic.prefix&amp;quot;: &amp;quot;ora-soe-07-LOGON&amp;quot;,
                &amp;quot;validate.non.null&amp;quot;:false
        }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Robin@asgard02 ~/cp&amp;gt; kafka-avro-console-consumer \
                        --bootstrap-server localhost:9092 \
                        --property schema.registry.url=http://localhost:8081 \
                        --topic ora-soe-07-LOGON --from-beginning --max-messages 1| jq &#39;.&#39;
{
  &amp;quot;LOGON_ID&amp;quot;: {
    &amp;quot;int&amp;quot;: 2
  },
  &amp;quot;CUSTOMER_ID&amp;quot;: {
    &amp;quot;long&amp;quot;: 48645
  },
  &amp;quot;LOGON_DATE&amp;quot;: {
    &amp;quot;long&amp;quot;: 962854648000
  }
}
Processed a total of 1 messages
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;use-a-view-in-the-source-database-to-cast-the-data-types&#34;&gt;Use a View in the source database to cast the data types&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Define a view in the source DB that casts the columns appropriately, and then use the connector against this instead (make sure to include &lt;code&gt;&amp;quot;table.types&amp;quot;:&amp;quot;VIEW&amp;quot;&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW VW_LOGON AS SELECT CAST(LOGON_ID AS NUMERIC(8,0)) AS LOGON_ID, CAST(CUSTOMER_ID AS NUMERIC(18,0)) AS CUSTOMER_ID, LOGON_DATE FROM LOGON;

curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
  -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
  -d &#39;{
        &amp;quot;name&amp;quot;: &amp;quot;jdbc_source_oracle_soe_logon_05&amp;quot;,
        &amp;quot;config&amp;quot;: {
                &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.jdbc.JdbcSourceConnector&amp;quot;,
                &amp;quot;connection.url&amp;quot;: &amp;quot;jdbc:oracle:thin:soe/soe@localhost:1521/ORCLPDB1&amp;quot;,
                &amp;quot;table.whitelist&amp;quot;:&amp;quot;VW_LOGON&amp;quot;,
                &amp;quot;table.types&amp;quot;:&amp;quot;VIEW&amp;quot;,
                &amp;quot;mode&amp;quot;: &amp;quot;incrementing&amp;quot;,
                &amp;quot;poll.interval.ms&amp;quot;: &amp;quot;1000&amp;quot;,
                &amp;quot;incrementing.column.name&amp;quot;:&amp;quot;LOGON_ID&amp;quot;,
                &amp;quot;topic.prefix&amp;quot;: &amp;quot;ora-soe-05-&amp;quot;,
                &amp;quot;validate.non.null&amp;quot;:false
        }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Happy data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Robin@asgard02 ~/cp&amp;gt; kafka-avro-console-consumer \
                        --bootstrap-server localhost:9092 \
                        --property schema.registry.url=http://localhost:8081 \
                        --topic ora-soe-05-VW_LOGON --from-beginning --max-messages 1| jq &#39;.&#39;
{
  &amp;quot;LOGON_ID&amp;quot;: {
    &amp;quot;int&amp;quot;: 2
  },
  &amp;quot;CUSTOMER_ID&amp;quot;: {
    &amp;quot;long&amp;quot;: 48645
  },
  &amp;quot;LOGON_DATE&amp;quot;: {
    &amp;quot;long&amp;quot;: 962854648000
  }
}
Processed a total of 1 messages
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-happens&#34;&gt;What happens&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; DESCRIBE LOGON;
 Name                                      Null?    Type
 ----------------------------------------- -------- ----------------------------
 LOGON_ID                                  NOT NULL NUMBER
 CUSTOMER_ID                               NOT NULL NUMBER
 LOGON_DATE                                         DATE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the ID column doesn&amp;rsquo;t work:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
  -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
  -d &#39;{
        &amp;quot;name&amp;quot;: &amp;quot;jdbc_source_oracle_soe_logon_01&amp;quot;,
        &amp;quot;config&amp;quot;: {
                &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.jdbc.JdbcSourceConnector&amp;quot;,
                &amp;quot;connection.url&amp;quot;: &amp;quot;jdbc:oracle:thin:soe/soe@localhost:1521/ORCLPDB1&amp;quot;,
                &amp;quot;table.whitelist&amp;quot;:&amp;quot;LOGON&amp;quot;,
                &amp;quot;mode&amp;quot;: &amp;quot;incrementing&amp;quot;,
                &amp;quot;poll.interval.ms&amp;quot;: &amp;quot;1000&amp;quot;,
                &amp;quot;incrementing.column.name&amp;quot;:&amp;quot;LOGON_ID&amp;quot;,
                &amp;quot;topic.prefix&amp;quot;: &amp;quot;ora-soe-&amp;quot;
        }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The task fails with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;org.apache.kafka.connect.errors.ConnectException: Scale of Decimal value for incrementing column must be 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;timestamp&lt;/code&gt; works but the data pulled through has the &lt;code&gt;NUMBER&lt;/code&gt; columns as bytes, which is no use.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
  -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
  -d &#39;{
        &amp;quot;name&amp;quot;: &amp;quot;jdbc_source_oracle_soe_logon_01&amp;quot;,
        &amp;quot;config&amp;quot;: {
                &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.jdbc.JdbcSourceConnector&amp;quot;,
                &amp;quot;connection.url&amp;quot;: &amp;quot;jdbc:oracle:thin:soe/soe@localhost:1521/ORCLPDB1&amp;quot;,
                &amp;quot;table.whitelist&amp;quot;:&amp;quot;LOGON&amp;quot;,
                &amp;quot;mode&amp;quot;: &amp;quot;timestamp&amp;quot;,
                &amp;quot;poll.interval.ms&amp;quot;: &amp;quot;1000&amp;quot;,
                &amp;quot;timestamp.column.name&amp;quot;:&amp;quot;LOGON_DATE&amp;quot;,
                &amp;quot;topic.prefix&amp;quot;: &amp;quot;ora-soe-&amp;quot;,
                &amp;quot;validate.non.null&amp;quot;:false
        }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sample message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;LOGON_ID&amp;quot;: {&amp;quot;bytes&amp;quot;: &amp;quot;\u0000ÖÝ³pÌ\u0081ä\u008E8\u0005µì4påI\u008DÍO;Ê¶÷SI1½éoUÙv\u0099\f\u0003ð5j|\u0080\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000&amp;quot;}, &amp;quot;CUSTOMER_ID&amp;quot;: {&amp;quot;bytes&amp;quot;: &amp;quot;\t±Ó\u001Cluº\u000B|8åÆM0jzÏXFioF.\u0084\u008B,\f%ïYÝ\u0011\u0082À*\fjÑ\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000&amp;quot;}, &amp;quot;LOGON_DATE&amp;quot;: 1526388662000}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using 4.1.1 and &lt;code&gt;&amp;quot;numeric.mapping&amp;quot;:&amp;quot;best_fit&amp;quot;,&lt;/code&gt;, no joy&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
  -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
  -d &#39;{
        &amp;quot;name&amp;quot;: &amp;quot;jdbc_source_oracle_soe_logon_04&amp;quot;,
        &amp;quot;config&amp;quot;: {
                &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.jdbc.JdbcSourceConnector&amp;quot;,
                &amp;quot;connection.url&amp;quot;: &amp;quot;jdbc:oracle:thin:soe/soe@localhost:1521/ORCLPDB1&amp;quot;,
                &amp;quot;table.whitelist&amp;quot;:&amp;quot;LOGON&amp;quot;,
                &amp;quot;mode&amp;quot;: &amp;quot;timestamp&amp;quot;,
                &amp;quot;poll.interval.ms&amp;quot;: &amp;quot;1000&amp;quot;,
                &amp;quot;timestamp.column.name&amp;quot;:&amp;quot;LOGON_DATE&amp;quot;,&amp;quot;numeric.mapping&amp;quot;:&amp;quot;best_fit&amp;quot;,
                &amp;quot;topic.prefix&amp;quot;: &amp;quot;ora-soe-04-&amp;quot;,&amp;quot;validate.non.null&amp;quot;:false,&amp;quot;numeric.mapping&amp;quot;:&amp;quot;best_fit&amp;quot;
        }
}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;same bytes output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Robin@asgard02 ~/cp&amp;gt; kafka-avro-console-consumer \
                        --bootstrap-server localhost:9092 \
                        --property schema.registry.url=http://localhost:8081 \
                        --topic ora-soe-04-LOGON --from-beginning --max-messages 1| jq &#39;.&#39;
{
  &amp;quot;LOGON_ID&amp;quot;: &amp;quot;&#39;ñK\u0001³èò~¯x6\&amp;quot;¤É^ãñ&amp;amp;Ý\u001cÐÀl)\u001f\u0019W¤¦ ­b»;ç\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000&amp;quot;,
  &amp;quot;CUSTOMER_ID&amp;quot;: &amp;quot;\u001e©/@sy/\tÍ`j;±èÂÃAâ#,ú1\u0003\u0017Ùg|ÙóNwEj\u001cH\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000&amp;quot;,
  &amp;quot;LOGON_DATE&amp;quot;: {
    &amp;quot;long&amp;quot;: 946687534000
  }
}
Processed a total of 1 messages
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-does-it-happen&#34;&gt;Why does it happen?&lt;/h3&gt;

&lt;p&gt;The source data is defined as &lt;code&gt;NUMERIC&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;NUMBER&lt;/code&gt; means &amp;ldquo;store the value as given&amp;rdquo;, and the JDBC metadata for the column returns a precision of 38 and scale of non-zero. The connector has to trust the metadata, so it maps that to smallest type it can: &lt;code&gt;Decimal&lt;/code&gt; logical type  (or &lt;code&gt;java.math.BigDecimal&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; DESCRIBE LOGON;
 Name                                      Null?    Type
 ----------------------------------------- -------- ----------------------------
 LOGON_ID                                  NOT NULL NUMBER
 CUSTOMER_ID                               NOT NULL NUMBER
 LOGON_DATE                                         DATE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to when a scale is given, e.g. :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; DESCRIBE WAREHOUSES;
 Name                                      Null?    Type
 ----------------------------------------- -------- ----------------------------
 WAREHOUSE_ID                                       NUMBER(6)
 WAREHOUSE_NAME                                     VARCHAR2(35)
 LOCATION_ID                                        NUMBER(4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works fine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Robin@asgard02 ~/cp&amp;gt; kafka-avro-console-consumer \
                        --bootstrap-server localhost:9092 \
                        --property schema.registry.url=http://localhost:8081 \
                        --topic ora-soe-03-WAREHOUSES --from-beginning --max-messages 1| jq &#39;.&#39;
{
  &amp;quot;WAREHOUSE_ID&amp;quot;: {
    &amp;quot;int&amp;quot;: 712
  },
  &amp;quot;WAREHOUSE_NAME&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;bFLB2&amp;quot;
  },
  &amp;quot;LOCATION_ID&amp;quot;: {
    &amp;quot;int&amp;quot;: 1564
  }
}
Processed a total of 1 messages
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Stream-Table Joins in KSQL: Stream events must be timestamped after the Table messages</title>
			<link>https://rmoff.github.io/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</link>
			<pubDate>Thu, 17 May 2018 10:16:43 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</guid>
			<description>

&lt;p&gt;(preserving &lt;a href=&#34;https://stackoverflow.com/questions/50371518/kafka-ksql-simple-join-does-not-work/50390022#50390022&#34;&gt;this StackOverflow&lt;/a&gt; answer for posterity and future Googlers)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; When doing a stream-table join, your &lt;em&gt;table&lt;/em&gt; messages must already exist (and must be timestamped) &lt;em&gt;before&lt;/em&gt; the stream messages. If you re-emit your source stream messages, after the table topic is populated, the join will succeed.&lt;/p&gt;

&lt;h3 id=&#34;example-data&#34;&gt;Example data&lt;/h3&gt;

&lt;p&gt;Use &lt;code&gt;kafakcat&lt;/code&gt; to populate topics:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafkacat -b localhost:9092 -P -t sessionDetails &amp;lt;&amp;lt;EOF
{&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1}
{&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2}
EOF

kafkacat -b localhost:9092 -P -t voipDetails &amp;lt;&amp;lt;EOF
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1a&amp;quot;}
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1b&amp;quot;}
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2,&amp;quot;Details&amp;quot;:&amp;quot;Bar2&amp;quot;}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate topic contents:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Robin@asgard02 ~&amp;gt; kafkacat -b localhost:9092 -C -t sessionDetails
{&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1}
{&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2}

Robin@asgard02 ~&amp;gt; kafkacat -b localhost:9092 -C -t voipDetails
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1a&amp;quot;}
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1b&amp;quot;}
{&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2,&amp;quot;Details&amp;quot;:&amp;quot;Bar2&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;declare-source-streams&#34;&gt;Declare source streams&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; CREATE STREAM session_details_stream \
      (Media varchar ,SessionIdTime varchar,SessionIdSeq long) \
      WITH (KAFKA_TOPIC = &#39;sessionDetails&#39;, VALUE_FORMAT = &#39;json&#39;);

 Message
----------------
 Stream created
----------------
ksql&amp;gt; CREATE STREAM voip_details_stream \
      (SessionIdTime varchar,SessionIdSeq long, Details varchar) \
      WITH (KAFKA_TOPIC = &#39;voipDetails&#39;, VALUE_FORMAT = &#39;json&#39;);

 Message
----------------
 Stream created
----------------
ksql&amp;gt; select * from session_details_stream;
1526553130864 | null | Foo | 2018-05-17 11:25:33 BST | 1
1526553130865 | null | Foo | 2018-05-17 11:26:33 BST | 2
^CQuery terminated
ksql&amp;gt; select * from voip_details_stream;
1526553143176 | null | 2018-05-17 11:25:33 BST | 1 | Bar1a
1526553143176 | null | 2018-05-17 11:25:33 BST | 1 | Bar1b
1526553143176 | null | 2018-05-17 11:26:33 BST | 2 | Bar2
^CQuery terminated
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;repartition-each-topic-on-sessionidtime-sessionidseq&#34;&gt;Repartition each topic on SessionIdTime+SessionIdSeq&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; CREATE STREAM SESSION AS \
      SELECT Media, CONCAT(SessionIdTime,SessionIdSeq) AS root \
      FROM session_details_stream \
      PARTITION BY root;

 Message
----------------------------
 Stream created and running
----------------------------


ksql&amp;gt; SELECT ROWTIME, ROWKEY, root, media FROM SESSION;
1526553130864 | 2018-05-17 11:25:33 BST1 | 2018-05-17 11:25:33 BST1 | Foo
1526553130865 | 2018-05-17 11:26:33 BST2 | 2018-05-17 11:26:33 BST2 | Foo


ksql&amp;gt; CREATE STREAM VOIP AS \
      SELECT CONCAT(SessionIdTime,SessionIdSeq) AS root, details \
      FROM voip_details_stream \
      PARTITION BY root;

 Message
----------------------------
 Stream created and running
----------------------------
ksql&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;declare-table&#34;&gt;Declare table&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; CREATE TABLE VOIP_TABLE (root VARCHAR, details VARCHAR) \
      WITH (KAFKA_TOPIC=&#39;VOIP&#39;, VALUE_FORMAT=&#39;JSON&#39;, KEY=&#39;root&#39;);

 Message
---------------
 Table created
---------------
ksql&amp;gt; SELECT ROWTIME, ROWKEY, root, details FROM VOIP;
1526553143176 | 2018-05-17 11:26:33 BST2 | 2018-05-17 11:26:33 BST2 | Bar2
1526553143176 | 2018-05-17 11:25:33 BST1 | 2018-05-17 11:25:33 BST1 | Bar1a
1526553143176 | 2018-05-17 11:25:33 BST1 | 2018-05-17 11:25:33 BST1 | Bar1b
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;join-session-stream-to-voip-table&#34;&gt;Join SESSION stream to VOIP table&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT s.ROWTIME, s.root, s.media, v.details \
      FROM SESSION s \
      LEFT OUTER JOIN VOIP_TABLE v ON S.root = V.root;
1526553130864 | 2018-05-17 11:25:33 BST1 | Foo | null
1526553130865 | 2018-05-17 11:26:33 BST2 | Foo | null
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Leave the above JOIN query running. Re-emit SESSION message to the source topic (using &lt;code&gt;kafkacat&lt;/code&gt; to send the same messages to &lt;code&gt;sessionDetails&lt;/code&gt; as above):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1526553862403 | 2018-05-17 11:25:33 BST1 | Foo | Bar1a
1526553988639 | 2018-05-17 11:26:33 BST2 | Foo | Bar2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Per Rohan Desai on the &lt;a href=&#34;https://slackpass.io/confluentcommunity&#34;&gt;Confluent Community Slack&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The problem is that the rowtime of the record from your stream is earlier than the rowtime of the record in your table that you expect it to join with. So when the stream record is processed there is no corresponding record in the table&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Looking at the message on the source table for one of the join keys using &lt;code&gt;ROWTIME&lt;/code&gt; to look at the message timestamp (&lt;em&gt;not to be confused with the timestamp-based &lt;code&gt;root&lt;/code&gt;&lt;/em&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT TIMESTAMPTOSTRING(ROWTIME, &#39;yyyy-MM-dd HH:mm:ss&#39;) , ROWTIME, root, details from VOIP WHERE root=&#39;2018-05-17 11:26:33 BST2&#39;;
2018-05-17 11:32:23 | 1526553143176 | 2018-05-17 11:26:33 BST2 | Bar2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compare this to the message on the source session stream topic:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ksql&amp;gt; SELECT TIMESTAMPTOSTRING(ROWTIME, &#39;yyyy-MM-dd HH:mm:ss&#39;) , ROWTIME, root, media from SESSION WHERE root=&#39;2018-05-17 11:26:33 BST2&#39;;
2018-05-17 11:32:10 | 1526553130865 | 2018-05-17 11:26:33 BST2 | Foo
2018-05-17 11:46:28 | 1526553988639 | 2018-05-17 11:26:33 BST2 | Foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;em&gt;first&lt;/em&gt; of these (at &lt;code&gt;11:32:10&lt;/code&gt; / &lt;code&gt;1526553130865&lt;/code&gt;) is prior to that of the corresponding &lt;code&gt;VOIP&lt;/code&gt; message (shown above), and resulted in the &lt;code&gt;null&lt;/code&gt; join result that we first saw. The &lt;em&gt;second&lt;/em&gt; of these is dated later (&lt;code&gt;11:46:28&lt;/code&gt; / &lt;code&gt;1526553988639&lt;/code&gt;) is produced the successful join that we subsequently saw:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1526553988639 | 2018-05-17 11:26:33 BST2 | Foo | Bar2
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
			<link>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
			<pubDate>Thu, 10 May 2018 12:56:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
			<description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Use &lt;code&gt;curl&lt;/code&gt; to pull data from the Mockaroo REST endpoint, and pipe it into &lt;code&gt;kafkacat&lt;/code&gt;, thus:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \
kafkacat -b localhost:9092 -t purchases -P
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Three things I love…Kafka, &lt;code&gt;kafkacat&lt;/code&gt;, and Mockaroo. And in this post I get to show all three 😁&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mockaroo.com/&#34;&gt;Mockaroo&lt;/a&gt; is a very cool online service that lets you quickly mock up test data.
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/05/2018-05-10_14-59-03.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What sets it apart from &lt;code&gt;SELECT RANDOM(100) FROM DUMMY;&lt;/code&gt; is that it has lots of different classes of test data for you to choose from. Wanting to simulate some users? Here you go:
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/05/2018-05-10_15-00-57.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So you can build up realistic datasets at a few clicks of the mouse, and then export them to a bunch of formats, including CSV, JSON, and even SQL &lt;code&gt;INSERT INTO&lt;/code&gt; statements (and, of course, it also provides the &lt;code&gt;CREATE TABLE&lt;/code&gt; DDL!).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve used Mockaroo many times over the years, often as a source for analytics visualisation tools that I&amp;rsquo;ve been working with. Now I&amp;rsquo;m doing a bunch of work with &lt;a href=&#34;https://www.confluent.io/product/ksql/&#34;&gt;KSQL&lt;/a&gt;, and want some useful test data with which to demonstrate certain queries and concepts. KSQL is the streaming SQL engine for Apache Kafka, and so as such I needed to get a bunch of test data into Kafka topics. First up, create my schema:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/05/2018-05-10_15-03-51.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Set the output to &lt;code&gt;JSON&lt;/code&gt; (make sure it&amp;rsquo;s &lt;strong&gt;not&lt;/strong&gt; as a JSON array).&lt;/p&gt;

&lt;p&gt;Mockaroo provides a REST endpoint from which you can pull the data for a given schema. To do this you need to save your schema, and you need to register (for free) to do this.&lt;/p&gt;

&lt;p&gt;With the REST endpoint you can get any number of records, using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;$ curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=1&amp;amp;key=ff7856d0&amp;quot;|jq &#39;.&#39;
{
  &amp;quot;order_id&amp;quot;: 1,
  &amp;quot;customer_name&amp;quot;: &amp;quot;Gustaf Lindro&amp;quot;,
  &amp;quot;date_of_birth&amp;quot;: &amp;quot;1916-06-24T09:00:55Z&amp;quot;,
  &amp;quot;product&amp;quot;: &amp;quot;Chicken - Ground&amp;quot;,
  &amp;quot;order_total_usd&amp;quot;: &amp;quot;9.45&amp;quot;,
  &amp;quot;town&amp;quot;: &amp;quot;Greensboro&amp;quot;,
  &amp;quot;country&amp;quot;: &amp;quot;United States&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now comes the Kafka bit. I&amp;rsquo;m using the most-excellent &lt;a href=&#34;https://github.com/edenhill/kafkacat/&#34;&gt;&lt;code&gt;kafkacat&lt;/code&gt;&lt;/a&gt; (&lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;about which you can read more here&lt;/a&gt;), which is a very simple—yet powerful—command line tool for producing data to and consuming data from Kafka.&lt;/p&gt;

&lt;p&gt;When using &lt;code&gt;kafkacat&lt;/code&gt; as a Producer you can do so interactively, feed it from flat files - or use &lt;code&gt;stdin&lt;/code&gt; as the input. Therefore we can simply pipe the output of the Mockaroo REST call directly into it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2000&amp;amp;key=ff7856d0&amp;quot;|\
kafkacat -b localhost:9092 -t purchases -P
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This writes &lt;code&gt;2000&lt;/code&gt; records from the given schema to the &lt;code&gt;purchases&lt;/code&gt; topic, using the Kafka broker at &lt;code&gt;localhost:9092&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can use &lt;code&gt;kafkacat&lt;/code&gt; to inspect the topic (&lt;code&gt;-C&lt;/code&gt;=run as consumer, &lt;code&gt;-c1&lt;/code&gt;=read just one message):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kafkacat -b localhost:9092 -t purchases -C -c1|jq &#39;.&#39;
{
  &amp;quot;order_id&amp;quot;: 1,
  &amp;quot;customer_name&amp;quot;: &amp;quot;Maryanna Andryszczak&amp;quot;,
  &amp;quot;date_of_birth&amp;quot;: &amp;quot;1922-06-06T02:21:59Z&amp;quot;,
  &amp;quot;product&amp;quot;: &amp;quot;Nut - Walnut, Pieces&amp;quot;,
  &amp;quot;order_total_usd&amp;quot;: &amp;quot;1.65&amp;quot;,
  &amp;quot;town&amp;quot;: &amp;quot;Portland&amp;quot;,
  &amp;quot;country&amp;quot;: &amp;quot;United States&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There you have it…a super-powerful but simple way to load test data into Kafka.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Courtesy of my colleague Chris Matta, you can also use &lt;code&gt;kafka-console-producer&lt;/code&gt; in this way:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=20&amp;amp;key=ff7856d0&amp;quot; | \
kafka-console-producer \
  --broker-list localhost:9092 \
  --topic users
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Want to slow it down a bit, and loop forever? Use &lt;code&gt;while&lt;/code&gt; to loop, and &lt;code&gt;awk&lt;/code&gt; to inject some delay:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    while [ 1 -eq 1 ]
      do curl &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=5000&amp;amp;key=ff7856d0&amp;quot; | \
         awk &#39;{print $$0;system(&amp;quot;sleep 0.5&amp;quot;);}&#39; | \
          kafkacat -b kafka:29092 -P -t purchases
      done
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
			<link>https://rmoff.github.io/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
			<pubDate>Tue, 27 Mar 2018 18:52:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
			<description>

&lt;p&gt;&lt;em&gt;Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;mongodb-config-enabling-replica-sets&#34;&gt;MongoDB config - enabling replica sets&lt;/h3&gt;

&lt;p&gt;For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:&lt;/p&gt;

&lt;p&gt;Docs: &lt;a href=&#34;https://docs.mongodb.com/manual/replication/&#34;&gt;Replication&lt;/a&gt; / &lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/convert-standalone-to-replica-set/&#34;&gt;Convert a Standalone to a Replica Set&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Stop Mongo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo service mongod stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add replica set config to &lt;code&gt;/etc/mongod.conf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[source,yaml]
----
replication:
   replSetName: mongo01
----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Optionally, also set the bindIp so that it listens on all IPs, not just loopback:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[source,yaml]
----
net:
  bindIp: 0.0.0.0
----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Start Mongo, and initiate the replica set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo service mongod start
rmoff@proxmox01 ~&amp;gt; mongo --host 127.0.0.1:27017
MongoDB shell version v3.6.3
connecting to: mongodb://127.0.0.1:27017/
&amp;gt; rs.initiate()
{
        &amp;quot;info2&amp;quot; : &amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;,
        &amp;quot;me&amp;quot; : &amp;quot;127.0.0.1:27017&amp;quot;,
        &amp;quot;ok&amp;quot; : 1,
        &amp;quot;operationTime&amp;quot; : Timestamp(1520428346, 1),
        &amp;quot;$clusterTime&amp;quot; : {
                &amp;quot;clusterTime&amp;quot; : Timestamp(1520428346, 1),
                &amp;quot;signature&amp;quot; : {
                        &amp;quot;hash&amp;quot; : BinData(0,&amp;quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&amp;quot;),
                        &amp;quot;keyId&amp;quot; : NumberLong(0)
                }
        }
}
mongo01:PRIMARY&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check &lt;code&gt;/var/log/mongodb/mongod.log&lt;/code&gt;, should see replica set config success and oplog get created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2018-03-07T13:12:26.007+0000 I REPL     [conn1] replSetInitiate admin command received from client
2018-03-07T13:12:26.007+0000 I REPL     [conn1] creating replication oplog of size: 2864MB...
2018-03-07T13:12:26.048+0000 I REPL     [conn1] New replica set config in use: { _id: &amp;quot;mongo01&amp;quot;, version: 1, protocolVersion: 1, members: [ { _id: 0, host: &amp;quot;127.0.0.1:27017&amp;quot;, arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId(&#39;5a9fe53ac81eed28a3bf207a&#39;) } }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setting-up-debezium-to-stream-changes-from-mongodb-into-apache-kafka&#34;&gt;Setting up Debezium to stream changes from MongoDB into Apache Kafka&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a &lt;a href=&#34;http://debezium.io/docs/connectors/mongodb/&#34;&gt;detailed explanation of how Debezium CDC works with Debezium&lt;/a&gt; on the Debezium doc site.&lt;/p&gt;

&lt;h4 id=&#34;install-debezium-mongo-plugin&#34;&gt;Install Debezium Mongo plugin&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;mkdir ~/connect-jars
cd ~/connect-jars/
wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mongodb/0.7.4/debezium-connector-mongodb-0.7.4-plugin.tar.gz
tar -xf debezium-connector-mongodb-0.7.4-plugin.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the plugin folder (e.g. &lt;code&gt;/home/rmoff/connect-jars&lt;/code&gt;) to the Connect worker config file (e.g. &lt;code&gt;./etc/schema-registry/connect-avro-distributed.properties&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugin.path=share/java,/home/rmoff/connect-jars
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;configure-debezium-mongodb-connector&#34;&gt;Configure Debezium MongoDB connector&lt;/h4&gt;

&lt;p&gt;Config file (&lt;code&gt;/home/rmoff/connect-config/mongodb.json&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;mongodb-connector&amp;quot;,
  &amp;quot;config&amp;quot;: {
    &amp;quot;connector.class&amp;quot;: &amp;quot;io.debezium.connector.mongodb.MongoDbConnector&amp;quot;,
    &amp;quot;mongodb.hosts&amp;quot;: &amp;quot;rs0/localhost:27017&amp;quot;,
    &amp;quot;mongodb.name&amp;quot;: &amp;quot;ubnt&amp;quot;,
    &amp;quot;database.whitelist&amp;quot;: &amp;quot;ace&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load connector:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
    -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8084/connectors/ \
    -d @/home/rmoff/connect-config/mongodb.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check Connect worker stdout:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2018-03-07 13:33:35,303] INFO Beginning initial sync of &#39;mongo01&#39; at {sec=1520429608, ord=1, h=-4885990198351632203} (io.debezium.connector.mongodb.Replicator:247)
[2018-03-07 13:33:35,324] INFO Preparing to use 1 thread(s) to sync 39 collection(s): mongo01.ace.wlangroup, mongo01.ace.extension, mongo01.ace.stat, mongo01.ace.networkconf, mongo01.ace.voucher, mongo01.ace.hotspotpackage, mongo01.ace.hotspotop, mongo01.ace.alarm, mongo01.ace.heatmap, mongo01.ace.map, mongo01.ace.dpigroup, mongo01.ace.setting, mongo01.ace.usergroup, mongo01.ace.verification, mongo01.ace.payment, mongo01.ace.heatmappoint, mongo01.ace.scheduletask, mongo01.ace.guest, mongo01.ace.admin, mongo01.ace.radiusprofile, mongo01.ace.portalfile, mongo01.ace.mediafile, mongo01.ace.device, mongo01.ace.firewallgroup, mongo01.ace.site, mongo01.ace.task, mongo01.ace.dynamicdns, mongo01.ace.portconf, mongo01.ace.wlanconf, mongo01.ace.rogue, mongo01.ace.routing, mongo01.ace.firewallrule, mongo01.ace.event, mongo01.ace.hotspot2conf, mongo01.ace.broadcastgroup, mongo01.ace.portforward, mongo01.ace.privilege, mongo01.ace.account, mongo01.ace.user (io.debezium.connector.mongodb.Replicator:276)
[2018-03-07 13:33:35,326] INFO Creating thread debezium-mongodbconnector-ubnt-copy-mongo01-0 (io.debezium.util.Threads:247)
[2018-03-07 13:33:35,327] INFO Starting initial sync of &#39;mongo01.ace.wlangroup&#39; (io.debezium.connector.mongodb.Replicator:286)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check topics:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~/connect-jars&amp;gt; kafka-topics --zookeeper localhost:2181 --list
ubnt.ace.admin
ubnt.ace.alarm
ubnt.ace.broadcastgroup
ubnt.ace.device
ubnt.ace.dpigroup
ubnt.ace.event
ubnt.ace.guest
ubnt.ace.map
ubnt.ace.networkconf
ubnt.ace.portconf
ubnt.ace.portforward
ubnt.ace.privilege
ubnt.ace.rogue
ubnt.ace.scheduletask
ubnt.ace.setting
ubnt.ace.site
ubnt.ace.user
ubnt.ace.usergroup
ubnt.ace.wlanconf
ubnt.ace.wlangroup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-$&#34;&gt;{
  &amp;quot;after&amp;quot;: {\&amp;quot;_id\&amp;quot; : {\&amp;quot;$oid\&amp;quot; : \&amp;quot;58385328e4b001431e4e497a\&amp;quot;},\&amp;quot;adopted\&amp;quot; : true,\&amp;quot;board_rev\&amp;quot; : 18,\&amp;quot;cfgversion\&amp;quot; : \&amp;quot;xxxxxxxxxxxxx\&amp;quot;,\&amp;quot;config_network\&amp;quot; : {\&amp;quot;ip\&amp;quot; : \&amp;quot;192.168.10.12\&amp;quot;,\&amp;quot;type\&amp;quot; : \&amp;quot;dhcp\&amp;quot;},\&amp;quot;ethernet_table\&amp;quot; : [{\&amp;quot;mac\&amp;quot; : \&amp;quot;xx:xx:xx:xx:xx:xx\&amp;quot;,\&amp;quot;num_port\&amp;quot; : 1,\&amp;quot;name\&amp;quot; : \&amp;quot;eth0\&amp;quot;}],\&amp;quot;fw_caps\&amp;quot; : 75,\&amp;quot;has_eth1\&amp;quot; : false,\&amp;quot;has_speaker\&amp;quot; : false,\&amp;quot;inform_ip\&amp;quot; : \&amp;quot;192.168.10.172\&amp;quot;,\&amp;quot;inform_url\&amp;quot; : \&amp;quot;http://192.168.10.172:8080/inform\&amp;quot;,\&amp;quot;ip\&amp;quot; : \&amp;quot;192.168.10.68\&amp;quot;,\&amp;quot;led_override\&amp;quot; : \&amp;quot;on\&amp;quot;,\&amp;quot;mac\&amp;quot; : \&amp;quot;xx:xx:xx:xx:xx:xx\&amp;quot;,\&amp;quot;model\&amp;quot; : \&amp;quot;BZ2\&amp;quot;,\&amp;quot;name\&amp;quot; : \&amp;quot;Unifi AP - Study\&amp;quot;,\&amp;quot;port_table\&amp;quot; : [],\&amp;quot;radio_table\&amp;quot; : [{\&amp;quot;radio\&amp;quot; : \&amp;quot;ng\&amp;quot;,\&amp;quot;min_txpower\&amp;quot; : 5,\&amp;quot;max_txpower\&amp;quot; : 23,\&amp;quot;builtin_antenna\&amp;quot; : true,\&amp;quot;builtin_ant_gain\&amp;quot; : 0,\&amp;quot;nss\&amp;quot; : 2,\&amp;quot;name\&amp;quot; : \&amp;quot;wifi0\&amp;quot;}],\&amp;quot;serial\&amp;quot; : \&amp;quot;xxx\&amp;quot;,\&amp;quot;site_id\&amp;quot; : \&amp;quot;xxx\&amp;quot;,\&amp;quot;type\&amp;quot; : \&amp;quot;uap\&amp;quot;,\&amp;quot;version\&amp;quot; : \&amp;quot;3.7.40.6115\&amp;quot;,\&amp;quot;vwire_table\&amp;quot; : [],\&amp;quot;wifi_caps\&amp;quot; : 117,\&amp;quot;wlangroup_id_ng\&amp;quot; : \&amp;quot;xx\&amp;quot;,\&amp;quot;x_authkey\&amp;quot; : \&amp;quot;xx\&amp;quot;,\&amp;quot;x_fingerprint\&amp;quot; : \&amp;quot;xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:3e:39:08:45\&amp;quot;,\&amp;quot;x_ssh_hostkey\&amp;quot; : \&amp;quot;xx/xx=\&amp;quot;,\&amp;quot;x_ssh_hostkey_fingerprint\&amp;quot; : \&amp;quot;xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx\&amp;quot;,\&amp;quot;x_vwirekey\&amp;quot; : \&amp;quot;xx\&amp;quot;,\&amp;quot;map_id\&amp;quot; : \&amp;quot;xx\&amp;quot;,\&amp;quot;x\&amp;quot; : 880.3939455186993,\&amp;quot;y\&amp;quot; : 966.2397514041841,\&amp;quot;locked\&amp;quot; : true,\&amp;quot;wlan_overrides\&amp;quot; : []}&amp;quot;
  },
  &amp;quot;patch&amp;quot;: null,
  &amp;quot;source&amp;quot;: {
    &amp;quot;version&amp;quot;: {
      &amp;quot;string&amp;quot;: &amp;quot;0.7.4&amp;quot;
    },
    &amp;quot;name&amp;quot;: &amp;quot;ubnt&amp;quot;,
    &amp;quot;rs&amp;quot;: &amp;quot;mongo01&amp;quot;,
    &amp;quot;ns&amp;quot;: &amp;quot;ace.device&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the MongoDB document is not as fields in the Kafka message, but instead everything is in the payload as a &lt;code&gt;string&lt;/code&gt; field as escaped JSON.&lt;/p&gt;

&lt;p&gt;Debezium does provide a &lt;a href=&#34;http://debezium.io/docs/configuration/mongodb-event-flattening/&#34;&gt;Single Message Transform (SMT) to flatten the MongoDB record&lt;/a&gt; out like this, but in using it I hit a bug (&lt;a href=&#34;https://issues.jboss.org/browse/DBZ-649&#34;&gt;DBZ-649&lt;/a&gt;) that seems to be down to the MongoDB collection documents having different fields between documents. The reported error was &lt;code&gt;org.apache.kafka.connect.errors.DataException: &amp;lt;field&amp;gt; is not a valid field name&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;However, using KSQL&amp;rsquo;s &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; you can still work with the data as-is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;ksql&amp;gt; CREATE STREAM DEVICE (after VARCHAR) WITH (KAFKA_TOPIC=&#39;ubnt.ace.device-07&#39;,VALUE_FORMAT=&#39;JSON&#39;);
ksql&amp;gt; select EXTRACTJSONFIELD(after,&#39;$.name&#39;),EXTRACTJSONFIELD(after,&#39;$.ip&#39;) from device;
Unifi AP - Study | 192.168.10.68
Unifi AP - Attic | 192.168.10.67
ubnt.moffatt.me | 77.102.5.159
Unifi AP - Pantry | 192.168.10.71
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Cloning Ubiquiti&#39;s MongoDB instance to a separate server</title>
			<link>https://rmoff.github.io/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/</link>
			<pubDate>Tue, 27 Mar 2018 18:45:20 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/</guid>
			<description>

&lt;p&gt;DISCLAIMER: I am not a MongoDB person (even if it is &lt;a href=&#34;http://www.mongodb-is-web-scale.com/&#34;&gt;Web Scale&lt;/a&gt; X-D) - below instructions may work for you, they may not. Use with care!&lt;/p&gt;

&lt;p&gt;For some work I&amp;rsquo;ve been doing I wanted to access the data in Ubiquiti&amp;rsquo;s Unifi controller which it stores in MongoDB. Because I didn&amp;rsquo;t want to risk my actual Unifi device by changing local settings to enable remote access, and also because the version of MongoDB on it is older than ideal, I wanted to clone the data elsewhere. This article shows you how.&lt;/p&gt;

&lt;h3 id=&#34;dump-data-from-source-server-unifi&#34;&gt;Dump data from source server (unifi)&lt;/h3&gt;

&lt;p&gt;To start with, SSH to the Unifi box (in my case, it&amp;rsquo;s a CloudKey). Username and password are the same as Unifi web GUI login.&lt;/p&gt;

&lt;p&gt;Inspect local MongoDB instance (&lt;code&gt;ace&lt;/code&gt; is the database that unifi uses):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@UniFi-CloudKey:~# mongo localhost:27117/ace
MongoDB shell version: 2.4.10
connecting to: localhost:27117/foo
Server has startup warnings:
Wed Nov  1 19:38:11.033 [initandlisten]
Wed Nov  1 19:38:11.033 [initandlisten] ** NOTE: This is a 32 bit MongoDB binary.
Wed Nov  1 19:38:11.033 [initandlisten] **       32 bit builds are limited to less than 2GB of data (or less with --journal).
Wed Nov  1 19:38:11.033 [initandlisten] **       Note that journaling defaults to off for 32 bit and is currently off.
Wed Nov  1 19:38:11.033 [initandlisten] **       See http://dochub.mongodb.org/core/32bit
Wed Nov  1 19:38:11.034 [initandlisten]

mongo01:PRIMARY&amp;gt; db.getCollection(&#39;device&#39;).find({},{name:1})
{ &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;58385328e4b001431e4e497a&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;Unifi AP - Study&amp;quot; }
{ &amp;quot;_id&amp;quot; : ObjectId(&amp;quot;583854cde4b001431e4e4982&amp;quot;), &amp;quot;name&amp;quot; : &amp;quot;Unifi AP - Attic&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run &lt;a href=&#34;https://docs.mongodb.com/manual/reference/program/mongodump/#bin.mongodump&#34;&gt;&lt;code&gt;mongodump&lt;/code&gt;&lt;/a&gt; to dump the DB to file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@UniFi-CloudKey:~# mongodump --port 27117
connected to: 127.0.0.1:27117
Wed Mar  7 12:14:05.092 all dbs
Wed Mar  7 12:14:05.098 DATABASE: ace	 to 	dump/ace
Wed Mar  7 12:14:05.103 	ace.system.indexes to dump/ace/system.indexes.bson
Wed Mar  7 12:14:05.108 		 120 objects
Wed Mar  7 12:14:05.110 	ace.account to dump/ace/account.bson
Wed Mar  7 12:14:05.112 		 0 objects
Wed Mar  7 12:14:05.112 	Metadata for ace.account to dump/ace/account.metadata.json
Wed Mar  7 12:14:05.114 	ace.admin to dump/ace/admin.bson
Wed Mar  7 12:14:05.116 		 1 objects
Wed Mar  7 12:14:05.117 	Metadata for ace.admin to dump/ace/admin.metadata.json
Wed Mar  7 12:14:05.118 	ace.alarm to dump/ace/alarm.bson
Wed Mar  7 12:14:05.124 		 152 objects
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-mongodb-locally&#34;&gt;Install MongoDB locally&lt;/h3&gt;

&lt;p&gt;Ref: &lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/install-mongodb-on-debian/&#34;&gt;Install docs&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2930ADAE8CAF5059EE73BB4B58712A2291FA4AD5
echo &amp;quot;deb http://repo.mongodb.org/apt/debian jessie/mongodb-org/3.6 main&amp;quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.6.list
sudo apt-get update
sudo apt-get install -y mongodb-org
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rmoff@proxmox01 ~&amp;gt; mongo --host 127.0.0.1:27017
MongoDB shell version v3.6.3
connecting to: mongodb://127.0.0.1:27017/
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;restore-data-to-new-server&#34;&gt;Restore data to new server&lt;/h3&gt;

&lt;p&gt;Copy dump file from CloudKey to local server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; scp -r robin@unifi.moffatt.me:/root/dump unifi-mongodump
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;a href=&#34;https://docs.mongodb.com/manual/reference/program/mongodump/#bin.mongodump&#34;&gt;&lt;code&gt;mongorestore&lt;/code&gt;&lt;/a&gt; to restore dump to local MongoDB instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; mongorestore unifi-mongodump/

----
connected to: 127.0.0.1
Wed Mar  7 12:17:31.910 unifi-mongodump/ace/alarm.bson
Wed Mar  7 12:17:31.910         going into namespace [ace.alarm]
152 objects found
Wed Mar  7 12:17:31.912         Creating index: { name: &amp;quot;_id_&amp;quot;, key: { _id: 1 }, ns: &amp;quot;ace.alarm&amp;quot; }
Wed Mar  7 12:17:31.930         Creating index: { name: &amp;quot;datetime_1&amp;quot;, key: { datetime: 1 }, ns: &amp;quot;ace.alarm&amp;quot; }
Wed Mar  7 12:17:31.931         Creating index: { name: &amp;quot;archived_1&amp;quot;, key: { archived: 1 }, ns: &amp;quot;ace.alarm&amp;quot; }
Wed Mar  7 12:17:31.932 unifi-mongodump/ace/radiusprofile.bson
[...]
----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Done!&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&#34;https://www.robomongo.org/&#34;&gt;Robo 3T&lt;/a&gt; (formerly Robomongo) it&amp;rsquo;s easy to explore the data in the restored instance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/03/mongo01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</title>
			<link>https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</link>
			<pubDate>Sat, 24 Mar 2018 14:58:14 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</guid>
			<description>

&lt;p&gt;&lt;a href=&#34;http://debezium.io/&#34;&gt;Debezium&lt;/a&gt; is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we&amp;rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.&lt;/p&gt;

&lt;p&gt;The software versions used here are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Confluent Platform 4.0&lt;/li&gt;
&lt;li&gt;Debezium 0.7.2&lt;/li&gt;
&lt;li&gt;MySQL 5.7.19 with &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-installation.html&#34;&gt;Sakila sample database&lt;/a&gt; installed&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;install-debezium&#34;&gt;Install Debezium&lt;/h3&gt;

&lt;p&gt;To use it, you need the relevant JAR for the source system (e.g. MySQL), and make that JAR available to Kafka Connect. Here we&amp;rsquo;ll set it up for MySQL.&lt;/p&gt;

&lt;p&gt;Download &lt;code&gt;debezium-connector-mysql-0.7.2-plugin.tar.gz&lt;/code&gt; jar from &lt;a href=&#34;https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/&#34;&gt;https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unpack the &lt;code&gt;.tar.gz&lt;/code&gt; into its own folder, for example &lt;code&gt;/u01/plugins&lt;/code&gt; so that you have:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/u01/plugins/debezium-connector-mysql/mysql-binlog-connector-java-0.13.0.jar
/u01/plugins/debezium-connector-mysql/debezium-core-0.7.2.jar
/u01/plugins/debezium-connector-mysql/mysql-binlog-connector-java-0.13.0.jar
/u01/plugins/debezium-connector-mysql/mysql-connector-java-5.1.40.jar
/u01/plugins/debezium-connector-mysql/debezium-connector-mysql-0.7.2.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now configure Kafka Connect to pick up the Debezium plugin, by updating the Kafka Connect worker config.&lt;/p&gt;

&lt;p&gt;Edit &lt;code&gt;./etc/kafka/connect-distributed.properties&lt;/code&gt; and append to &lt;code&gt;plugin.path&lt;/code&gt; the value for the &lt;em&gt;folder containing the Debezium JAR&lt;/em&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plugin.path=share/java,/u01/plugins/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;plugin.path&lt;/code&gt; is based on this expected structure: &lt;img src=&#34;https://rmoff.github.io/content/images/2018/03/KafkaConnect_pluginpath.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;mysql-config&#34;&gt;MySQL config&lt;/h3&gt;

&lt;p&gt;Debezium uses MySQL&amp;rsquo;s binlog facility to extract events, and you need to configure MySQL to enable it. Here is the bare-basics necessary to get this working - fine for demo purposes, but not a substitute for an actual MySQL DBA doing this properly :)&lt;/p&gt;

&lt;p&gt;Doc: &lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/server-configuration.html&#34;&gt;Server Config reference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check current state of binlog replication:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mysqladmin variables -uroot|grep log_bin
| log_bin                                                  | OFF
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enable binlog &lt;a href=&#34;http://debezium.io/docs/connectors/mysql/#enabling-the-binlog&#34;&gt;per the doc&lt;/a&gt;. On the Mac I&amp;rsquo;d installed MySQL with homebrew, and enabled binlog by creating the following file at &lt;code&gt;/usr/local/opt/mysql/my.cnf&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mysqld]
server-id         = 42
log_bin           = mysql-bin
binlog_format     = row
binlog_row_image  = full
expire_logs_days  = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I restarted &lt;code&gt;mysqld&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew services restart mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and verified that binlog was now enabled:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mysqladmin variables -uroot|grep log_bin
| log_bin                                                  | ON
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create user with required permissions;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mysql -uroot

mysql&amp;gt; GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#39;debezium&#39; IDENTIFIED BY &#39;dbz&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;kafka-connect-setup&#34;&gt;Kafka Connect setup&lt;/h3&gt;

&lt;p&gt;Load the connector configuration into Kafka Connect using the REST API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i -X POST -H &amp;quot;Accept:application/json&amp;quot; \
    -H  &amp;quot;Content-Type:application/json&amp;quot; http://localhost:8083/connectors/ \
    -d &#39;{
      &amp;quot;name&amp;quot;: &amp;quot;mysql-connector&amp;quot;,
      &amp;quot;config&amp;quot;: {
            &amp;quot;connector.class&amp;quot;: &amp;quot;io.debezium.connector.mysql.MySqlConnector&amp;quot;,
            &amp;quot;database.hostname&amp;quot;: &amp;quot;localhost&amp;quot;,
            &amp;quot;database.port&amp;quot;: &amp;quot;3306&amp;quot;,
            &amp;quot;database.user&amp;quot;: &amp;quot;debezium&amp;quot;,
            &amp;quot;database.password&amp;quot;: &amp;quot;dbz&amp;quot;,
            &amp;quot;database.server.id&amp;quot;: &amp;quot;42&amp;quot;,
            &amp;quot;database.server.name&amp;quot;: &amp;quot;demo&amp;quot;,
            &amp;quot;database.history.kafka.bootstrap.servers&amp;quot;: &amp;quot;localhost:9092&amp;quot;,
            &amp;quot;database.history.kafka.topic&amp;quot;: &amp;quot;dbhistory.demo&amp;quot; ,
            &amp;quot;include.schema.changes&amp;quot;: &amp;quot;true&amp;quot;
       }
    }&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now check that the connector is running successfully:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;http://localhost:8083/connectors&amp;quot; | jq &#39;.[]&#39; | \
xargs -I{connector_name} curl -s &amp;quot;http://localhost:8083/connectors/&amp;quot;{connector_name}&amp;quot;/status&amp;quot; | \
jq -c -M &#39;[.name,.connector.state,.tasks[].state] | \
join(&amp;quot;:|:&amp;quot;)&#39;| column -s : -t| sed &#39;s/\&amp;quot;//g&#39;| sort

mysql-connector  |  RUNNING  |  RUNNING
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it&amp;rsquo;s &lt;code&gt;FAILED&lt;/code&gt; then check the Connect Worker log for errors - often this will be down to mistakes with the plugin&amp;rsquo;s JAR path or availability, so check that carefully.&lt;/p&gt;

&lt;p&gt;Assuming it&amp;rsquo;s &lt;code&gt;RUNNING&lt;/code&gt;, you should see in the Connect Worker logs something like this, indicating that Debezium has successfully pulled data from MySQL:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2018-02-09 15:27:40,268] INFO Starting snapshot for jdbc:mysql://localhost:3306/?useInformationSchema=true&amp;amp;nullCatalogMeansCurrent=false&amp;amp;useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;characterSetResults=UTF-8&amp;amp;zeroDateTimeBehavior=convertToNull with user &#39;debezium&#39; (io.debezium.connector.mysql.SnapshotReader:220)
[...]
[2018-02-09 15:27:57,297] INFO Step 8: scanned 97354 rows in 24 tables in 00:00:15.617 (io.debezium.connector.mysql.SnapshotReader:579)
[2018-02-09 15:27:57,297] INFO Step 9: committing transaction (io.debezium.connector.mysql.SnapshotReader:611)
[2018-02-09 15:27:57,299] INFO Completed snapshot in 00:00:17.032 (io.debezium.connector.mysql.SnapshotReader:661)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;inspect-the-mysql-data-in-kafka&#34;&gt;Inspect the MySQL data in Kafka&lt;/h3&gt;

&lt;p&gt;Use &lt;code&gt;kafka-topics&lt;/code&gt; to see all the topics created by Debezium:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-topics --zookeeper localhost:2181 --list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each &lt;strong&gt;table&lt;/strong&gt; in the database becomes one &lt;strong&gt;topic&lt;/strong&gt; in Kafka. You&amp;rsquo;ll see that the topic name is in the format of &lt;code&gt;database.schema.table&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fullfillment.sakila.actor
fullfillment.sakila.address
fullfillment.sakila.category
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the messages. Each &lt;strong&gt;table row&lt;/strong&gt; becomes a &lt;strong&gt;message&lt;/strong&gt; on a kafka topic.&lt;/p&gt;

&lt;p&gt;Run the Avro Console consumer:  (using the excellent &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;jq&lt;/a&gt; for easy formatting of the JSON)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/kafka-avro-console-consumer \
--bootstrap-server localhost:9092 \
--property schema.registry.url=http://localhost:8081 \
--topic fullfillment.sakila.customer \
--from-beginning | jq &#39;.&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will show the current contents of the topic. Leave the above command running, and in a separate window make a change to the table in MySQL, for example, an update:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mysql&amp;gt; UPDATE CUSTOMER SET FIRST_NAME=&#39;Rick&#39; WHERE CUSTOMER_ID=603;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the Kafka consumer you&amp;rsquo;ll see the change record come through pretty much instantaneously.&lt;/p&gt;

&lt;script src=&#34;https://asciinema.org/a/vzt7YhIBHdcuYz9Zp4UsYPuaS.js&#34; id=&#34;asciicast-vzt7YhIBHdcuYz9Zp4UsYPuaS&#34; async&gt;&lt;/script&gt;

&lt;p&gt;The records from Debezium look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;before&amp;quot;: null,
  &amp;quot;after&amp;quot;: {
    &amp;quot;fullfillment.sakila.rental.Value&amp;quot;: {
      &amp;quot;rental_id&amp;quot;: 13346,
      &amp;quot;rental_date&amp;quot;: 1124483301000,
      &amp;quot;inventory_id&amp;quot;: 4541,
      &amp;quot;customer_id&amp;quot;: 131,
      &amp;quot;return_date&amp;quot;: {
        &amp;quot;long&amp;quot;: 1125188901000
      },
      &amp;quot;staff_id&amp;quot;: 2,
      &amp;quot;last_update&amp;quot;: &amp;quot;2006-02-15T21:30:53Z&amp;quot;
    }
  },
  &amp;quot;source&amp;quot;: {
    &amp;quot;name&amp;quot;: &amp;quot;fullfillment&amp;quot;,
    &amp;quot;server_id&amp;quot;: 0,
    &amp;quot;ts_sec&amp;quot;: 0,
    &amp;quot;gtid&amp;quot;: null,
    &amp;quot;file&amp;quot;: &amp;quot;mysql-bin.000002&amp;quot;,
    &amp;quot;pos&amp;quot;: 832,
    &amp;quot;row&amp;quot;: 0,
    &amp;quot;snapshot&amp;quot;: {
      &amp;quot;boolean&amp;quot;: true
    },
    &amp;quot;thread&amp;quot;: null,
    &amp;quot;db&amp;quot;: {
      &amp;quot;string&amp;quot;: &amp;quot;sakila&amp;quot;
    },
    &amp;quot;table&amp;quot;: {
      &amp;quot;string&amp;quot;: &amp;quot;rental&amp;quot;
    }
  },
  &amp;quot;op&amp;quot;: &amp;quot;c&amp;quot;,
  &amp;quot;ts_ms&amp;quot;: {
    &amp;quot;long&amp;quot;: 1518190060267
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the structure of the messages - you get an &lt;code&gt;before&lt;/code&gt; and &lt;code&gt;after&lt;/code&gt; view of the record, plus a bunch of metadata (&lt;code&gt;source&lt;/code&gt;, &lt;code&gt;op&lt;/code&gt;, &lt;code&gt;ts_ms&lt;/code&gt;). Depending on what you&amp;rsquo;re using the CDC events for, you&amp;rsquo;ll want to retain some or all of this structure.&lt;/p&gt;

&lt;h3 id=&#34;event-message-flattening-with-single-message-transform&#34;&gt;Event Message Flattening with Single Message Transform&lt;/h3&gt;

&lt;p&gt;For simply streaming into Kafka the &lt;em&gt;current&lt;/em&gt; state of the record, it can be useful to take just the &lt;code&gt;after&lt;/code&gt; section of the message. Kafka Connect includes functionality called Single Message Transform (SMT). As the name suggests, it enables you to transform single messages! You can read more about it and examples of its usage &lt;a href=&#34;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-3/&#34;&gt;here&lt;/a&gt;. As well as the &lt;a href=&#34;http://kafka.apache.org/documentation.html#connect_transforms&#34;&gt;Transforms that ship with Apache Kafka&lt;/a&gt;, you can write your own using the &lt;a href=&#34;https://kafka.apache.org/10/javadoc/org/apache/kafka/connect/transforms/Transformation.html&#34;&gt;documented API&lt;/a&gt;. This is exactly what the Debezium project have done, shipping their own SMT as part of it, providing an easy way to &lt;a href=&#34;http://debezium.io/docs/configuration/event-flattening/&#34;&gt;flatten the events that Debezium emits&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Using SMT you can amend the message inbound/outbound from Kafka to show just the new record:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;c1&amp;quot;: {
    &amp;quot;int&amp;quot;: 100
  },
  &amp;quot;c2&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;wibble&amp;quot;
  },
  &amp;quot;create_ts&amp;quot;: &amp;quot;2018-01-23T22:47:09Z&amp;quot;,
  &amp;quot;update_ts&amp;quot;: &amp;quot;2018-02-09T15:35:48Z&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead of the full change:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;before&amp;quot;: {
    &amp;quot;fullfillment.demo.foobar.Value&amp;quot;: {
      &amp;quot;c1&amp;quot;: {
        &amp;quot;int&amp;quot;: 100
      },
      &amp;quot;c2&amp;quot;: {
        &amp;quot;string&amp;quot;: &amp;quot;bar&amp;quot;
      },
      &amp;quot;create_ts&amp;quot;: &amp;quot;2018-01-23T22:47:09Z&amp;quot;,
      &amp;quot;update_ts&amp;quot;: &amp;quot;2018-01-23T22:47:09Z&amp;quot;
    }
  },
  &amp;quot;after&amp;quot;: {
    &amp;quot;fullfillment.demo.foobar.Value&amp;quot;: {
      &amp;quot;c1&amp;quot;: {
        &amp;quot;int&amp;quot;: 100
      },
      &amp;quot;c2&amp;quot;: {
        &amp;quot;string&amp;quot;: &amp;quot;wibble&amp;quot;
      },
      &amp;quot;create_ts&amp;quot;: &amp;quot;2018-01-23T22:47:09Z&amp;quot;,
      &amp;quot;update_ts&amp;quot;: &amp;quot;2018-02-09T15:35:48Z&amp;quot;
    }
  },
  &amp;quot;source&amp;quot;: {
    &amp;quot;name&amp;quot;: &amp;quot;fullfillment&amp;quot;,
    &amp;quot;server_id&amp;quot;: 42,
    &amp;quot;ts_sec&amp;quot;: 1518190548,
    &amp;quot;gtid&amp;quot;: null,
    &amp;quot;file&amp;quot;: &amp;quot;mysql-bin.000002&amp;quot;,
    &amp;quot;pos&amp;quot;: 1025,
    &amp;quot;row&amp;quot;: 3,
    &amp;quot;snapshot&amp;quot;: null,
    &amp;quot;thread&amp;quot;: {
      &amp;quot;long&amp;quot;: 11
    },
    &amp;quot;db&amp;quot;: {
      &amp;quot;string&amp;quot;: &amp;quot;demo&amp;quot;
    },
    &amp;quot;table&amp;quot;: {
      &amp;quot;string&amp;quot;: &amp;quot;foobar&amp;quot;
    }
  },
  &amp;quot;op&amp;quot;: &amp;quot;u&amp;quot;,
  &amp;quot;ts_ms&amp;quot;: {
    &amp;quot;long&amp;quot;: 1518190548539
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SMT can also be used to modify the target topic (which unmodified is &lt;code&gt;server.database.table&lt;/code&gt;), using the &lt;code&gt;RegexRouter&lt;/code&gt; transform.&lt;/p&gt;

&lt;p&gt;With these two SMT included, this is how our configuration looks now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;mysql-connector-flattened&amp;quot;,
  &amp;quot;config&amp;quot;: {
    &amp;quot;connector.class&amp;quot;: &amp;quot;io.debezium.connector.mysql.MySqlConnector&amp;quot;,
    &amp;quot;database.hostname&amp;quot;: &amp;quot;localhost&amp;quot;,
    &amp;quot;database.port&amp;quot;: &amp;quot;3306&amp;quot;,
    &amp;quot;database.user&amp;quot;: &amp;quot;debezium&amp;quot;,
    &amp;quot;database.password&amp;quot;: &amp;quot;dbz&amp;quot;,
    &amp;quot;database.server.id&amp;quot;: &amp;quot;42&amp;quot;,
    &amp;quot;database.server.name&amp;quot;: &amp;quot;fullfillment&amp;quot;,
    &amp;quot;database.history.kafka.bootstrap.servers&amp;quot;: &amp;quot;localhost:9092&amp;quot;,
    &amp;quot;database.history.kafka.topic&amp;quot;: &amp;quot;dbhistory.fullfillment&amp;quot; ,
    &amp;quot;include.schema.changes&amp;quot;: &amp;quot;true&amp;quot; ,
    &amp;quot;transforms&amp;quot;: &amp;quot;unwrap,changetopic&amp;quot;,
    &amp;quot;transforms.unwrap.type&amp;quot;: &amp;quot;io.debezium.transforms.UnwrapFromEnvelope&amp;quot;,
    &amp;quot;transforms.changetopic.type&amp;quot;:&amp;quot;org.apache.kafka.connect.transforms.RegexRouter&amp;quot;,
    &amp;quot;transforms.changetopic.regex&amp;quot;:&amp;quot;(.*)&amp;quot;,
    &amp;quot;transforms.changetopic.replacement&amp;quot;:&amp;quot;$1-smt&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;To see how streaming events from a RDBMS such as MySQL into Kafka can be even more powerful when combined with KSQL for stream processing check out &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-enriching-csv-events-with-data-from-rdbms-into-AWS/&#34;&gt;KSQL in Action: Enriching CSV Events with Data from RDBMS into AWS&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>KSQL: Topic … does not conform to the requirements</title>
			<link>https://rmoff.github.io/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/</link>
			<pubDate>Tue, 06 Mar 2018 23:08:11 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/</guid>
			<description>&lt;pre&gt;&lt;code&gt;io.confluent.ksql.exception.KafkaTopicException: Topic &#39;KSQL_NOTIFY&#39; does not conform to the requirements Partitions:1 v 4. Replication: 1 v 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Why? Because the topic KSQL creates to underpin a &lt;code&gt;CREATE STREAM AS SELECT&lt;/code&gt; or &lt;code&gt;CREATE TABLE AS SELECT&lt;/code&gt; already exists, and doesn&amp;rsquo;t match what it expects. By default it will create partitions &amp;amp; replicas based on the same values of the input topic.&lt;/p&gt;

&lt;p&gt;Options:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Use a different topic, via the &lt;code&gt;WITH (KAFKA_TOPIC=&#39;FOO&#39;)&lt;/code&gt; syntax, e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE STREAM TEST WITH (KAFKA_TOPIC=&#39;FOO&#39;) AS SELECT * FROM BAR;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tell KSQL to use values that match the existing topic, with the &lt;code&gt;PARTITIONS&lt;/code&gt; and &lt;code&gt;REPLICAS&lt;/code&gt; parameters. So if the existing topic only has one partition, then tell KSQL that&amp;rsquo;s what you want:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE STREAM TEST WITH (PARTITIONS=1) AS SELECT * FROM BAR;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
		</item>
		
		<item>
			<title>Streaming data from Kafka into Elasticsearch</title>
			<link>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
			<pubDate>Tue, 06 Mar 2018 22:21:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
			<description>&lt;p&gt;&lt;em&gt;This article is part of a series exploring Streaming ETL in practice. You can read about &lt;a href=&#34;https://rmoff.net/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/&#34;&gt;setting up the ingest of realtime events from a standard Oracle platform&lt;/a&gt;, and &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;building streaming ETL using KSQL&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use Kafka Connect to stream the Avro topics directly into Elasticsearch. Because we&amp;rsquo;re using Avro and the schema registry all of our Elasticsearch mappings will be created automagically and with the correct datatypes. You can read more about using Kafka Connect to build pipelines in an earlier blog series here: &lt;a href=&#34;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/&#34;&gt;1&lt;/a&gt; &lt;a href=&#34;https://www.confluent.io/blog/blogthe-simplest-useful-kafka-connect-data-pipeline-in-the-world-or-thereabouts-part-2/&#34;&gt;2&lt;/a&gt; &lt;a href=&#34;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-3/&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/connectsrwin.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Create the Connect sink configuration file—note that we&amp;rsquo;re using Single Message Transforms (SMT) to set Timestamp datatype for &lt;code&gt;op_ts&lt;/code&gt; and &lt;code&gt;current_ts&lt;/code&gt;. We&amp;rsquo;re doing this to get around a limitation in the current release of GoldenGate in which date/timestamps are simply passed as strings. In order for Elasticsearch to work seamlessly, we want the Kafka Connect sink to pass the datatype as a timestamp—which using the SMT will enable.&lt;/p&gt;

&lt;p&gt;Write &lt;a href=&#34;https://gist.github.com/rmoff/975707be38b452f79347cde065b2322b&#34;&gt;this&lt;/a&gt; to &lt;code&gt;/home/oracle/es-sink-soe-all.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Load the sink:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent load es-sink-soe-all -d /home/oracle/es-sink-soe-all.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check status:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent status connectors|  jq &#39;.[]&#39;|  xargs -I{connector} confluent status {connector}|  jq -c -M &#39;[.name,.connector.state,.tasks[].state]|join(&amp;quot;:|:&amp;quot;)&#39;|  column -s : -t|  sed &#39;s/\&amp;quot;//g&#39;|  sort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it&amp;rsquo;s &lt;code&gt;FAILED&lt;/code&gt; then check the Connect log:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent log connect
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check Elasticsearch doc count:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;http://localhost:9200/soe.warehouses/_search&amp;quot; | jq &#39;.hits.total&#39;
1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, with the data now in Elasticsearch, you can go and build Kibana dashboards to your heart&amp;rsquo;s content.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/ogg01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/ogg02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a very simple one of median/max Order values and counts over time, along with a histogram plot showing the distribution of order values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/oggkib01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Remember that this is based on data streaming through from our source transactional system, with two particular benefits:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We&amp;rsquo;ve not had to modify the source application at all to provide this data&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s extremely low latency, giving us a near-realtime analytics view of our data&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;To read more about this, and see the awesome KSQL in action, head over to the &lt;a href=&#34;https://www.confluent.io/blog/&#34;&gt;Confluent blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Installing the Python Kafka library from Confluent - troubleshooting some silly errors…</title>
			<link>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</link>
			<pubDate>Tue, 06 Mar 2018 22:18:24 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</guid>
			<description>&lt;p&gt;System:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01:~$ uname -a
Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux

rmoff@proxmox01:~$ head -n1 /etc/os-release
PRETTY_NAME=&amp;quot;Debian GNU/Linux 8 (jessie)&amp;quot;

rmoff@proxmox01:~$ python --version
Python 2.7.9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/&#34;&gt;https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/confluentinc/confluent-kafka-python&#34;&gt;https://github.com/confluentinc/confluent-kafka-python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Install &lt;code&gt;librdkafka&lt;/code&gt;, which is a pre-req for the Python library:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add -
sudo add-apt-repository &amp;quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main&amp;quot;
sudo apt-get install librdkafka-dev python-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Setup virtualenv:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install virtualenv
virtualenv kafka_push_notify
source ./kafka_push_notify/bin/activate.fish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Try to install &lt;code&gt;confluent-kafka&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install confluent-kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fails:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Cleaning up...
Command /home/rmoff/kafka_push_notify/bin/python2 -c &amp;quot;import setuptools, tokenize;__file__=&#39;/tmp/pip-build-Nkr6wJ/confluent-kafka/setup.py&#39;;exec(compile(getattr(tokenize, &#39;open&#39;, open)(__file__).read().replace(&#39;\r\n&#39;, &#39;\n&#39;), __file__, &#39;exec&#39;))&amp;quot; install --record /tmp/pip-OlKYHm-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/rmoff/kafka_push_notify/include/site/python2.7 failed with error code 1 in /tmp/pip-build-Nkr6wJ/confluent-kafka
Traceback (most recent call last):
  File &amp;quot;/home/rmoff/kafka_push_notify/bin/pip&amp;quot;, line 11, in &amp;lt;module&amp;gt;
    sys.exit(main())
  File &amp;quot;/home/rmoff/kafka_push_notify/local/lib/python2.7/site-packages/pip/__init__.py&amp;quot;, line 248, in main
    return command.main(cmd_args)
  File &amp;quot;/home/rmoff/kafka_push_notify/local/lib/python2.7/site-packages/pip/basecommand.py&amp;quot;, line 161, in main
    text = &#39;\n&#39;.join(complete_log)
UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe2 in position 75: ordinal not in range(128)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same error if I clone the repo (&lt;a href=&#34;https://github.com/confluentinc/confluent-kafka-python.git&#34;&gt;https://github.com/confluentinc/confluent-kafka-python.git&lt;/a&gt;) and do &lt;code&gt;pip install .&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If I try to install it outside of virtualenv it fails with :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
confluent_kafka/src/confluent_kafka.c: In function ‘_init_cimpl’:
confluent_kafka/src/confluent_kafka.c:1590:56: error: ‘RD_KAFKA_TIMESTAMP_NOT_AVAILABLE’ undeclared (first use in this function)
  PyModule_AddIntConstant(m, &amp;quot;TIMESTAMP_NOT_AVAILABLE&amp;quot;, RD_KAFKA_TIMESTAMP_NOT_AVAILABLE);
                                                        ^
confluent_kafka/src/confluent_kafka.c:1591:54: error: ‘RD_KAFKA_TIMESTAMP_CREATE_TIME’ undeclared (first use in this function)
  PyModule_AddIntConstant(m, &amp;quot;TIMESTAMP_CREATE_TIME&amp;quot;, RD_KAFKA_TIMESTAMP_CREATE_TIME);
                                                      ^
confluent_kafka/src/confluent_kafka.c:1592:58: error: ‘RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME’ undeclared (first use in this function)
  PyModule_AddIntConstant(m, &amp;quot;TIMESTAMP_LOG_APPEND_TIME&amp;quot;, RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME);
                                                          ^
confluent_kafka/src/confluent_kafka.c:1597:54: error: ‘RD_KAFKA_OFFSET_INVALID’ undeclared (first use in this function)
         PyModule_AddIntConstant(m, &amp;quot;OFFSET_INVALID&amp;quot;, RD_KAFKA_OFFSET_INVALID);
                                                      ^
error: command &#39;x86_64-linux-gnu-gcc&#39; failed with exit status 1

----------------------------------------
Command &amp;quot;/usr/bin/python -u -c &amp;quot;import setuptools, tokenize;__file__=&#39;/tmp/pip-build-8U7Wwr/confluent-kafka/setup.py&#39;;f=getattr(tokenize, &#39;open&#39;, open)(__file__);code=f.read().replace(&#39;\r\n&#39;, &#39;\n&#39;);f.close();exec(compile(code, __file__, &#39;exec&#39;))&amp;quot; install --record /tmp/pip-k1Yd7x-record/install-record.txt --single-version-externally-managed --compile&amp;quot; failed with error code 1 in /tmp/pip-build-8U7Wwr/confluent-kafka/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check &lt;a href=&#34;https://github.com/confluentinc/confluent-kafka-python/issues/&#34;&gt;the issues on github&lt;/a&gt; - many point to problem being old version of &lt;code&gt;librdkafka&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ dpkg -l librdkafka-dev
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                                                 Version                         Architecture                    Description
+++-====================================================-===============================-===============================-==============================================================================================================
ii  librdkafka-dev:amd64                                 0.8.5-2                         amd64                           library implementing the Apache Kafka protocol (development headers)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s weird - the latest version is 0.11 or so.&lt;/p&gt;

&lt;p&gt;The problem? I forgot to run &lt;code&gt;apt-get update&lt;/code&gt; after adding the repo. Looking back at my session history I can see that it pulled down:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://ftp.uk.debian.org/debian/ jessie/main librdkafka-dev amd64 0.8.5-2 [106 kB]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So let&amp;rsquo;s fix that!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install librdkafka-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now the package comes from the horse&amp;rsquo;s mouth, as it were:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://packages.confluent.io/deb/4.0/ stable/main librdkafka-dev amd64 0.11.1~1confluent4.0.0-1 [412 kB]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the version :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dpkg -l librdkafka-dev
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                                                 Version                         Architecture                    Description
+++-====================================================-===============================-===============================-==============================================================================================================
ii  librdkafka-dev:amd64                                 0.11.1~1confluent4.0.0-1        amd64                           library implementing the Apache Kafka protocol (development headers)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Happy days. Now let&amp;rsquo;s try the install again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(kafka_push_notify)rmoff@proxmox01 ~/confluent-kafka-python&amp;gt; pip install confluent-kafka
Downloading/unpacking confluent-kafka
  Downloading confluent-kafka-0.11.0.tar.gz (42kB): 42kB downloaded
  Running setup.py (path:/tmp/pip-build-O1tSQm/confluent-kafka/setup.py) egg_info for package confluent-kafka

Installing collected packages: confluent-kafka
  Running setup.py install for confluent-kafka
    building &#39;confluent_kafka.cimpl&#39; extension
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c confluent_kafka/src/confluent_kafka.c -o build/temp.linux-x86_64-2.7/confluent_kafka/src/confluent_kafka.o
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c confluent_kafka/src/Producer.c -o build/temp.linux-x86_64-2.7/confluent_kafka/src/Producer.o
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c confluent_kafka/src/Consumer.c -o build/temp.linux-x86_64-2.7/confluent_kafka/src/Consumer.o
    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/confluent_kafka/src/confluent_kafka.o build/temp.linux-x86_64-2.7/confluent_kafka/src/Producer.o build/temp.linux-x86_64-2.7/confluent_kafka/src/Consumer.o -lrdkafka -o build/lib.linux-x86_64-2.7/confluent_kafka/cimpl.so

Successfully installed confluent-kafka
Cleaning up...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phew. All good.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Why Do We Need Streaming ETL?</title>
			<link>https://rmoff.github.io/2018/03/06/why-do-we-need-streaming-etl/</link>
			<pubDate>Tue, 06 Mar 2018 22:18:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/03/06/why-do-we-need-streaming-etl/</guid>
			<description>&lt;p&gt;&lt;em&gt;(This is an expanded version of the intro to an article I posted over on the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog&lt;/a&gt;. Here I get to be as verbose as I like &lt;code&gt;;)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe. From there it was checked and loaded, and then reports generated on it. This was nearly twenty years ago as my greying beard will attest—and not a lot has changed in the large majority of reporting and analytics systems since then. COBOL is maybe less common, but what has remained constant is the batch-driven nature of processing. Sometimes batches are run more frequently, and get given fancy names like intra-day ETL or even micro-batching. But batch processing it is, and as such latency is built into our reporting &lt;em&gt;by design&lt;/em&gt;. When we opt for batch processing we voluntarily inject delays into the availability of data to our end users. Much better is to build our systems around a streaming platform instead.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/streaming-platform.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Apache Kafka is a distributed streaming platform, and as well as powering a large number of stream-based mission-critical systems around the world, it has a huge role to play in &lt;strong&gt;data integration&lt;/strong&gt; too. Back in 2016 Neha Narkhede wrote that &lt;a href=&#34;https://www.infoq.com/presentations/etl-streams&#34;&gt;ETL Is Dead, Long Live Streams&lt;/a&gt;, and since then we&amp;rsquo;ve seen more and more companies moving to adopt Apache Kafka as the backbone of their architectures. Through &lt;a href=&#34;https://www.confluent.io/product/connectors/&#34;&gt;Kafka&amp;rsquo;s Connect API&lt;/a&gt; pretty much any standard system can serve as the source of streaming data into Kafka. Once data is in Kafka, it is &amp;ldquo;just&amp;rdquo; a message; its source is irrelevant when it comes to how we want to use it. From within Kafka we can transform the data further, drive microservices, and stream the data out through Kafka&amp;rsquo;s Connect API to myriad targets. Which targets? Many targets!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.confluent.io/wp-content/uploads/etl_streaming-768x410.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Often we&amp;rsquo;ll want to use the data not landed in a store, but as the input for &lt;a href=&#34;https://www.confluent.io/blog/building-a-microservices-ecosystem-with-kafka-streams-and-ksql/&#34;&gt;&lt;em&gt;event driven architectures&lt;/em&gt;&lt;/a&gt; that drive business processes.&lt;/li&gt;
&lt;li&gt;For long-term analytics we have the data lake concept, often hosted on technologies such as HDFS/Hive, S3/Athena, BigQuery, and Snowflake.&lt;/li&gt;
&lt;li&gt;Often we want refine elements of the data lake into what&amp;rsquo;s generally recognised as a data warehouse/mart for performance as well as formal modelling and audit of the data lineage. These may also reside on the same platforms as the data lake, or perhaps standard RDBMS such as Postgres, MySQL, Oracle etc.&lt;/li&gt;
&lt;li&gt;Real-time analytics can be served through slices of our events streamed to NoSQL stores such as Cassandra, or search-based tools such as Elasticsearch&lt;/li&gt;
&lt;li&gt;Elasticsearch and similar tools such as Solr are frequently used for search replicas/caches of data both to drive search in end-user applications and websites, as well as ad-hoc analytics and data exploration by analysts.&lt;/li&gt;
&lt;li&gt;More specific tasks often demand dedicated technologies, such as time series databases like InfluxDB, Graph analytics with Neo4j, and so on&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As the above list demonstrates, how we think about data and what we want to do with it helps define the kind of technology we&amp;rsquo;re going to use to store it. A very important point to realise here is that &lt;em&gt;you generally will not have just one target for your data&lt;/em&gt;. Kafka enables two vital architectural principles here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stream your data from source into Kafka, and from there to target. Decouple your architecture to make it flexible and agile for future development. Resist the temptation to load it into your data lake &lt;em&gt;first&lt;/em&gt; and then process it onwards. Why introduce completely unnecessary latencies and dependencies in your processing?&lt;/li&gt;
&lt;li&gt;Following on from the above, be aware that you can stream data from Kafka to multiple targets &lt;em&gt;concurrently&lt;/em&gt;. If you want all your data in Hadoop for audit purposes, or just because it gives you a warm fuzzy feeling - you can do. But if you want to use that data somewhere else, you can stream it directly from Kafka. Perhaps you want to drive some alerting based on the events streaming in, or you just want the data in a second data store. Either way, you decouple that secondary use from Hadoop, making the pipeline simpler to maintain, less fragile—and without the latency!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From a point of view of data latency this first point above is critical. Getting data where you want it when you (and your users) want it is one of the key drivers of technology choice. Reducing the latency of data made available to users enables them to make more accurate business decisions. And if you don&amp;rsquo;t care about latency? Well that&amp;rsquo;s fine too; Kafka supports batch-concepts too, by virtue of the fact that it &lt;em&gt;persists data&lt;/em&gt; which means that it is there for a consumer to read when the consumer wants to—even if that&amp;rsquo;s just as part of a once-a-day batch load.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/kafka_streaming_etl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As well as chosing the most appropriate technology for a particular task and being able to maintain a correct state of data in it via Kafka, using Kafka as our data backbone also lets us embrace and rationalise the proliferation of technologies in use across organisations as control moves away from central IT and out to individual business units. Instead of the futility of railing against this and trying to prevent it, with Kafka we can easily provide a feed of clean data to anyone who wants it, without impacting the design or availability of our central data architecture. &lt;em&gt;Embrace the Anarchy!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Even if you don&amp;rsquo;t face the challenge of disparate systems and have a perfectly controlled and rational technology footprint, Kafka gives you the power to &lt;strong&gt;evolve&lt;/strong&gt; this in a beautifully flexible manner. With Kafka at the heart of your architecture, you can replace sources without impacting downstream users of the data. You can add additional targets, or evaluate alternative technologies, alongside existing ones. All of this is done in a &lt;strong&gt;decoupled&lt;/strong&gt; manner, enabling agile development of systems.&lt;/p&gt;

&lt;p&gt;One last point for now is that in some cases, we don&amp;rsquo;t even want to store the data outside of Kafka. &lt;em&gt;What is the nonsense&lt;/em&gt;, you may ask? Surely we must always land data to a database, to a data store? Not always. Here are two examples:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Consider a source of data that you want to enrich and load to a target. Standard design would be to load the raw data into a database as the staging layer, and then enrich and transform it from there. &lt;strong&gt;Kafka itself is the staging layer&lt;/strong&gt; here, acting as the central point for all inbound data.&lt;/li&gt;
&lt;li&gt;Kafka Streams has &lt;strong&gt;interactive query&lt;/strong&gt; capabilities meaning that it can serve up the state of a stream (such as a point in time aggregation) directly from its local state store. Doing this, external applications can query a dedicated stream job to directly access data, without needing to land it at an intermediary data source. Interactive query generally fits operational systems better, such as realtime embedded dashboards, but where you do have these, challenge any assumptions that you have around the necessity of a database.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To read more about storing data in Kafka see &lt;a href=&#34;https://www.confluent.io/blog/okay-store-data-apache-kafka/&#34;&gt;Jay Krep&amp;rsquo;s recent article&lt;/a&gt; as well as an example of it in action at the &lt;a href=&#34;https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/&#34;&gt;New York Times&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>HOWTO: Oracle GoldenGate &#43; Apache Kafka &#43; Schema Registry &#43; Swingbench</title>
			<link>https://rmoff.github.io/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/</link>
			<pubDate>Thu, 01 Feb 2018 23:15:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/</guid>
			<description>

&lt;p&gt;&lt;em&gt;This is the detailed step-by-step if you want to recreate the process I describe in the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I used Oracle&amp;rsquo;s &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt;, which comes preinstalled with Oracle 12cR2. You can see the notes on &lt;a href=&#34;https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12-3-1-with-kafka-connect-and-confluent-platform/&#34;&gt;how to do this here&lt;/a&gt;. These notes take you through installing and configuring:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Swingbench, to create a sample &amp;ldquo;Order Entry&amp;rdquo; schema and simulate events on the Oracle database&lt;/li&gt;
&lt;li&gt;Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith)

&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m using Oracle GoldenGate 12.3.1 which includes the Kafka Connect handler as part of its distribution. A connector for earlier versions can be &lt;a href=&#34;http://www.oracle.com/technetwork/middleware/goldengate/oracle-goldengate-exchange-3805527.html&#34;&gt;found here&lt;/a&gt;. Some of the syntax may differ in the configuration below - if you hit problems then check out [an article that I wrote]() with an earlier version of the tool.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OGG &lt;code&gt;extract&lt;/code&gt; from the Order Entry schema&lt;/li&gt;
&lt;li&gt;Confluent Platform&lt;/li&gt;
&lt;li&gt;KSQL&lt;/li&gt;
&lt;li&gt;Elasticsearch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From this point, I&amp;rsquo;ll now walk through configuring OGG-BD with the Kafka Connect handler&lt;/p&gt;

&lt;h3 id=&#34;configuring-the-kafka-connect-replicat&#34;&gt;Configuring the Kafka Connect replicat&lt;/h3&gt;

&lt;p&gt;The OGG-BD replicat takes the trail file of events written by the &lt;code&gt;extract&lt;/code&gt; job, and replays those events to the target. In our case, the target is the Kafka Connect handler.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/02/oggkaf01sm.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can find full documentation for the OGG-BD replicat &lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-81730248-AC12-438E-AF82-48C7002178EC&#34;&gt;here&lt;/a&gt;. Each replicat has three configuration files:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;parameter&lt;/strong&gt; file, which can be used to specify configuration including the selection of specific tables or schemas from the Extract trail file. This file also points to the primary &lt;strong&gt;properties&lt;/strong&gt; file&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;properties&lt;/strong&gt; file is where we tell OGG-BD to use the Kafka Connect handler (&lt;code&gt;gg.handler.kafkaconnect.type=kafkaconnect&lt;/code&gt;), as well as the &lt;strong&gt;topic name format&lt;/strong&gt; and &lt;strong&gt;Kafka message key&lt;/strong&gt;. These latter two items are new and improved in 12.3.1, and are template-driven. You can see the full syntax for them &lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GUID-A87CAFFA-DACF-43A0-8C6C-5C64B578D606&#34;&gt;here&lt;/a&gt;. Another useful configuration item in this file is &lt;code&gt;gg.log.level&lt;/code&gt; which you can set to &lt;code&gt;DEBUG&lt;/code&gt; if you need to dig into what&amp;rsquo;s going on.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Kafka Connect configuration&lt;/strong&gt; file, which points to the Kafka brokers, and the Converters that we want to use (Json or Avro). If we&amp;rsquo;re using Avro, the URL for the Schema Registry is also defined here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article we&amp;rsquo;re going to use OGG-BD to populate a topic for each table, using Avro encoding. &lt;a href=&#34;https://gist.github.com/rmoff/221b4a1903a85568042e3a1b9b07ab95&#34;&gt;Here are&lt;/a&gt; the three configuration files you should put in &lt;code&gt;/u01/app/ogg-bd/dirprm&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Be very careful with copying &amp;amp; pasting these configuration files, as trailing whitespace can cause problems, &lt;a href=&#34;https://rmoff.net/2017/09/12/oracle-goldengate-kafka-connect-handler-troubleshooting/&#34;&gt;detailed here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;starting-the-replicat&#34;&gt;Starting the replicat&lt;/h3&gt;

&lt;p&gt;From the bash shell, start the OGG-BD &lt;code&gt;ggsci&lt;/code&gt; tool:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# set to orcl12c when prompted
. oraenv

export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/amd64/server/

cd /u01/app/ogg-bd
rlwrap ./ggsci
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then from the &lt;code&gt;ggsci&amp;gt;&lt;/code&gt; prompt run the following to start each replicat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;add replicat rkafavro, exttrail /u01/app/ogg/dirdat/oe
start rkafavro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having done this, we&amp;rsquo;ll want to do a little smoke test just to see that data modified in the SOE schema streams through via OGG-BD into Kafka.&lt;/p&gt;

&lt;p&gt;Insert row to an SOE table&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ~]$ rlwrap sqlplus SYS/oracle@orcl as sysdba

SQL*Plus: Release 12.2.0.1.0 Production on Mon Sep 11 10:14:28 2017

Copyright (c) 1982, 2016, Oracle.  All rights reserved.


Connected to:
Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production

SQL&amp;gt; insert into soe.logon values(42,42,sysdate);

1 row created.

SQL&amp;gt; commit;

Commit complete.

SQL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that the topics have been created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafka-topics --zookeeper localhost:2181 --list
__confluent.support.metrics
__consumer_offsets
_schemas
connect-configs
connect-offsets
connect-statuses
ora-ogg-SOE-LOGON-avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View the record&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-avro-console-consumer \
--bootstrap-server localhost:9092 \
--property schema.registry.url=http://localhost:8081 \
--property print.key=true \
--from-beginning \
--topic ora-ogg-SOE-LOGON-avro | jq &#39;.&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;42_42_2017-09-11 10:14:30&amp;quot;
{
  &amp;quot;table&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;ORCL.SOE.LOGON&amp;quot;
  },
  &amp;quot;op_type&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;I&amp;quot;
  },
  &amp;quot;op_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-11 14:14:39.000000&amp;quot;
  },
  &amp;quot;current_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-11 10:14:43.164000&amp;quot;
  },
  &amp;quot;pos&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;00000000010000002172&amp;quot;
  },
  &amp;quot;LOGON_ID&amp;quot;: {
    &amp;quot;double&amp;quot;: 42
  },
  &amp;quot;CUSTOMER_ID&amp;quot;: {
    &amp;quot;double&amp;quot;: 42
  },
  &amp;quot;LOGON_DATE&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-11 10:14:30&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having confirmed that the replication is working, we can now run Swingbench to simulate some work on our source transactional system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/swingbench/bin/charbench -cs localhost:1521/orcl -u soe -p soe -v trans,users -c /opt/swingbench/configs/SOE_Client_Side.xml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Author  :        Dominic Giles
Version :        2.6.0.1046

Results will be written to results.xml.
Hit Return to Terminate Run...

Time            NCR     UCD     BP      OP      PO      BO      SQ      WQ      WA      Users
09:01:04        0       0       0       0       0       0       0       0       0       [0/4]
09:01:06        0       0       0       0       0       0       0       0       0       [0/4]
09:01:08        15      7       68      28      3       12      1       0       0       [4/4]
09:01:10        47      15      179     75      9       31      2       0       0       [4/4]
09:01:12        74      20      248     119     11      45      3       2       0       [4/4]
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hit Ctrl-C when you want to quit Swingbench. I&amp;rsquo;ve shown the command line version (&lt;code&gt;charbench&lt;/code&gt;) above; there is also a GUI version that you can use if you want to easily tweak the simulation characteristics.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;To read more about this, and see the awesome KSQL in action, head over to the &lt;a href=&#34;https://www.confluent.io/blog/&#34;&gt;Confluent blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available</title>
			<link>https://rmoff.github.io/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</link>
			<pubDate>Wed, 03 Jan 2018 11:26:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</guid>
			<description>&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;See also &lt;a href=&#34;https://rmoff.net/2018/08/02/kafka-listeners-explained/&#34;&gt;Kafka Listeners - Explained&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;KSQL was throwing a similar error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KSQL cannot initialize AdminCLient.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I had correctly set the machine&amp;rsquo;s hostname in my Kafka &lt;code&gt;server.properties&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;listeners=PLAINTEXT://proxmox01.moffatt.me:9092
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but in Schema Registry, Connect etc I had not, and so they were using the default (&lt;code&gt;localhost&lt;/code&gt;). The &lt;a href=&#34;https://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/admin/AdminClient.html&#34;&gt;&lt;code&gt;AdminClient&lt;/code&gt;&lt;/a&gt; bit comes in because when they launch each creates its own internal topics.&lt;/p&gt;

&lt;p&gt;Based on my &lt;code&gt;/etc/hosts&lt;/code&gt; we can see &lt;code&gt;localhost&lt;/code&gt; has a different IP from the hostname (&lt;code&gt;proxmox01&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; cat /etc/hosts
127.0.0.1 localhost.localdomain localhost
192.168.10.250 proxmox01.moffatt.me proxmox01 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thus, Kafka was listening on one IP (192.168.10.25):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo netstat -plnt|grep 9092
tcp6       0      0 192.168.10.250:9092     :::*                    LISTEN      30345/java
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But Schema Registry and Kafka Connect were trying (and failing) to connect to it on another (&lt;code&gt;localhost&lt;/code&gt; → &lt;code&gt;127.0.0.1&lt;/code&gt;). With the appropriate files fixed (&lt;code&gt;connect-avro-distributed.properties&lt;/code&gt;, &lt;code&gt;schema-registry.properties&lt;/code&gt;) all was well with the world!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I also hit a hostname/networking related error earlier in this process, which stopped Kafka launching entirely:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka.common.KafkaException: Socket server failed to bind to proxmox01.moffatt.me:9092: Cannot assign requested address.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Turns out my &lt;code&gt;/etc/hosts&lt;/code&gt; was fubar - it had the wrong IP address listed for the hostname. Instead of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.10.250 proxmox01.moffatt.me proxmox01 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it had&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.1.250 proxmox01.moffatt.me proxmox01 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which was wrong. Fixing this solved the problem.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform</title>
			<link>https://rmoff.github.io/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</link>
			<pubDate>Tue, 21 Nov 2017 17:31:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</guid>
			<description>

&lt;p&gt;&lt;em&gt;Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I used the &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt; for this - it&amp;rsquo;s preinstalled with Oracle 12cR2. &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;Big Data Lite&lt;/a&gt; is nice but currently has an older version of GoldenGate.&lt;/p&gt;

&lt;p&gt;Login to the VM (oracle/oracle) and then install some useful things:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop
sudo su -
cd /etc/yum.repos.d/
wget http://download.opensuse.org/repositories/shells:fish:release:2/CentOS_7/shells:fish:release:2.repo
yum install fish
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check Oracle version etc:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/dedaa1a2ef4b3225a6299a36629dcb67.js&#34;&gt;&lt;/script&gt;

&lt;h1 id=&#34;install-ogg&#34;&gt;Install OGG&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html&#34;&gt;Download both&lt;/a&gt; &lt;strong&gt;Oracle GoldenGate&lt;/strong&gt; 12.3.0.1 and &lt;strong&gt;Oracle GoldenGate for Big Data&lt;/strong&gt; 12.3.1.1.0. For reference, here is the &lt;a href=&#34;http://docs.oracle.com/goldengate/c1221/gg-winux/GIORA/GUID-B5B88238-C74D-487B-AD7D-7809ED5125EE.htm#GIORA162&#34;&gt;Install guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Make sure installers are present on VM&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ~]$ ls -l ~/Downloads/
total 610368
-rw-r--r--. 1 oracle oinstall 543200432 Sep  5 08:45 123010_fbo_ggs_Linux_x64_shiphome.zip
-rw-r--r--. 1 oracle oinstall  81812011 Sep  5 08:38 123110_ggs_Adapters_Linux_x64.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unzip the OGG installer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost Downloads]$ unzip 123010_fbo_ggs_Linux_x64_shiphome.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Build a response file (e.g. &lt;code&gt;/tmp/oggcore.rsp&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oracle.install.responseFileVersion=/oracle/install/rspfmt_ogginstall_response_schema_v12_1_2
INSTALL_OPTION=ORA12c
SOFTWARE_LOCATION=/u01/app/ogg
START_MANAGER=true
MANAGER_PORT=7809
DATABASE_LOCATION=/u01/app/oracle/product/12.2/db_1/
INVENTORY_LOCATION=/u01/app/oraInventory/
UNIX_GROUP_NAME=oracle
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install OGG:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost Disk1]$ ~/Downloads/fbo_ggs_Linux_x64_shiphome/Disk1/runInstaller -silent -nowait -responseFile /tmp/oggcore.rsp
Starting Oracle Universal Installer...

Checking Temp space: must be greater than 120 MB.   Actual 13557 MB    Passed
Checking swap space: must be greater than 150 MB.   Actual 4088 MB    Passed
Preparing to launch Oracle Universal Installer from /tmp/OraInstall2017-09-05_09-34-29AM. Please wait ...[oracle@localhost Disk1]$ [WARNING] [INS-75014] Database version cannot be determined from the location specified.
CAUSE: The components inventory may be missing or corrupted in the location specified.
ACTION: Specify an alternate database location.
You can find the log of this install session at:
/u01/installervb/logs/installActions2017-09-05_09-34-29AM.log

…
…

The installation of Oracle GoldenGate Core was successful.
Please check &#39;/u01/installervb/logs/silentInstall2017-09-05_09-34-29AM.log&#39; for more details.
Successfully Setup Software.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check that OGG Manager is running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ogg]$ . oraenv
ORACLE_SID = [oracle] ? orcl12c
ORACLE_BASE environment variable is not being set since this
information is not available for the current user ID oracle.
You can set ORACLE_BASE manually if it is required.
Resetting ORACLE_BASE to its previous value or ORACLE_HOME
The Oracle base has been set to /u01/app/oracle/product/12.2/db_1
[oracle@localhost ogg]$ cd /u01/app/ogg/
[oracle@localhost ogg]$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter for Oracle
Version 12.3.0.1.0 OGGCORE_12.3.0.1.0_PLATFORMS_170721.0154_FBO
Linux, x64, 64bit (optimized), Oracle 12c on Jul 21 2017 23:31:13
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2017, Oracle and/or its affiliates. All rights reserved.



GGSCI (localhost.localdomain) 1&amp;gt; info mgr

Manager is running (IP port localhost.localdomain.7809, Process ID 23231).


GGSCI (localhost.localdomain) 2&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configure-db-for-ogg&#34;&gt;Configure DB for OGG&lt;/h3&gt;

&lt;p&gt;Since the DB is multitenant, &lt;a href=&#34;http://docs.oracle.com/goldengate/c1221/gg-winux/GIORA/GUID-1A6D7483-BF6D-4354-904D-E9BBD0E7DD59.htm#GIORA558&#34;&gt;need to use&lt;/a&gt; &lt;a href=&#34;http://docs.oracle.com/goldengate/c1221/gg-winux/GIORA/GUID-6C0E8B93-FA67-4700-AC33-6E57F4DBF9B2.htm#GIORA212&#34;&gt;integrated capture mode&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Add TNS entry (per &lt;a href=&#34;http://docs.oracle.com/goldengate/c1221/gg-winux/GIORA/GUID-A72C7E33-6AA6-4F88-9F01-E9FC0FDE0C46.htm#GIORA982&#34;&gt;doc&lt;/a&gt;) to &lt;code&gt;/u01/app/oracle/product/12.2/db_1/network/admin/tnsnames.ora&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OGG_ORCL12C =
  (DESCRIPTION =
    (ADDRESS = (PROTOCOL = TCP)(HOST = 0.0.0.0)(PORT = 1521))
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = orcl12c)
    )
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, set up &lt;a href=&#34;http://docs.oracle.com/goldengate/c1221/gg-winux/GIORA/GUID-55E7046C-0550-40C2-A855-904A2049F31B.htm#GIORA367&#34;&gt;Minimum Database-level Supplemental Logging&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Launch SQL*Plus: &lt;code&gt;rlwrap sqlplus SYS/oracle@orcl12c as sysdba&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
ALTER DATABASE FORCE LOGGING;
SHUTDOWN IMMEDIATE
STARTUP MOUNT
ALTER DATABASE ARCHIVELOG;
ALTER DATABASE OPEN;
ALTER SYSTEM SWITCH LOGFILE;
ALTER SYSTEM SET ENABLE_GOLDENGATE_REPLICATION=TRUE SCOPE=BOTH;
EXIT
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;configure-ogg-extract&#34;&gt;Configure OGG Extract&lt;/h4&gt;

&lt;p&gt;Launch OGG command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ~]$ . oraenv
ORACLE_SID = [oracle] ? orcl12c
ORACLE_BASE environment variable is not being set since this
information is not available for the current user ID oracle.
You can set ORACLE_BASE manually if it is required.
Resetting ORACLE_BASE to its previous value or ORACLE_HOME
The Oracle base has been set to /u01/app/oracle/product/12.2/db_1
[oracle@localhost ~]$
[oracle@localhost ~]$ cd /u01/app/ogg/
[oracle@localhost ogg]$ rlwrap ./ggs
ggsci       ggserr.log
[oracle@localhost ogg]$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter for Oracle
Version 12.3.0.1.0 OGGCORE_12.3.0.1.0_PLATFORMS_170721.0154_FBO
Linux, x64, 64bit (optimized), Oracle 12c on Jul 21 2017 23:31:13
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2017, Oracle and/or its affiliates. All rights reserved.



GGSCI (localhost.localdomain) 1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Register the integrated Extract process, for the Container Database (&lt;code&gt;orcl&lt;/code&gt;) - this&amp;rsquo;ll take a minute or two to complete:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DBLOGIN USERID SYSTEM@localhost:1521/orcl12c PASSWORD oracle
REGISTER EXTRACT EXT1 DATABASE CONTAINER (ORCL)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Enter commands to enable schema logging with all columns captured, on schema &lt;code&gt;HR&lt;/code&gt; in the pluggable DB (&lt;code&gt;ORCL&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD SCHEMATRANDATA ORCL.HR ALLCOLS
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now define the extract itself:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD EXTRACT EXT1, INTEGRATED TRANLOG, BEGIN NOW
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write a trail file for the extract&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD EXTTRAIL ./dirdat/lt EXTRACT EXT1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Specify parameters for the extract:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;EDIT PARAM EXT1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the edit session paste:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;EXTRACT EXT1
USERID SYSTEM@OGG_ORCL12C, PASSWORD oracle
EXTTRAIL ./dirdat/lt
SOURCECATALOG ORCL
TABLE HR.*;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save and close the file. Now we&amp;rsquo;re ready to start the extract.&lt;/p&gt;

&lt;p&gt;From the &lt;code&gt;GGSCI&lt;/code&gt; prompt issue:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;START EXT1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and check that it&amp;rsquo;s running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INFO EXT1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expected status:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;EXTRACT    EXT1      Last Started 2017-09-05 11:21   Status RUNNING
Checkpoint Lag       00:00:00 (updated 00:00:05 ago)
Process ID           27550
Log Read Checkpoint  Oracle Integrated Redo Logs
                     2017-09-05 11:21:18
                     SCN 0.1957461 (1957461)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If it doesn&amp;rsquo;t start successfully then check &lt;code&gt;/u01/app/ogg/ggserr.log&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;smoketest&#34;&gt;Smoketest&lt;/h4&gt;

&lt;p&gt;Log into SQL*Plus&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rlwrap sqlplus SYS/oracle@orcl as sysdba
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Insert a row and commit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; INSERT INTO HR.REGIONS VALUES (42,&#39;FOO&#39;);

1 row created.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fire up OGG&amp;rsquo;s &lt;code&gt;logdump&lt;/code&gt; (&lt;a href=&#34;https://www.rittmanmead.com/blog/2016/09/using-logdump-to-debug-oracle-goldengate-and-kafka/&#34;&gt;ref&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ogg]$ rlwrap ./logdump

Oracle GoldenGate Log File Dump Utility for Oracle
Version 12.3.0.1.0 OGGCORE_12.3.0.1.0_PLATFORMS_170721.0154

Copyright (C) 1995, 2017, Oracle and/or its affiliates. All rights reserved.



Logdump 11 &amp;gt;GHDR ON
Logdump 12 &amp;gt;DETAIL ON
Logdump 13 &amp;gt;DETAIL DATA
Logdump 14 &amp;gt;OPEN /u01/app/ogg/dirdat/lt000000000
Current LogTrail is /u01/app/ogg/dirdat/lt000000000
Logdump 15 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the extract trail file and see the record added (preceeded by the table metadata):&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/db86d4b12a295aea51b55f3c5abf5236.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;install-ogg-bd&#34;&gt;Install OGG-BD&lt;/h3&gt;

&lt;p&gt;Doc: &lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GBDIG/toc.htm&#34;&gt;Installing Oracle GoldenGate for Big Data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unpack OGG-BD into target folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /u01/app/ogg-bd
cp ~/Downloads/123110_ggs_Adapters_Linux_x64.zip /u01/app/ogg-bd/
cd /u01/app/ogg-bd/
unzip 123110_ggs_Adapters_Linux_x64.zip
tar -xf ggs_Adapters_Linux_x64.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; environment variable (this needs to be set each time you prior to launching the MGR process through &lt;code&gt;ggsci&lt;/code&gt;, otherwise the replicat will abort with the error &lt;code&gt;OGG-15050  Oracle GoldenGate Delivery, rkconn.prm:  Error loading Java VM runtime library: (2 No such file or directory).&lt;/code&gt;). &lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/introduction1.htm#GADBD113&#34;&gt;Ref&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ogg-bd]$ export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/amd64/server/
[oracle@localhost ~]$ echo $JAVA_HOME
/home/oracle/java/jdk1.8.0_131
[oracle@localhost ~]$ echo $LD_LIBRARY_PATH
/home/oracle/java/jdk1.8.0_131/jre/lib/amd64/server/
[oracle@localhost ~]$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create initial folders and create manager config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oracle@localhost /u/a/ogg-bd&amp;gt; rlwrap ./ggsci
Oracle GoldenGate for Big Data
Version 12.3.1.1.0

Oracle GoldenGate Command Interpreter
Version 12.3.0.1.0 OGGCORE_OGGADP.12.3.0.1.0GA_PLATFORMS_170810.0015
Linux, x64, 64bit (optimized), Generic on Aug 10 2017 01:26:22
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2017, Oracle and/or its affiliates. All rights reserved.



GGSCI (localhost.localdomain) 1&amp;gt; CREATE SUBDIRS

Creating subdirectories under current directory /u01/app/ogg-bd

Parameter file                 /u01/app/ogg-bd/dirprm: created.
Report file                    /u01/app/ogg-bd/dirrpt: created.
Checkpoint file                /u01/app/ogg-bd/dirchk: created.
Process status files           /u01/app/ogg-bd/dirpcs: created.
SQL script files               /u01/app/ogg-bd/dirsql: created.
Database definitions files     /u01/app/ogg-bd/dirdef: created.
Extract data files             /u01/app/ogg-bd/dirdat: created.
Temporary files                /u01/app/ogg-bd/dirtmp: created.
Credential store files         /u01/app/ogg-bd/dircrd: created.
Masterkey wallet files         /u01/app/ogg-bd/dirwlt: created.
Dump files                     /u01/app/ogg-bd/dirdmp: created.


GGSCI (localhost.localdomain) 2&amp;gt; EDIT PARAM MGR
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the config file put:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PORT 7801
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then from the &lt;code&gt;ggsci&lt;/code&gt; prompt start the manager and confirm that it&amp;rsquo;s running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (localhost.localdomain) 3&amp;gt; start mgr
Manager started.


GGSCI (localhost.localdomain) 4&amp;gt; info mgr

Manager is running (IP port localhost.localdomain.7801, Process ID 28707).
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;install-confluent-platform-3-3&#34;&gt;Install Confluent Platform 3.3&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;sudo rpm --import http://packages.confluent.io/rpm/3.3/archive.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add to &lt;code&gt;/etc/yum.repos.d/confluent.repo&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Confluent.dist]
name=Confluent repository (dist)
baseurl=http://packages.confluent.io/rpm/3.3/7
gpgcheck=1
gpgkey=http://packages.confluent.io/rpm/3.3/archive.key
enabled=1

[Confluent]
name=Confluent repository
baseurl=http://packages.confluent.io/rpm/3.3
gpgcheck=1
gpgkey=http://packages.confluent.io/rpm/3.3/archive.key
enabled=1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install Confluent Enterprise&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum clean all
sudo yum install confluent-platform-2.11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify Oracle to shut down HTTP listener on port 8081 since we don&amp;rsquo;t need it and it clashes with Schema Registry. As SYSDBA run on &lt;strong&gt;each&lt;/strong&gt; CDB/PDB run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;exec dbms_xdb.sethttpport(0);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then stop/start the listener.&lt;/p&gt;

&lt;p&gt;To start Confluent Platform run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;confluent start
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configure-smoke-test-ogg-kafka-connect-kafka&#34;&gt;Configure &amp;amp; Smoke Test OGG-Kafka Connect → Kafka&lt;/h1&gt;

&lt;h3 id=&#34;configure-for-ogg-bd-kafka-connect-handler&#34;&gt;Configure for OGG-BD Kafka Connect handler&lt;/h3&gt;

&lt;p&gt;(&lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-81730248-AC12-438E-AF82-48C7002178EC&#34;&gt;Doc&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Put these files in &lt;code&gt;/u01/app/ogg-bd/dirprm&lt;/code&gt;:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/0b658cccc625eed827ade52d7abab048.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Be very careful with the above configuration files for any trailing whitespace&lt;/strong&gt; - it can cause problem, &lt;a href=&#34;https://rmoff.net/2017/09/12/oracle-goldengate-kafka-connect-handler-troubleshooting/&#34;&gt;detailed here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Launch &lt;code&gt;ggsci&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /u01/app/ogg-bd &amp;amp;&amp;amp; rlwrap ./ggsci
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[Re-]Add replicat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;stop rkconn
pause 1
delete replicat rkconn
pause 1
add replicat rkconn, exttrail /u01/app/ogg/dirdat/lt
pause 1
start rkconn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check status&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;info all
info rkconn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expect:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (localhost.localdomain) 37&amp;gt; info all

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING
REPLICAT    RUNNING     RKCONN      00:00:00      00:00:05


GGSCI (localhost.localdomain) 38&amp;gt; info rkconn

REPLICAT   RKCONN    Last Started 2017-09-27 13:50   Status RUNNING
Checkpoint Lag       00:00:00 (updated 00:00:09 ago)
Process ID           15843
Log Read Checkpoint  File /u01/app/ogg/dirdat/lt000000002
2017-09-27 13:43:27.000000  RBA 2393

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check logfile &lt;code&gt;/u01/app/ogg-bd/ggserr.log&lt;/code&gt; and &lt;code&gt;/u01/app/ogg-bd/dirrpt/*&lt;/code&gt; for any errors.&lt;/p&gt;

&lt;h3 id=&#34;smoke-test-ogg-kafka-connect-kafka&#34;&gt;Smoke test OGG &amp;ndash; Kafka Connect &amp;ndash;&amp;gt; Kafka&lt;/h3&gt;

&lt;p&gt;Insert a row in Oracle (as done already above)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Log into SQL*Plus&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rlwrap sql SYS/oracle@orcl as sysdba
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Insert a row and commit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; INSERT INTO HR.REGIONS VALUES (42,&#39;FOO&#39;);

1 row created.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check that the Kafka topic has been created&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-topics --zookeeper localhost:2181 --list
...
ora-ogg-HR-COUNTRIES-avro
ora-ogg-HR-REGIONS-avro
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;View the record&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-avro-console-consumer \
--bootstrap-server localhost:9092 \
--property schema.registry.url=http://localhost:8081 \
--property print.key=true \
--from-beginning \
--topic ora-ogg-HR-COUNTRIES-avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Optionally install &lt;code&gt;jq&lt;/code&gt; (&lt;code&gt;sudo yum install jq&lt;/code&gt;) to pretty-print the JSON displayed (remember the message is still in Avro in Kafka internally though)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kafka-avro-console-consumer \
  --bootstrap-server localhost:9092 \
  --property schema.registry.url=http://localhost:8081 \
  --from-beginning \
  --topic ora-ogg-HR-COUNTRIES-avro|jq &#39;.&#39;
{
  &amp;quot;table&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;ORCL.HR.COUNTRIES&amp;quot;
  },
  &amp;quot;op_type&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;I&amp;quot;
  },
  &amp;quot;op_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-12 22:26:11.000000&amp;quot;
  },
  &amp;quot;current_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-27 13:50:59.279000&amp;quot;
  },
  &amp;quot;pos&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;00000000010000002739&amp;quot;
  },
  &amp;quot;COUNTRY_ID&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;XX&amp;quot;
  },
  &amp;quot;COUNTRY_NAME&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;FOO&amp;quot;
  },
  &amp;quot;REGION_ID&amp;quot;: {
    &amp;quot;double&amp;quot;: 42
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;bonus-install-swingbench-and-build-seed-schema&#34;&gt;Bonus: Install Swingbench and build/seed schema&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://dominicgiles.com/downloads.html&#34;&gt;Download Swingbench 2.6&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip swingbench261046.zip
sudo mv swingbench /opt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the tablespace and user manually so that we can capture everything with GoldenGate (there&amp;rsquo;s probably a better way to do this?)&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/959a7b090c67725abcb009017545406c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Define the Extract properties&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt;&amp;gt; /u01/app/ogg/dirprm/EXT_SOE.prm&amp;lt;&amp;lt;EOF
EXTRACT EXT_SOE
USERID SYSTEM@OGG_ORCL12C, PASSWORD oracle
EXTTRAIL ./dirdat/oe
SOURCECATALOG ORCL
TABLE SOE.*;
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then launch OGG&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /u01/app/ogg/
rlwrap ./ggsci
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and set up capture of the schema&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DBLOGIN USERID SYSTEM@localhost:1521/orcl12c PASSWORD oracle
REGISTER EXTRACT EXT_SOE DATABASE CONTAINER (ORCL)
ADD SCHEMATRANDATA ORCL.SOE ALLCOLS
ADD EXTRACT EXT_SOE, INTEGRATED TRANLOG, BEGIN NOW
ADD EXTTRAIL ./dirdat/oe EXTRACT EXT_SOE
START EXT_SOE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run Swingbench &lt;code&gt;oewizard&lt;/code&gt;, and the creation of the tables and data load will be captured in OGG:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/swingbench/bin/oewizard -cs localhost:1521/orcl -cl -scale 0.1 -dbap oracle -u soe -p soe -v -create
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swingbench output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SwingBench Wizard
Author  :        Dominic Giles
Version :        2.6.0.1046

Running in Lights Out Mode using config file : ../wizardconfigs/oewizard.xml

[...]

============================================
|           Datagenerator Run Stats        |
============================================
Connection Time                        0:00:00.003
Data Generation Time                   0:01:03.240
DDL Creation Time                      0:00:43.730
Total Run Time                         0:01:46.979
Rows Inserted per sec                       19,200
Data Generated (MB) per sec                    1.9
Actual Rows Generated                    2,116,908

Connecting to : jdbc:oracle:thin:@localhost:1521/orcl
Connected

Post Creation Validation Report
===============================
The schema appears to have been created successfully.

Valid Objects
=============
Valid Tables : &#39;ORDERS&#39;,&#39;ORDER_ITEMS&#39;,&#39;CUSTOMERS&#39;,&#39;WAREHOUSES&#39;,&#39;ORDERENTRY_METADATA&#39;,&#39;INVENTORIES&#39;,&#39;PRODUCT_INFORMATION&#39;,&#39;PRODUCT_DESCRIPTIONS&#39;,&#39;ADDRESSES&#39;,&#39;CARD_DETAILS&#39;
Valid Indexes : &#39;PRD_DESC_PK&#39;,&#39;PROD_NAME_IX&#39;,&#39;PRODUCT_INFORMATION_PK&#39;,&#39;PROD_SUPPLIER_IX&#39;,&#39;PROD_CATEGORY_IX&#39;,&#39;INVENTORY_PK&#39;,&#39;INV_PRODUCT_IX&#39;,&#39;INV_WAREHOUSE_IX&#39;,&#39;ORDER_PK&#39;,&#39;ORD_SALES_REP_IX&#39;,&#39;ORD_CUSTOMER_IX&#39;,&#39;ORD_ORDER_DATE_IX&#39;,&#39;ORD_WAREHOUSE_IX&#39;,&#39;ORDER_ITEMS_PK&#39;,&#39;ITEM_ORDER_IX&#39;,&#39;ITEM_PRODUCT_IX&#39;,&#39;WAREHOUSES_PK&#39;,&#39;WHS_LOCATION_IX&#39;,&#39;CUSTOMERS_PK&#39;,&#39;CUST_EMAIL_IX&#39;,&#39;CUST_ACCOUNT_MANAGER_IX&#39;,&#39;CUST_FUNC_LOWER_NAME_IX&#39;,&#39;ADDRESS_PK&#39;,&#39;ADDRESS_CUST_IX&#39;,&#39;CARD_DETAILS_PK&#39;,&#39;CARDDETAILS_CUST_IX&#39;
Valid Views : &#39;PRODUCTS&#39;,&#39;PRODUCT_PRICES&#39;
Valid Sequences : &#39;CUSTOMER_SEQ&#39;,&#39;ORDERS_SEQ&#39;,&#39;ADDRESS_SEQ&#39;,&#39;LOGON_SEQ&#39;,&#39;CARD_DETAILS_SEQ&#39;
Valid Code : &#39;ORDERENTRY&#39;
Schema Created
oracle@localhost /o/swingbench&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more details on how to do cool stuff with Swingbench data, including in KSQL for live joining of events with reference data, keep an eye on the &lt;a href=&#34;https://www.confluent.io/blog/&#34;&gt;Confluent blog&lt;/a&gt;…&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Where will I be at OpenWorld / Oak Table World?</title>
			<link>https://rmoff.github.io/2017/09/29/where-will-i-be-at-openworld-oak-table-world/</link>
			<pubDate>Fri, 29 Sep 2017 19:02:55 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/29/where-will-i-be-at-openworld-oak-table-world/</guid>
			<description>&lt;p&gt;Here&amp;rsquo;s where I&amp;rsquo;ll be!&lt;/p&gt;

&lt;iframe src=&#34;https://calendar.google.com/calendar/embed?title=rmoff%20%40%20OOW17%2FOTW17&amp;amp;showNav=0&amp;amp;showDate=0&amp;amp;showPrint=0&amp;amp;showTabs=0&amp;amp;showCalendars=0&amp;amp;showTz=0&amp;amp;mode=AGENDA&amp;amp;height=600&amp;amp;wkst=1&amp;amp;bgcolor=%23FFFFFF&amp;amp;src=confluent.io_0bq6fa55a27pqun24uec7jm8sk%40group.calendar.google.com&amp;amp;color=%23B1365F&amp;amp;ctz=America%2FLos_Angeles&#34; style=&#34;border-width:0&#34; width=&#34;800&#34; height=&#34;600&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;If you use Google Calendar you can click on individual entries above and select &lt;code&gt;copy to my calendar&lt;/code&gt; - which of course you&amp;rsquo;ll want to do for all the ones I&amp;rsquo;ve marked as &lt;code&gt;[SPEAKING]&lt;/code&gt; :-)&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a list of all the &lt;a href=&#34;https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/&#34;&gt;Apache Kafka talks at OpenWorld and JavaOne&lt;/a&gt;, most of which I&amp;rsquo;ll be trying to get to.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Apache Kafka™ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017</title>
			<link>https://rmoff.github.io/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</link>
			<pubDate>Wed, 20 Sep 2017 15:46:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</guid>
			<description>&lt;p&gt;There&amp;rsquo;s an impressive 19 sessions that cover Apache Kafka™ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogoow17?search=kafka&amp;amp;showEnrolled=false&#34;&gt;OOW&lt;/a&gt;, &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogjavaone17?search=kafka&amp;amp;showEnrolled=false&#34;&gt;JavaOne&lt;/a&gt;, and &lt;a href=&#34;http://www.oaktable.net/blog/oak-table-world-2017-oracle-open-world&#34;&gt;Oak Table World&lt;/a&gt;. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Check out the writeup of my previous visit to OOW including useful tips &lt;a href=&#34;https://www.rittmanmead.com/blog/2014/10/first-timer-tips-for-oracle-open-world/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My talks :-)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kafka&amp;rsquo;s Role in Implementing Oracle&amp;rsquo;s Big Data Reference Architecture&lt;/strong&gt; Sunday, Oct 01, 1:45 p.m. - 2:30 p.m. | Marriott Marquis (Yerba Buena Level) - Salon 12&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What&amp;rsquo;s this stream processing thing anyway?&lt;/strong&gt; &lt;a href=&#34;http://www.oaktable.net/blog/oak-table-world-2017-oracle-open-world&#34;&gt;Oak Table World&lt;/a&gt; Oct 2nd - 14:30  (stick around too for the beer &amp;amp; wine tasting at 16:30!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;An Enterprise Databus: Oracle GoldenGate in the Cloud Working with Kafka and Spark Wednesday&lt;/strong&gt;, Oct 04, 3:30 p.m. - 4:15 p.m. | Moscone West - Room 3003&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the others!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Apache Kafka: Scalable Message Processing and More&lt;/strong&gt; [CON6156] Monday, Oct 02, 4:30 p.m. - 5:15 p.m. | Moscone West - Room 2004&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bring Your Favorite Kafka to Java EE with CDI&lt;/strong&gt; [CON3194] Tuesday, Oct 03, 3:00 p.m. - 3:45 p.m. | Moscone West - Room 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Choreographing Microservices Through Messaging&lt;/strong&gt; [CON1662] Monday, Oct 02, 12:15 p.m. - 1:00 p.m. | Moscone West - Room 2024&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect Java EE to the Cloud with JCA&lt;/strong&gt; [CON3250] Tuesday, Oct 03, 11:00 a.m. - 11:45 a.m. | Moscone West - Room 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Pipeline for a Hyperscale, Real-Time Applications Using Kafka and Cassandra&lt;/strong&gt; [CON7867] Wednesday, Oct 04, 9:30 a.m. - 10:15 a.m. | Moscone West - Room 2002&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Spark Streaming for Real-Time Data Analysis&lt;/strong&gt; [BOF1594] Monday, Oct 02, 6:30 p.m. - 7:15 p.m. | Moscone West - Room 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kafka Streams + TensorFlow + H2O.ai = Highly Scalable Deep Learning&lt;/strong&gt; [CON1191] Wednesday, Oct 04, 8:30 a.m. - 9:15 a.m. | Moscone West - Room 2022&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kafka, Data Streaming, and Analytics Microservices&lt;/strong&gt; Sunday, Oct 01, 1:45 p.m. - 2:30 p.m. | Moscone South - Room 157&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oracle Data Integration Platform: A Cornerstone for Big Data Tuesday&lt;/strong&gt;, Oct 03, 4:45 p.m. - 5:30 p.m. | Moscone West - Room 3024&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oracle Event Hub Cloud Service (Kafka as Managed Service)&lt;/strong&gt; Workshop Monday, Oct 02, 6:00 p.m. - 7:00 p.m. | Hilton San Francisco Union Square (Lobby Level) - Plaza Room B&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Oracle GoldenGate for Big Data&lt;/strong&gt; Wednesday, Oct 04, 4:30 p.m. - 5:15 p.m. | Moscone West - Room 3005&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RDBMS to Kafka: Stories from the Message Bus Stop&lt;/strong&gt; [CON7374] Monday, Oct 02, 4:30 p.m. - 5:15 p.m. | Moscone West - Room 2002&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reactive Stream Processing with WildFly Swarm and Apache Kafka&lt;/strong&gt; [CON2526] Monday, Oct 02, 4:30 p.m. - 5:15 p.m. | Moscone West - Room 2024&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-Time UI with Apache Kafka Streaming Analytics of Fast Data and Server Push&lt;/strong&gt; [CON2854] Tuesday, Oct 03, 8:30 a.m. - 9:15 a.m. | Moscone West - Room 2020&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Solutions for Real-Time Problems&lt;/strong&gt; [CON6059] Monday, Oct 02, 12:15 p.m. - 1:00 p.m. | Moscone West - Room 2003&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Open Source and Cloud Part of Oracle Big Data Cloud Service for Beginners&lt;/strong&gt; [CON7368] Wednesday, Oct 04, 12:45 p.m. - 1:30 p.m. | Moscone West - Room 2002&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Oracle GoldenGate / Kafka Connect Handler troubleshooting</title>
			<link>https://rmoff.github.io/2017/09/12/oracle-goldengate-kafka-connect-handler-troubleshooting/</link>
			<pubDate>Tue, 12 Sep 2017 21:55:16 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/12/oracle-goldengate-kafka-connect-handler-troubleshooting/</guid>
			<description>&lt;p&gt;The Replicat was kapput:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (localhost.localdomain) 3&amp;gt; info rkconnoe

REPLICAT   RKCONNOE  Last Started 2017-09-12 17:06   Status ABENDED
Checkpoint Lag       00:00:00 (updated 00:46:34 ago)
Log Read Checkpoint  File /u01/app/ogg/dirdat/oe000000
                     First Record  RBA 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So checking the OGG error log &lt;code&gt;ggserr.log&lt;/code&gt; showed&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-09-12T17:06:17.572-0400  ERROR   OGG-15051  Oracle GoldenGate Delivery, rkconnoe.prm:  Java or JNI exception:
                              oracle.goldengate.util.GGException: Error detected handling operation added event.
2017-09-12T17:06:17.572-0400  ERROR   OGG-01668  Oracle GoldenGate Delivery, rkconnoe.prm:  PROCESS ABENDING.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So checking the replicat log &lt;code&gt;dirrpt/RKCONNOE_info_log4j.log&lt;/code&gt; showed:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/3e1fe8153d3a72068af1fb612fe4b839.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Something odd going on here. From the above stack trace I focussed on&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java.lang.NullPointerException at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So something to do with the &lt;code&gt;sendHttpRequest&lt;/code&gt; to the Schema Registry. Checking the Schema Registry log showed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2017-09-12 16:50:59,372] WARN badMessage: 400 Unknown Version for HttpChannelOverHttp@27e54a61{r=0,c=false,a=IDLE,uri=} (org.eclipse.jetty.http.HttpParser:1317)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Most bizarre. A normal OGG Kafka Connect handler interaction with Schema Registry looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2017-09-12 18:26:14,058] INFO 127.0.0.1 - - [12/Sep/2017:18:26:13 -0400] &amp;quot;POST /subjects/ora-ogg-COUNTRIES-key/versions HTTP/1.1&amp;quot; 200 8  468 (io.confluent.rest-utils.requests:77)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s dig a bit deeper. Since the debug log of the OGG Kafka Connect handler doesn&amp;rsquo;t tell us anything more, let&amp;rsquo;s see if we can spot anything in what is being sent to the Schema Registry in that HTTP call. Enter &lt;code&gt;tcpdump&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@localhost ~]$ sudo tcpdump -i venet0 -i lo -nnA &#39;port 8081&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Firstly, here&amp;rsquo;s what we get when a successful message is processed by the OGG Kafka Connect handler (matching the above &lt;code&gt;POST&lt;/code&gt; seen in the schema registry log):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;18:26:13.496951 IP 127.0.0.1.10420 &amp;gt; 127.0.0.1.8081: Flags [P.], seq 1:312, ack 1, win 342, options [nop,nop,TS val 1779726 ecr 1779721], length 311
E..k6m@.@...........(....&amp;amp;.c.ZR....V._.....
..(...( POST /subjects/ora-ogg-COUNTRIES-key/versions HTTP/1.1
Content-Type: application/vnd.schemaregistry.v1+json
Cache-Control: no-cache
Pragma: no-cache
User-Agent: Java/1.8.0_131
Host: localhost:8081
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive
Content-Length: 23
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and now here&amp;rsquo;s what we see when the dodgy replicat abends and the schema registry logs &lt;code&gt;WARN badMessage: 400 Unknown Version&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;17:06:17.533978 IP 127.0.0.1.10112 &amp;gt; 127.0.0.1.8081: Flags [P.], seq 1:314, ack 1, win 342, options [nop,nop,TS val 1147447 ecr 1147444], length 313
E..my0@.@..X........&#39;...7W...#.^...V.a.....
...7...4POST /subjects/ora-ogg-SOE-LOGON  -key/versions HTTP/1.1
Content-Type: application/vnd.schemaregistry.v1+json
Cache-Control: no-cache
Pragma: no-cache
User-Agent: Java/1.8.0_131
Host: localhost:8081
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive
Content-Length: 23
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check out the space between &lt;code&gt;LOGON&lt;/code&gt; and &lt;code&gt;-key/versions&lt;/code&gt; in the &lt;code&gt;POST&lt;/code&gt; shown in the &lt;code&gt;tcpdump&lt;/code&gt; output above – I&amp;rsquo;m pretty sure that shouldn&amp;rsquo;t be there. Where did it come from, and importantly, how do we get rid of it?&lt;/p&gt;

&lt;p&gt;The OGG Kafka Connect handler takes its configuration from the properties file that you define (&lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-23F5CCE3-845C-43F0-A08E-42C2BD1824FB&#34;&gt;syntax here&lt;/a&gt;). New in 12.3.1 is the ability to define key and topic &lt;a href=&#34;http://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-A87CAFFA-DACF-43A0-8C6C-5C64B578D606&#34;&gt;_templates&lt;/a&gt;_. Here&amp;rsquo;s the snippet from my properties file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.kafkaconnect.topicMappingTemplate=ora-ogg-${schemaName}-${tableName}
gg.handler.kafkaconnect.keyMappingTemplate=${primaryKeys}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks pretty innocuous right? It&amp;rsquo;s the same that I used for the replicat that &lt;em&gt;was&lt;/em&gt; working. Or at least, I thought it was. Check out what it looks like in &lt;code&gt;vi&lt;/code&gt; if I issue a &lt;code&gt;:set list&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.kafkaconnect.topicMappingTemplate=ora-ogg-${schemaName}-${tableName}  $
gg.handler.kafkaconnect.keyMappingTemplate=${primaryKeys}$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each of those dollar &lt;code&gt;$&lt;/code&gt; are end of line characters; and you&amp;rsquo;ll notice that on the first line there are spaces after the configuration value and before the end of line! Working backwards from this we can actually spot in the &lt;code&gt;dirrpt/RKCONNOE.rpt&lt;/code&gt; this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DEBUG [main] (PropertyWrapper.java:375) - Setting property on &#39;class oracle.goldengate.handler.kafkaconnect.KafkaConnectHandler&#39;: &#39;topicMappingTemplate&#39;=&#39;ora-ogg-${schemaName}-${tableName}  &#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the trailing spaces! (to get DEBUG in the replicat log/report file, set &lt;code&gt;gg.log.level=DEBUG&lt;/code&gt; in the handler properties configuration).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;To fix the problem, I just used some regex in vi:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:%s/ .$//g
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(throughout the document, replace one or more spaces immediately before the end of line, with nothing - i.e. remove them.)&lt;/p&gt;

&lt;p&gt;Having updated the handler properties, I then restarted the replicat&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (localhost.localdomain) 2&amp;gt; start rkconnoe

Sending START request to MANAGER ...
REPLICAT RKCONNOE starting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The replicat stayed running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (localhost.localdomain) 4&amp;gt; info rkconnoe

REPLICAT   RKCONNOE  Last Started 2017-09-12 18:45   Status RUNNING
Checkpoint Lag       00:00:00 (updated 00:00:03 ago)
Process ID           19129
Log Read Checkpoint  File /u01/app/ogg/dirdat/oe000000001
                     2017-09-12 16:48:33.849941  RBA 1482
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And I got my data in Kafka Connect, streaming through from Oracle GoldenGate!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-avro-console-consumer \
--bootstrap-server localhost:9092 \
--property schema.registry.url=http://localhost:8081 \
--property print.key=true \
--from-beginning \
--topic ora-ogg-SOE-LOGON | jq &#39;.&#39;

&amp;quot;42_42_2017-09-12 16:21:48&amp;quot;
{
  &amp;quot;table&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;ORCL.SOE.LOGON&amp;quot;
  },
  &amp;quot;op_type&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;I&amp;quot;
  },
  &amp;quot;op_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-12 20:21:59.000000&amp;quot;
  },
  &amp;quot;current_ts&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-12 18:45:09.244000&amp;quot;
  },
  &amp;quot;pos&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;00000000000000001940&amp;quot;
  },
  &amp;quot;LOGON_ID&amp;quot;: {
    &amp;quot;double&amp;quot;: 42
  },
  &amp;quot;CUSTOMER_ID&amp;quot;: {
    &amp;quot;double&amp;quot;: 42
  },
  &amp;quot;LOGON_DATE&amp;quot;: {
    &amp;quot;string&amp;quot;: &amp;quot;2017-09-12 16:21:48&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>What is Markdown, and Why is it Awesome?</title>
			<link>https://rmoff.github.io/2017/09/12/what-is-markdown-and-why-is-it-awesome/</link>
			<pubDate>Tue, 12 Sep 2017 19:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/12/what-is-markdown-and-why-is-it-awesome/</guid>
			<description>

&lt;p&gt;Markdown is a plain-text formatting syntax. It enables you write documents in plain text, readable by others in plain text, and optionally rendered into nicely formatted PDF, HTML, DOCX etc.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s used widely in software documentation, particularly open-source, because it enables richer formatting than plain-text alone, but without constraining authors or readers to a given software platform.&lt;/p&gt;

&lt;p&gt;Platforms such as github natively support Markdown rendering - so you write your &lt;code&gt;README&lt;/code&gt; etc in markdown, and when viewed on github it is automagically rendered - without you needing to actually do anything.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve used Markdown for years now, after being introduced to it at my previous company. When I worked as a consultant I would be delivering client reports, and I wrote all of them in Markdown. The final delivered copy was in DOCX or maybe HTML - but the master copy remained as Markdown.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-write-markdown&#34;&gt;How Do I Write Markdown?&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;comprehensive reference guide here&lt;/a&gt;.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/8f31d89ad60297b3c63301613a204b85.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;One of the cool things about Markdown is that it is widely supported. For example, if you&amp;rsquo;re using github (or gist), any file with a &lt;code&gt;.md&lt;/code&gt; extension automagically gets rendered as Markdown, thus:&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/rmoff/fd71d2f97f2ff4eec41dde180cd03e73.js&#34;&gt;&lt;/script&gt;

&lt;h1 id=&#34;how-do-i-write-markdown-what-tools-do-i-need&#34;&gt;How Do I Write Markdown? What Tools Do I Need?&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;vi&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Seriously. Well, &lt;strong&gt;emacs&lt;/strong&gt; works great too.&lt;/p&gt;

&lt;p&gt;Markdown is plain text. &lt;strong&gt;ANY&lt;/strong&gt; plain text editor can be used to write Markdown. That&amp;rsquo;s why it&amp;rsquo;s so awesome - it is completely portable. TextWrangler, FoldingText, TextEdit, Notepad, Notepad++, Ultraedit&amp;hellip; take your pick&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a &lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&#34;&gt;comprehensive reference guide here&lt;/a&gt; that guides you through the syntax.&lt;/p&gt;

&lt;p&gt;But, there are other tools that can make your Markdown life even better, such as previewers.&lt;/p&gt;

&lt;h3 id=&#34;markdown-previewers&#34;&gt;Markdown Previewers&lt;/h3&gt;

&lt;p&gt;The best of these by a mile on the Mac is &lt;a href=&#34;http://marked2app.com/&#34;&gt;Marked2&lt;/a&gt;. It gives you a nice rendering of any markdown document that you&amp;rsquo;re working on. Used with the El Capitan split-screen it&amp;rsquo;s pretty-near perfect way to write Markdown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/09/screenshot.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Marked2 can also export to DOCX, PDF, HTML, etc.&lt;/p&gt;

&lt;h3 id=&#34;other-useful-tools&#34;&gt;Other Useful Tools&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://omz-software.com/editorial/&#34;&gt;Editorial&lt;/a&gt; is a good iOS Markdown editor &amp;amp; previewer&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://25.io/mou/&#34;&gt;Mou&lt;/a&gt; does quite a good job of an all-in-one Markdown editor &amp;amp; previewer for the Mac. Personally, I prefer running two separate tools that excel at editing and previewing respectively, instead of one single one trying to do both. YMMV.&lt;/li&gt;
&lt;li&gt;Emacs has a good &lt;a href=&#34;http://jblevins.org/projects/markdown-mode/&#34;&gt;markdown-mode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;ways-to-generate-output-from-markdown&#34;&gt;Ways to Generate Output from Markdown&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://marked2app.com/&#34;&gt;Marked2&lt;/a&gt; can also export to DOCX, PDF, HTML, etc.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pandoc.org/&#34;&gt;pandoc&lt;/a&gt; can generate a huge range of output formats from markdown input.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;markdown-geek-out&#34;&gt;Markdown geek-out&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;Rmd&lt;/a&gt; is a combination of R and Markdown. Pretty cool for writing reports with embedded R output.&lt;/li&gt;
&lt;li&gt;When you get into the outer-reaches of Markdown, you&amp;rsquo;ll find that there are different &amp;ldquo;flavours&amp;rdquo;. There&amp;rsquo;s the &lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;original markdown&lt;/a&gt;, there&amp;rsquo;s github markdown, there&amp;rsquo;s &lt;a href=&#34;http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/&#34;&gt;Common Markdown&lt;/a&gt;. This only really matters when it comes to trying to do some funky formatting, and if the previewer/renderer that you&amp;rsquo;re using supports a different flavour. Some, like Marked2, will let you choose which to use.&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Conferences &amp; Meetups at which I&#39;ll be speaking - 2017</title>
			<link>https://rmoff.github.io/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/</link>
			<pubDate>Mon, 11 Sep 2017 06:45:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/</guid>
			<description>

&lt;p&gt;I&amp;rsquo;m excited to be speaking at several conferences and meetups over the next few months. Unsurprisingly, the topic will be Apache Kafka!&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re at any of these, please do come and say hi :)&lt;/p&gt;

&lt;h3 id=&#34;apache-kafka-meetup-london&#34;&gt;Apache Kafka Meetup - London&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;My first time talking at the London Apache Kafka Meetup - always a sold-out crowd, this will be fun!&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;September 20th, 19:00 : &lt;strong&gt;&lt;a href=&#34;https://www.meetup.com/Apache-Kafka-London/events/242981989/&#34;&gt;Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka&lt;/a&gt;&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Slides are &lt;a href=&#34;https://speakerdeck.com/rmoff/look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka&#34;&gt;available here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;oracle-openworld-san-francisco&#34;&gt;Oracle OpenWorld - San Francisco&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;This will be my second time at OOW - I &lt;a href=&#34;https://www.rittmanmead.com/blog/2014/10/first-timer-tips-for-oracle-open-world/&#34;&gt;wrote up my previous trip here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Oct 1st, 10:45 &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogoow17?search=SUN2413&amp;amp;showEnrolled=false&#34;&gt;&lt;strong&gt;EOUC Database ACES Share Their Favorite Database Things&lt;/strong&gt; (SUN2413)&lt;/a&gt; (Marriott Marquis (Golden Gate Level) - Golden Gate C1/C2)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Oct 1st, 13:45 : &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogoow17?search=SUN6259&amp;amp;showEnrolled=false&#34;&gt;&lt;strong&gt;Kafka&amp;rsquo;s Role in Implementing Oracle&amp;rsquo;s Big Data Reference Architecture&lt;/strong&gt; (SUN6259)&lt;/a&gt; (Marriott Marquis (Yerba Buena Level) - Salon 12)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Oct 4th, 15:30 &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogoow17?search=CON6895&amp;amp;showEnrolled=false&#34;&gt;&lt;strong&gt;An Enterprise Databus: Oracle GoldenGate in the Cloud Working with Kafka and Spark&lt;/strong&gt; (CON6895)&lt;/a&gt; (Moscone West - Room 3003)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;oak-table-world-san-francisco&#34;&gt;Oak Table World - San Francisco&lt;/h3&gt;

&lt;p&gt;*Oak Table World is &lt;strong&gt;awesome&lt;/strong&gt;–go along if you can make it, as it is literally just next door to OOW itself. I&amp;rsquo;m really honoured to speaking here with my colleague Gwen Shapira.*&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Oct 2nd - 14:30 &lt;a href=&#34;http://www.oaktable.net/blog/oak-table-world-2017-oracle-open-world&#34;&gt;&lt;strong&gt;What&amp;rsquo;s this stream processing thing anyway?&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;hadoop-user-group-dublin&#34;&gt;Hadoop User Group - Dublin&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;October 19th, 18:00: &lt;strong&gt;&lt;a href=&#34;https://www.meetup.com/hadoop-user-group-ireland/events/243387159/&#34;&gt;Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;doag-nuremberg&#34;&gt;DOAG - Nuremberg&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;My first time presenting in Germany - looking forward to this!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://2017.doag.org/fileadmin/2017-K-A-Banner-180x180_Speaker-ENG.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;November 23rd, 16:00 &lt;a href=&#34;https://www.doag.org/konferenz/konferenzplaner/konferenzplaner_details.php?locS=1&amp;amp;id=535509&amp;amp;vid=545631&#34;&gt;&lt;strong&gt;Building a Real-Time Streaming Platform with Oracle, Apache Kafka, and KSQL&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ukoug-birmingham&#34;&gt;UKOUG - Birmingham&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;There was lots of interest in Kafka last year - I&amp;rsquo;ve got my fingers crossed for a room with &lt;a href=&#34;https://twitter.com/lasjen/status/805768105578401792&#34;&gt;bigger capacity&lt;/a&gt; ;)&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;December 4th, 17:55 &lt;a href=&#34;http://tech17.ukoug.org/default.asp?p=16630&amp;amp;dlgact=shwprs&amp;amp;prs_prsid=13504&amp;amp;day_dayid=118&#34;&gt;&lt;strong&gt;ETL in the Streaming Age : Building Kafka Pipelines from Oracle&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;December 5th, 09:00 - &lt;a href=&#34;http://tech17.ukoug.org/default.asp?p=16630&amp;amp;dlgact=shwprs&amp;amp;prs_prsid=13505&amp;amp;day_dayid=118&#34;&gt;&lt;strong&gt;Building a Real-Time Streaming Platform with Oracle, Apache Kafka, and KSQL&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect - JsonDeserializer with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields</title>
			<link>https://rmoff.github.io/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</link>
			<pubDate>Wed, 06 Sep 2017 12:00:25 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</guid>
			<description>&lt;p&gt;An error that I see coming up frequently in the Kafka Connect community (e.g. &lt;a href=&#34;https://groups.google.com/forum/#!forum/confluent-platform&#34;&gt;mailing list&lt;/a&gt;, &lt;a href=&#34;https://slackpass.io/confluentcommunity&#34;&gt;Slack group&lt;/a&gt;, &lt;a href=&#34;https://stackoverflow.com/questions/tagged/apache-kafka-connect&#34;&gt;StackOverflow&lt;/a&gt;) is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JsonDeserializer with schemas.enable requires &amp;quot;schema&amp;quot; and &amp;quot;payload&amp;quot; fields and may not contain additional fields
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;No fields found using key and value schemas for table: foo-bar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see an explanation, and solution, for the issue in my StackOverflow answer here: &lt;a href=&#34;https://stackoverflow.com/a/45940013/350613&#34;&gt;https://stackoverflow.com/a/45940013/350613&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re using &lt;code&gt;schemas.enable&lt;/code&gt; in the Connector configuration, you must have &lt;code&gt;schema&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt; as the root-level elements of your JSON message (
Which is pretty much verbatim what the error says 😁), like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{
    &amp;quot;schema&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;struct&amp;quot;,
        &amp;quot;fields&amp;quot;: [{
            &amp;quot;type&amp;quot;: &amp;quot;int32&amp;quot;,
            &amp;quot;optional&amp;quot;: true,
            &amp;quot;field&amp;quot;: &amp;quot;c1&amp;quot;
        }, {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;optional&amp;quot;: true,
            &amp;quot;field&amp;quot;: &amp;quot;c2&amp;quot;
        }, {
            &amp;quot;type&amp;quot;: &amp;quot;int64&amp;quot;,
            &amp;quot;optional&amp;quot;: false,
            &amp;quot;name&amp;quot;: &amp;quot;org.apache.kafka.connect.data.Timestamp&amp;quot;,
            &amp;quot;version&amp;quot;: 1,
            &amp;quot;field&amp;quot;: &amp;quot;create_ts&amp;quot;
        }, {
            &amp;quot;type&amp;quot;: &amp;quot;int64&amp;quot;,
            &amp;quot;optional&amp;quot;: false,
            &amp;quot;name&amp;quot;: &amp;quot;org.apache.kafka.connect.data.Timestamp&amp;quot;,
            &amp;quot;version&amp;quot;: 1,
            &amp;quot;field&amp;quot;: &amp;quot;update_ts&amp;quot;
        }],
        &amp;quot;optional&amp;quot;: false,
        &amp;quot;name&amp;quot;: &amp;quot;foobar&amp;quot;
    },
    &amp;quot;payload&amp;quot;: {
        &amp;quot;c1&amp;quot;: 10000,
        &amp;quot;c2&amp;quot;: &amp;quot;bar&amp;quot;,
        &amp;quot;create_ts&amp;quot;: 1501834166000,
        &amp;quot;update_ts&amp;quot;: 1501834166000
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So either make sure your JSON message adheres to this format, or tell the JSON Converter not to try and fetch a schema, by setting the following in the Connector config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;value.converter.schemas.enable&amp;quot;: &amp;quot;false&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Simple export/import of Data Sources in Grafana</title>
			<link>https://rmoff.github.io/2017/08/08/simple-export-import-of-data-sources-in-grafana/</link>
			<pubDate>Tue, 08 Aug 2017 19:32:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/08/08/simple-export-import-of-data-sources-in-grafana/</guid>
			<description>

&lt;p&gt;&lt;a href=&#34;http://docs.grafana.org/http_api/data_source/&#34;&gt;Grafana API Reference&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;export-all-grafana-data-sources-to-data-sources-folder&#34;&gt;Export all Grafana data sources to data_sources folder&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p data_sources &amp;amp;&amp;amp; curl -s &amp;quot;http://localhost:3000/api/datasources&amp;quot;  -u admin:admin|jq -c -M &#39;.[]&#39;|split -l 1 - data_sources/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This exports each data source to a separate JSON file in the &lt;code&gt;data_sources&lt;/code&gt; folder.&lt;/p&gt;

&lt;h3 id=&#34;load-data-sources-back-in-from-folder&#34;&gt;Load data sources back in from folder&lt;/h3&gt;

&lt;p&gt;This submits every file that exists in the &lt;code&gt;data_sources&lt;/code&gt; folder to Grafana as a new data source definition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in data_sources/*; do \
    curl -X &amp;quot;POST&amp;quot; &amp;quot;http://localhost:3000/api/datasources&amp;quot; \
    -H &amp;quot;Content-Type: application/json&amp;quot; \
     --user admin:admin \
     --data-binary @$i
done
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Linux - USB disk connection problems - uas: probe failed with error -12</title>
			<link>https://rmoff.github.io/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/</link>
			<pubDate>Wed, 21 Jun 2017 06:14:45 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/</guid>
			<description>&lt;p&gt;Usually connecting external disks in Linux is easy. Plug it in, run &lt;code&gt;fdisk -l&lt;/code&gt; or &lt;code&gt;lsblk | grep disk&lt;/code&gt; to identify the device ID, and then &lt;code&gt;mount&lt;/code&gt; it.&lt;/p&gt;

&lt;p&gt;Unfortunately in this instance, plugging in my Seagate 2TB wasn&amp;rsquo;t so simple. The server is running Proxmox:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# uname -a
Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No device showed up on &lt;code&gt;lsblk&lt;/code&gt; or &lt;code&gt;fdisk -l&lt;/code&gt;. In &lt;code&gt;dmesg&lt;/code&gt; I saw:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uas: probe of 4-2:1.0 failed with error -12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and &lt;code&gt;/var/log/syslog&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jun 20 10:28:23 proxmox01 kernel: [8025519.475079] usb 3-1: new high-speed USB device number 10 using xhci_hcd
Jun 20 10:28:23 proxmox01 kernel: [8025519.604683] usb 3-1: Manufacturer: Seagate
Jun 20 10:28:23 proxmox01 kernel: [8025519.605666] CPU: 0 PID: 31152 Comm: kworker/0:0 Tainted: P           O    4.4.6-1-pve #1
Jun 20 10:28:23 proxmox01 kernel: [8025519.605681]  000000000208c020 0000000000000000 ffff880cacdd3560 ffffffff81192b5c
Jun 20 10:28:23 proxmox01 kernel: [8025519.605705]  [&amp;lt;ffffffff810c3914&amp;gt;] ? __wake_up+0x44/0x50
Jun 20 10:28:23 proxmox01 kernel: [8025519.605723]  [&amp;lt;ffffffff811b2264&amp;gt;] kmalloc_order_trace+0x24/0xb0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605742]  [&amp;lt;ffffffff813bdfc4&amp;gt;] blk_init_tags+0x14/0x20
Jun 20 10:28:23 proxmox01 kernel: [8025519.605763]  [&amp;lt;ffffffff8154e522&amp;gt;] __device_attach_driver+0x72/0x80
Jun 20 10:28:23 proxmox01 kernel: [8025519.605779]  [&amp;lt;ffffffff8154d282&amp;gt;] bus_probe_device+0x92/0xa0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605795]  [&amp;lt;ffffffff8154e164&amp;gt;] driver_probe_device+0x224/0x4b0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605811]  [&amp;lt;ffffffff8154e6a3&amp;gt;] device_initial_probe+0x13/0x20
Jun 20 10:28:23 proxmox01 kernel: [8025519.605827]  [&amp;lt;ffffffff81617130&amp;gt;] hub_event+0x1020/0x1580
Jun 20 10:28:23 proxmox01 kernel: [8025519.605843]  [&amp;lt;ffffffff810a0baa&amp;gt;] kthread+0xea/0x100
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860] active_anon:6721278 inactive_anon:2537813 isolated_anon:0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860]  active_file:1818599 inactive_file:16022602 isolated_file:0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860]  unevictable:9057 dirty:343 writeback:0 unstable:0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860]  slab_reclaimable:808602 slab_unreclaimable:40370
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860]  mapped:105858 shmem:280137 pagetables:44195 bounce:0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605860]  free:4793036 free_pcp:3685 free_cma:0
Jun 20 10:28:23 proxmox01 kernel: [8025519.605884] Node 0 Normal free:9716940kB min:22276kB low:27844kB high:33412kB active_anon:15658136kB inactive_anon:5117636kB active_file:2095992kB inactive_file:29493156kB unevictable:32376kB isolated(anon):0kB isolated(file):0kB present:65011712kB managed:63964020kB mlocked:32376kB dirty:728kB writeback:0kB mapped:221248kB shmem:485280kB slab_reclaimable:1345308kB slab_unreclaimable:74956kB kernel_stack:29856kB pagetables:72468kB unstable:0kB bounce:0kB free_pcp:5720kB local_pcp:200kB free_cma:0kB writeback_tmp:0kB pages_scanned:1100 all_unreclaimable? no
Jun 20 10:28:23 proxmox01 kernel: [8025519.605912] Node 0 DMA32: 2931*4kB (UME) 12003*8kB (UME) 6685*16kB (UME) 1735*32kB (UME) 338*64kB (UME) 47*128kB (UME) 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 297876kB
Jun 20 10:28:23 proxmox01 kernel: [8025519.605945] Node 1 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB
Jun 20 10:28:23 proxmox01 kernel: [8025519.605953] Free swap  = 928kB
Jun 20 10:28:23 proxmox01 kernel: [8025519.605958] 0 pages cma reserved
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A bit of Googling led me to &lt;a href=&#34;https://unix.stackexchange.com/questions/270945/why-cant-i-mount-the-disk-now-with-limited-memory&#34;&gt;here&lt;/a&gt;, which suggested memory was causing a problem with &lt;code&gt;uas&lt;/code&gt; that is responsible for making the drive device available. The existing memory usage looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# free -m
             total       used       free     shared    buffers     cached
Mem:        128811     110118      18692       1094       3198      67616
-/+ buffers/cache:      39303      89508
Swap:         8191       8191          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Taking a bit of a gamble on running random commands from the internet I ran :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# echo 3 | sudo tee /proc/sys/vm/drop_caches
3

root@proxmox01:~# free -m
             total       used       free     shared    buffers     cached
Mem:        128811      38275      90536       1094         23       1435
-/+ buffers/cache:      36815      91996
Swap:         8191       8191          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After which I reconnected the drive and &amp;hellip; it was picked up and registered just fine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usb 4-2: USB disconnect, device number 3
usb 4-5: new SuperSpeed USB device number 4 using xhci_hcd
usb 4-5: New USB device found, idVendor=0bc2, idProduct=3321
usb 4-5: New USB device strings: Mfr=2, Product=3, SerialNumber=1
usb 4-5: Product: Expansion Desk
usb 4-5: Manufacturer: Seagate
usb 4-5: SerialNumber: NA4N4ZC9
scsi host16: uas
scsi 16:0:0:0: Direct-Access     Seagate  Expansion Desk   0604 PQ: 0 ANSI: 6
sd 16:0:0:0: Attached scsi generic sg10 type 0
sd 16:0:0:0: [sdj] Spinning up disk...
..........ready
sd 16:0:0:0: [sdj] 488378645 4096-byte logical blocks: (2.00 TB/1.82 TiB)
sd 16:0:0:0: [sdj] 16384-byte physical blocks
sd 16:0:0:0: [sdj] Write Protect is off
sd 16:0:0:0: [sdj] Mode Sense: 4f 00 00 00
sd 16:0:0:0: [sdj] Write cache: enabled, read cache: enabled, doesn&#39;t support DPO or FUA
sd 16:0:0:0: [sdj] 488378645 4096-byte logical blocks: (2.00 TB/1.82 TiB)
 sdj: sdj1
sd 16:0:0:0: [sdj] 488378645 4096-byte logical blocks: (2.00 TB/1.82 TiB)
sd 16:0:0:0: [sdj] Attached SCSI disk
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Configuring Kafka Connect to log REST HTTP messages to a separate file</title>
			<link>https://rmoff.github.io/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</link>
			<pubDate>Mon, 12 Jun 2017 15:28:15 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</guid>
			<description>&lt;p&gt;Kafka&amp;rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from &lt;a href=&#34;https://www.confluent.io/product/connectors/&#34;&gt;the many available&lt;/a&gt;, it&amp;rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the &lt;a href=&#34;https://www.confluent.io/product/control-center/&#34;&gt;Confluent Control Center&lt;/a&gt;, for both management and monitoring:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/05/Control_Center.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;BUT &amp;hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured. What then? Well of course, it&amp;rsquo;s diagnostics time! And for diagnostics, you need logs. When you launch Kafka Connect it logs everything to &lt;code&gt;stdout&lt;/code&gt;, and this output includes content from the Kafka Connect &lt;a href=&#34;http://docs.confluent.io/current/connect/restapi.html&#34;&gt;REST interface&lt;/a&gt;. This REST interface is for configuration and control of the connectors (status/pause/resume) - and whilst Control Center is being used on the Connect configuration screens, you&amp;rsquo;ll notice that the REST interface gets polled frequently - every couple of seconds, with a greater number of requests the more connectors you have. All of this goes into the log:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/05/1__screen-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This logging is great, but it does make it tricky to spot errors in the Connect log that you might get as a result of, say, misconfiguring a connector.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/05/1__screen_and_3__oracle_vbgeneric____ssh_-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To filter out REST logs from Connect&amp;rsquo;s &lt;code&gt;stdout&lt;/code&gt; into their own file, we&amp;rsquo;ll add some manual overrides to the configuration for the log4j logging system. You can validate which log4j configuration is in use by examining &lt;code&gt;ps -ef&lt;/code&gt; and looking at the value of &lt;code&gt;-Dlog4j.configuration&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/05/1__screen-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Add the following to &lt;code&gt;etc/kafka/connect-log4j.properties&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;
log4j.appender.kafkaConnectRestAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.kafkaConnectRestAppender.DatePattern=&#39;.&#39;yyyy-MM-dd-HH
log4j.appender.kafkaConnectRestAppender.File=${kafka.logs.dir}/connect-rest.log
log4j.appender.kafkaConnectRestAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.kafkaConnectRestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.logger.org.apache.kafka.connect.runtime.rest=INFO, kafkaConnectRestAppender
log4j.additivity.org.apache.kafka.connect.runtime.rest=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a result of this, you get a cleaner Connect &lt;code&gt;stdout&lt;/code&gt;, and a new file in the Kafka logs folder with all of the REST logs on their own. Now you have no excuses for not examining the logs when troubleshooting Connect!&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>kafka.common.KafkaException: No key found on line 1</title>
			<link>https://rmoff.github.io/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</link>
			<pubDate>Fri, 12 May 2017 00:52:41 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</guid>
			<description>&lt;p&gt;A very silly &lt;a href=&#34;https://en.wiktionary.org/wiki/PEBCAK&#34;&gt;PEBCAK&lt;/a&gt; problem this one, but Google hits weren&amp;rsquo;t so helpful so here goes.&lt;/p&gt;

&lt;p&gt;Running a console producer, specifying keys:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-console-producer \
--broker-list localhost:9092 \
--topic test_topic \
--property parse.key=true \
--property key.seperator=,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Failed when I entered a key/value:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1,foo
kafka.common.KafkaException: No key found on line 1: 1,foo
        at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314)
        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55)
        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;kafka.common.KafkaException: No key found on line&lt;/strong&gt; &amp;hellip; but I specified the key, didn&amp;rsquo;t I?&lt;/p&gt;

&lt;p&gt;It would help if I could spell &amp;hellip;  &lt;code&gt;key.sep&lt;/code&gt;&lt;strong&gt;e&lt;/strong&gt;&lt;code&gt;rator&lt;/code&gt; isn&amp;rsquo;t a valid property to configure. &lt;code&gt;sep&lt;/code&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;code&gt;rator&lt;/code&gt; on the other hand, is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-console-producer \
--broker-list localhost:9092 \
--topic test_topic \
--property parse.key=true \
--property key.separator=,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Keeping Up with the Deluge</title>
			<link>https://rmoff.github.io/2017/03/11/keeping-up-with-the-deluge/</link>
			<pubDate>Sat, 11 Mar 2017 15:30:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/03/11/keeping-up-with-the-deluge/</guid>
			<description>

&lt;p&gt;&lt;em&gt;How do you try and stay current on technical affairs, given only 24 hours in a day and a job to do as well? Here&amp;rsquo;s my take on it&amp;hellip;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;One of the many things that has changed perceptibly since the beginning of this century when I started working in IT is the amount of information freely available, and being created all the time. Back then, printed books and manuals were still the primary source of definitive information about a piece of software. Remember these? I bet you still have a few of them keeping your monitor at the right height still&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51PDHEZZ65L.jpg&#34; alt=&#34;SAMS Active Serve Pages 2.0 Unleashed&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The internet existed, sure, but believe it or not finding information was hard.
Back in the day, before Google became the ubiquitous entry point to everything, you&amp;rsquo;d find information through methods including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Browser bookmarks of pages already visited&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Recommended links&amp;rdquo; pages from one site to others&lt;/li&gt;
&lt;li&gt;Blogrolls&lt;/li&gt;
&lt;li&gt;Directories, such as DMOZ&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Webring&#34;&gt;Web Rings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sure, there were search engines, but they weren&amp;rsquo;t comprehensive, and they were dumb. You had to hit on the exact search terms. Nowadays Google&amp;rsquo;s auto complete is taken for granted, but if you stop and think about what it does in terms of information discovery and direction of search it&amp;rsquo;s damn impressive.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Fast-forward 17 years, and look how the world&amp;rsquo;s changed. Everyone and her dog is creating content, continually. Whether it&amp;rsquo;s pictures of fried breakfasts, humblebrag photos of holiday destinations - or useful content about new technologies. &lt;strong&gt;And this is where it gets tricky&lt;/strong&gt;. If you work in technology, part of your job - whether explicit or not - is keeping abreast of general trends, and sometimes specifically detailed changes in a given technology.&lt;/p&gt;

&lt;p&gt;Even if this is not part of your job role, &lt;strong&gt;it&amp;rsquo;s certainly part of keeping yourself employable&lt;/strong&gt;. Who knows what opportunities will present themselves to you next? Being &lt;em&gt;aware&lt;/em&gt; of technology, even if not au fait with it, is &lt;strong&gt;better than sheer ignorance&lt;/strong&gt;. Sometimes you&amp;rsquo;ll want to digest new information right away, and at others simply be aware of its presence for some time in the future when you come to need the detail in it.&lt;/p&gt;

&lt;p&gt;But - how on earth does one try and keep on top of the deluge of information? Broadly put, there&amp;rsquo;s information coming at us from all angles in the form of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Conference papers and videos&lt;/li&gt;
&lt;li&gt;Blogs&lt;/li&gt;
&lt;li&gt;Podcasts&lt;/li&gt;
&lt;li&gt;Whitepapers&lt;/li&gt;
&lt;li&gt;Twitter discussions&lt;/li&gt;
&lt;li&gt;Books&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &amp;lsquo;throughput&amp;rsquo; of these vary, as does their importance - and quality.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Generally &lt;strong&gt;books&lt;/strong&gt; are released infrequently, and one only reads a book to get a detailed understanding of the particular subject - but if you need that level of detail a book can be the definitive place to go for it.&lt;/li&gt;
&lt;li&gt;At the other end of the scale, &lt;strong&gt;Tweets&lt;/strong&gt; are created continually, and arguably a great deal are complete tosh. However, there are gems in there which make it worth trying to sift through the stream to pick them out. For example, nowhere else do you get such frequent opportunity to see key experts in a given field discussing and debating points openly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blogs&lt;/strong&gt; range in quality and relevance greatly, sometimes being an author&amp;rsquo;s &amp;lsquo;scratchpad&amp;rsquo; of notes (like this one!), through to carefully written and argued articles that would be as well published as whitepapers. They can also range from hugely specific and low-level all the way up to general industry commentary - and depending on your interest either or both are going to be interesting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Whitepapers&lt;/strong&gt; and &lt;strong&gt;Conference papers and videos&lt;/strong&gt; can be an excellent source of learning material and in effect free training, as can &lt;strong&gt;Podcasts&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;If you tried to read all blogs, follow all tweets, and read all the books you&amp;rsquo;d go completely mad trying, and also not get any other work done because of the sheer volume of it. So &amp;hellip; how does one even attempt to keep up?&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I do, and I&amp;rsquo;m really keen to know how others approach this problem - let me know in the comments below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I&amp;rsquo;m a massive fan of &lt;strong&gt;RSS feeds&lt;/strong&gt;. They seem to be somewhat &amp;lsquo;off-trend&amp;rsquo; in recent years, particularly with the death of Google Reader, but I think there&amp;rsquo;s no sensible alternative.&lt;/p&gt;

&lt;p&gt;I use &lt;strong&gt;&lt;a href=&#34;http://feedly.com/&#34;&gt;Feedly&lt;/a&gt;&lt;/strong&gt; to track RSS feeds, splitting them up into &amp;lsquo;collections&amp;rsquo;. &lt;a href=&#34;http://feedly.com/rmoff/&#34;&gt;I&amp;rsquo;ve shared mine here&lt;/a&gt; if you want to add them to your RSS reader or Feedly profile.&lt;/p&gt;

&lt;p&gt;With Feedly I skim through feeds (sometimes daily, sometimes weekly/monthly, depending on workload), and anything interesting I read there and then (for short pieces), or send to Pocket (for longer articles). I also like to share on &lt;strong&gt;Twitter&lt;/strong&gt; interesting articles using the handy in-built twitter button (or via Buffer).&lt;/p&gt;

&lt;p&gt;In the past I&amp;rsquo;ve used Flipboard but found it too gimmicky and easy to miss articles.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I use &lt;a href=&#34;https://twitter.com/rmoff/&#34;&gt;Twitter&lt;/a&gt; a great deal. There&amp;rsquo;s no way I can keep up with the c.1k people I follow on it, so I&amp;rsquo;ve created my own private list (&amp;ldquo;TopRead&amp;rdquo;) on which I put key people who I wouldn&amp;rsquo;t want to miss what they&amp;rsquo;re saying. This includes people who are key in my area of tech, who often post quality articles or comments, and who don&amp;rsquo;t post lots of random crap.&lt;/p&gt;

&lt;p&gt;I rely on people [re-]tweeting interesting content on Twitter as a way of discovering content that I might otherwise miss through not reading every tweet on my timeline. Twitter&amp;rsquo;s &amp;ldquo;In case you missed it&amp;rdquo; algorithm does a pretty good job too of highlighting interesting tweets.&lt;/p&gt;

&lt;p&gt;Tweets linking to interesting articles/blogs I save to Pocket, using the Chrome Pocket extension which adds a Pocket link directly to the Tweet page (or the Pocket iOS extensions when reading on my phone).&lt;/p&gt;

&lt;p&gt;Twitter is a funny mix of personal and professional, and I like it for that. I post pictures of my fried breakfasts and beers that I&amp;rsquo;ve drunk, but I also post techie content. I&amp;rsquo;ve had some great conversations (including technical related ones), online and offline, as a result of this - conversations that wouldn&amp;rsquo;t have come about if I&amp;rsquo;d posted technical content only.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://getpocket.com&#34;&gt;Pocket&lt;/a&gt;&lt;/strong&gt; is a great tool for making available articles that you want to read, with the massive bonus that it works offline so is perfect for when you&amp;rsquo;re travelling. As you read stuff you mark it as read and it goes into your &amp;lsquo;archive&amp;rsquo; so can still be found, but it&amp;rsquo;s nice and clear what you&amp;rsquo;ve read and not.&lt;/p&gt;

&lt;p&gt;When I do read articles in Pocket, I like the ability to highlight and &amp;lsquo;recommend&amp;rsquo; passages, sharing them on Twitter.&lt;/p&gt;

&lt;p&gt;My Pocket reading list (not archive) goes back five years 😳 and tends to be a &lt;a href=&#34;https://en.wikipedia.org/wiki/LIFO&#34;&gt;LIFO&lt;/a&gt; system, never reaching zero (or anywhere near it). You can search and tag and all that good organisational stuff.&lt;/p&gt;

&lt;p&gt;I always think of Pocket as my &lt;a href=&#34;http://theelectricmonk.com/ElectricMonk.html&#34;&gt;Electric Monk&lt;/a&gt;, doing my reading for me so that I don&amp;rsquo;t have to. Part of the problem is that an article could be interesting and I&amp;rsquo;d ideally like to read it, but not have time. So it&amp;rsquo;d be wrong to archive it off, but similarly it&amp;rsquo;s not going to get read now. So my use of Pocket is not perfect, but it scratches my itch of worrying about forever missing an article I saw once, as I can always search through it for keywords. Maybe I should be saving whole articles to Evernote as a matter of routine instead.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I use &lt;strong&gt;Evernote&lt;/strong&gt; for a lot of my digital life. In the context of this article, I use the &lt;a href=&#34;https://evernote.com/webclipper/&#34;&gt;Web Clipper&lt;/a&gt; to save copies of an article or PDF directly into Evernote, from where it is then fully searchable and can be located in the future.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Saving full articles into Evernote tends to address the issue with Pocket above, where it&amp;rsquo;s *something I want to have available to me one day when I need it, but I&amp;rsquo;m not going to read and consume it *now**. For example, a comprehensive list of good practices for working with &lt;code&gt;&amp;lt;x&amp;gt;&lt;/code&gt;. As and when I work with &lt;code&gt;&amp;lt;x&amp;gt;&lt;/code&gt;, it&amp;rsquo;ll be brilliant to have these to hand - but as of now all I really want to be aware of is what &lt;code&gt;&amp;lt;x&amp;gt;&lt;/code&gt; does and how it fits in my current technical life, so I&amp;rsquo;m not going to spend time on a greater level of detail than that.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I&amp;rsquo;ve recently started listening to &lt;strong&gt;Podcasts&lt;/strong&gt; when I go running, and have found them a great way to ingest a lot more information than reading blogs. Partly because I have nothing to do but concentrate on the podcast, but also because the medium works well for detailed discussion and explanation of technology. I&amp;rsquo;d easily trade one quality podcast on a topic over a dozen fairly-good blogs. Some podcasts I&amp;rsquo;d recommend:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Techie

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://exponent.fm/&#34;&gt;Exponent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.drilltodetail.com/&#34;&gt;Drill to Detail&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;full disclosure : I was a guest on episode 14 :-)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/oreilly-radar/sets/the-oreilly-data-show-podcast&#34;&gt;O&amp;rsquo;Reilly Data Show&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.se-radio.net/category/episodes/&#34;&gt;Software Engineering Radio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Non-techie

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bbc.co.uk/programmes/b006qt55/episodes/downloads&#34;&gt;Crossing Continents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bbc.co.uk/programmes/p02nrtpm/episodes/downloads&#34;&gt;From Our Own Correspondent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Something you may note is missing from the above list is &lt;strong&gt;books&lt;/strong&gt;. I do still read books, but very, very few. Mostly they are non-work - believe it or not I do have a life ;-)
The technical books that I do read I get as &lt;code&gt;epub&lt;/code&gt; to consume on my Mac or Kindle, and quite often will dig into a few chapters thoroughly and skim the rest. It really does depend on the subject.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;your-turn&#34;&gt;Your Turn!&lt;/h3&gt;

&lt;p&gt;What&amp;rsquo;s your system for trying to capture, consume, and comprehend the deluge of information out there? Let me know in the comments section below :)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(image credit: &lt;a href=&#34;https://unsplash.com/@danielcgold&#34;&gt;https://unsplash.com/@danielcgold&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Install qemu on AWS EC2 Amazon Linux</title>
			<link>https://rmoff.github.io/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/</link>
			<pubDate>Sat, 11 Mar 2017 15:04:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/</guid>
			<description>&lt;p&gt;Mucking about with virtual disks, I wanted to install &lt;code&gt;qemu&lt;/code&gt; on a AWS EC2 instance in order to use &lt;code&gt;qemu-img&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Not finding it in a &lt;code&gt;yum&lt;/code&gt; repo, I built it from scratch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ uname -a

Linux ip-10-0-1-238 4.4.41-36.55.amzn1.x86_64 #1 SMP Wed Jan 18 01:03:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install -y ghc-glib-devel ghc-glib autoconf autogen intltool libtool

wget http://download.qemu-project.org/qemu-2.8.0.tar.xz
tar xvJf qemu-2.8.0.tar.xz
cd qemu-2.8.0
./configure
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;I hit a few errors, recorded here for passing Googlers:&lt;/p&gt;

&lt;p&gt;First error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./configure

ERROR: glib-2.22 gthread-2.0 is required to compile QEMU
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To fix it, first enable EPEL repository - in &lt;code&gt;/etc/yum.repos.d/epel.repo&lt;/code&gt; set &lt;code&gt;enabled=1&lt;/code&gt; for &lt;code&gt;[epel]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then install the Haskell glib library:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install -y ghc-glib-devel ghc-glib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Three more missing dependencies during the build, errors were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/bin/sh: autoreconf: command not found&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Can&#39;t exec &amp;quot;aclocal&amp;quot;: No such file or directory at /usr/share/autoconf/Autom4te/FileUtils.pm line 326.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;configure.ac:75: error: possibly undefined macro: AC_PROG_LIBTOOL&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fixed with :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum install -y autoconf autogen intltool libtool
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;
</description>
		</item>
		
		<item>
			<title>Mount VMDK/OVF/OVA on Amazon Web Services (AWS) EC2</title>
			<link>https://rmoff.github.io/2017/03/11/mount-vmdk-ovf-ova-on-amazon-web-services-aws-ec2/</link>
			<pubDate>Sat, 11 Mar 2017 14:21:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/03/11/mount-vmdk-ovf-ova-on-amazon-web-services-aws-ec2/</guid>
			<description>&lt;p&gt;So you&amp;rsquo;ve got a Linux VM that you want to access the contents of in EC2 - how do you do it? Let&amp;rsquo;s see how. First up, convert the VMDK to raw image file. If you&amp;rsquo;ve got a &lt;code&gt;ova&lt;/code&gt;/&lt;code&gt;ovf&lt;/code&gt; then just untar it first (&lt;code&gt;tar -xvf my_vm.ova&lt;/code&gt;), from which you should get the VMDK. With that, convert it using &lt;code&gt;qemu-img&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ time qemu-img convert -f vmdk -O raw SampleAppv607p-appliance-disk1.vmdk SampleAppv607p-appliance-disk1.raw

real    16m36.740s
user    6m44.136s
sys     0m11.000s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Inspect the image file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ file /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw
/u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw: DOS/MBR boot sector; GRand Unified Bootloader, stage1 version 0x3, boot drive 0x80, 1st sector stage2 0x8480e, GRUB version 0.94

$ sudo fdisk -l /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw

Disk /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw: 214.7 GB, 214748364800 bytes, 419430400 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000274a9

                                                   Device Boot      Start         End      Blocks   Id  System
/u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw1   *        2048     1026047      512000   83  Linux
/u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw2         1026048   419430399   209202176   8e  Linux LVM
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mounting it straight out won&amp;rsquo;t work:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mount /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw /mnt/sampleapp/
mount: wrong fs type, bad option, bad superblock on /dev/loop1,
       missing codepage or helper program, or other error

       In some cases useful info is found in syslog - try
       dmesg | tail or so.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the offset (&lt;strong&gt;2048&lt;/strong&gt;) from &lt;code&gt;fdisk&lt;/code&gt; output above, mount the first partition:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir /mnt/sampleapp
$ sudo mount -o offset=$((2048 * 512)) /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw /mnt/sampleapp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Success!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -l sampleapp/
total 134841
-rw-r--r--. 1 root root   106308 Oct 14  2014 config-2.6.32-504.el6.x86_64
-rw-r--r--. 1 root root   107139 Mar 22  2016 config-2.6.32-573.22.1.el6.x86_64
-rw-r--r--. 1 root root   131020 Mar 23  2016 config-3.8.13-118.4.2.el6uek.x86_64
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now to mount the rest of the disk. Not so simple, as it uses Logical Volume Management (LVM):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mount -o offset=$((1026048 * 512)) /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw /mnt/sampleapp/
mount: unknown filesystem type &#39;LVM2_member&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Courtesy of &lt;a href=&#34;http://www.hutsky.cz/blog/2014/06/mount-a-disk-image-containing-lvm/&#34;&gt;this article&lt;/a&gt;, we use &lt;code&gt;losetup&lt;/code&gt; to make the volumes available via the loop device, run as root. We can skip the use of &lt;code&gt;kpartx&lt;/code&gt; by using the &lt;code&gt;-P&lt;/code&gt; flag on &lt;code&gt;losetup&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# losetup -P /dev/loop0 /u01/stage/vm/extract/SampleAppv607p-appliance-disk1.raw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then activate the volume groups:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# vgscan
  Reading all physical volumes.  This may take a while...
  Found volume group &amp;quot;vg_demo&amp;quot; using metadata type lvm2

# vgchange -ay vg_demo
  3 logical volume(s) in volume group &amp;quot;vg_demo&amp;quot; now active
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, mount each volume group:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# ls -l /dev/mapper/
total 0
crw------- 1 root root 10, 236 Feb 14 10:21 control
lrwxrwxrwx 1 root root       7 Feb 14 14:21 loop0p1 -&amp;gt; ../dm-0
lrwxrwxrwx 1 root root       7 Feb 14 14:21 loop0p2 -&amp;gt; ../dm-1
lrwxrwxrwx 1 root root       7 Feb 14 14:21 vg_demo-lv_home -&amp;gt; ../dm-4
lrwxrwxrwx 1 root root       7 Feb 14 14:21 vg_demo-lv_root -&amp;gt; ../dm-2
lrwxrwxrwx 1 root root       7 Feb 14 14:21 vg_demo-lv_swap -&amp;gt; ../dm-3

# mkdir sampleapp/home sampleapp/root
# mount /dev/mapper/vg_demo-lv_root /mnt/sampleapp/root/
# mount /dev/mapper/vg_demo-lv_home /mnt/sampleapp/home/
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;To unmount the image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# umount /mnt/sampleapp/home/
# umount /mnt/sampleapp/root/
# umount /mnt/sampleapp/
# losetup -d /dev/loop0
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Convert back to VMDK:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ec2-user@ip-10-0-1-238 extract]$ time qemu-img convert -f raw -O vmdk SampleAppv607p-appliance-disk1.raw SampleAppv607p-appliance-disk1-mod.vmdk

real    19m34.931s
user    0m4.780s
sys     3m25.332s
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Little Technology Wins</title>
			<link>https://rmoff.github.io/2017/03/11/little-technology-wins/</link>
			<pubDate>Sat, 11 Mar 2017 11:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/03/11/little-technology-wins/</guid>
			<description>

&lt;h3 id=&#34;wireless-headset-for-voip-with-no-30-minute-dalek-timebomb&#34;&gt;Wireless Headset for VOIP With No 30-Minute Dalek Timebomb&lt;/h3&gt;

&lt;p&gt;A lot of my work is done remotely, with colleagues and customers. Five years ago I bought a &lt;a href=&#34;https://www.amazon.co.uk/Microsoft-JUG-00014-LifeChat-LX-3000-Headset&#34;&gt;Microsoft LifeChat LX-3000&lt;/a&gt; which plugged into the USB port on my Mac. It did the job kinda fine, with two gripes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;it wasn&amp;rsquo;t wireless. I like to wander whilst I chat, and I didn&amp;rsquo;t like being tethered. But this in itself wasn&amp;rsquo;t a reason to ditch it&lt;/li&gt;
&lt;li&gt;After c.30 minutes on a call, my voice would turn into a dalek. or rather, my voice wouldn&amp;rsquo;t but the audio that others heard was.
This happened regardless of platform (Hangouts / Zoom / Skype / etc). I figured it must be a software or network issue. Never got to the bottom of it, until I switched to using a Snowball microphone for some proper &lt;a href=&#34;https://www.drilltodetail.com/podcast/2016/12/20/drill-to-detail-ep14-christmas-new-year-special-with-special-guest-robin-moffatt&#34;&gt;voice recording&lt;/a&gt; - and any calls I happened to also make on it no longer had the Dalek problem.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So I switched, on a colleague&amp;rsquo;s recommendation, to the &lt;a href=&#34;https://www.amazon.co.uk/Logitech-H600-Wireless-Headset-Mac/&#34;&gt;Logitech H600&lt;/a&gt;. I love it. The wireless works flawlessly, and no dalek effect. Gripes? Well there&amp;rsquo;s no pleasing some people. The audio quality is great for calls, but for music I switch back to my wireless &lt;a href=&#34;https://www.amazon.co.uk/Avantree-Comfortable-Bluetooth-Headphones-Lightweight&#34;&gt;Avantree Auditions&lt;/a&gt;. The Logitech headset also feels a bit plastic, which I don&amp;rsquo;t care about unless I&amp;rsquo;m back here in six months complaining that it&amp;rsquo;s broken&amp;hellip;.&lt;/p&gt;

&lt;h3 id=&#34;apple-watch-2-plus-bluetooth-headphones&#34;&gt;Apple Watch 2 plus Bluetooth Headphones&lt;/h3&gt;

&lt;p&gt;Eighteen months ago I took up running, since for the past 37 years I&amp;rsquo;d done all I could to avoid exercise, and with my fondness for [beer]() and [fryups](), it was time to do something to try and influence the statistics of health problems back in my favour&amp;hellip;&lt;/p&gt;

&lt;p&gt;Being a geek, I obviously had to track my runs, which I did at first with MapMyRun on my iPhone, eventually investing in a &lt;a href=&#34;https://www.polar.com/uk-en/products/sport/M400&#34;&gt;Polar M400&lt;/a&gt;. It&amp;rsquo;s a great phone for tracking running, and synced nicely with &lt;a href=&#34;https://www.strava.com/athletes/10250052/&#34;&gt;Strava&lt;/a&gt; - but it didn&amp;rsquo;t do anything else as a &amp;lsquo;smart watch&amp;rsquo;. It mirrored notifications from my iPhone but in a fairly lame way. Then along came Apple Watch 2, which unlike the original Apple Watch has in-built GPS meaning that you can track your runs without needing to have a phone - and being an Apple Watch does lots of very cool &amp;lsquo;smart watch&amp;rsquo; stuff.&lt;/p&gt;

&lt;p&gt;On my runs I listen to podcasts, and previously had done this using an old Nexus 4 that I had, with a pair of MPow Swift Bluetooth Headphones. This worked great. Then I realised that the Apple Watch 2 can pair directly with headphones! At first it refused to pair with the MPow Swift, but after a bit of Googling I found that by first deleting the pairing from my Nexus 4 to the headphones, I could then pair them fine to the Apple Watch 2. Following &lt;a href=&#34;http://www.idownloadblog.com/2015/04/28/how-to-sync-podcasts-apple-watch/&#34;&gt;this convoluted method&lt;/a&gt; I can get podcasts on my Apple Watch 2 &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/02/IMG_2057.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So my little tech win is being able to go for a run with just my Apple Watch 2 and MPow Swift headphones, track my run with GPS (using Strava&amp;rsquo;s &lt;a href=&#34;https://www.strava.com/apple-watch?hl=en-GB&#34;&gt;new support for Watch-only tracking&lt;/a&gt;), and listen to podcasts!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Little technology fail : I&amp;rsquo;ve actually reverted to going running with my Nexus 4, because I got fed up manually trying to sync podcasts to my watch. For the benefit of not carrying a phone, it just wasn&amp;rsquo;t worth it. The Podcast Addict app on the Nexus does a great job of automatically downloading new episodes so that they&amp;rsquo;re just there and ready for me to listen to when I go out.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(Photo credit: &lt;a href=&#34;https://unsplash.com/@dsoodmand&#34;&gt;https://unsplash.com/@dsoodmand&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Time For a Change</title>
			<link>https://rmoff.github.io/2017/03/10/time-for-a-change/</link>
			<pubDate>Fri, 10 Mar 2017 17:30:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/03/10/time-for-a-change/</guid>
			<description>&lt;p&gt;After 5 years at Rittman Mead, &lt;a href=&#34;https://www.rittmanmead.com/blog/author/robin-moffatt/&#34;&gt;126 blog posts&lt;/a&gt;, &lt;a href=&#34;https://speakerdeck.com/rmoff/&#34;&gt;16 conferences&lt;/a&gt;, &lt;a href=&#34;https://community.oracle.com/docs/DOC-993649&#34;&gt;four&lt;/a&gt; &lt;a href=&#34;https://community.oracle.com/docs/DOC-1010305&#34;&gt;published&lt;/a&gt; &lt;a href=&#34;https://community.oracle.com/docs/DOC-1009358&#34;&gt;OTN&lt;/a&gt; &lt;a href=&#34;https://community.oracle.com/docs/DOC-1006400&#34;&gt;articles&lt;/a&gt;, an &lt;a href=&#34;https://apex.oracle.com/pls/otn/f?p=19297:4:::NO:4:P4_ID:10100&#34;&gt;Oracle ACE award&lt;/a&gt; - not to mention, of course, a whole heap of interesting and challenging client work - I&amp;rsquo;ve decided that it&amp;rsquo;s time to do something different.&lt;/p&gt;

&lt;p&gt;Later this month I&amp;rsquo;ll be joining &lt;a href=&#34;https://confluent.io&#34;&gt;&lt;strong&gt;Confluent&lt;/strong&gt;&lt;/a&gt; as a &lt;strong&gt;Partner Technology Evangelist&lt;/strong&gt;, helping spread the good word of Apache Kafka and the &lt;a href=&#34;https://www.confluent.io/product/&#34;&gt;Confluent platform&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/03/66021689.jpg&#34; alt=&#34;I&#39;m so excited!&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As always you can find me on Twitter &lt;a href=&#34;https://twitter.com/rmoff/&#34;&gt;@rmoff&lt;/a&gt;, for beer tweets, fried breakfast pics - and lots of Apache Kafka! You can also email me direct at robin@rmoff.net.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>HBase crash after resuming suspended VM</title>
			<link>https://rmoff.github.io/2017/01/20/hbase-crash-after-resuming-suspended-vm/</link>
			<pubDate>Fri, 20 Jan 2017 09:36:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/01/20/hbase-crash-after-resuming-suspended-vm/</guid>
			<description>&lt;p&gt;I use &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;BigDataLite&lt;/a&gt; for a lot of my sandboxing work. This is a &lt;code&gt;OVA&lt;/code&gt; provided by Oracle which can be run on VirtualBox, VMWare, etc and has the Cloudera Hadoop platform (CDH) along with all of Oracle&amp;rsquo;s Big Data goodies including Big Data Discovery and Big Data Spatial and Graph (BDSG).&lt;/p&gt;

&lt;p&gt;Something that kept tripping me up during my work with BDSG was that HBase would become unavailable. Not being an HBase expert and simply using it as a data store for my property graph data, I wrote it off as mistakes on my part. But, the issue kept reoccuring enough for me to dig into it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[oracle@bigdatalite ~]$ sudo service hbase-master status;sudo service hbase-regionserver status;sudo service hbase-thrift status;sudo service zookeeper-server status
HBase master daemon is not running                         [FAILED]
hbase-regionserver is not running.
HBase thrift daemon is running                             [  OK  ]
zookeeper-server is running
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Turns out that HBase throws its toys out when I suspend the VM. I don&amp;rsquo;t know if it&amp;rsquo;s the clock jumping too much, or simply a session expiring and it not exiting gracefully. I don&amp;rsquo;t know if this is a VirtualBox fault, host machine (Mac), Hbase, or even Zookeeper; nor do I especially care now that I&amp;rsquo;ve found the cause and know to look for it whilst doing sandbox work ;-)&lt;/p&gt;

&lt;p&gt;This is the log from the HBase master:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-01-20 09:19:05,430 ERROR [master/bigdatalite.localdomain/127.0.0.1:60000] zookeeper.ZooKeeperWatcher: master:60000-0x159b8f3800e0014, quorum=localhost:2181, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/master
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1151)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:359)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:623)
        at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:148)
        at org.apache.hadoop.hbase.master.ActiveMasterManager.stop(ActiveMasterManager.java:267)
        at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:1150)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1092)
        at java.lang.Thread.run(Thread.java:745)
2017-01-20 09:19:05,431 ERROR [master/bigdatalite.localdomain/127.0.0.1:60000] master.ActiveMasterManager: master:60000-0x159b8f3800e0014, quorum=localhost:2181, baseZNode=/hbase Error deleting our own master address node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/master
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1151)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:359)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData(ZKUtil.java:623)
        at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:148)
        at org.apache.hadoop.hbase.master.ActiveMasterManager.stop(ActiveMasterManager.java:267)
        at org.apache.hadoop.hbase.master.HMaster.stopServiceThreads(HMaster.java:1150)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1092)
        at java.lang.Thread.run(Thread.java:745)
2017-01-20 09:19:05,432 INFO  [master/bigdatalite.localdomain/127.0.0.1:60000] hbase.ChoreService: Chore service for: bigdatalite.localdomain,60000,1484898484413_splitLogManager_ had [] on shutdown
2017-01-20 09:19:05,432 INFO  [master/bigdatalite.localdomain/127.0.0.1:60000] flush.MasterFlushTableProcedureManager: stop: server shutting down.
2017-01-20 09:19:05,432 INFO  [master/bigdatalite.localdomain/127.0.0.1:60000] ipc.RpcServer: Stopping server on 60000
2017-01-20 09:19:05,434 INFO  [RpcServer.listener,port=60000] ipc.RpcServer: RpcServer.listener,port=60000: stopping
2017-01-20 09:19:05,440 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2017-01-20 09:19:05,440 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2017-01-20 09:19:20,449 ERROR [master/bigdatalite.localdomain/127.0.0.1:60000] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts
2017-01-20 09:19:20,449 WARN  [master/bigdatalite.localdomain/127.0.0.1:60000] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/bigdatalite.localdomain,60000,1484898484413
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:178)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1236)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1225)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1431)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1100)
        at java.lang.Thread.run(Thread.java:745)
2017-01-20 09:19:20,511 INFO  [master/bigdatalite.localdomain/127.0.0.1:60000] regionserver.HRegionServer: stopping server bigdatalite.localdomain,60000,1484898484413; zookeeper connection closed.
2017-01-20 09:19:20,511 INFO  [master/bigdatalite.localdomain/127.0.0.1:60000] regionserver.HRegionServer: master/bigdatalite.localdomain/127.0.0.1:60000 exiting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The solution, crude as it is, is just to turn it off and on again - HBase, that is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo service hbase-master restart;sudo service hbase-regionserver restart;sudo service hbase-thrift restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Happy days.&lt;/p&gt;

&lt;p&gt;And if anyone can tell me the proper resolution to this (other than not suspending my VM), I&amp;rsquo;m all ears!&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kibana Timelion - Anomaly Detection</title>
			<link>https://rmoff.github.io/2017/01/18/kibana-timelion-anomaly-detection/</link>
			<pubDate>Wed, 18 Jan 2017 19:53:10 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/01/18/kibana-timelion-anomaly-detection/</guid>
			<description>&lt;p&gt;Using the &lt;code&gt;holt&lt;/code&gt; function in Timelion to do anomaly detection on Metricbeat data in Kibana:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2017/01/holt_-_Timelion_-_Kibana.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Expression:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$thres=0.02, .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).color(#eee).lines(10).label(&#39;Prediction&#39;), .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).color(#666).lines(1).label(Actual), .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).subtract(.es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;)).abs().if(lt, $thres, null, .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;)).points(10,3,0).color(#c66).label(&#39;Anomaly&#39;).title(&#39;max:system.cpu.user.pct / @rmoff&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/rashidkpc/status/762754396111327232&#34;&gt;https://twitter.com/rashidkpc/status/762754396111327232&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/timelion/issues/87&#34;&gt;https://github.com/elastic/timelion/issues/87&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/timelion/blob/master/FUNCTIONS.md&#34;&gt;https://github.com/elastic/timelion/blob/master/FUNCTIONS.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Streaming / Unbounded Data - Resources</title>
			<link>https://rmoff.github.io/2017/01/16/streaming-unbounded-data-resources/</link>
			<pubDate>Mon, 16 Jan 2017 11:10:38 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2017/01/16/streaming-unbounded-data-resources/</guid>
			<description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101&#34;&gt;The world beyond batch: Streaming 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102&#34;&gt;The world beyond batch: Streaming 102&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/ideas/data-architectures-for-streaming-applications&#34;&gt;Data architectures for streaming applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.se-radio.net/2016/10/se-radio-episode-272-frances-perry-on-apache-beam/&#34;&gt;SE-Radio Episode 272: Frances Perry on Apache Beam&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(&lt;a href=&#34;https://unsplash.com/@jacksonjost]&#34;&gt;img credit&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException</title>
			<link>https://rmoff.github.io/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</link>
			<pubDate>Fri, 02 Dec 2016 11:35:57 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</guid>
			<description>&lt;p&gt;By default, the &lt;code&gt;kafka-avro-console-producer&lt;/code&gt; will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 &lt;em&gt;already&lt;/em&gt;!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite tmp]$ kafka-avro-console-producer \
&amp;gt;  --broker-list localhost:9092 --topic kudu_test \
&amp;gt;  --property value.schema=&#39;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}]}&#39;

{&amp;quot;id&amp;quot;: 999, &amp;quot;random_field&amp;quot;: &amp;quot;foo&amp;quot;}

org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;}]}
Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character (&#39;&amp;lt;&#39; (code 60)): expected a valid value (number, String, array, object, &#39;true&#39;, &#39;false&#39; or &#39;null&#39;)
 at [Source: sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@4e0ae11f; line: 1, column: 2]; error code: 50005
        at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:170)
        at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:187)
        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:238)
        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:230)
        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:225)
        at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.registerAndGetId(CachedSchemaRegistryClient.java:59)
        at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.register(CachedSchemaRegistryClient.java:91)
        at io.confluent.kafka.serializers.AbstractKafkaAvroSerializer.serializeImpl(AbstractKafkaAvroSerializer.java:72)
        at io.confluent.kafka.formatter.AvroMessageReader.readMessage(AvroMessageReader.java:158)
        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55)
        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Solution? Make sure you specify the schema URL when you launch the producer, using &lt;code&gt;--property schema.registry.url=http://localhost:18081&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-avro-console-producer \
--broker-list localhost:9092 --topic kudu_test \
--property value.schema=&#39;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}]}&#39; \
--property schema.registry.url=http://localhost:18081
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Oracle GoldenGate -&gt; Kafka Connect - &#34;Failed to serialize Avro data&#34;</title>
			<link>https://rmoff.github.io/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</link>
			<pubDate>Tue, 29 Nov 2016 22:04:38 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</guid>
			<description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; &lt;em&gt;Make sure that &lt;code&gt;key.converter.schema.registry.url&lt;/code&gt; and &lt;code&gt;value.converter.schema.registry.url&lt;/code&gt; are specified, and that there are no trailing whitespaces.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I&amp;rsquo;ve been building on &lt;a href=&#34;https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/&#34;&gt;previous work&lt;/a&gt; I&amp;rsquo;ve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the &lt;a href=&#34;https://java.net/projects/oracledi/downloads/directory/GoldenGate/Oracle%20GoldenGate%20Adapter%20for%20Kafka%20Connect&#34;&gt;sample configuration&lt;/a&gt; gives.&lt;/p&gt;

&lt;p&gt;Simply changing the Kafka Connect OGG configuration file (&lt;code&gt;confluent.properties&lt;/code&gt;) from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter=org.apache.kafka.connect.json.JsonConverter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;value.converter=io.confluent.connect.avro.AvroConverter
key.converter=io.confluent.connect.avro.AvroConverter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;isn&amp;rsquo;t enough - the OGG replicat abends with the error (in &lt;code&gt;ggserr.log&lt;/code&gt;) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-11-29 20:50:23  ERROR   OGG-15051  Oracle GoldenGate Delivery, rconf.prm:  Java or JNI exception:
oracle.goldengate.util.GGException: org.apache.kafka.common.config.ConfigException: Missing Schema registry url!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A similar configuration attempt (OGG -&amp;gt; Kafka Connect Avro) can be found on the &lt;a href=&#34;https://groups.google.com/forum/#!msg/confluent-platform/hTaL0z9WJhA/ZA5fb-DJBAAJ&#34;&gt;Confluent Platform Google Group&lt;/a&gt;, where the advice is to make sure that the schema registry URL is configured. I already had &lt;code&gt;schema.registry.url&lt;/code&gt; in my config, but added the  &lt;a href=&#34;https://groups.google.com/d/msg/confluent-platform/hTaL0z9WJhA/0Yf0c0GTBQAJ&#34;&gt;sample config given&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;key.converter.schema.registry.url=http://localhost:18081
value.converter.schema.registry.url=http://localhost:18081 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that my schema registry is running on 18081 (not 8081).&lt;/p&gt;

&lt;p&gt;I then got the replicat abending with a different error:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;org.apache.kafka.connect.errors.DataException: Failed to serialize Avro data&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The replicat RPT (in OGG&amp;rsquo;s &lt;code&gt;dirrpt&lt;/code&gt; folder) shows&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Exception in thread &amp;quot;main&amp;quot; org.apache.kafka.connect.errors.DataException: Failed to serialize Avro data:
        at io.confluent.connect.avro.AvroConverter.fromConnectData(AvroConverter.java:92)
        at oracle.goldengate.kafkaconnect.GGProducer.send(GGProducer.java:64)
        at oracle.goldengate.kafkaconnect.KafkaConnectHandler.processData(KafkaConnectHandler.java:337)
[...]
Caused by: org.apache.kafka.common.errors.SerializationException: Error serializing Avro message
Caused by: java.net.MalformedURLException: For input string: &amp;quot;18081 &amp;quot;
        at java.net.URL.&amp;lt;init&amp;gt;(URL.java:627)
        at java.net.URL.&amp;lt;init&amp;gt;(URL.java:490)
        at java.net.URL.&amp;lt;init&amp;gt;(URL.java:439)
        at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:124)
[...]
Caused by: java.lang.NumberFormatException: For input string: &amp;quot;18081 &amp;quot;
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:580)
        at java.lang.Integer.parseInt(Integer.java:615)
        at java.net.URLStreamHandler.parseURL(URLStreamHandler.java:216)
        at java.net.URL.&amp;lt;init&amp;gt;(URL.java:622)
        ... 21 more
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The schema registry URL is evidently valid at some level, because in the schema registry stdout I can see a POST being made when the OGG replicat runs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-11-25 03:12:53,591] INFO 127.0.0.1 - - [25/Nov/2016:03:12:53 +0000] &amp;quot;POST /subjects/ORCL.SOE.LOGON-key/versions HTTP/1.1&amp;quot; 200 9  15 (io.confluent.rest-utils.requests:77)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking at the error in the above note &lt;code&gt;MalformedURLException: For input string: &amp;quot;18081 &amp;quot;&lt;/code&gt; and the space suffix on 18081.&lt;/p&gt;

&lt;p&gt;Going back to the RPT output some more:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  Contents of Kafka producer configuration file
    key [schema.registry.url]  value [http://localhost:18081]
    key [key.converter]  value [io.confluent.connect.avro.AvroConverter]
    key [value.converter]  value [io.confluent.connect.avro.AvroConverter]
    key [bootstrap.servers]  value [localhost:9092]
    key [value.serializer]  value [org.apache.kafka.common.serialization.ByteArraySerializer]
    key [value.converter.schema.registry.url]  value [http://localhost:18081 ]
    key [key.converter.schema.registry.url]  value [http://localhost:18081]
    key [key.serializer]  value [org.apache.kafka.common.serialization.ByteArraySerializer]
    key [internal.key.converter]  value [org.apache.kafka.connect.json.JsonConverter]
    key [internal.value.converter]  value [org.apache.kafka.connect.json.JsonConverter]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the &lt;strong&gt;trailing space&lt;/strong&gt; on the configuration value for &lt;code&gt;value.converter.schema.registry.url&lt;/code&gt;! After removing the trailing space from &lt;code&gt;confluent.properties&lt;/code&gt;, all was well, and OGG successfully sends data to Kafka in Avro format.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect - java.lang.IncompatibleClassChangeError</title>
			<link>https://rmoff.github.io/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</link>
			<pubDate>Thu, 24 Nov 2016 20:58:44 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</guid>
			<description>&lt;p&gt;I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties

[...]
Exception in thread &amp;quot;main&amp;quot; java.lang.IncompatibleClassChangeError: Implementing class
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The fix was to unset the &lt;code&gt;CLASSPATH&lt;/code&gt; first:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unset CLASSPATH
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>boto / S3 errors</title>
			<link>https://rmoff.github.io/2016/10/14/boto-s3-errors/</link>
			<pubDate>Fri, 14 Oct 2016 08:41:30 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/10/14/boto-s3-errors/</guid>
			<description>

&lt;p&gt;Presented without comment, warranty, or context -  other than these might help a wandering code hacker.&lt;/p&gt;

&lt;h3 id=&#34;when-using-sigv4-you-must-specify-a-host-parameter&#34;&gt;When using SigV4, you must specify a &amp;lsquo;host&amp;rsquo; parameter&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;boto.s3.connection.HostRequiredError: BotoClientError: When using SigV4, you must specify a &#39;host&#39; parameter.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To fix, switch&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conn_s3 = boto.connect_s3()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;for&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conn_s3 = boto.connect_s3(host=&#39;s3.amazonaws.com&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see a list of endpoints &lt;a href=&#34;http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;boto-exception-s3responseerror-s3responseerror-400-bad-request&#34;&gt;boto.exception.S3ResponseError: S3ResponseError: 400 Bad Request&lt;/h3&gt;

&lt;p&gt;Make sure you&amp;rsquo;re specifying the correct hostname (see above) for the bucket&amp;rsquo;s region. Determine the bucket&amp;rsquo;s region from the S3 control panel, and then use the &lt;a href=&#34;http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region&#34;&gt;endpoint listed here&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OGG-15051 oracle.goldengate.util.GGException:  Class not found: &#34;kafkahandler&#34;</title>
			<link>https://rmoff.github.io/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</link>
			<pubDate>Fri, 29 Jul 2016 07:47:30 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</guid>
			<description>&lt;p&gt;Similar to the &lt;a href=&#34;http://rmoff.net/2016/07/28/ogg-class-not-found-com-company-kafka-customproducerrecord/&#34;&gt;previous issue&lt;/a&gt;, the &lt;a href=&#34;http://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD457&#34;&gt;sample config&lt;/a&gt; in the docs causes another snafu:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OGG-15051  Java or JNI exception:
oracle.goldengate.util.GGException:  Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler
 	Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This time it&amp;rsquo;s in the &lt;code&gt;kafka.props&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.kafkahandler.Type = kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Should be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.kafkahandler.type = kafka
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No capital T in Type!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(Image credit: &lt;a href=&#34;https://unsplash.com/@vanschneider&#34;&gt;https://unsplash.com/@vanschneider&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OGG -  Class not found: &#34;com.company.kafka.CustomProducerRecord&#34;</title>
			<link>https://rmoff.github.io/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</link>
			<pubDate>Thu, 28 Jul 2016 16:34:37 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</guid>
			<description>&lt;p&gt;In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there&amp;rsquo;s a &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD457&#34;&gt;helpful sample configuration&lt;/a&gt;, which isn&amp;rsquo;t so helpful &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This value for &lt;code&gt;gg.handler.kafkahandler.ProducerRecordClass&lt;/code&gt; will cause a failure when you start the replicat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
Class not found: &amp;quot;com.company.kafka.CustomProducerRecord&amp;quot;
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you comment this configuration item out, it&amp;rsquo;ll use &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD455&#34;&gt;the default&lt;/a&gt; (&lt;code&gt;oracle.goldengate.handler.kafka.DefaultProducerRecord&lt;/code&gt;) and work swimingly!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(Image credit: &lt;a href=&#34;https://unsplash.com/@vanschneider&#34;&gt;https://unsplash.com/@vanschneider&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect JDBC - Oracle - Number of groups must be positive</title>
			<link>https://rmoff.github.io/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</link>
			<pubDate>Wed, 27 Jul 2016 15:23:14 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</guid>
			<description>&lt;p&gt;There are &lt;a href=&#34;https://groups.google.com/forum/#!searchin/confluent-platform/%22Number$20of$20groups$20must$20be$20positive%22&#34;&gt;various reasons for this error&lt;/a&gt;, but the one I hit was that &lt;strong&gt;the table name is case sensitive&lt;/strong&gt;, and returned from Oracle by the JDBC driver in uppercase.&lt;/p&gt;

&lt;p&gt;If you specify the tablename in your connecter config in lowercase, it won&amp;rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit &lt;code&gt;etc/kafka/connect-log4j.properties&lt;/code&gt; to set &lt;code&gt;log4j.rootLogger=DEBUG, stdout&lt;/code&gt;), and observe:  (&lt;em&gt;I&amp;rsquo;ve truncated some of the output for legibility&lt;/em&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-07-27 17:00:26,594] DEBUG Got the following tables: [...], SRSNAMESPACE_TABLE, ADDRESSES, CARD_DETAILS, CUSTOMERS, INVENTORIES, KAFKATEST, KAFKA_TEST, LOGON,[...] (io.confluent.connect.jdbc.TableMonitorThread:108)

[2016-07-27 17:00:26,594] DEBUG After filtering we got tables: [] (io.confluent.connect.jdbc.TableMonitorThread:135)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Changing the connector config from&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
table.whitelist=kafka_test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
table.whitelist=KAFKA_TEST
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Resolved this particular problem.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required</title>
			<link>https://rmoff.github.io/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</link>
			<pubDate>Tue, 19 Jul 2016 14:36:52 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</guid>
			<description>&lt;p&gt;I&amp;rsquo;ve been doing some noodling around with Confluent&amp;rsquo;s Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you&amp;rsquo;re not familiar with Kafka Connect &lt;a href=&#34;http://docs.confluent.io/3.0.0/connect/design.html&#34;&gt;this page&lt;/a&gt; gives a good idea of the thinking behind it.&lt;/p&gt;

&lt;p&gt;One issue that I hit defeated my Google-fu so I&amp;rsquo;m recording it here to hopefully help out fellow n00bs.&lt;/p&gt;

&lt;p&gt;The pipeline that I&amp;rsquo;d set up looked like this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Eneco/kafka-connect-twitter&#34;&gt;Eneco&amp;rsquo;s Twitter Source&lt;/a&gt; streaming tweets to a Kafka topic&lt;/li&gt;
&lt;li&gt;Confluent&amp;rsquo;s &lt;a href=&#34;docs.confluent.io/3.0.0/connect/connect-hdfs/docs/hdfs_connector.html&#34;&gt;HDFS Sink&lt;/a&gt; to stream tweets to HDFS and define Hive table automagically over them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It worked great, but only if I didn&amp;rsquo;t enable the Hive integration part. For me the integration with Hive to automatically define schemas was one of the key interests for this platform, so I wanted to see if I could get it to work. The error I got was&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;org.apache.kafka.connect.errors.SchemaProjectorException: Schema version required for BACKWARD compatibility
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The long and short of it was that I was using the wrong &lt;a href=&#34;http://docs.confluent.io/2.0.0/connect/userguide.html#common-worker-configs&#34;&gt;&lt;code&gt;Converter&lt;/code&gt; class&lt;/a&gt; for the data that was being written and read by Kafka - instead of Avro I&amp;rsquo;d used Json.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d used &lt;code&gt;/etc/kafka/connect-standalone.properties&lt;/code&gt;, just copying and pasting examples from the docs, and then gone off on my own config without thinking about it much further. This meant that instead of &lt;code&gt;io.confluent.connect.avro.AvroConverter&lt;/code&gt; for &lt;code&gt;key.converter&lt;/code&gt; and &lt;code&gt;value.converter&lt;/code&gt; I was using &lt;code&gt;org.apache.kafka.connect.json.JsonConverter&lt;/code&gt;. When you think about it, if you want a schema defined in Hive it&amp;rsquo;s got to come from somewhere; the Avro schema registry that Kafka Connect provides. Once I switched my config to use &lt;code&gt;/etc/schema-registry/connect-avro-standalone.properties&lt;/code&gt; everything worked just perfectly!&lt;/p&gt;

&lt;p&gt;You can find the configuration files &lt;a href=&#34;https://gist.github.com/rmoff/a2a9fd1cf24a9cf0b3537c7e47360583&#34;&gt;on gist here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(photo credit: &lt;a href=&#34;https://unsplash.com/@dan_carl5on&#34;&gt;https://unsplash.com/@dan_carl5on&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Configuring UPS/apcupsd</title>
			<link>https://rmoff.github.io/2016/07/18/configuring-ups-apcupsd/</link>
			<pubDate>Mon, 18 Jul 2016 07:59:51 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/18/configuring-ups-apcupsd/</guid>
			<description>

&lt;p&gt;With my new server I bought a UPS, partly just as a Good Thing, but also because I suspect a powercut fried the motherboard on a previous machine that I had, and this baby is too precious to lose ;)&lt;/p&gt;

&lt;p&gt;The idea is that the UPS will smooth out the power supply to my server, protecting it from surges or temporarily blips in power loss. If there&amp;rsquo;s a proper power cut, the UPS is connected to my server and can initiate a graceful shutdown instead of system crash. It seems unintuitive in this day and age of laptops and iPads that you just close or switch off to &amp;ldquo;suspend&amp;rdquo; them that killing the power to a server can damage it, but when you think about it just a moment more, it&amp;rsquo;s hardly surprising.&lt;/p&gt;

&lt;p&gt;The software for enabling the server (running Debian 8/Proxmox 4, kernel 4.4.6-1-pve), is a textbook example of why many people took against Linux in the early days - esoteric, out of date documentation. In this short article I&amp;rsquo;ll record how I actually got the thing to work, and a couple of errors I hit along the way.&lt;/p&gt;

&lt;p&gt;The UPS I&amp;rsquo;m running is a &lt;a href=&#34;https://www.scan.co.uk/products/650va-apc-bk650ei-tower-ups-with-internet-dsl-fax-modem-protection-retail-box&#34;&gt;APC Back-UPS 650&lt;/a&gt;, which includes a cable to connect its data port to the USB of a server/workstation&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;detour-nuts&#34;&gt;Detour - nuts&lt;/h3&gt;

&lt;p&gt;My first port of call was the top of the Google hits, &lt;a href=&#34;http://networkupstools.org/&#34;&gt;&lt;code&gt;nut&lt;/code&gt; (Network UPS Tools)&lt;/a&gt;, using the &lt;code&gt;usbhid-ups&lt;/code&gt; driver. This failed to even start up:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;usbhid-ups[11055]: segfault at 0 ip 00007f4e9fe3ca87 sp 00007fffcbd48228 error 4 in libc-2.19.so[7f4e9fd1d000+1a2000]
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;success-apcupsd&#34;&gt;Success - apcupsd&lt;/h3&gt;

&lt;p&gt;Next I found &lt;a href=&#34;http://www.apcupsd.org/&#34;&gt;&lt;code&gt;apcupsd&lt;/code&gt;&lt;/a&gt; (APC UPS Daemon).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get install apcupsd
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;finding-the-device&#34;&gt;Finding the device&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apcupsd.org/manual/manual.html&#34;&gt;The docs&lt;/a&gt; suggested I&amp;rsquo;d find my UPS device listed at &lt;code&gt;/proc/bus/usb/drivers&lt;/code&gt;, and if not the drivers at least at &lt;code&gt;/proc/bus/usb/drivers&lt;/code&gt; - neither of these worked for me:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# cat /proc/bus/usb/devices
cat: /proc/bus/usb/devices: No such file or directory
root@proxmox01:/proc/bus# cat /proc/bus/usb/drivers
cat: /proc/bus/usb/drivers: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But a fortunate &lt;a href=&#34;https://forums.linuxmint.com/viewtopic.php?t=91483&#34;&gt;Google hit&lt;/a&gt; reminded me about &lt;code&gt;lsusb&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# lsusb
[...]
Bus 003 Device 007: ID 051d:0002 American Power Conversion Uninterruptible Power Supply
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there it is. Phew. It&amp;rsquo;s also found using &lt;code&gt;udev&lt;/code&gt; (per the apcupsd documentation - maybe I shouldn&amp;rsquo;t be quite so rude about it):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# udevadm info --attribute-walk --name=/dev/usb/hiddev0
[...]
  looking at parent device &#39;/devices/pci0000:00/0000:00:14.0/usb3/3-2&#39;:
    KERNELS==&amp;quot;3-2&amp;quot;
    SUBSYSTEMS==&amp;quot;usb&amp;quot;
    DRIVERS==&amp;quot;usb&amp;quot;
    ATTRS{bDeviceSubClass}==&amp;quot;00&amp;quot;
    ATTRS{bDeviceProtocol}==&amp;quot;00&amp;quot;
    ATTRS{devpath}==&amp;quot;2&amp;quot;
    ATTRS{idVendor}==&amp;quot;051d&amp;quot;
    ATTRS{speed}==&amp;quot;1.5&amp;quot;
    ATTRS{bNumInterfaces}==&amp;quot; 1&amp;quot;
    ATTRS{bConfigurationValue}==&amp;quot;1&amp;quot;
    ATTRS{bMaxPacketSize0}==&amp;quot;8&amp;quot;
    ATTRS{busnum}==&amp;quot;3&amp;quot;
    ATTRS{devnum}==&amp;quot;7&amp;quot;
    ATTRS{configuration}==&amp;quot;&amp;quot;
    ATTRS{bMaxPower}==&amp;quot;0mA&amp;quot;
    ATTRS{authorized}==&amp;quot;1&amp;quot;
    ATTRS{bmAttributes}==&amp;quot;e0&amp;quot;
    ATTRS{bNumConfigurations}==&amp;quot;1&amp;quot;
    ATTRS{maxchild}==&amp;quot;0&amp;quot;
    ATTRS{bcdDevice}==&amp;quot;0006&amp;quot;
    ATTRS{avoid_reset_quirk}==&amp;quot;0&amp;quot;
    ATTRS{quirks}==&amp;quot;0x0&amp;quot;
    ATTRS{serial}==&amp;quot;4B1517P07342  &amp;quot;
    ATTRS{version}==&amp;quot; 1.10&amp;quot;
    ATTRS{urbnum}==&amp;quot;86&amp;quot;
    ATTRS{ltm_capable}==&amp;quot;no&amp;quot;
    ATTRS{manufacturer}==&amp;quot;American Power Conversion&amp;quot;
    ATTRS{removable}==&amp;quot;removable&amp;quot;
    ATTRS{idProduct}==&amp;quot;0002&amp;quot;
    ATTRS{bDeviceClass}==&amp;quot;00&amp;quot;
    ATTRS{product}==&amp;quot;Back-UPS CS 650 FW:817.v9.I USB FW:v9&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, it&amp;rsquo;s also there under &lt;code&gt;usb-devices&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# usb-devices
[...]
T:  Bus=03 Lev=01 Prnt=01 Port=01 Cnt=02 Dev#=  7 Spd=1.5 MxCh= 0
D:  Ver= 1.10 Cls=00(&amp;gt;ifc ) Sub=00 Prot=00 MxPS= 8 #Cfgs=  1
P:  Vendor=051d ProdID=0002 Rev=00.06
S:  Manufacturer=American Power Conversion
S:  Product=Back-UPS CS 650 FW:817.v9.I USB FW:v9
S:  SerialNumber=4B1517P07342
C:  #Ifs= 1 Cfg#= 1 Atr=e0 MxPwr=0mA
I:  If#= 0 Alt= 0 #EPs= 1 Cls=03(HID  ) Sub=00 Prot=00 Driver=usbhid
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;found-the-device-now-configure-apcupsd&#34;&gt;Found the device, now configure apcupsd&lt;/h4&gt;

&lt;p&gt;In &lt;code&gt;/etc/apcupsd/apcupsd.conf&lt;/code&gt; I set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UPSCABLE usb
UPSTYPE usb
#DEVICE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I started the daemon, which didn&amp;rsquo;t actually run based on the &lt;code&gt;status&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# service apcupsd start
root@proxmox01:/proc/bus# service apcupsd status
● apcupsd.service - LSB: Starts apcupsd daemon
   Loaded: loaded (/etc/init.d/apcupsd)
   Active: active (exited) since Fri 2016-06-17 13:56:25 BST; 28min ago

Jun 17 13:56:25 proxmox01 apcupsd[15897]: Please check your configuration ISCONFIGURED in /etc/default/apcupsd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After editing &lt;code&gt;/etc/default/apcupsd&lt;/code&gt; to set&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ISCONFIGURED=yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The service came up and stayed up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# service apcupsd restart
root@proxmox01:/proc/bus# service apcupsd status
● apcupsd.service - LSB: Starts apcupsd daemon
   Loaded: loaded (/etc/init.d/apcupsd)
   Active: active (running) since Fri 2016-06-17 14:30:01 BST; 1s ago
  Process: 1242 ExecStop=/etc/init.d/apcupsd stop (code=exited, status=0/SUCCESS)
  Process: 1249 ExecStart=/etc/init.d/apcupsd start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/apcupsd.service
           └─1254 /sbin/apcupsd

Jun 17 14:30:01 proxmox01 apcupsd[1249]: Starting UPS power management: apcupsd.
Jun 17 14:30:01 proxmox01 apcupsd[1254]: apcupsd 3.14.12 (29 March 2014) debian startup succeeded
Jun 17 14:30:01 proxmox01 apcupsd[1254]: NIS server startup succeeded
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The successful service start can also be seen in &lt;code&gt;/var/log/apcupsd.events&lt;/code&gt; and &lt;code&gt;/var/log/daemon.log&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;testing-it&#34;&gt;Testing it&lt;/h3&gt;

&lt;p&gt;Now we&amp;rsquo;re up and running, let&amp;rsquo;s &lt;a href=&#34;http://www.apcupsd.org/manual/manual.html#communications-test&#34;&gt;test it&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;disconnect-ups-data-cable-from-server&#34;&gt;Disconnect UPS data cable from Server&lt;/h4&gt;

&lt;p&gt;The unscary one first - disconnect the data comms between UPS and server. It took a few more than the 6 seconds than the doc says, but within a minute or so I got a system beep on the server and :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Broadcast message from root@proxmox01 (somewhere) (Fri Jun 17 14:39:48 2016):

Warning communications lost with UPS proxmox01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;as well as an entry in the &lt;code&gt;/var/log/daemon.log&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-06-17 14:39:48 +0100  Communications with UPS lost.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I plugged the USB back in to my server, and got:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jun 17 14:41:14 proxmox01 apcupsd[1254]: Communications with UPS restored.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;disconnect-ups-from-mains-power&#34;&gt;Disconnect UPS from Mains Power&lt;/h4&gt;

&lt;p&gt;The document entertainingly says:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The first time that you do this, psychologically it won&amp;rsquo;t be easy, but after you have pulled the plug a few times, you may even come to enjoy it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first step is to replace the actual script that would shut the server down if necessary on power failure and battery exhaustion with a dummy one:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mv /etc/apcupsd/apccontrol /etc/apcupsd/apccontrol.bak
cp /usr/share/doc/apcupsd/examples/safe.apccontrol /etc/apcupsd/apccontrol
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then &amp;hellip; pull the plug on the USB from the mains &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Broadcast message from root@proxmox01 (somewhere) (Fri Jun 17 14:49:41 2016):

apccontrol: Warning power loss detected.



Broadcast message from root@proxmox01 (somewhere) (Fri Jun 17 14:49:47 2016):

apccontrol: Power failure. Running on UPS batteries.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Entries in the &lt;code&gt;/var/log/daemon.log&lt;/code&gt;, and my UPS starts beeping too.&lt;/p&gt;

&lt;p&gt;Plugging the UPS back in (phew), and:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Broadcast message from root@proxmox01 (somewhere) (Fri Jun 17 14:51:04 2016):

apccontrol: Off battery. Mains returned.



Broadcast message from root@proxmox01 (somewhere) (Fri Jun 17 14:51:04 2016):

apccontrol: Power has returned...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In real life when we had a powercut the UPS worked perfectly too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jul 18 02:19:00 proxmox01 apcupsd[1254]: Power failure.
Jul 18 02:19:06 proxmox01 apcupsd[1254]: Running on UPS batteries.
Jul 18 02:25:40 proxmox01 apcupsd[1254]: Mains returned. No longer on UPS batteries.
Jul 18 02:25:40 proxmox01 apcupsd[1254]: Power is back. UPS running on mains.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;full-power-outage&#34;&gt;Full Power Outage&lt;/h4&gt;

&lt;p&gt;TODO!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;And yes this is bad. Just like backups being only as good as the last successful restore, a UPS graceful shutdown routine really does need to be tested. Watch this space!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;bonus-data-extract&#34;&gt;Bonus - data extract&lt;/h3&gt;

&lt;p&gt;Because we left &lt;code&gt;NETSERVER&lt;/code&gt; enabled in the config, we can probe the stats of the UPS:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/proc/bus# apcaccess status
APC      : 001,045,1060
DATE     : 2016-06-17 14:30:35 +0100
HOSTNAME : proxmox01
VERSION  : 3.14.12 (29 March 2014) debian
UPSNAME  : proxmox01
CABLE    : USB Cable
DRIVER   : USB UPS Driver
UPSMODE  : Stand Alone
STARTTIME: 2016-06-17 14:30:01 +0100
MODEL    : Back-UPS CS 650
STATUS   : ONLINE
LINEV    : 244.0 Volts
LOADPCT  : 40.0 Percent
BCHARGE  : 100.0 Percent
TIMELEFT : 14.9 Minutes
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I might brew a bash-based extract of this data into InfluxDB to track in Grafana (of course!), or maybe look at &lt;a href=&#34;https://bitbucket.org/snippets/wnasich/7Kg89/telegraf-input-for-apc-ups-status-using&#34;&gt;this&lt;/a&gt; custom &lt;a href=&#34;https://influxdata.com/time-series-platform/telegraf/&#34;&gt;Telegraf&lt;/a&gt; plugin.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Spark sqlContext.read.json - java.io.IOException: No input paths specified in job</title>
			<link>https://rmoff.github.io/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</link>
			<pubDate>Wed, 13 Jul 2016 04:50:16 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</guid>
			<description>&lt;p&gt;Trying to use &lt;a href=&#34;http://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets&#34;&gt;SparkSQL to read a JSON file&lt;/a&gt;, from either pyspark or spark-shell, I got this error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java.io.IOException: No input paths specified in job
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sqlContext.read.json(&amp;quot;/u02/custom/twitter/twitter.json&amp;quot;)
java.io.IOException: No input paths specified in job
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Despite the reference articles that I found using this local path syntax (&lt;code&gt;/u02/custom/twitter/twitter.json&lt;/code&gt;), it turned out that I needed to prefix it with &lt;code&gt;file://&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sqlContext.read.json(&amp;quot;file:///u02/custom/twitter/twitter.json&amp;quot;)
res3: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: string, created_at: string, entities: struct&amp;lt;hashtags:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,media:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string,id:bigint,id_str:string,indices:array&amp;lt;bigint&amp;gt;,media_url:string,media_url_https:string,sizes:struct&amp;lt;large:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,medium:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,small:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,thumb:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;&amp;gt;,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string&amp;gt;&amp;gt;,symbols:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,urls:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string...
scala&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An alternative to &lt;code&gt;file://&lt;/code&gt; is &lt;code&gt;hdfs://&lt;/code&gt;, assuming you have some data residing there too:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; sqlContext.read.json(&amp;quot;hdfs:///user/oracle/incoming/twitter/2016/07/12/FlumeData.1468339844123&amp;quot;)
res5: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: struct&amp;lt;coordinates:array&amp;lt;double&amp;gt;,type:string&amp;gt;, created_at: string, entities: struct&amp;lt;hashtags:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,media:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string,id:bigint,id_str:string,indices:array&amp;lt;bigint&amp;gt;,media_url:string,media_url_https:string,sizes:struct&amp;lt;large:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,medium:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,small:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,thumb:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;&amp;gt;,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string&amp;gt;&amp;gt;,symbols:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,urls:array&amp;lt;struct...
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Proxmox 4 Containers - ssh - ssh_exchange_identification: read: Connection reset by peer</title>
			<link>https://rmoff.github.io/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/</link>
			<pubDate>Tue, 05 Jul 2016 15:20:37 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/</guid>
			<description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; When defining networking on Proxmox 4 LXC containers, use an appropriate CIDR suffix (e.g. 24) - don&amp;rsquo;t use 32!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;On my &lt;a href=&#34;http://rmoff.net/2016/06/07/commissioning-my-proxmox-server/&#34;&gt;Proxmox 4 server&lt;/a&gt; I&amp;rsquo;m running a whole load of lovely LXC containers. Unfortunately, I had trouble connecting to them. From a client machine, I got the error&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh_exchange_identification: read: Connection reset by peer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the server I was connecting to (which I could get a console for through the Proxmox GUI, or a session on using &lt;code&gt;pct enter&lt;/code&gt; from the Proxmox host) I ran a SSHD process with debug to see what was happening:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$(which sshd) -D -ddd -P 2000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which showed a bunch of various errors including one or more of the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fatal: Write failed: Connection reset by peer [preauth]
getpeername failed: Transport endpoint is not connected
get_remote_port failed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It wasn&amp;rsquo;t just SSH that was affected - any inbound network requests to the server failed, with connection reset type messages.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After a bunch of fruitless Googling and dead-ends, I hit upon the problem. One thing that I had found was that if I changed the network configuration for the container from a static IP to DHCP, it worked fine and I could ssh to it. Looking closer at the network configuration, I saw the problem:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/07/proxmox01_-_Proxmox_Virtual_Environment.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By setting the &lt;a href=&#34;https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing&#34;&gt;CIDR&lt;/a&gt; suffix to &lt;strong&gt;32&lt;/strong&gt; it was only routing traffic from that particular IP. Changing it to &lt;strong&gt;24&lt;/strong&gt; (or indeed, DHCP) fixed the problem and traffic began flowing freely.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note - &lt;a href=&#34;https://en.wikipedia.org/wiki/IANAL&#34;&gt;IANANE&lt;/a&gt; (I Am Not A Network Engineer) so the above may be an inaccurate description of the cause/resolution &amp;ndash; but it did well enough for me on my home rig to work!&lt;/em&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Reset Hue password</title>
			<link>https://rmoff.github.io/2016/07/05/reset-hue-password/</link>
			<pubDate>Tue, 05 Jul 2016 13:27:06 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/07/05/reset-hue-password/</guid>
			<description>&lt;p&gt;(&lt;a href=&#34;http://gethue.com/password-management-in-hue/&#34;&gt;Ref&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;The bit that caught me out was this kept failing with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: Password not present 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and a Python stack trace that ended with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subprocess.CalledProcessError: Command &#39;/var/run/cloudera-scm-agent/process/78-hue-HUE_SERVER/altscript.sh sec-1-secret_key&#39; returned non-zero exit status 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The answer (it &lt;em&gt;seems&lt;/em&gt;) is to ensure that &lt;code&gt;HUE_SECRET_KEY&lt;/code&gt; is set (to any value!)&lt;/p&gt;

&lt;p&gt;Launch shell:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HUE_SECRET_KEY=foobar
/opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/hue/build/env/bin/hue shell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Reset password for &lt;code&gt;hue&lt;/code&gt;, activate account and make it superuser&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from django.contrib.auth.models import User
user = User.objects.get(username=&#39;hue&#39;)
user.is_active=True
user.save()
user.is_superuser=True
user.save()
user.set_password(&#39;hue&#39;)
user.save()
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Apache Drill - conflicting jar problem - &#34;No current connection&#34;</title>
			<link>https://rmoff.github.io/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/</link>
			<pubDate>Mon, 20 Jun 2016 19:04:18 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/</guid>
			<description>&lt;p&gt;Vanilla download of Apache Drill 1.6, attempting to follow the Followed the &lt;a href=&#34;https://drill.apache.org/docs/drill-in-10-minutes/&#34;&gt;Drill in 10 Minutes&lt;/a&gt; tutorial - but kept just getting the error &lt;code&gt;No current connection&lt;/code&gt;. Here&amp;rsquo;s an example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[oracle@bigdatalite apache-drill-1.6.0]$ ./bin/drill-embedded
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0
com.fasterxml.jackson.databind.JavaType.isReferenceType()Z
apache drill 1.6.0
&amp;quot;the only truly happy people are children, the creative minority and drill users&amp;quot;
0: jdbc:drill:zk=local&amp;gt; SELECT version FROM sys.version;
No current connection
0: jdbc:drill:zk=local&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whether &lt;code&gt;SELECT version FROM sys.version;&lt;/code&gt; or any other command - same result - &lt;code&gt;No current connection&lt;/code&gt;. Trying to run Drill in distributed mode also failed, with a class error&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.JavaType.isReferenceType()Z
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:400)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:352)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)
        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)
        at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)
        at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:477)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;It turned out that there&amp;rsquo;s a conflicting jar on my machine, as &lt;strong&gt;starting with a clean shell, it worked just fine&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[oracle@bigdatalite ~]$ env -i HOME=&amp;quot;$HOME&amp;quot; LC_CTYPE=&amp;quot;${LC_ALL:-${LC_CTYPE:-$LANG}}&amp;quot; PATH=&amp;quot;$PATH&amp;quot; USER=&amp;quot;$USER&amp;quot; /opt/apache-drill-1.6.0/bin/drill-embedded
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0
Jun 18, 2016 11:01:00 PM org.glassfish.jersey.server.ApplicationHandler initialize
INFO: Initiating Jersey application, version Jersey: 2.8 2014-04-29 01:25:26...
apache drill 1.6.0
&amp;quot;start your sql engine&amp;quot;
0: jdbc:drill:zk=local&amp;gt;
0: jdbc:drill:zk=local&amp;gt;
0: jdbc:drill:zk=local&amp;gt; SELECT version FROM sys.version;
+----------+
| version  |
+----------+
| 1.6.0    |
+----------+
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>ClassNotFoundException with MongoDB-Hadoop in Hive</title>
			<link>https://rmoff.github.io/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/</link>
			<pubDate>Wed, 15 Jun 2016 17:58:19 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/</guid>
			<description>

&lt;p&gt;I wasted &lt;em&gt;literally&lt;/em&gt; two hours on this one, so putting down a note to hopefully help future Googlers.&lt;/p&gt;

&lt;h3 id=&#34;symptom&#34;&gt;Symptom&lt;/h3&gt;

&lt;p&gt;Here&amp;rsquo;s all the various errors that I got in the &lt;code&gt;hive-server2.log&lt;/code&gt; during my attempts to get a &lt;code&gt;CREATE EXTERNABLE TABLE&lt;/code&gt; to work against a MongoDB table in Hive:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Caused by: java.lang.ClassNotFoundException: com.mongodb.hadoop.io.BSONWritable
Caused by: java.lang.ClassNotFoundException: com.mongodb.util.JSON
Caused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson
Caused by: java.lang.ClassNotFoundException: org.bson.io.OutputBuffer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whilst Hive would throw errors along the lines of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/bson/io/OutputBuffer (state=08S01,code=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re using the &lt;a href=&#34;https://github.com/mongodb/mongo-hadoop/&#34;&gt;MongoDB-Hadoop&lt;/a&gt; connector with Hive, you need three JARs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;mongo-java-driver&lt;/li&gt;
&lt;li&gt;mongo-hadoop-core&lt;/li&gt;
&lt;li&gt;mongo-hadoop-hive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The latter two are part of the Mongo-Hadoop package and can be &lt;a href=&#34;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.mongodb.mongo-hadoop%22&#34;&gt;downloaded pre-compiled here&lt;/a&gt;. It&amp;rsquo;s the first one on the list, &lt;code&gt;mongo-java-driver&lt;/code&gt;, that caused me much gnashing of teeth and wailing &amp;ndash; because I mistakenly downloaded &lt;code&gt;mongodb-driver&lt;/code&gt; instead. Stupid me, right? Because to be fair, &lt;a href=&#34;https://github.com/mongodb/mongo-hadoop/wiki/Hive-Usage&#34;&gt;the documentation&lt;/a&gt; does say:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The connector requires at least version 3.0.0 of the driver &amp;ldquo;uber&amp;rdquo; jar (called &amp;ldquo;&lt;strong&gt;mongo-java-driver.jar&lt;/strong&gt;&amp;rdquo;).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(my emphasis)&lt;/p&gt;

&lt;p&gt;But the link to the download leads to &lt;a href=&#34;http://mongodb.github.io/mongo-java-driver/&#34;&gt;http://mongodb.github.io/mongo-java-driver/&lt;/a&gt;, on which &lt;code&gt;mongodb-driver&lt;/code&gt; is the default, not &lt;code&gt;mongo-java-driver&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Hey ho, lesson learnt&amp;hellip;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Erroneous SwapFree on LXC causes problems with CDH install</title>
			<link>https://rmoff.github.io/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/</link>
			<pubDate>Wed, 15 Jun 2016 17:52:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/</guid>
			<description>&lt;p&gt;Installing CDH 5.7 on Linux Containers (LXC) hosted on Proxmox 4. Everything was going well until &lt;strong&gt;Cluster Setup&lt;/strong&gt;, and which point it failed on &lt;strong&gt;Start YARN (MR2 included)&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Completed only 0/1 steps. First failure: Failed to execute command Start on service YARN (MR2 Included)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/cdh-yarn-01-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Log &lt;code&gt;/var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh57-01-node-02.moffatt.me.log.out&lt;/code&gt; showed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.NumberFormatException: For input string: &amp;quot;18446744073709550364&amp;quot;
java.lang.NumberFormatException: For input string: &amp;quot;18446744073709550364&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking down the stack trace, this came from &lt;code&gt;org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin.readProcMemInfoFile&lt;/code&gt;, which the &lt;a href=&#34;http://grepcode.com/file/repo1.maven.org/maven2/org.apache.hadoop/hadoop-yarn-common/0.23.1/org/apache/hadoop/yarn/util/LinuxResourceCalculatorPlugin.java#LinuxResourceCalculatorPlugin.readProcMemInfoFile%28boolean%29&#34;&gt;source code&lt;/a&gt; shows is reading &lt;code&gt;/proc/meminfo&lt;/code&gt;. Looking at this file on each node showed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@cdh57-01-node-02 hadoop-yarn]# cat /proc/meminfo
MemTotal:       24576000 kB
MemFree:        22123008 kB
MemAvailable:   22123008 kB
Buffers:               0 kB
Cached:          1194376 kB
SwapCached:            0 kB
Active:         73536116 kB
Inactive:       21903364 kB
Active(anon):   64138128 kB
Inactive(anon): 11784916 kB
Active(file):    9397988 kB
Inactive(file): 10118448 kB
Unevictable:    26832052 kB
Mlocked:        26832052 kB
SwapTotal:             0 kB
SwapFree:       18446744073709550384 kB
Dirty:              2008 kB
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Erm &amp;hellip; &lt;strong&gt;SwapFree&lt;/strong&gt; is 16 &lt;strong&gt;million petabytes&lt;/strong&gt;???&lt;/p&gt;

&lt;p&gt;In my LXC configuration in Proxmox I&amp;rsquo;d set zero swap, thinking that disabling swap would be a good idea. Evidently not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/cdh-yarn-02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As soon as I updated the container Swap to 128Mb, the SwapFree looked better:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@cdh57-01-node-02 hadoop-yarn]# cat /proc/meminfo
[...]
SwapTotal:        131072 kB
SwapFree:         129840 kB
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To apply this to all the six container nodes, I could have used the Proxmox web GUI, but took advantage of the CLI to save some time with a little bash iteration over the six container IDs (111 to 116) and the &lt;code&gt;pct set&lt;/code&gt; command&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;for i in 11{1..6}; do pct set $i -swap 512;done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To check the value across each node at once, I used &lt;a href=&#34;http://www.rittmanmead.com/2014/12/linux-cluster-sysadmin-parallel-command-execution-with-pdsh/&#34;&gt;pdsh&lt;/a&gt; from my laptop to run the same command on each node directly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rmoff@asgard:~&amp;gt; pdsh -l root -w cdh57-01-node-0[1-6] &amp;quot;grep Swap /proc/meminfo&amp;quot;|sort
cdh57-01-node-01: SwapCached:            0 kB
cdh57-01-node-01: SwapFree:         515496 kB
cdh57-01-node-01: SwapTotal:        524288 kB
cdh57-01-node-02: SwapCached:            0 kB
cdh57-01-node-02: SwapFree:         523056 kB
cdh57-01-node-02: SwapTotal:        524288 kB
cdh57-01-node-03: SwapCached:            0 kB
cdh57-01-node-03: SwapFree:         523476 kB
cdh57-01-node-03: SwapTotal:        524288 kB
cdh57-01-node-04: SwapCached:            0 kB
cdh57-01-node-04: SwapFree:         523760 kB
cdh57-01-node-04: SwapTotal:        524288 kB
cdh57-01-node-05: SwapCached:            0 kB
cdh57-01-node-05: SwapFree:         522272 kB
cdh57-01-node-05: SwapTotal:        524288 kB
cdh57-01-node-06: SwapCached:            0 kB
cdh57-01-node-06: SwapFree:         519456 kB
cdh57-01-node-06: SwapTotal:        524288 kB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the Cloudera Manager &lt;strong&gt;Cluster Setup&lt;/strong&gt; page I then clicked &lt;strong&gt;Retry&lt;/strong&gt; and YARN came up successfully.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Reviving a bricked EdgeRouter Lite (ERL) from a Mac</title>
			<link>https://rmoff.github.io/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/</link>
			<pubDate>Wed, 08 Jun 2016 15:58:30 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/</guid>
			<description>&lt;p&gt;I&amp;rsquo;ve got an &lt;a href=&#34;https://www.ubnt.com/edgemax/edgerouter-lite/&#34;&gt;EdgeRouter LITE&lt;/a&gt; (ERL) which I used as my home router until a powercut fried it a while ago (&lt;a href=&#34;https://community.ubnt.com/t5/EdgeMAX/2nd-Failed-ERLite/m-p/601815&#34;&gt;looks like I&amp;rsquo;m not the only one to have this issue&lt;/a&gt;). The symptoms were it powering on but not giving any DHCP addresses, or after a factory reset responding on the default IP of 192.168.1.1. It was a real shame, because it had been a great bit of kit up until then. I am a complete hack when it comes to networking, and it struck the balance right between letting me do what I needed to do, without overwhelming me with complexity.
I&amp;rsquo;d replaced it with a SonicWall TZ105 but having utterly failed to get the latter to permit OpenVPN traffic (so I can access my home server when on the road), which I had done with no problem on the ERL I thought I&amp;rsquo;d try and resurrect the ERL using &lt;a href=&#34;https://help.ubnt.com/hc/en-us/articles/204959514-EdgeMAX-Last-resort-recovery-of-failed-EdgeOS-device&#34;&gt;the instructions here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I bought a &lt;a href=&#34;https://www.amazon.co.uk/dp/B01A6OS6FK&#34;&gt;RJ45-USB&lt;/a&gt; cable from Amazon, and connected it to &lt;strong&gt;Console&lt;/strong&gt; on the ERL and the USB to my Macbook Pro.&lt;/p&gt;

&lt;p&gt;On my Mac, I determined USB device:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rmoff@asgard:~&amp;gt; ls -l /dev/tty.usbserial*
crw-rw-rw-  1 root  wheel   18,   4  8 Jun 17:01 /dev/tty.usbserial-AI038A4A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the eternally brilliant GNU &lt;code&gt;screen&lt;/code&gt; as serial terminal client from standard Terminal/iTerm on the Mac:
(the 115200 is the &lt;a href=&#34;https://help.ubnt.com/hc/en-us/articles/205202630-EdgeMAX-Connect-to-serial-console-port-default-settings&#34;&gt;baud rate&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;screen /dev/tty.usbserial-AI038A4A 115200
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Powering up the router showed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
SQUASHFS error: zlib_inflate error, data probably corrupt
SQUASHFS error: squashfs_read_data failed to read block 0x29767f3
SQUASHFS error: Unable to read fragment cache entry [29767f3]
SQUASHFS error: Unable to read page, block 29767f3, size b168
SQUASHFS error: Unable to read fragment cache entry [29767f3]
SQUASHFS error: Unable to read page, block 29767f3, size b168
SQUASHFS error: Unable to read fragment cache entry [29767f3]
SQUASHFS error: Unable to read page, block 29767f3, size b168
SQUASHFS error: Unable to read fragment cache entry [29767f3]
SQUASHFS error: Unable to read page, block 29767f3, size b168
SQUASHFS error: Unable to read fragment cache entry [29767f3]
SQUASHFS error: Unable to read page, block 29767f3, size b168
start-stop-daemon: unable to start /usr/sbin/atd (Input/output error)
Starting routing daemon: rib.
Starting EdgeOS router: migrate rl-system configure.

Welcome to EdgeOS ubnt ttyS0

By logging in, accessing, or using the Ubiquiti product, you
acknowledge that you have read and understood the Ubiquiti
License Agreement (available in the Web UI at, by default,
http://192.168.1.1) and agree to be bound by its terms.

ubnt login:

[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And on a second occasion showed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0:0:0:0: [sda] No Caching mode page found
sd 0:0:0:0: [sda] Assuming drive cache: write through
sd 0:0:0:0: [sda] Attached SCSI removable disk
VFS: Cannot open root device &amp;quot;sda2&amp;quot; or unknown-block(8,2): error -17
Please append a correct &amp;quot;root=&amp;quot; boot option; here are the available partitions:
1f00             512 mtdblock0  (driver?)
1f01             512 mtdblock1  (driver?)
1f02              64 mtdblock2  (driver?)
0800         3789504 sda  driver: sd
  0801          145408 sda1 35e60000-01
  0802         1709056 sda2 35e60000-02
Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(8,2)

*** NMI Watchdog interrupt on Core 0x00 ***
        $0      0x0000000000000000      at      0x0000000010000ce0
        v0      0xffffffffc0620000      v1      0x0000000000000000
        a0      0xffffffffc07b0478      a1      0x0000000000000001
        a2      0x0000000000000000      a3      0xffffffffc05f8028
        a4      0x0000000000000000      a5      0x0000000000000002
        a6      0x0000000000000002      a7      0x0000000000000000
        t0      0xffffffffc07c0000      t1      0xffffffffc05f8028
        t2      0xffffffffc07b0000      t3      0x0000000000000006
        s0      0xffffffffc0590000      s1      0xffffffffc058bdd0
        s2      0xffffffffc0590000      s3      0x0000000000000014
        s4      0xffffffffc05df940      s5      0xffffffffc0688a60
        s6      0x0000000000000004      s7      0x0000000000000001
        t8      0x0000000000000000      t9      0x000000000000032b
        k0      0xdab0f2c75931f243      k1      0xd0f77c4765a7af0b
        gp      0xffffffffc05dc000      sp      0xffffffffc05df6f0
        s8      0x0000000862f59974      ra      0xffffffffc04ea078
        err_epc 0xffffffffc00a1070      epc     0xffffffffc007a140
        status  0x0000000010480ce4      cause   0x0000000040808800
        sum0    0x000000f000000000      en0     0x0100000400000000
*** Chip soft reset soon ***
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;a href=&#34;https://community.ubnt.com/t5/EdgeMAX/SQUASHFS-error-and-repair/td-p/892730&#34;&gt;this also looks worth trying&lt;/a&gt;, but means opening up the ERL)&lt;/p&gt;

&lt;p&gt;Started TFTP server on my Mac (&lt;a href=&#34;http://www.barryodonovan.com/2014/11/08/os-x-built-in-tftp-server&#34;&gt;h/t&lt;/a&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo launchctl load -F /System/Library/LaunchDaemons/tftp.plist
sudo launchctl start com.apple.tftpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Downloaded &lt;code&gt;emrk-0.9c.bin&lt;/code&gt; from &lt;a href=&#34;http://packages.vyos.net/tools/emrk/&#34;&gt;http://packages.vyos.net/tools/emrk/&lt;/a&gt; and copied it to the TFTP folder, to be used as a boot image for the ERL to run from across the network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://packages.vyos.net/tools/emrk/0.9c/emrk-0.9c.bin
sudo cp emrk-0.9c.bin /private/tftpboot/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Power up the ERL (or restart it if it&amp;rsquo;s already on) and keep pressing a key in the Console window, which will interupt the boot sequence and land you at the bootloader console:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Looking for valid bootloader image....
Jumping to start of image at address 0xbfc80000


U-Boot 1.1.1 (UBNT Build ID: 4493936-g009d77b) (Build time: Sep 20 2012 - 15:48:51)

BIST check passed.
UBNT_E100 r1:2, r2:14, serial #: DC9FDB282DDB
Core clock: 500 MHz, DDR clock: 266 MHz (532 Mhz data rate)
DRAM:  512 MB
Clearing DRAM....... done
Flash:  4 MB
Net:   octeth0, octeth1, octeth2

USB:   (port 0) scanning bus for devices... 1 USB Devices found
       scanning bus for storage devices...
  Device 0: Vendor:          Prod.: USB DISK 2.0     Rev: PMAP
            Type: Removable Hard Disk
            Capacity: 3700.6 MB = 3.6 GB (7579008 x 512)                                                                                                                           0
Octeon ubnt_e100#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Connect the ERL to your network (I used port 0), and then configure its network details:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Octeon ubnt_e100# set ipaddr 192.168.10.2
Octeon ubnt_e100# set netmask 255.255.255.0
Octeon ubnt_e100# set serverip 192.168.10.79
Octeon ubnt_e100# set gatewayip 192.168.10.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ipaddr&lt;/code&gt; - the IP address to assign to the ERL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;netmask&lt;/code&gt; - &lt;code&gt;255.255.255.0&lt;/code&gt; (I don&amp;rsquo;t profess to be a networking expert; maybe this would vary in some cases?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serverip&lt;/code&gt; - the IP address of the TFTP server (my Mac, in this case)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gatewayip&lt;/code&gt; - the IP address of the router currently in use (not the ERL, but the one acting as your current gateway). &lt;a href=&#34;https://help.ubnt.com/hc/en-us/articles/204959514-EdgeMAX-Last-resort-recovery-of-failed-EdgeOS-device&#34;&gt;According to the doc&lt;/a&gt; this is optional.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then define the boot image to use, and transfer it to the ERL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Octeon ubnt_e100# set bootfile emrk-0.9c.bin
Octeon ubnt_e100# tftpboot
Using octeth0 device
TFTP from server 192.168.10.79; our IP address is 192.168.10.2
Filename &#39;emrk-0.9c.bin&#39;.
Load address: 0x9f00000
Loading: #################################################################
         #############################################
done
Bytes transferred = 15665511 (ef0967 hex), 9502 Kbytes/sec
Octeon ubnt_e100#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, initiate the boot sequence from the newly-transferred image:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Octeon ubnt_e100# bootoctlinux $loadaddr
ELF file is 64 bit
Allocating memory for ELF segment: addr: 0xffffffff81100000 (adjusted to: 0x1100000), size 0xe83940
Allocated memory for ELF segment: addr: 0xffffffff81100000, size 0xe83940
Processing PHDR 0
  Loading e23d80 bytes at ffffffff81100000
  Clearing 5fbc0 bytes at ffffffff81f23d80
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On boot I got some errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Checking system image MD5 sum
sd 0:0:0:0: [sda] Unhandled sense code
sd 0:0:0:0: [sda] Result: hostbyte=0x00 driverbyte=0x08
sd 0:0:0:0: [sda] Sense Key : 0x3 [current]
sd 0:0:0:0: [sda] ASC=0x11 ASCQ=0x0
sd 0:0:0:0: [sda] CDB: cdb[0]=0x28: 28 00 00 12 a4 18 00 00 f0 00
end_request: I/O error, dev sda, sector 1221656
sd 0:0:0:0: [sda] Unhandled sense code
sd 0:0:0:0: [sda] Result: hostbyte=0x00 driverbyte=0x08
sd 0:0:0:0: [sda] Sense Key : 0x3 [current]
sd 0:0:0:0: [sda] ASC=0x11 ASCQ=0x0
sd 0:0:0:0: [sda] CDB: cdb[0]=0x28: 28 00 00 13 d6 60 00 00 f0 00
end_request: I/O error, dev sda, sector 1300064
System image MD5 sum is not correct! Your image may be corrupted.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then a prompt for accepting the licence and whether I wanted to use DHCP, to both of which I answered &lt;code&gt;yes&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;**********************************************
Welcome to EdgeMax Rescue Kit!

This tool is distributed under the terms of
GNU General Public License and other licenses

Brought to you by SO3 Group

[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At the &lt;code&gt;EMRK&amp;gt;&lt;/code&gt; prompt, I opted for the complete reinstall:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;EMRK&amp;gt;emrk-reinstall
WARNING: This script will reinstall EdgeOS from scratch
If you have any usable data on your router storage,
it will be irrecoverably destroyed!
Do you want to continue?
yes or no: yes
Unmounting boot partition
Unmounting root partition
Re-creating partition table
Creating boot partition
Formatting boot partition
mkfs.vfat 3.0.9 (31 Jan 2010)
Creating root partition
Formatting root partition
Mounting boot parition
Mounting root partition
kjournald starting.  Commit interval 5 seconds
EXT3 FS on sda2, internal journal
EXT3-fs: mounted filesystem with writeback data mode.
Enter EdgeOS image url:http://dl.ubnt.com/firmwares/edgemax/v1.8.0/ER-e100.v1.8.0.4853089.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;re then prompted for the EdgeOS image, which you can find on the &lt;a href=&#34;https://www.ubnt.com/download/edgemax/&#34;&gt;Ubiquiti website&lt;/a&gt;. I went for the one matching my router, &lt;strong&gt;ERLite&lt;/strong&gt; (&lt;code&gt;EdgeRouter ERLite-3/ERPoe-5 Firmware v1.8.0&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Unpacking EdgeOS release image
Verifying EdgeOS kernel
Copying EdgeOS kernel to boot partition
Verifying EdgeOS system image
Copying EdgeOS system image to root partition
Copying version file to the root partition
Creating EdgeOS writable data directory
Cleaning up
Installation finished
Please reboot your router
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now enter &lt;code&gt;reboot&lt;/code&gt; to restart your hopefully-healthy ERL!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;EMRK&amp;gt;reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the reboot things looked &lt;em&gt;much&lt;/em&gt; healthier!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scsi 0:0:0:0: Direct-Access              USB DISK 2.0     PMAP PQ: 0 ANSI: 4
sd 0:0:0:0: [sda] 7579008 512-byte logical blocks: (3.88 GB/3.61 GiB)
sd 0:0:0:0: [sda] Write Protect is off
sd 0:0:0:0: [sda] No Caching mode page found
sd 0:0:0:0: [sda] Assuming drive cache: write through
sd 0:0:0:0: [sda] No Caching mode page found
sd 0:0:0:0: [sda] Assuming drive cache: write through
 sda: sda1 sda2
sd 0:0:0:0: [sda] No Caching mode page found
sd 0:0:0:0: [sda] Assuming drive cache: write through
sd 0:0:0:0: [sda] Attached SCSI removable disk
kjournald starting.  Commit interval 3 seconds
EXT3-fs (sda2): using internal journal
EXT3-fs (sda2): mounted filesystem with journal data mode
VFS: Mounted root (unionfs filesystem) on device 0:11.
Freeing unused kernel memory: 288K (ffffffffc0648000 - ffffffffc0690000)
Algorithmics/MIPS FPU Emulator v1.5
INIT: version 2.88 booting
INIT: Entering runlevel: 2
[ ok ] Starting routing daemon: rib nsm ribd.
[ ok ] Starting EdgeOS router: migrate rl-system configure.

Welcome to EdgeOS ubnt ttyS0

By logging in, accessing, or using the Ubiquiti product, you
acknowledge that you have read and understood the Ubiquiti
License Agreement (available in the Web UI at, by default,
http://192.168.1.1) and agree to be bound by its terms.

ubnt login:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;If you accidently close your &lt;code&gt;screen&lt;/code&gt; you&amp;rsquo;ll find you get a &amp;ldquo;Resource Busy&amp;rdquo; and &amp;ldquo;Sorry, could not find a PTY.&amp;rdquo; error. Simply unplug the USB cable from your Mac and then plug it back in, and you&amp;rsquo;ll be good to go again from where you left off&lt;/em&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Running a Docker Container on Proxmox for BitTorrent Sync</title>
			<link>https://rmoff.github.io/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/</link>
			<pubDate>Tue, 07 Jun 2016 21:43:26 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/</guid>
			<description>&lt;p&gt;(&lt;a href=&#34;http://rmoff.net/2016/06/07/a-new-arrival/&#34;&gt;Previously&lt;/a&gt;, &lt;a href=&#34;http://rmoff.net/2016/06/07/commissioning-my-proxmox-server/&#34;&gt;previously&lt;/a&gt;, &lt;a href=&#34;http://rmoff.net/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/&#34;&gt;previously&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Since Proxmox 4 has a recent Linux kernel and mainline one at that, it means that Docker can be run on it. I&amp;rsquo;ve yet to really dig into Docker and work out when it makes sense in place of Linux Containers (LXC), so this is going to be a learning experience for me.&lt;/p&gt;

&lt;p&gt;To install Docker, add Backports repo to apt:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# cat /etc/apt/sources.list.d/backports.list
deb http://ftp.debian.org/debian jessie-backports main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then install:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get update &amp;amp;&amp;amp; apt-get install docker.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once installed, run a test to validate it&amp;rsquo;s all working:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# docker run --rm hello-world

Hello from Docker.
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the &amp;quot;hello-world&amp;quot; image from the Docker Hub.
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker Hub account:
 https://hub.docker.com

For more examples and ideas, visit:
 https://docs.docker.com/engine/userguide/
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://getsync.com&#34;&gt;Resilio Sync&lt;/a&gt; (previously known as BitTorrent Sync) is a Peer-To-Peer (P2P) tool that is a great way for sharing and synchronising folders across machines, both local and remote. Think Dropbox, but without the central &amp;ldquo;Cloud&amp;rdquo; bit. I like using it for sharing VM images particularly, for two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It trickles data down as and when and as fast or slow as the internet connection permits. If your wifi drops, it&amp;rsquo;s no biggie. If someone&amp;rsquo;s transferring data from you, you can reboot your machine without causing everything to fail. Sync just keeps plodding away at transferring the data. Whether transferring between two LAN machines, or LAN to another person&amp;rsquo;s machine, or even up to Amazon S3 (via an EC2 machine running Sync), it&amp;rsquo;s a great tool.&lt;/li&gt;
&lt;li&gt;Because it&amp;rsquo;s P2P, the more people who are sharing a file, the faster you can receive it. If two people have the file you want, and one goes offline, you can still continue to receive it from the other. If both are online an your bandwidth supports it, you can get it twice as fast.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can use it for one-off transfers of single huge files, or just folders of documents that you want to keep in sync.&lt;/p&gt;

&lt;p&gt;BitTorrent|Resilio Sync is nothing to do with the somewhat-infamous BitTorrent, other than similar technology &amp;ndash; which is presumably why they replaced the &amp;lsquo;BitTorrent&amp;rsquo; part of the name.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve run BitTorrent Sync in the past in an OpenVZ container, but thought this would be a good chance to see if Docker was going to be useful for me. I found a &lt;a href=&#34;(https://hub.docker.com/r/bittorrent/sync/)&#34;&gt;Docker image existing for Sync already&lt;/a&gt;, so ran it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rmoff@proxmox01:~$ DATA_FOLDER=/data04/sync
rmoff@proxmox01:~$ mkdir $DATA_FOLDER
rmoff@proxmox01:~$ WEBUI_PORT=8888
rmoff@proxmox01:~$ sudo docker run -d --name Sync   -p $WEBUI_PORT:8888 -p 55555   -v $DATA_FOLDER:/mnt/sync -v /data04:/mnt/mounted_folders/data04  --restart on-failure   bittorrent/sync
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It was as simple that that. I pointed my web browser at port 8888 on my Proxmox server (the Docker host), and it worked perfectly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/Sync___c7415250d7a3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To see what Docker containers are running use &lt;code&gt;ps&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# docker ps
CONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS              PORTS                                              NAMES
b9081e03570b        bittorrent/sync:latest   &amp;quot;/opt/run_sync --log   11 hours ago        Up 11 hours         0.0.0.0:8888-&amp;gt;8888/tcp, 0.0.0.0:32770-&amp;gt;55555/tcp   Sync
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and to terminate one use &lt;code&gt;docker rm&lt;/code&gt; (with &lt;code&gt;-f&lt;/code&gt; if you want to just crash it and get rid)&lt;/p&gt;

&lt;p&gt;So why&amp;rsquo;s that better as a Docker container than a VM? Or a Linux Container (LXC)? Well the VM one is easy - way fewer resources needed on the host machine to run it. Better than a LXC? Not sure yet. On the plus side, it&amp;rsquo;s even more minimalistic than LXC. On the downside&amp;hellip; it&amp;rsquo;s more minimalistic than LXC. This may be my inexperience with Docker, but I like the fact that an LXC I can still SSH into and it&amp;rsquo;s (up to a certain point) still a &amp;ldquo;real&amp;rdquo; server. Another upside to LXC is that Proxmox&amp;rsquo;s web GUI can be used to manage them. I&amp;rsquo;ve yet to dig into whether there are good Web GUIs for Docker.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Importing VMWare and VirtualBox VMs to Proxmox</title>
			<link>https://rmoff.github.io/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/</link>
			<pubDate>Tue, 07 Jun 2016 21:14:26 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/</guid>
			<description>&lt;p&gt;(&lt;a href=&#34;http://rmoff.net/2016/06/07/a-new-arrival/&#34;&gt;Previously&lt;/a&gt;, &lt;a href=&#34;http://rmoff.net/2016/06/07/commissioning-my-proxmox-server/&#34;&gt;previously&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve got a bunch of existing VirtualBox and VMWare VMs that I want to run on Proxmox. Eventually I&amp;rsquo;ll migrate them to containers, but for the time being run them as &amp;ldquo;fat&amp;rdquo; VMs using Proxmox&amp;rsquo;s KVM virtualisation. After copying the OVA files that I had to the server, I uncompressed them:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/data04/vms/bdl44-biwa# cd ../bdl44
root@proxmox01:/data04/vms/bdl44# ll
total 27249328
-rw------- 1 root root 27903306752 Jun  1 10:14 BigDataLite440.ova
root@proxmox01:/data04/vms/bdl44# tar -xf BigDataLite440.ova
root@proxmox01:/data04/vms/bdl44# ll
total 54498668
-rw------- 1 root root  7300486656 Feb 18 21:25 BigDataLite440-disk1.vmdk
-rw------- 1 root root  1261044224 Feb 18 21:26 BigDataLite440-disk2.vmdk
-rw------- 1 root root 19295202816 Feb 18 21:48 BigDataLite440-disk3.vmdk
-rw------- 1 root root    46550528 Feb 18 21:48 BigDataLite440-disk4.vmdk
-rw------- 1 root root 27903306752 Jun  1 10:14 BigDataLite440.ova
-rw------- 1 root root       19619 Feb 18 21:15 BigDataLite440.ovf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then converted each disk image to qcow2 format:
(_You can read more about how and why &lt;a href=&#34;https://www.jamescoyle.net/how-to/1218-upload-ova-to-proxmox-kvm&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://pve.proxmox.com/wiki/Migration_of_servers_to_Proxmox_VE#VMware_to_Proxmox_VE_.28KVM.29&#34;&gt;here&lt;/a&gt;_).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/data04/vms/bdl44# time qemu-img convert -f vmdk BigDataLite440-disk1.vmdk -O qcow2 BigDataLite440-disk1.qcow2

real    2m14.986s
user    1m39.976s
sys     0m23.548s
root@proxmox01:/data04/vms/bdl44# time qemu-img convert -f vmdk BigDataLite440-disk2.vmdk -O qcow2 BigDataLite440-disk2.qcow2

real    0m47.865s
user    0m35.556s
sys     0m3.684s
root@proxmox01:/data04/vms/bdl44# time qemu-img convert -f vmdk BigDataLite440-disk3.vmdk -O qcow2 BigDataLite440-disk3.qcow2

real    5m21.469s
user    3m49.736s
sys     1m0.348s
root@proxmox01:/data04/vms/bdl44# time qemu-img convert -f vmdk BigDataLite440-disk4.vmdk -O qcow2 BigDataLite440-disk4.qcow2

real    0m13.419s
user    0m5.716s
sys     0m0.296s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For re-usability, this will convert all found vmdk files in one fell swoop:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;for file in *.vmdk; do   i=${file##*/};qemu-img convert -f vmdk $i -O qcow2 $(echo $i |sed &#39;s/vmdk/qcow2/g&#39;); done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I then created a new VM using the Proxmox web interface, specifying the disk to match the size of the first disk listed in the &lt;code&gt;OVF&lt;/code&gt; of the unpacked &lt;code&gt;OVA&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;  &amp;lt;DiskSection&amp;gt;
    &amp;lt;Info&amp;gt;List of the virtual disks used in the package&amp;lt;/Info&amp;gt;
    &amp;lt;Disk ovf:capacity=&amp;quot;41943040000&amp;quot; ovf:diskId=&amp;quot;vmdisk2&amp;quot; ovf:fileRef=&amp;quot;file1&amp;quot; ovf:format=&amp;quot;http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized&amp;quot; vbox:uuid=&amp;quot;150dbbe8-0c88-48d0-9fcf-e80d7d7d4c2f&amp;quot;/&amp;gt;
    &amp;lt;Disk ovf:capacity=&amp;quot;104857600000&amp;quot; ovf:diskId=&amp;quot;vmdisk3&amp;quot; ovf:fileRef=&amp;quot;file2&amp;quot; ovf:format=&amp;quot;http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized&amp;quot; vbox:uuid=&amp;quot;64101bef-46af-4e89-8c02-0e6315d6be41&amp;quot;/&amp;gt;
    &amp;lt;Disk ovf:capacity=&amp;quot;62914560000&amp;quot; ovf:diskId=&amp;quot;vmdisk4&amp;quot; ovf:fileRef=&amp;quot;file3&amp;quot; ovf:format=&amp;quot;http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized&amp;quot; vbox:uuid=&amp;quot;d7fae10b-aac3-4675-b295-6a5ab9db3e7f&amp;quot;/&amp;gt;
    &amp;lt;Disk ovf:capacity=&amp;quot;20971520000&amp;quot; ovf:diskId=&amp;quot;vmdisk5&amp;quot; ovf:fileRef=&amp;quot;file4&amp;quot; ovf:format=&amp;quot;http://www.vmware.com/interfaces/specifications/vmdk.html#streamOptimized&amp;quot; vbox:uuid=&amp;quot;e29ccf7f-69f1-4338-ac2d-909344d74f75&amp;quot;/&amp;gt;
  &amp;lt;/DiskSection&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and once created added the three remaining disks to the VM configuration. I removed the CD device so as to be able to sequence the four IDE disks as per the original OVF configuration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/prox01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The config for the server looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/data04/vms/bdl44# cat /etc/pve/nodes/proxmox01/qemu-server/100.conf
bootdisk: ide0
cores: 4
ide0: data01:100/vm-100-disk-1.qcow2,size=40G
ide1: data01:100/vm-100-disk-2.qcow2,size=100G
ide2: data01:100/vm-100-disk-4.qcow2,size=60G
ide3: data01:100/vm-100-disk-5.qcow2,size=20G
memory: 16000
name: bdl44
net0: bridge=vmbr0,e1000=66:65:31:38:36:64
numa: 1
ostype: l26
smbios1: uuid=58e05db8-1bae-4ccf-90fe-9ea036a58056
sockets: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I copied the qcow2 files (converted from VMDK) &lt;strong&gt;over&lt;/strong&gt; the existing qcow2 files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time mv -f /data04/vms/bdl44/BigDataLite440-disk1.qcow2 /data01/images/100/vm-100-disk-1.qcow2
time mv -f /data04/vms/bdl44/BigDataLite440-disk2.qcow2 /data01/images/100/vm-100-disk-2.qcow2
time mv -f /data04/vms/bdl44/BigDataLite440-disk3.qcow2 /data01/images/100/vm-100-disk-3.qcow2
time mv -f /data04/vms/bdl44/BigDataLite440-disk4.qcow2 /data01/images/100/vm-100-disk-4.qcow2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the Proxmox GUI I started the migrated VM, but it failed with the error &lt;code&gt;Unable to resolve &#39;LABEL=oracle_sw&#39;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/prox02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The reason being I&amp;rsquo;d got the disks wrong - look at the above configuration (1,2,4,5) and list of disk images (1-4). After fixing this the VM (a version of Big Data Lite 4.4) started up just fine. As did SampleApp v506:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/prox03.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want you can tidy up by uninstalling the VirtualBox support modules:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo /opt/VBoxGuestAdditions-4.3.22/uninstall.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For reference:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I&amp;rsquo;m not sure if it makes too much difference on the exact sizing of the disk images in the qemu configuration file, since they grow as needed - hence sticking a size of 200G is going to do no harm (unless you actually don&amp;rsquo;t have that disk space) and makes importing the image easier.&lt;/li&gt;
&lt;li&gt;You can update the configuration file to make the disk image names match those that you have, instead of renaming them to match the pattern that qemu generates. Swings &amp;amp; roundabouts.&lt;/li&gt;
&lt;/ol&gt;
</description>
		</item>
		
		<item>
			<title>Commissioning my Proxmox Server - OS and filesystems</title>
			<link>https://rmoff.github.io/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/</link>
			<pubDate>Tue, 07 Jun 2016 21:03:22 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/</guid>
			<description>&lt;p&gt;(&lt;a href=&#34;http://rmoff.net/2016/06/07/a-new-arrival/&#34;&gt;Previously&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;With my server in place, I ran a memtest on it &amp;hellip; which with 128G took a while ;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/IMG_7889.jpg&#34; alt=&#34;memtest&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And then installed &lt;a href=&#34;https://www.proxmox.com/en/&#34;&gt;Proxmox 4&lt;/a&gt;, using a bootable USB that I&amp;rsquo;d created on my Mac from the ISO downloaded from Proxmox&amp;rsquo;s website. To create the bootable USB, create the &lt;code&gt;img&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hdiutil convert -format UDRW -o target.img source.iso
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then burn it to USB:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo dd if=target.img of=/dev/rdiskN bs=1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Replace &lt;strong&gt;&lt;code&gt;N&lt;/code&gt;&lt;/strong&gt; with the correct device based on &lt;code&gt;diskutil list&lt;/code&gt; output. Don&amp;rsquo;t get it wrong, else you&amp;rsquo;ll properly knacker your machine :D&lt;/p&gt;

&lt;p&gt;After booting the machine from the USB the Proxmox installation is a simple click-click-next-next job, after which it reboots and you&amp;rsquo;re good to go. Proxmox is a Debian-based Linux distribution, so you SSH to it as you would any other box, and it also has a GUI at &lt;code&gt;https://&amp;lt;IP&amp;gt;:8006&lt;/code&gt; (note HTTPS not HTTP).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Once the OS was in place, I then set up the filesystems. There&amp;rsquo;s a SDD I&amp;rsquo;m using for the main OS and binaries, along with four 3TB spinning disks. &lt;em&gt;I&amp;rsquo;ve opted to keep them un-raided, since one of the things I&amp;rsquo;m interested in is performance of things like HDFS and Kafka where from my limited understanding the raw spindles are as important as the logical devices that&amp;rsquo;d be presented on top of RAID. Maybe I&amp;rsquo;ll be wrong and have to rebuild the machine in a few months&amp;rsquo; time ;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I formatted each 3TB disk as ext4:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkfs.ext4 /dev/sdb
mkfs.ext4 /dev/sdc
mkfs.ext4 /dev/sdd
mkfs.ext4 /dev/sde
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;I did wonder about looking at other filesystems such as XFS, but figured that the standard mainstream option was probably absolutely fine for my needs. Quite possibly others are &amp;ldquo;better&amp;rdquo;, but as long as there&amp;rsquo;s nothing &amp;ldquo;wrong&amp;rdquo; with this one, it&amp;rsquo;ll do just fine. I&amp;rsquo;ve a long list of topics to investigate once the server&amp;rsquo;s up and running already, without adding FS comparisons to it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Added a label to each one (to make mounting easier, and identifying them if I physically swap them in/out):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;e2label /dev/sdb data01
e2label /dev/sdc data02
e2label /dev/sdd data03
e2label /dev/sde data04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Strangely, only one of the labels showed up:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/# ls -l /dev/disk/by-label/
total 0
lrwxrwxrwx 1 root root  9 Jun  1 09:34 data03 -&amp;gt; ../../sdd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking at the block devices, it seemed &lt;code&gt;sdd&lt;/code&gt; was different from the other three:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:/# lsblk
NAME               MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                  8:0    0 447.1G  0 disk
├─sda1               8:1    0  1007K  0 part
├─sda2               8:2    0   127M  0 part
└─sda3               8:3    0   447G  0 part
  ├─pve-root       251:0    0    96G  0 lvm  /
  ├─pve-swap       251:1    0     8G  0 lvm  [SWAP]
  ├─pve-data_tmeta 251:2    0    84M  0 lvm
  │ └─pve-data     251:4    0   327G  0 lvm
  └─pve-data_tdata 251:3    0   327G  0 lvm
    └─pve-data     251:4    0   327G  0 lvm
sdb                  8:16   0   2.7T  0 disk
└─sdb1               8:17   0   128M  0 part
sdc                  8:32   0   2.7T  0 disk
└─sdc1               8:33   0   128M  0 part
sdd                  8:48   0   2.7T  0 disk
sde                  8:64   0   2.7T  0 disk
└─sde1               8:65   0   128M  0 part
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So a reboot (maybe there&amp;rsquo;s a better way?) fixed this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# lsblk
NAME               MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda                  8:0    0 111.8G  0 disk
├─sda1               8:1    0   200M  0 part
└─sda2               8:2    0 111.5G  0 part
sdb                  8:16   0 447.1G  0 disk
├─sdb1               8:17   0  1007K  0 part
├─sdb2               8:18   0   127M  0 part
└─sdb3               8:19   0   447G  0 part
  ├─pve-root       251:0    0    96G  0 lvm  /
  ├─pve-swap       251:1    0     8G  0 lvm  [SWAP]
  ├─pve-data_tmeta 251:2    0    84M  0 lvm
  │ └─pve-data     251:4    0   327G  0 lvm
  └─pve-data_tdata 251:3    0   327G  0 lvm
    └─pve-data     251:4    0   327G  0 lvm
sdc                  8:32   0   2.7T  0 disk /data01
sdd                  8:48   0   2.7T  0 disk /data02
sde                  8:64   0   2.7T  0 disk /data03
sdf                  8:80   0   2.7T  0 disk /data04

root@proxmox01:~# ls -l /dev/disk/by-label/
total 0
lrwxrwxrwx 1 root root  9 Jun  1 09:49 data01 -&amp;gt; ../../sdc
lrwxrwxrwx 1 root root  9 Jun  1 09:49 data02 -&amp;gt; ../../sdd
lrwxrwxrwx 1 root root  9 Jun  1 09:49 data03 -&amp;gt; ../../sde
lrwxrwxrwx 1 root root  9 Jun  1 09:49 data04 -&amp;gt; ../../sdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Created host folders&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /data01 /data02 /data03 /data04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Added entry to &lt;code&gt;/etc/fstab&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LABEL=data01 /data01 ext4 defaults 0 2
LABEL=data02 /data02 ext4 defaults 0 2
LABEL=data03 /data03 ext4 defaults 0 2
LABEL=data04 /data04 ext4 defaults 0 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another reboot to test the auto-mounting, and it&amp;rsquo;s all good:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;root@proxmox01:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             10M     0   10M   0% /dev
tmpfs            26G  9.6M   26G   1% /run
/dev/dm-0        95G  1.7G   88G   2% /
tmpfs            63G   28M   63G   1% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            63G     0   63G   0% /sys/fs/cgroup
/dev/sdd        2.7T   73M  2.6T   1% /data02
/dev/sde        2.7T   73M  2.6T   1% /data03
/dev/sdc        2.7T   73M  2.6T   1% /data01
/dev/sdf        2.7T   73M  2.6T   1% /data04
tmpfs           100K     0  100K   0% /run/lxcfs/controllers
cgmfs           100K     0  100K   0% /run/cgmanager/fs
/dev/fuse        30M   16K   30M   1% /etc/pve
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;With the disks set up, I installed some important packages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get update
apt-get install screen collectl atop pve-headers-4.4.6-1-pve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set up my &lt;code&gt;.screenrc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; ~/.screenrc &amp;lt;&amp;lt;EOF
hardstatus alwayslastline &amp;quot;%{= RY}%H %{kG}%{G} Screen(s): %{c}%w %=%{kG}%c  %D, %M %d %Y  LD:%l&amp;quot;
startup_message off
msgwait 1
defscrollback 100000
nethack on
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Installed sysdig&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s https://s3.amazonaws.com/download.draios.com/stable/install-sysdig | sudo bash
# Make sure you&#39;ve installed the pve-headers first, and then run this to build the necessary sysdig bit:
sysdig-probe-loader
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>A New Arrival</title>
			<link>https://rmoff.github.io/2016/06/07/a-new-arrival/</link>
			<pubDate>Tue, 07 Jun 2016 20:43:20 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/07/a-new-arrival/</guid>
			<description>&lt;p&gt;After a long and painful delivery, I&amp;rsquo;m delighted to announce the arrival of a new addition to my household &amp;hellip; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/IMG_3813.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This &lt;a href=&#34;https://www.scan.co.uk/3xs/shared/98f6ed5b-7fc4-492c-b66c-3c0e4117dd9c&#34;&gt;custom-build from Scan 3XS&lt;/a&gt; is sat in my study quietly humming away. I&amp;rsquo;m going to use it for hosting VMs for R&amp;amp;D on OBIEE, Big Data Lite, Elastic, InfluxDB, Kafka, etc.
I&amp;rsquo;ll blog various installations that I&amp;rsquo;ve done on it as a reference for myself, and anyone else interested. Which I guess means, myself ;)&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m running &lt;a href=&#34;https://www.proxmox.com/en/&#34;&gt;Proxmox 4&lt;/a&gt; on it, which is a bare-metal hypervisor. The other option I looked at was VMWare ESXi but I went for Proxmox because:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;As well as &amp;lsquo;fat&amp;rsquo; VMs, it supports Linux Containers (LXC), which means I get a lot more bang for my buck in terms of capacity, as well as componentisation of different tools that I run on it&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve run Proxmox 3 on an old laptop for several years previously and found it to be a perfect fit&lt;/li&gt;
&lt;li&gt;VMWare didn&amp;rsquo;t look like it supported containers, and looked like a potentially confusing/complex stack for a single node in a home environment, and heavy on the GUI tools&lt;/li&gt;
&lt;li&gt;Whilst VMWare ESXi offers easier import/export of VMs (eg to VMWare Fusion on a laptop, or of OVAs such as Oracle&amp;rsquo;s SampleApp) it is relatively easy still to do on Proxmox&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Earlier versions of Proxmox used OpenVZ containers and a custom kernel that I sometimes hit issues with not being compatible (e.g. with &lt;a href=&#34;https://github.com/draios/sysdig/issues/415&#34;&gt;sysdig&lt;/a&gt;). Proxmox 4 uses the mainstream Linux Containers (LXC), and also supports Docker which is something I&amp;rsquo;ve yet to get to grips with but I think could be useful.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/06/proxmox01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>New version of BigDataLite VM from Oracle</title>
			<link>https://rmoff.github.io/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/</link>
			<pubDate>Mon, 06 Jun 2016 22:28:25 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/</guid>
			<description>&lt;p&gt;Oracle&amp;rsquo;s excellent &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;Big Data Lite VM&lt;/a&gt; has been updated, to version 4.5.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html#introduction&#34;&gt;Download it here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CDH 5.5 -&amp;gt; 5.7&lt;/li&gt;
&lt;li&gt;Big Data Spatial and Graph 1.1 -&amp;gt; 1.2&lt;/li&gt;
&lt;li&gt;Big Data Discovery 1.1 -&amp;gt; 1.2&lt;/li&gt;
&lt;li&gt;Oracle Big Data Connectors 4.4 -&amp;gt; 4.5&lt;/li&gt;
&lt;li&gt;Oracle NoSQL 3.5 -&amp;gt; 4.0&lt;/li&gt;
&lt;li&gt;GoldenGate 12.2.0.1 -&amp;gt; 12.2.0.1.1&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE 12c blog posts</title>
			<link>https://rmoff.github.io/2016/06/01/obiee-12c-blog-posts/</link>
			<pubDate>Wed, 01 Jun 2016 22:30:14 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/01/obiee-12c-blog-posts/</guid>
			<description>&lt;p&gt;I&amp;rsquo;ve been spending some interesting hours digging into OBIEE 12c recently, with some interesting blog posts to show for it. Some of it is just curiosities discovered along the way, but the real meaty stuff is the in the RESTful APIs - lots of potential here for cool integrations I think&amp;hellip;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rmoff.net/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/&#34;&gt;Lifting the Lid on OBIEE 12c Web Services - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rmoff.net/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/&#34;&gt;Lifting the Lid on OBIEE 12c Web Services - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ritt.md/obiee12c-xsa-dss&#34;&gt;Extended Subject Areas (XSA) and the Data Set Service &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://ritt.md/obi-12c-cache&#34;&gt;Changes in BI Server Cache Behaviour in OBIEE 12c : &lt;code&gt;OBIS_REFRESH_CACHE&lt;/code&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rmoff.net/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/&#34;&gt;Dynamic Naming of OBIEE 12c Service Instance Exports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rmoff.net/2016/05/27/obiee-12c-add-data-source-in-answers/&#34;&gt;OBIEE 12c - &amp;ldquo;Add Data Source&amp;rdquo; in Answers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;(Photo credit: &lt;a href=&#34;https://unsplash.com/@jluebke&#34;&gt;https://unsplash.com/@jluebke&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Presentation Services Logsources in OBIEE 12c</title>
			<link>https://rmoff.github.io/2016/06/01/presentation-services-logsources-in-obiee-12c/</link>
			<pubDate>Wed, 01 Jun 2016 11:03:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/06/01/presentation-services-logsources-in-obiee-12c/</guid>
			<description>&lt;p&gt;Presentation Services can provide some very detailed logs, useful for troubleshooting, performance tracing, and general poking around. &lt;a href=&#34;http://www.rittmanmead.com/2014/11/auditing-obiee-presentation-catalog-activity-with-custom-log-filters/&#34;&gt;See here&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no &lt;code&gt;bi-init.sh&lt;/code&gt; in 12c, so need to set up the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; ourselves:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/oracle/biee/bi/bifoundation/web/bin/:/app/oracle/biee/bi/lib/:/app/oracle/biee/lib/:/app/oracle/biee/bi/bifoundation/odbc/lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run &lt;code&gt;sawserver&lt;/code&gt; with flag to list all log sources&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/app/oracle/biee/bi/bifoundation/web/bin/sawserver -logsources &amp;gt; saw_logsources_12.2.1.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Full list: &lt;a href=&#34;https://gist.github.com/rmoff/e3be9009da6130839c71181cb58509a0&#34;&gt;https://gist.github.com/rmoff/e3be9009da6130839c71181cb58509a0&lt;/a&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Lifting the Lid on OBIEE 12c Web Services - Part 2</title>
			<link>https://rmoff.github.io/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/</link>
			<pubDate>Sat, 28 May 2016 20:30:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/</guid>
			<description>&lt;p&gt;In OBIEE 12c &lt;code&gt;data-model-cmd&lt;/code&gt; is a wrapper for some java code which ultimately calls an internal RESTful web service in OBIEE 12c, &lt;code&gt;bi-lcm&lt;/code&gt;. We saw in the &lt;a href=&#34;http://rmoff.net/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/&#34;&gt;previous post&lt;/a&gt; how these internal web services can be opened up slightly, and we&amp;rsquo;re going to do the same again here. Which means, time for the same caveat:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;With that out of the way, let&amp;rsquo;s go! Firstly, a simple example of downloading the RPD. The way that OBIEE 12c works with RPDs is significantly different from previous versions, since there is now the concept of customisations and layering. You may have noticed &lt;code&gt;liverpd.rpd&lt;/code&gt; or &lt;code&gt;default_diff.xml&lt;/code&gt;, both of which are related to this. The nett result is that there is no longer a single RPD file uploaded to the server which can be downloaded again for editing, as there was in 11g and before. You now need to submit a request to OBIEE to retrieve the latest version of the RPD, which it will pull together on the fly and pass back to you. This is done using &lt;code&gt;data-model-cmd&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /app/oracle/biee/user_projects/domains/bi/bitools/bin/data-model-cmd.sh downloadrpd -O ~/myRPD.rpd -W Admin123 -U weblogic -P Admin123 -SI ssi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This writes a local file, myRPD.rpd. In the background, it&amp;rsquo;s calling &lt;code&gt;bi-lcm&lt;/code&gt; to retrieve the file, as we can see from two diagnostic routes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, my favourite, &lt;code&gt;sysdig&lt;/code&gt;, sniffing any traffic on ports 7780 (managed server public listen port) and 7783 (managed server internal listen port, as used by intra-component REST calls etc):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo sysdig -s 2000 -A -c echo_fds &amp;quot;(fd.port=7780 or fd.port=7783)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This captures the following &lt;code&gt;POST&lt;/code&gt; request:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST /bi-lcm/v1/si/ssi/rpd/downloadrpd HTTP/1.1
Content-Type: application/x-www-form-urlencoded
Authorization: Basic d2VibG9naWM6QWRtaW4xMjM=
User-Agent: Jersey/2.21.1 (HttpUrlConnection 1.8.0_51)
Host: demo.us.oracle.com:7780
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Connection: keep-alive
Content-Length: 24
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Just as useful, in this case, is the &lt;code&gt;bi-lcm&lt;/code&gt; log file itself, &lt;code&gt;DOMAIN_HOME/servers/bi_server1/logs/bi-lcm-rest.log.0&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;47 &amp;gt; POST http://demo.us.oracle.com:7780/bi-lcm/v1/si/ssi/rpd/downloadrpd
47 &amp;gt; Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
47 &amp;gt; Authorization: Basic d2VibG9naWM6QWRtaW4xMjM=
47 &amp;gt; Connection: keep-alive
47 &amp;gt; Content-Length: 24
47 &amp;gt; Content-Type: application/x-www-form-urlencoded
47 &amp;gt; Host: demo.us.oracle.com:7780
47 &amp;gt; User-Agent: Jersey/2.21.1 (HttpUrlConnection 1.8.0_51)
target-password=Admin123
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So the syntax of the RESTful call is pretty simple &amp;ndash; call the &lt;code&gt;downloadrpd&lt;/code&gt; endpoint, passing in the password that is to be used to encrypt the generated RPD. Authentication is done by Basic HTTP.&lt;/p&gt;

&lt;p&gt;What can we do with this? Two things - neither of which are spectacularly useful as such, but additional ways and means to achieve the same. It may be you&amp;rsquo;ve got a bespoke code integration system that needs to integrate with a RESTful endpoint, or simply it&amp;rsquo;s just another useful tool to have at one&amp;rsquo;s disposal. Before we get into it, it&amp;rsquo;s probably time for that caveat again:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://curl.haxx.se/&#34;&gt;&lt;code&gt;cURL&lt;/code&gt;&lt;/a&gt; is a commandline utility for sending HTTP (amongst other) requests. We can use it to request the RPD to be downloaded to any machine on which we can run cURL. Because it adheres to the *nix philosophy of making the output available as a pipe, we can redirect it to a file:  (&lt;em&gt;if you don&amp;rsquo;t, you&amp;rsquo;ll get a screen full of gibberish&amp;hellip;&lt;/em&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/bi-lcm/v1/si/ssi/rpd/downloadrpd&amp;quot; \
--data-urlencode &amp;quot;target-password=Admin123&amp;quot; \
--basic --user weblogic:Admin123 \
&amp;gt; downloadedRPD.rpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I&amp;rsquo;ve used &lt;code&gt;\&lt;/code&gt; line continuation character to break the command over multiple lines simply for legibility.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s pretty clear, hopefully, how to parameterise this for your own use. The server and port are your OBIEE managed server and port (the same that &lt;code&gt;/analytics&lt;/code&gt; is on). The rest are credentials, for the downloaded RPD, and to access the OBIEE system, respectively.&lt;/p&gt;

&lt;p&gt;Taking the same method, but via a web page, we can run this HTML file anywhere we want:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
   &amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;
   &amp;lt;body&amp;gt;
      &amp;lt;FORM action=&amp;quot;http://192.168.56.101:7780/bi-lcm/v1/si/ssi/rpd/downloadrpd&amp;quot;
         enctype=&amp;quot;application/x-www-form-urlencoded&amp;quot;
         method=&amp;quot;post&amp;quot;&amp;gt;
         &amp;lt;P&amp;gt;
           New password for downloaded RPD file? &amp;lt;INPUT type=&amp;quot;text&amp;quot; name=&amp;quot;target-password&amp;quot;&amp;gt;&amp;lt;BR&amp;gt;
            &amp;lt;INPUT type=&amp;quot;submit&amp;quot; value=&amp;quot;Send&amp;quot;&amp;gt; &amp;lt;INPUT type=&amp;quot;reset&amp;quot;&amp;gt;
      &amp;lt;/FORM&amp;gt;
   &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load it into a web browser, enter a password for the generated RPD:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rpd_download_html.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and enter your credentials for OBIEE when prompted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rpddl02.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The RPD file is downloaded as &lt;code&gt;downloadrpd&lt;/code&gt;, which you can rename to whatever you want.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So, we can do &lt;code&gt;downloadrpd&lt;/code&gt; &amp;ndash; what about &lt;code&gt;uploadrpd&lt;/code&gt;? Because that really would be useful for some people, being able to deploy an RPD without needing to dive into the commandline.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the &lt;code&gt;uploadrpd&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ /app/oracle/biee/user_projects/domains/bi/bitools/bin/data-model-cmd.sh \ 
uploadrpd -I /home/oracle/chng1.rpd -W Admin123 -U weblogic -P Admin123 -SI ssi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The resulting POST request looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST http://demo.us.oracle.com:7780/bi-lcm/v1/si/ssi/rpd/uploadrpd
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Authorization: Basic d2VibG9naWM6QWRtaW4xMjM=
Connection: keep-alive
Content-Length: 1221056
Content-Type: multipart/form-data;boundary=Boundary_1_1010953501_1464461679148
Host: demo.us.oracle.com:7780
MIME-Version: 1.0
User-Agent: Jersey/2.21.1 (HttpUrlConnection 1.8.0_51)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The upload has two parts; the RPD password, and the RPD itself.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--Boundary_1_1010953501_1464461679148
Content-Type: text/plain
Content-Disposition: form-data; name=&amp;quot;rpd-password&amp;quot;

Admin123
--Boundary_1_1010953501_1464461679148
Content-Type: application/vnd.oracle.rpd
Content-Disposition: form-data; filename=&amp;quot;myRPD.rpd&amp;quot;; modification-date=&amp;quot;Sat, 28 May 2016 18:53:14 GMT&amp;quot;; size=1220668; name=&amp;quot;file&amp;quot;

[whole bunch of encoded RPD binary file]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;del&gt;But - I couldn&amp;rsquo;t replicate this HTTP request successfully; called manually through Paw, or cURL, or a manual webpage like the above, it would fail with the error:&lt;/del&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oracle.bi.lcm.rest.si.rpd.RpdEndpointV1 SEVERE - Exception during RPD file upload: java.lang.NullPointerException
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;del&gt;Or on other attempts:&lt;/del&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java.net.SocketTimeoutException : Read time out after 30000 millis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;del&gt;My guess (based on ruling out all the other stuff) is it&amp;rsquo;s down to how I was encoding the file and/or specifying its length. A challenge for &lt;a href=&#34;https://oracleus.activeevents.com/2014/connect/fileDownload/session/75F302D730BDB9CA14ADAB3477574D4D/UGF9144_Berg-OOW_Neos_Voyage_2014.pdf&#34;&gt;you Neos&lt;/a&gt; out there to crack?&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/fomin_andrew/status/739189964097851393&#34;&gt;Neo has risen to the challenge&lt;/a&gt;, showing that the force is strong in this one&amp;hellip; Andrew Fomin (&lt;a href=&#34;https://twitter.com/fomin_andrew&#34;&gt;twitter&lt;/a&gt; | &lt;a href=&#34;https://bisoftdiary.com/&#34;&gt;blog&lt;/a&gt;) has figured out the correct syntax in order to use &lt;code&gt;curl&lt;/code&gt; to upload an RPD:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST \
     &amp;quot;http://192.168.1.41:7780/bi-lcm/v1/si/ssi/rpd/uploadrpd&amp;quot; \
     --form &amp;quot;file=@myRPD.rpd;type=application/vnd.oracle.rpd&amp;quot; \
     --form &amp;quot;rpd-password=Password01&amp;quot; \
     --basic --user weblogic:Admin123
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The key bit in this is that the content-type of the upload RPD file is set to &lt;code&gt;application/vnd.oracle.rpd&lt;/code&gt; &amp;ndash; without this the upload fails.&lt;/p&gt;

&lt;p&gt;When run, the JSON payload returned shows success (parsed here through &lt;code&gt;jq&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;clazz&amp;quot;: [
    &amp;quot;rpd-response&amp;quot;
  ],
  &amp;quot;links&amp;quot;: [
    {
      &amp;quot;href&amp;quot;: &amp;quot;http://192.168.1.41:7780/bi-lcm/v1/si/ssi/rpd/uploadrpd&amp;quot;,
      &amp;quot;rel&amp;quot;: [
        &amp;quot;self&amp;quot;
      ]
    }
  ],
  &amp;quot;properties&amp;quot;: {
    &amp;quot;entry&amp;quot;: [
      {
        &amp;quot;key&amp;quot;: &amp;quot;si&amp;quot;,
        &amp;quot;value&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
          &amp;quot;value&amp;quot;: &amp;quot;ssi&amp;quot;
        }
      },
      {
        &amp;quot;key&amp;quot;: &amp;quot;description&amp;quot;,
        &amp;quot;value&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
          &amp;quot;value&amp;quot;: &amp;quot;RPD upload completed successfully.&amp;quot;
        }
      },
      {
        &amp;quot;key&amp;quot;: &amp;quot;desc_code&amp;quot;,
        &amp;quot;value&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
          &amp;quot;value&amp;quot;: &amp;quot;DESC_CODE_RPD_UPLOAD_SUCCESSFUL&amp;quot;
        }
      },
      {
        &amp;quot;key&amp;quot;: &amp;quot;status&amp;quot;,
        &amp;quot;value&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
          &amp;quot;value&amp;quot;: &amp;quot;SUCCESS&amp;quot;
        }
      }
    ]
  },
  &amp;quot;title&amp;quot;: &amp;quot;RPD-LCM response, SI=ssi, action=Upload RPD&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Thanks Andrew!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
		</item>
		
		<item>
			<title>Dynamic Naming of OBIEE 12c Service Instance Exports</title>
			<link>https://rmoff.github.io/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/</link>
			<pubDate>Fri, 27 May 2016 09:13:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/</guid>
			<description>&lt;p&gt;&lt;a href=&#34;http://docs.oracle.com/middleware/1221/biee/BIESG/configrepos.htm#BIESG9314&#34;&gt;&lt;code&gt;exportServiceInstance&lt;/code&gt;&lt;/a&gt; will export the RPD, Presentation Catalog, and Security model (application roles &amp;amp; policies etc &amp;ndash; but &lt;em&gt;not&lt;/em&gt; WLS LDAP) into a single &lt;code&gt;.bar&lt;/code&gt; file, from which they can be imported to another environment, or restored to the same one at a later date (e.g. for backup/restore).&lt;/p&gt;

&lt;p&gt;To run &lt;code&gt;exportServiceInstance&lt;/code&gt; you need to launch WLST first. The following demonstrates how to call it, and embeds the current timestamp &amp;amp; machine details in the backup (useful info, and also makes the backup name unique each time).&lt;/p&gt;

&lt;p&gt;To include the timestamp and hostname in the bar file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time, socket
ts=time.strftime(&#39;%Y%m%dT%H%M%S&#39;,time.localtime())
hostname = socket.gethostname()
ip = socket.gethostbyname(hostname)
exportServiceInstance(&#39;C:/app/oracle/fmw/user_projects/domains/bi/&#39;,&#39;ssi&#39;,&#39;c:/&#39;,(&#39;C:/%s_%s_%s&#39; % (hostname,ip,ts) ))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All in one line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time, socket;ts=time.strftime(&#39;%Y%m%dT%H%M%S&#39;,time.localtime());hostname = socket.gethostname();ip = socket.gethostbyname(hostname);exportServiceInstance(&#39;C:/app/oracle/fmw/user_projects/domains/bi/&#39;,&#39;ssi&#39;,&#39;c:/&#39;,(&#39;C:/%s_%s_%s&#39; % (hostname,ip,ts) ))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On bash, including WLST invocation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/app/oracle/biee/oracle_common/common/bin/wlst.sh &amp;lt;&amp;lt;EOF
import time, socket;ts=time.strftime(&#39;%Y%m%dT%H%M%S&#39;,time.localtime());hostname = socket.gethostname();ip = socket.gethostbyname(hostname);exportServiceInstance(&#39;/app/oracle/biee/user_projects/domains/bi/&#39;,&#39;ssi&#39;,&#39;/home/oracle&#39;,(&#39;/home/oracle/%s_%s_%s&#39; % (hostname,ip,ts) ))
EOF
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE 12c - &#34;Add Data Source&#34; in Answers</title>
			<link>https://rmoff.github.io/2016/05/27/obiee-12c-add-data-source-in-answers/</link>
			<pubDate>Fri, 27 May 2016 08:44:24 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/27/obiee-12c-add-data-source-in-answers/</guid>
			<description>&lt;p&gt;So this had me scratching my head for a good hour today. Comparing SampleApp v511 against a vanilla OBIEE 12c install I&amp;rsquo;d done, one had &amp;ldquo;Add Data Source&amp;rdquo; as an option in Answers, the other didn&amp;rsquo;t. The strange thing was that the option &lt;em&gt;wasn&amp;rsquo;t&lt;/em&gt; there in SampleApp &amp;ndash; and usually that has all the bells and whistles enabled.&lt;/p&gt;

&lt;p&gt;After checking and re-checking the &lt;strong&gt;Manage Privileges&lt;/strong&gt; option, and even the Application Policy grants, and the manual, I hit MoS - and turned up &lt;a href=&#34;https://support.oracle.com/epmos/faces/DocContentDisplay?id=2093886.1&#34;&gt;Doc ID 2093886.1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Apparently, it is a &lt;em&gt;bug&lt;/em&gt; (go figure) that this option shows up when you use the SampleAppLite BAR file, and that the only proper way to upload datasets is through Visual Analyzer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Pasted_Image_27_05_2016__10_30.png&#34; alt=&#34;Add Data Source&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is on version 12.2.1.0.0; perhaps the option will be formally added (or properly removed) in the future.&lt;/p&gt;

&lt;p&gt;For the reasoning behind it being unavailable in Answers, see bug &lt;a href=&#34;https://support.oracle.com/epmos/faces/BugDisplay?id=22347229&#34;&gt;22347229&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>York Fry Ups</title>
			<link>https://rmoff.github.io/2016/05/24/york-fry-ups/</link>
			<pubDate>Tue, 24 May 2016 21:45:09 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/24/york-fry-ups/</guid>
			<description>&lt;p&gt;I had the pleasure of not one but two fry-ups in York, UK last weekend.&lt;/p&gt;

&lt;p&gt;The first was courtesy of &lt;a href=&#34;https://bills-website.co.uk/&#34;&gt;Bill&amp;rsquo;s Restaurant&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/CjDJFSBWYAAt6W3-jpg-large.jpg&#34; alt=&#34;Fry Up at Bill&#39;s Restaurant&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Overall, pretty good, and I&amp;rsquo;ve had much worse. All the ingredients seemed decent. The black pudding was overcooked and almost biscuit-like, but that&amp;rsquo;s my only serious grumble. The bacon was cooked well. That black pudding, beans and the mashed/friend potato thing were each extra charges annoyed me. Particularly with a hangover, I just want to be able to order a full english, without playing &lt;a href=&#34;https://en.wikipedia.org/wiki/Mastermind_(board_game)&#34;&gt;Mastermind&lt;/a&gt; to work out what&amp;rsquo;s in or not.&lt;/p&gt;

&lt;p&gt;Verdict: &lt;sup&gt;6&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;The next day I went to &lt;a href=&#34;http://www.starinnthecity.co.uk/&#34;&gt;Star Inn the City&lt;/a&gt;, where this beauty was served:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/CjDpUPaXIAAiSRV-jpg-large.jpg&#34; alt=&#34;Fry Up at Star Inn the City, York&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My first reaction was that it didn&amp;rsquo;t look like much - but the quantities were perfect, and the quality superb. The black pudding as you can see from the photo was substantial and not dried out. The only thing that let it down was the bacon, which if I had to wager had seen the inside of a microwave, or something else to give it a funny cardboard-like bite. Bonus points for the presence of a fried slice, but unfortunately this one had dried out somewhat so disintegrated on contact with the fork instead of giving that satisfying slight ooze that a proper fried slice does. Everything else, particularly the scrambled egg, was great.&lt;/p&gt;

&lt;p&gt;Verdict: &lt;sup&gt;8&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Lifting the Lid on OBIEE 12c Web Services - Part 1</title>
			<link>https://rmoff.github.io/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/</link>
			<pubDate>Tue, 24 May 2016 21:15:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/</guid>
			<description>

&lt;p&gt;Architecturally, OBIEE 12c is - on the surface - pretty similar to OBIEE 11g. Sure, we&amp;rsquo;ve lost &lt;a href=&#34;https://docs.oracle.com/middleware/1221/biee/BIESG/whatsnew.htm#CJAFBCJC&#34;&gt;OPMN in favour of Node Manager&lt;/a&gt;, but all the old favourites are there - WebLogic Servers, BI Server (nqsserver / OBIS), Presentation Services (sawserver / OBIPS), and so on.&lt;/p&gt;

&lt;p&gt;But, scratch beneath the surface, or have a gander at &lt;a href=&#34;http://www.ioug.org/p/cm/ld/fid=985&amp;amp;tid=743&amp;amp;sid=7207&#34;&gt;slide decks such as this one from BIWA this year&lt;/a&gt;, and you realise that change is afoot. Whilst the OBIEE core is still built around proprietary &amp;lsquo;black box&amp;rsquo; protocols (SAW from analytics to sawserver on port 9710, NQS ODBC from sawserver to nqsserver, cluster management on 9706 to nqsclustercontroller), there are now &lt;a href=&#34;https://en.wikipedia.org/wiki/Representational_state_transfer&#34;&gt;REST-based&lt;/a&gt; web services springing up (in addition to the &lt;a href=&#34;https://docs.oracle.com/middleware/1221/biee/BIEIT/soa_overview.htm#BABHJJAC&#34;&gt;existing SOAP&lt;/a&gt; services that have been there since at least 10g). Whilst the REST services are there under the covers, &lt;strong&gt;they are not documented nor user-servicable&lt;/strong&gt;, but they are there. But let me re-iterate:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So with that caveat out of the way, let&amp;rsquo;s have a poke around. My tool of choice, &lt;a href=&#34;http://www.rittmanmead.com/2016/05/under-the-covers-of-obiee-12c-configuration-with-sysdig/&#34;&gt;as before&lt;/a&gt;, is sysdig. This time I&amp;rsquo;m dumping out at &lt;code&gt;GET&lt;/code&gt; or &lt;code&gt;POST&lt;/code&gt; traffic with the Managed Server (which hosts the bulk of the java deployments, including Visual Analyzer, BI Publisher, and so on). I&amp;rsquo;m using sysdig on the server, because I&amp;rsquo;m also interested to pick up intra-component communications.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo sysdig -s 2000 -A  &amp;quot;fd.port=7780 and (evt.buffer contains GET or evt.buffer contains POST)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To explain the syntax:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-s 2000&lt;/code&gt; : include 2000 bytes of the captured event buffer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-A&lt;/code&gt; : Print ASCII, i.e. human-readable data&lt;/li&gt;
&lt;li&gt;A filter condition made up of two clauses:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fd.port=7780&lt;/code&gt; : traffic on port 7780, on which the Managed Server listens&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(evt.buffer contains GET or evt.buffer contains POST)&lt;/code&gt; : Only include traffic that includes GET or POST. Without this, you get a dump of &lt;em&gt;all&lt;/em&gt; traffic to and from the Managed Server&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let&amp;rsquo;s do something on the front end, and see what catch in our trap&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;dataset-storage-limits&#34;&gt;Dataset Storage Limits&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d already logged into Visual Analyzer, and went to the &lt;strong&gt;Data Sources&lt;/strong&gt; page, which triggered this capture:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;7240765 15:35:40.448761171 3 java (6623) &amp;lt; read res=677 data=
GET /va/api/v1/dataset/limits HTTP/1.1
Host: 192.168.56.101:7780
Connection: keep-alive
Accept: */*
Cache-Control: no-cache
X-CSRF-Token: K9grcZRjBJsfCgd0tmADK54uwvZtXMPeOcTtXqz4itGuyCit
X-Requested-With: XMLHttpRequest
Accept-Language: en-GB,en;q=0.8,en-US;q=0.6
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Postman/4.2.0 Chrome/47.0.2526.73 Electron/0.36.2 Safari/537.36
Content-Type: application/json; charset=UTF-8
Accept-Encoding: gzip, deflate
Cookie: JSESSIONID=ZzzePh4y0-JXAQqQZiSl9RyVKCeD2MFurbj-SQt3BdTLa-ThG3kW!-946065168; JSESSIONID=iUfeNud-YlgVcSWMocIIs18T28ea2ckDbSmwLyQHEylz-d4aKl2H!-946065168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which presumably drives this in the UI:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_storage_01.png&#34; alt=&#34;va_storage_01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we&amp;rsquo;ve got an endpoint (&lt;code&gt;/va/api/v1/dataset/limits&lt;/code&gt;), but can we use it ourselves, just by requesting that address? Yep:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_rest_01.png&#34; alt=&#34;va_rest_01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But what happens if I try it on another machine, or from a clean browsing session? If I do that I just get dumped back to the VA login page. Using Chrome&amp;rsquo;s Developer Tools I can see that I&amp;rsquo;m getting served a &lt;code&gt;302 Moved Temporarily&lt;/code&gt; which tells the browser that the document I&amp;rsquo;ve asked for (&lt;code&gt;limits&lt;/code&gt;) has moved somewhere else - or in plain speak, there&amp;rsquo;s nothing to see here, move along now&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_login_01.png&#34; alt=&#34;va_login_01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So I&amp;rsquo;m guessing my first attempt worked because I was in the same session as my &amp;ldquo;real&amp;rdquo; VA session, and had some cookies that identified me as such, whereas on my re-started browser session I didn&amp;rsquo;t. This is obviously a Good Thing, because we don&amp;rsquo;t want VA giving out information to non-authorised users.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see now what we do actually need in order to call this REST API ourselves. Less so because we want to programatically query the amount of space used by datasets (although that could well be useful), but more because it&amp;rsquo;s a fair bet that the framework will be the same and so with this figured out, the rest of the REST (see what I did there&amp;hellip;) API will be open for business.&lt;/p&gt;

&lt;p&gt;Looking at VA traffic in Chrome&amp;rsquo;s dev tools again, we can see two cookies are submitted as part of a successful call to the &lt;code&gt;limits&lt;/code&gt; API:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_rest_02.png&#34; alt=&#34;va_rest_02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s also a bunch of other headers in here too, such as User Agent. We can&amp;rsquo;t assume that just cookies alone will do the job - for example, from past hacking on OBIEE I&amp;rsquo;ve seen Presentation Services behave differently based on User Agent (presumably to detect whether JavaScript was available?). So here we want to work out the minimum set of headers that we need to send across in order for the request to be valid. Enter &lt;a href=&#34;https://luckymarmot.com/paw&#34;&gt;Paw&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://luckymarmot.com/paw&#34;&gt;Paw&lt;/a&gt; is a REST API client for the Mac, and an awesome one at that. I also tried out &lt;a href=&#34;https://www.getpostman.com/&#34;&gt;Postman&lt;/a&gt;, but Paw &amp;lsquo;just worked&amp;rsquo; and had the better UX for me. Here&amp;rsquo;s a cool trick to transition from session traffic sniffing into full-blown REST API hacking.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Perform the action of interest in Chrome, with Developer Tools capturing the traffic. Make sure the request succeeds (i.e. you have a &amp;ldquo;known good&amp;rdquo; traffic capture).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Right-click on the request and select &lt;strong&gt;Copy as cURL&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_rest_03.png&#34; alt=&#34;va_rest_03.png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[cURL](https://curl.haxx.se/) is a tool in its own right for making HTTP (and other) requests from the command line, and we&#39;ll revisit it later on in this article. For our use here, cURL is sometimes used as a common format of HTTP requests across tools, enabling us to transport it from Chrome to Paw
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Open Paw and select &lt;strong&gt;Edit&lt;/strong&gt; &amp;gt; &lt;strong&gt;Paste and Import&lt;/strong&gt;, selecting (and installing if necessary) &lt;strong&gt;cURL Importer&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now you&amp;rsquo;ve got the HTTP request in Paw you can work with it in a great more detail. First up, go to the &lt;strong&gt;Request&lt;/strong&gt; menu and hit &lt;strong&gt;Send&lt;/strong&gt;. You should get a successful response - because this is the same request as you fired from Chrome. That it&amp;rsquo;s a different program sending it makes no difference - it&amp;rsquo;s only the HTTP request headers and body that actually get sent to and parsed by the receiving web server. Note how Paw automagically presents the response as formatted JSON:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/paw01.png&#34; alt=&#34;paw01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we can start whittling down the base request that we can send in order for it still to be valid. Firstly untick &lt;em&gt;every header&lt;/em&gt;, and resend the request. You&amp;rsquo;ll see the HTML source code for the &amp;ldquo;302 Moved Temporarily&amp;rdquo; response, which if you followed the URL would give you the VA login page&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/paw02-1.png&#34; alt=&#34;paw02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So, we definitely need some headers. Let&amp;rsquo;s add back in the Cookie header; it still works. Now looking at the Cookies, there are two; &lt;code&gt;ADMINCONSOLESESSION&lt;/code&gt; and &lt;code&gt;JSESSIONID&lt;/code&gt;. Trying the request with just &lt;code&gt;JSESSIONID&lt;/code&gt; still works. Bingo! So we&amp;rsquo;ve got a minimum viable REST request - it just needs a valid &lt;code&gt;JSESSIONID&lt;/code&gt; cookie.&lt;/p&gt;

&lt;p&gt;Now in Paw go to the &lt;strong&gt;View&lt;/strong&gt; menu and select &lt;strong&gt;Show Code Generator&lt;/strong&gt;. You might need to install another addin here, but you can then see the cURL equivalent for the current request:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/paw03-1.png&#34; alt=&#34;paw03.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Taking this cURL and running it from the terminal gives&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rest01.png&#34; alt=&#34;rest01.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;an error?! Wat? And what&amp;rsquo;s &amp;ldquo;event not found&amp;rdquo;? &lt;a href=&#34;https://en.wikipedia.org/wiki/Representational_state_transfer&#34;&gt;Bash quoting&lt;/a&gt; fun times &amp;hellip; replace the double quotes (which bash will parse the contents of and get upset at the &lt;code&gt;!&lt;/code&gt;) with single quotes (bash &lt;a href=&#34;https://www.youtube.com/watch?v=2eMkth8FWno&#34;&gt;shall not pass&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rest02.png&#34; alt=&#34;rest02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Job&amp;rsquo;s a good &amp;lsquo;un. And for bonus points, let&amp;rsquo;s use the excellent  &lt;a href=&#34;https://stedolan.github.io/jq/&#34;&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt; to format the JSON:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rest03-1.png&#34; alt=&#34;rest03.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;or even parse out specific values from it (&lt;em&gt;use &lt;a href=&#34;https://jqplay.org/&#34;&gt;jqplay&lt;/a&gt; to help figure out the syntax of filters&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rest04.png&#34; alt=&#34;rest04.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And why&amp;rsquo;s this useful? Because we can now programatically do things with the data that we can access. Like, check a user&amp;rsquo;s remaining quota:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &#39;User has &#39; $(curl -s -X &#39;GET&#39; &#39;http://192.168.56.101:7780/va/api/v1/dataset/limits&#39; -H &#39;Cookie: JSESSIONID=Dp3h6i4FcuIfoTYAYsvb8Z8TYDOcnuxDl52KVdRREYlNfn8w1D49!-946065168&#39; | jq &#39;.limits.&amp;quot;user-remaining-quota-kilobytes&amp;quot;&#39;) &#39; KB remaining in their quota&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/rest05.png&#34; alt=&#34;rest05.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Sure, you need the user&amp;rsquo;s session cookie to literally do this, but it still gives you an idea of what&amp;rsquo;s possible. A final mention for now for Paw &amp;ndash; as well as exporting to cURL, you can generate code to many formats, not least including Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Install the Python Requests library:
# `pip install requests`

import requests


def send_request():
    # cURL
    # GET http://192.168.56.101:7780/va/api/v1/dataset/limits

    try:
        response = requests.get(
            url=&amp;quot;http://192.168.56.101:7780/va/api/v1/dataset/limits&amp;quot;,
            headers={
                &amp;quot;Cookie&amp;quot;: &amp;quot;JSESSIONID=hR7h7JkB-rTZq-QiYtTfZx6kYe-bpWoeSH9xxhv-2P1J1W6ZWQJk!-946065168&amp;quot;,
            },
        )
        print(&#39;Response HTTP Status Code: {status_code}&#39;.format(
            status_code=response.status_code))
        print(&#39;Response HTTP Response Body: {content}&#39;.format(
            content=response.content))
    except requests.exceptions.RequestException:
        print(&#39;HTTP Request failed&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;getting-a-session-cookie&#34;&gt;Getting a Session Cookie&lt;/h3&gt;

&lt;p&gt;So all we need to call the &lt;code&gt;va&lt;/code&gt; web service is a valid &lt;code&gt;JSESSIONID&lt;/code&gt; cookie, and then the world is our oyster &amp;hellip; but how do we get one in the first place?&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve got to assume that it gets set as part of the authentication process when we login to VA. Let&amp;rsquo;s stick Chrome Dev Tools on and see what we get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/va_login_02.png&#34; alt=&#34;va_login_02.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see from this the URL that gets called, how it&amp;rsquo;s called (HTTP &lt;code&gt;POST&lt;/code&gt;), the format of the body with the username/password we&amp;rsquo;re logging in as &amp;ndash; and the all-important response header that gives us our &lt;code&gt;JSESSIONID&lt;/code&gt; value.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;As a side note, observe that the credentials are in plain text. This is why TLS/SSL is so important in general on the internet, because otherwise anyone on the network can observe these (assuming they can access the network traffic)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Using the same trick as above (Copy as cURL from Chrome, import to Paw), I can run the same request in Paw:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/paw04.png&#34; alt=&#34;paw04.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Clicking on the &lt;strong&gt;Headers&lt;/strong&gt; option in the response pane shows the cookie being set:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/paw05-2.png&#34; alt=&#34;paw05.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using the Paw Code Generator, I can run this as a cURL command - but by default cURL won&amp;rsquo;t give me the cookie:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/va/j_security_check&amp;quot; -H &amp;quot;Content-Type: application/x-www-form-urlencoded; charset=utf-8&amp;quot; --data-urlencode &amp;quot;j_password=Admin123&amp;quot; --data-urlencode &amp;quot;j_username=prodney&amp;quot;
&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;303 See Other&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
&amp;lt;body bgcolor=&amp;quot;#FFFFFF&amp;quot;&amp;gt;
&amp;lt;p&amp;gt;This document you requested has moved
temporarily.&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;It&#39;s now at &amp;lt;a href=&amp;quot;http://192.168.56.101:7780/va&amp;quot;&amp;gt;http://192.168.56.101:7780/va&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So I force it to, using the &lt;code&gt;--cookie-jar&lt;/code&gt; option and a clever bash trick called &lt;a href=&#34;http://tldp.org/LDP/abs/html/process-sub.html&#34;&gt;process substitution&lt;/a&gt; so that it writes to stdout instead of the file (as would usually be used for the cookie jar).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/va/j_security_check&amp;quot; --data-urlencode &amp;quot;j_password=Admin123&amp;quot; --data-urlencode &amp;quot;j_username=prodney&amp;quot; --cookie-jar &amp;gt;(cat)
&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;303 See Other&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
&amp;lt;body bgcolor=&amp;quot;#FFFFFF&amp;quot;&amp;gt;
&amp;lt;p&amp;gt;This document you requested has moved
temporarily.&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;It&#39;s now at &amp;lt;a href=&amp;quot;http://192.168.56.101:7780/va&amp;quot;&amp;gt;http://192.168.56.101:7780/va&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
# Netscape HTTP Cookie File
# http://curl.haxx.se/docs/http-cookies.html
# This file was generated by libcurl! Edit at your own risk.

#HttpOnly_192.168.56.101	FALSE	/va	FALSE	0	JSESSIONID	XLjkp1KgOt8KBTFr1cLz-cXUaAf32yRESzk1r6yr0Yq9aE4IrODd!-946065168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Still a bit noisy, so let&amp;rsquo;s add &lt;code&gt;-s&lt;/code&gt; (silent - suppresses the progress bar which otherwise gets echo&amp;rsquo;d ) and &lt;code&gt;-o /dev/null&lt;/code&gt; (send the main output to /dev/null)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/va/j_security_check&amp;quot; \ 
--data-urlencode &amp;quot;j_password=Admin123&amp;quot; --data-urlencode &amp;quot;j_username=prodney&amp;quot; \ 
--cookie-jar &amp;gt;(cat) -o /dev/null -s
# Netscape HTTP Cookie File
# http://curl.haxx.se/docs/http-cookies.html
# This file was generated by libcurl! Edit at your own risk.

#HttpOnly_192.168.56.101	FALSE	/va	FALSE	0	JSESSIONID	-QrkqNASv-4aKk8UUvtD5CWBvXT4qgTnx50QhdLU8VgG8vBUiVI9!-946065168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s round off with a bit more bash-magic, to capture just the value of &lt;code&gt;JSESSIONID&lt;/code&gt; using &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/va/j_security_check&amp;quot; \ 
--data-urlencode &amp;quot;j_password=Admin123&amp;quot; --data-urlencode &amp;quot;j_username=prodney&amp;quot; \
--cookie-jar &amp;gt;(cat) -o /dev/null -s|grep JSESSIONID|awk &#39;{print $7}&#39;
EePkqTVa7OCVqO7h5j-4FPfkNnyVTitdZNIDU4SSiWJq0q3AKSh5!-946065168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final party trick - using &lt;a href=&#34;http://www.tldp.org/LDP/abs/html/commandsub.html&#34;&gt;command substitution&lt;/a&gt; and combining it with the the &lt;code&gt;limits&lt;/code&gt; call above, passing the dynamically-obtained session cookie:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;USER=$1
PW=$2
JSESSIONID=$(curl -X &amp;quot;POST&amp;quot; &amp;quot;http://192.168.56.101:7780/va/j_security_check&amp;quot; --data-urlencode &amp;quot;j_password=$PW&amp;quot; --data-urlencode &amp;quot;j_username=$USER&amp;quot; --cookie-jar &amp;gt;(cat) -o /dev/null -s|grep JSESSIONID|awk &#39;{print $7}&#39;)
echo &amp;quot;Session cookie for $USER is $JSESSIONID&amp;quot;

echo &#39;User has &#39; $(curl -s -X &#39;GET&#39; &#39;http://192.168.56.101:7780/va/api/v1/dataset/limits&#39; -H &amp;quot;Cookie: JSESSIONID=$JSESSIONID&amp;quot; | jq &#39;.limits.&amp;quot;user-remaining-quota-kilobytes&amp;quot;&#39;) &#39; KB remaining in their quota&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./tmp.sh prodney Admin123
Session cookie for prodney is f0Pkyfb_sVxDzjIGL8NlZHtS2XY-cDO9Hy5m81NUcdnemxElukYC!-946065168
User has  20955017  KB remaining in their quota
$ ./tmp.sh weblogic Admin123
Session cookie for weblogic is QsLkzFahsLcJDMZbeoruCN1xBG6XmYdxKAi9UmDANju7YVTQwAZ5!-946065168
User has  20971462  KB remaining in their quota
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So there we have it - a flavour of what the REST web services in OBIEE 12c can do, and how to go about accessing them. Next time we&amp;rsquo;ll dig a bit more into VA, and uncover the Data Set Service (&lt;code&gt;datasetsvc&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;And did I mention yet &amp;hellip; :&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle. This article is purely for geek interest. Using undocumented APIs leaves you at risk of the API changing at any time.&lt;/strong&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Kibana Timelion - Series Calculations - Difference from One Week Ago</title>
			<link>https://rmoff.github.io/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/</link>
			<pubDate>Mon, 23 May 2016 09:46:28 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/</guid>
			<description>&lt;p&gt;I wrote recently about &lt;a href=&#34;http://rmoff.net/2016/03/29/experiments-with-kibana-timelion-2/&#34;&gt;Kibana&amp;rsquo;s excellent Timelion feature&lt;/a&gt;, which brings time-series visualisations to Kibana. In the comments Ben Huang asked:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;do you know how to show whats the difference between this Friday and last Friday by Timelion?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So I thought I&amp;rsquo;d answer properly here.&lt;/p&gt;

&lt;p&gt;Timelion includes mathematical functions including &lt;code&gt;add&lt;/code&gt; and &lt;code&gt;subtract&lt;/code&gt;, as well as the ability to show data &lt;code&gt;offset&lt;/code&gt; by an amount of time. So to answer Ben&amp;rsquo;s query, we combine the two.&lt;/p&gt;

&lt;p&gt;First, our starter series, simply showing a count of all documents across all indices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s check out the &lt;code&gt;offset&lt;/code&gt; function, showing the same data but for the previous week:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(offset=-1w)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can combine the two on the same chart:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(*),.es(offset=-1w)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And then we can subtract one from the other:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es().subtract(.es(offset=-1w))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tarting it up a bit, we can show all three series, adding &lt;code&gt;label&lt;/code&gt; for each, and formatting the difference series as bars instead of lines clearly to identify it better:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es().label(&amp;quot;Original&amp;quot;),.es(offset=-1w).label(&amp;quot;One week offset&amp;quot;),.es().subtract(.es(offset=-1w)).label(&amp;quot;Difference&amp;quot;).bars()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mucking about with the &lt;code&gt;lines&lt;/code&gt; syntax, setting a &lt;code&gt;fill&lt;/code&gt; and zero-&lt;code&gt;width&lt;/code&gt; lines, we can show bars but with width of each data point (1 day):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es().label(&amp;quot;Original&amp;quot;),.es(offset=-1w).label(&amp;quot;One week offset&amp;quot;),.es().subtract(.es(offset=-1w)).label(&amp;quot;Difference&amp;quot;).lines(steps=1,fill=2,width=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Timelion_-_Kibana-5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So there you have it - the difference calculation between two time points in Timelion, with a bit of formatting fun thrown in for a bonus.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE 12c hangs at startup - Starting AdminServer ...</title>
			<link>https://rmoff.github.io/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../</link>
			<pubDate>Fri, 20 May 2016 14:22:21 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../</guid>
			<description>&lt;p&gt;Running the OBIEE 12c startup on Windows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\app\oracle\fmw\user_projects\domains\bi\bitools\bin\start.cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Just hangs at:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Starting AdminServer ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No CPU being consumed, very odd. But then &amp;hellip; looking at &lt;code&gt;DOMAIN_HOME\servers\AdminServer\logs\AdminServer.out&lt;/code&gt; shows the last log entry was:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Enter username to boot WebLogic server:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s bad news, cos that&amp;rsquo;s an interactive prompt, but not echo&amp;rsquo;d to the console output of the startup command, and there&amp;rsquo;s no way to interact with it.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;start.cmd&lt;/code&gt; was being called by adding it to the Startup folder (&lt;code&gt;C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp&lt;/code&gt;), and I guess it was something about this that stopped the prompt coming back to the console, because when I ran it manually from the command prompt, I got this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C:\Users\Administrator&amp;gt;C:\app\oracle\fmw\user_projects\domains\bi\bitools\bin\start.cmd
BI_PRODUCT_HOME set as C:\app\oracle\fmw\bi\
[...]
Requesting credentials ...
Enter Weblogic login details at prompt
Weblogic Username: 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If I entered the credentials, the AdminServer still failed to start, appearing to already be running.&lt;/p&gt;

&lt;p&gt;So why was it even prompting for the credentials in the first place? This was a server that had booted just fine previously.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;start.cmd&lt;/code&gt; is a wrapper for a Jython WLST script which uses Node Manager to start up all the necessary components. Taking the &lt;code&gt;startWebLogic.cmd&lt;/code&gt; and running it independently works fine, and the AdminServer comes up with no boot credentials required. But, from &lt;code&gt;start.cmd&lt;/code&gt; the whole process stalls at the interactive prompt for credentials. Weird.&lt;/p&gt;

&lt;p&gt;Digging around a bit shows that &lt;code&gt;start.cmd&lt;/code&gt; passes an argument to the AdminServer that we can guess is relevant to the problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-Dweblogic.system.BootIdentityFile=
C:\app\oracle\fmw\user_projects\domains\bi\servers\AdminServer\data\nodemanager\boot.properties
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loading this file showed an AES-encrypted username/password as you&amp;rsquo;d expected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Fri May 20 15:53:35 UTC 2016
password={AES}7zL9O1AP+5yVmpG1t71wu22m1VCBPyixC9tg8H78m+A\=
username={AES}IIqpK/FkTSV0CKPdigpMuaI0ECplaqv5Oplv9AoDRWM\=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the &lt;code&gt;{AES}&lt;/code&gt; prefix. If you remove that you can replace it with plain text values. On a guess that maybe the credentials are invalid or corrupted, I reset them:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Fri May 20 15:53:35 UTC 2016
password=Password01
username=weblogic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After doing this, the stack came up just fine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; this is just hacking around on a test server &amp;ndash; I wouldn&amp;rsquo;t recommend changing files that aren&amp;rsquo;t meant to be changed unless you&amp;rsquo;re sure it&amp;rsquo;s the answer, otherwise you can just end up compounding problems&amp;hellip;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>oracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.</title>
			<link>https://rmoff.github.io/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./</link>
			<pubDate>Thu, 19 May 2016 10:06:03 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./</guid>
			<description>&lt;p&gt;Another quick note on OBIEE 12c, this time on the &lt;a href=&#34;https://docs.oracle.com/middleware/1221/biee/BIESG/configrepos.htm#BIESG9316&#34;&gt;importServiceInstance&lt;/a&gt; command. If you run it with a BAR file that doesn&amp;rsquo;t exist, it&amp;rsquo;ll fail (obviously), but the error at the end of the stack trace is slightly confusing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;oracle.bi.bar.exceptions.UnSupportedBarException: 
The Bar file provided as input is not supported in this BI Platfrom release.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scrolling back up the stack trace does show the error message:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SEVERE: Failed in reading bar file. [...]
java.io.FileNotFoundException: [...] 
(The system cannot find the file specified)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &amp;hellip; RTEM (Read the Fantastic Error Message) in full, don&amp;rsquo;t just skim to the end&amp;hellip;&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;PS C:\OracleBI-BVT&amp;gt; C:\app\oracle\fmw\oracle_common\common\bin\wlst.cmd

Initializing WebLogic Scripting Tool (WLST) ...

Welcome to WebLogic Server Administration Scripting Shell

Type help() for help on available commands

wls:/offline&amp;gt; importServiceInstance(&#39;C:/app/oracle/fmw/user_projects/domains/bi/&#39;,&#39;ssi&#39;,&#39;C:/app/oracle/fmw/bi/bifoundation/samples/sampleapplite/SampleAppLite.bar&#39;)
Starting Import Service Instance
May 19, 2016 10:54:05 AM oracle.bi.bar.util.DomainIntrospectUtils obtainSingletonDataDirectory
INFO: Obtained Singleton Data Directory as C:\app\oracle\fmw\user_projects\domains\bi\bidata
May 19, 2016 10:54:05 AM oracle.bi.bar.framework.ConfigFileUtility getConfigProperty
INFO: property value found in config file: ALL
May 19, 2016 10:54:05 AM oracle.bi.bar.framework.ConfigFileUtility getConfigProperty
INFO: property value found in config file: 100000
May 19, 2016 10:54:05 AM oracle.bi.bar.framework.ConfigFileUtility getConfigProperty
INFO: property value found in config file: 500000
May 19, 2016 10:54:05 AM oracle.bi.bar.framework.ConfigFileUtility getConfigProperty
INFO: property value found in config file: V1
May 19, 2016 10:54:06 AM oracle.bi.bar.si.ServiceInstanceLifeCycleFactory getServiceInstanceLifeCycleImpl
INFO: Service Instance lifecyle impl version used:V1
May 19, 2016 10:54:06 AM oracle.bi.bar.log.LogUtils doesLogHandlerExist
INFO: Path from ODL Handler = C:\app\oracle\fmw\user_projects\domains\bi\bilogs\service_instances\ssi\metadata\si20160519_105405.log
May 19, 2016 10:54:06 AM oracle.bi.bar.log.LogUtils doesLogHandlerExist
INFO: Found odl handler logging to log file C:\app\oracle\fmw\user_projects\domains\bi\bilogs\service_instances\ssi\metadata\si20160519_105405.log
May 19, 2016 10:54:06 AM oracle.bi.bar.util.ValidateFileTypeUtil checkBarFile
SEVERE: Failed in reading bar file. C:\app\oracle\fmw\bi\bifoundation\samples\sampleapplite\SampleAppLite.bar
java.io.FileNotFoundException: C:\app\oracle\fmw\bi\bifoundation\samples\sampleapplite\SampleAppLite.bar (The system cannot find the file specified)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.&amp;lt;init&amp;gt;(FileInputStream.java:138)
        at oracle.bi.bar.util.ValidateFileTypeUtil.checkBarFile(ValidateFileTypeUtil.java:50)
        at oracle.bi.bar.si.ServiceInstanceLifeCycleImpl.importServiceInstance(ServiceInstanceLifeCycleImpl.java:240)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.python.core.PyReflectedFunction.__call__(Unknown Source)
        at org.python.core.PyMethod.__call__(Unknown Source)
        at org.python.core.PyObject.__call__(Unknown Source)
        at org.python.core.PyInstance.invoke(Unknown Source)
        at org.python.pycode._pyx133.importServiceInstance$3(/C:/app/oracle/fmw/bi/lib/bi-bar.jar!/wlstScriptDir/ServiceInstanceLifeCycle.py:63)
        at org.python.pycode._pyx133.call_function(/C:/app/oracle/fmw/bi/lib/bi-bar.jar!/wlstScriptDir/ServiceInstanceLifeCycle.py)
        at org.python.core.PyTableCode.call(Unknown Source)
        at org.python.core.PyTableCode.call(Unknown Source)
        at org.python.core.PyTableCode.call(Unknown Source)
        at org.python.core.PyFunction.__call__(Unknown Source)
        at org.python.pycode._pyx140.f$0(&amp;lt;console&amp;gt;:1)
        at org.python.pycode._pyx140.call_function(&amp;lt;console&amp;gt;)
        at org.python.core.PyTableCode.call(Unknown Source)
        at org.python.core.PyCode.call(Unknown Source)
        at org.python.core.Py.runCode(Unknown Source)
        at org.python.core.Py.exec(Unknown Source)
        at org.python.util.PythonInterpreter.exec(Unknown Source)
        at org.python.util.InteractiveInterpreter.runcode(Unknown Source)
        at org.python.util.InteractiveInterpreter.runsource(Unknown Source)
        at org.python.util.InteractiveInterpreter.runsource(Unknown Source)
        at weblogic.management.scripting.utils.WLSTInterpreter.runsource(WLSTInterpreter.java:1093)
        at weblogic.management.scripting.WLST.main(WLST.java:227)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at weblogic.WLST.main(WLST.java:47)

May 19, 2016 10:54:06 AM oracle.bi.bar.log.LogUtils removeServiceInstanceODLHandler
INFO: Path from ODL Handler = C:\app\oracle\fmw\user_projects\domains\bi\bilogs\service_instances\ssi\metadata\si20160519_105405.log
May 19, 2016 10:54:06 AM oracle.bi.bar.log.LogUtils removeServiceInstanceODLHandler
INFO: Removing odl handler after completion of the run for log file C:\app\oracle\fmw\user_projects\domains\bi\bilogs\service_instances\ssi\metadata\si20160519_105405.log
Traceback (innermost last):
  File &amp;quot;&amp;lt;console&amp;gt;&amp;quot;, line 1, in ?
  File &amp;quot;/C:/app/oracle/fmw/bi/lib/bi-bar.jar!/wlstScriptDir/ServiceInstanceLifeCycle.py&amp;quot;, line 63, in importServiceInstance
        at oracle.bi.bar.si.ServiceInstanceLifeCycleImpl.importServiceInstance(ServiceInstanceLifeCycleImpl.java:244)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)

oracle.bi.bar.exceptions.UnSupportedBarException: oracle.bi.bar.exceptions.UnSupportedBarException: The Bar fileC:\app\oracle\fmw\bi\bifoundation\samples\sampleapplite\SampleAppLite.bar provided as input is not supported in this BI Platfrom release.
wls:/offline&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;(Photo credit: &lt;a href=&#34;https://unsplash.com/@joerobot&#34;&gt;https://unsplash.com/@joerobot&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE Baseline Validation Tool - Parameter &#39;directory&#39; is not a directory</title>
			<link>https://rmoff.github.io/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/</link>
			<pubDate>Wed, 18 May 2016 15:35:46 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/</guid>
			<description>&lt;p&gt;Interesting quirk in running Baseline Validation Tool for OBIEE here. If you invoke &lt;code&gt;obibvt&lt;/code&gt; from the &lt;code&gt;bin&lt;/code&gt; folder, it errors with &lt;strong&gt;Parameter &amp;lsquo;directory&amp;rsquo; is not a directory&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\OracleBI-BVT&amp;gt; cd bin
PS C:\OracleBI-BVT\bin&amp;gt; .\obibvt -config C:\OracleBI-BVT\bin\bvt-config.xml -deployment current
 INFO: Result folder: Results\current
Throwable: Parameter &#39;directory&#39; is not a directory
Thread[main,5,main]
SEVERE: Unhandled Exception
SEVERE: java.lang.IllegalArgumentException: Parameter &#39;directory&#39; is not a directory
       at org.apache.commons.io.FileUtils.validateListFilesParameters(FileUtils.java:545)
       at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:521)
       at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:691)
       at com.oracle.biee.bvt.UpgradeTool.loadPlugins(UpgradeTool.java:537)
       at com.oracle.biee.bvt.UpgradeTool.runPluginTests(UpgradeTool.java:644)
       at com.oracle.biee.bvt.UpgradeTool.run(UpgradeTool.java:812)
       at com.oracle.biee.bvt.UpgradeTool.main(UpgradeTool.java:999)

PS C:\OracleBI-BVT\bin&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Solution? Run the exact same command, but from the folder above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\OracleBI-BVT&amp;gt; .\bin\obibvt -config C:\OracleBI-BVT\bin\bvt-config.xml -deployment current
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;(Photo credit: &lt;a href=&#34;https://unsplash.com/@rooszan&#34;&gt;https://unsplash.com/@rooszan&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion</title>
			<link>https://rmoff.github.io/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</link>
			<pubDate>Fri, 13 May 2016 05:45:19 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</guid>
			<description>

&lt;p&gt;Yesterday I wrote about &lt;a href=&#34;http://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/&#34;&gt;Monitoring Logstash Ingest Rates with InfluxDB and Grafana&lt;/a&gt;, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/warkolm/&#34;&gt;Mark Walkom&lt;/a&gt; reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it&amp;rsquo;ll integrate back into the Elastic stack:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/rmoff&#34;&gt;@rmoff&lt;/a&gt; nice, LS 5.0 will have APIs exposing metrics too. they’ll be integrated back into Marvel/Monitoring! :)&lt;/p&gt;&amp;mdash; Mark Walkom (@warkolm) &lt;a href=&#34;https://twitter.com/warkolm/status/730900473226485764&#34;&gt;May 12, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Which then got me thinking &amp;ndash; why add in InfluxDB and Grafana, if you&amp;rsquo;re already using another datastore and front end (Elasticsearch and Kibana)? Well, I touched on this yesterday, and I would still opt for InfluxDB &amp;amp; Grafana when deploying a metrics-based monitoring solution. But, if your primary focus is on text based data (such as log files), rather than metrics alone, Elastic stack will be just great for you. And so in this scenario, let&amp;rsquo;s bring the ingest rate monitoring back in house!&lt;/p&gt;

&lt;h3 id=&#34;logstash-configuration&#34;&gt;Logstash Configuration&lt;/h3&gt;

&lt;p&gt;This is the same &lt;a href=&#34;http://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/&#34;&gt;as before&lt;/a&gt;, except the &lt;strong&gt;output&lt;/strong&gt; stanza points to Elasticsearch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;input {
    # Input code goes here
}
filter {
    # Any other filter code goes here
    # 
    # [...] 
    #
    metrics {
        meter =&amp;gt; &amp;quot;events&amp;quot;
        add_tag =&amp;gt; &amp;quot;metric&amp;quot;
    }
}

output {
    if &amp;quot;metric&amp;quot; in [tags] {
        elasticsearch { hosts =&amp;gt; &#39;localhost&#39;
                        index =&amp;gt; &amp;quot;logstash-metrics&amp;quot;
        }
    } else {
    # Output code goes here
    # 
    # [...] 
    #
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After making this change, restart your Logstash agent.&lt;/p&gt;

&lt;h3 id=&#34;checking-the-data-s-arriving&#34;&gt;Checking the data&amp;rsquo;s arriving&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve been working with Elastic stack for a few years now, and can&amp;rsquo;t believe that it&amp;rsquo;s only recently I&amp;rsquo;ve discovered &lt;a href=&#34;https://www.elastic.co/guide/en/sense/current/installing.html&#34;&gt;Sense&lt;/a&gt;. It&amp;rsquo;s a plugin for Kibana, and makes working with the Elasticsearch REST API a real pleasure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Sense_-_Kibana.png&#34; alt=&#34;Sense&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we can see in the new Elasticsearch index &lt;strong&gt;logstash-metrics&lt;/strong&gt; the data&amp;rsquo;s coming through. All good - now onto the graphs!&lt;/p&gt;

&lt;h3 id=&#34;graphing-it-in-kibana&#34;&gt;Graphing it in Kibana&lt;/h3&gt;

&lt;p&gt;You&amp;rsquo;ve two options for visualising the data here - the Line Chart, or &lt;a href=&#34;https://www.elastic.co/blog/timelion-timeline&#34;&gt;Timelion&lt;/a&gt;. Timelion is still in beta, but longer-term will absolutely be the right choice for this kind of visualisation. So, let&amp;rsquo;s do both!&lt;/p&gt;

&lt;p&gt;The Line Chart is pretty simple. Set the metric aggregation to &lt;strong&gt;Max&lt;/strong&gt; (instead of &lt;em&gt;Count&lt;/em&gt;), and choose the relevant metric field. I&amp;rsquo;ve gone for &lt;code&gt;rate_1m&lt;/code&gt; and added a second Y-axis metric for &lt;code&gt;rate_5m&lt;/code&gt;. On the X-axis it&amp;rsquo;s just split out as a date histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir14.png&#34; alt=&#34;Kibana Line Chart&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Timelion chart is a tad more complex to build, but ultimately better. Since I&amp;rsquo;ve got Timelion installed and am running Kibana 4.5, &amp;ldquo;Timeseries&amp;rdquo; shows up as a Visualisation option for me when I create a new one. To start with the blank configuration is a bit daunting:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Visualize_-_Kibana.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Set the Interval to &lt;strong&gt;other&lt;/strong&gt; and then &lt;code&gt;5s&lt;/code&gt; in the box that appears. Amend the Timelion Expression to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-metrics)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hit the play button:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Visualize_-_Kibana-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Ah - not quite what we expected. That&amp;rsquo;s because we&amp;rsquo;re seeing by default a &lt;strong&gt;Count&lt;/strong&gt;, which is generally 1 per Interval. Let&amp;rsquo;s fix that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-metrics,metric=max:events.rate_1m)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Visualize_-_Kibana-2.png&#34; alt=&#34;Kibana Timelion&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Tada!&lt;/p&gt;

&lt;p&gt;But, let&amp;rsquo;s not stop there, let&amp;rsquo;s see what Timelion can do. A second series? Sure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-metrics,metric=max:events.rate_1m), .es(index=logstash-metrics,metric=max:events.rate_5m)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Visualize_-_Kibana-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looks good - but which is which? And how about a title for the chart? &lt;strong&gt;label&lt;/strong&gt; and &lt;strong&gt;title&lt;/strong&gt; functions to the rescue!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-metrics,metric=max:events.rate_1m).label(&amp;quot;1 minute moving average&amp;quot;), .es(index=logstash-metrics,metric=max:events.rate_5m).label(&amp;quot;5 minute moving average&amp;quot;).title(&amp;quot;Events per second&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Visualize_-_Kibana_and_untitled_and_1__screen.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s save the visualisation, and include it on the dashboard with our actual data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Dashboard_-_Kibana.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So - ingest rate monitoring within the Elastic stack? Done. Ingest rate monitoring if you&amp;rsquo;re also using InfluxDB &amp;amp; Grafana? &lt;a href=&#34;http://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/&#34;&gt;Done&lt;/a&gt;! And just to round off all permutations - you want to store your data in Elasticsearch, but just love how Grafana looks? Not a problem, since &lt;a href=&#34;http://docs.grafana.org/datasources/elasticsearch/&#34;&gt;Grafana support Elasticsearch as a data source&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/Grafana_-_Twitter_Ingest_Monitor.png&#34; alt=&#34;Grafana visualising Elasticsearch data&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Monitoring Logstash Ingest Rates with InfluxDB and Grafana</title>
			<link>https://rmoff.github.io/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/</link>
			<pubDate>Thu, 12 May 2016 20:56:38 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/</guid>
			<description>

&lt;p&gt;In this article I&amp;rsquo;m going to show you how to easily monitor the rate at which Logstash is ingesting data, as well as in future articles the rate at which Elasticsearch is indexing it. It&amp;rsquo;s a nice little touch to add to any project involving Logstash, and it&amp;rsquo;s easy to do.&lt;/p&gt;

&lt;p&gt;Logstash is powerful tool for data ingest, processing, and distribution. It originated as simply the pipe to slurp at log files and put them into Elasticsearch, but has evolved into a whole bunch more. With connectors to JDBC and Kafka, as well as many other &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/input-plugins.html&#34;&gt;input&lt;/a&gt; and &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/output-plugins.html&#34;&gt;output&lt;/a&gt; options (not to mention the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/filter-plugins.html&#34;&gt;filtering&lt;/a&gt; possibilities), it really is a great bit of software to use. I&amp;rsquo;ve used it over the years with &lt;a href=&#34;http://www.rittmanmead.com/2014/10/monitoring-obiee-with-elasticsearch-logstash-and-kibana/&#34;&gt;OBIEE&lt;/a&gt;, as well as more recently to &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;pull data from Oracle&lt;/a&gt;, and even &lt;a href=&#34;http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;IRC&lt;/a&gt;. Another great set of tools is &lt;a href=&#34;http://influxdb.com&#34;&gt;InfluxDB&lt;/a&gt; and &lt;a href=&#34;http://grafana.org&#34;&gt;Grafana&lt;/a&gt;, which for me really round off the standalone Elastic platform (previously known as ELK - Elasticsearch, Logstash, and Kibana). What InfluxDB and Grafana give is a powerful dedicated time series database and flexible time series-based dashboarding tool respectively. A topic for another day is the Elasticsearch vs InfluxDB overlap, and Kibana vs Grafana - but for now, just take it as read that it&amp;rsquo;s horses for course, right tool for the right job, etc.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s get started&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-Requisites&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m not going to cover setup &amp;amp; install here - I&amp;rsquo;m assuming that you&amp;rsquo;ve got Logstash &amp;gt;=2.3.1, InfluxDB &amp;gt;= 0.12, Grafana &amp;gt;= 2.6 running. In this example it&amp;rsquo;s all running on a single node, localhost, default ports for everything. The only non-standard configuration is that I&amp;rsquo;ve &lt;a href=&#34;https://github.com/influxdata/influxdb/blob/master/services/graphite/README.md&#34;&gt;enabled the &lt;strong&gt;graphite&lt;/strong&gt; listener in InfluxDB&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ll get Logstash to send event rates over to InfluxDB, from where we&amp;rsquo;ll visualise it in Grafana.&lt;/p&gt;

&lt;p&gt;The example I&amp;rsquo;m using it based on pulling some data in from a Kafka topic (similar to the &lt;a href=&#34;http://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/&#34;&gt;pattern described here&lt;/a&gt;) and indexing it into Elasticsearch. I can start and stop my Logstash configuration when I want, and it picks up from where it left off in consuming the data from Kafka.&lt;/p&gt;

&lt;h3 id=&#34;logstash-instrumentation&#34;&gt;Logstash Instrumentation&lt;/h3&gt;

&lt;p&gt;First job is to get Logstash to track, and then output, the rate at which it&amp;rsquo;s processing events. One row read from a log, one message pulled from Kafka - each is one &amp;ldquo;event&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use the &lt;strong&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-filters-metrics.html&#34;&gt;metric&lt;/a&gt;&lt;/strong&gt; filter to do this. In the &lt;strong&gt;filter&lt;/strong&gt; stanza of my Logstash configuration, I add:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;filter {
    # Any other filter code here
    # [...]
    #
    # Add events per second metric
    metrics {
        meter =&amp;gt; &amp;quot;events&amp;quot;
        add_tag =&amp;gt; &amp;quot;metric&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then to get it written over to InfluxDB, via the graphite protocol, I amend my &lt;strong&gt;output&lt;/strong&gt; stanza to split out the events based on tag - metrics go to Influx (and stdout for debug), everything else to Elasticsearch as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;output {
    if &amp;quot;metric&amp;quot; in [tags] {
        stdout {
            codec =&amp;gt; line {
            format =&amp;gt; &amp;quot;Events per second ingest rate (1/5/15 min avg): %{[events][rate_1m]} | %{[events][rate_5m]} | %{[events][rate_15m]}&amp;quot;
            }
        }
        graphite {
            host =&amp;gt; &amp;quot;localhost&amp;quot;
            metrics_format =&amp;gt; &amp;quot;logstash.*&amp;quot;
            include_metrics =&amp;gt; [ &amp;quot;events.*&amp;quot; ]
            fields_are_metrics =&amp;gt; true
        }
    } else {
        # Output configuration as before,
        # to Elasticsearch or wherever
        # [...]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fire up your Logstash agent (the new &lt;code&gt;--auto-reload&lt;/code&gt; parameter I&amp;rsquo;ve found great for development stuff like this):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bin/logstash --auto-reload --config logstash-twitter-kafka.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And, aside from any other stdout that your script is writing, you&amp;rsquo;ll now see the 1/5/15 minute moving averages for events per second being processed:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Events per second ingest rate (1/5/15 min avg): 2.0609812156329577 | 2.327820782492659 | 2.3756847177898033
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But &amp;hellip; that stdout is just debug, remember? Where we really want it is over in InfluxDB, so we can build some lovely charts against it.&lt;/p&gt;

&lt;h3 id=&#34;checking-the-data-in-influxdb&#34;&gt;Checking the data in InfluxDB&lt;/h3&gt;

&lt;p&gt;You can use the &lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.12/tools/web_admin/&#34;&gt;InfluxDB GUI&lt;/a&gt; for this, or the &lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.12/tools/shell/&#34;&gt;command line&lt;/a&gt;. Here I&amp;rsquo;ll use the command line.&lt;/p&gt;

&lt;p&gt;Launch the client&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ influx
Visit https://enterprise.influxdata.com to register for updates, InfluxDB server management, and monitoring.
Connected to http://localhost:8086 version 0.12.1
InfluxDB shell 0.12.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Switch to the &lt;code&gt;graphite&lt;/code&gt; database (used by default for graphite protocol data; can be changed in the influxDB configuration)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; use graphite
Using database graphite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List the series that exist so far:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; show measurements
name: measurements
------------------
name
logstash.events.count
logstash.events.rate_15m
logstash.events.rate_1m
logstash.events.rate_5m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Show a sample of the data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; select * from /logstash.events.rate_1m/ limit 5
name: logstash.events.rate_1m
-----------------------------
time                    value
1463044923000000000     0
1463044928000000000     16.8
1463044933000000000     15.472737282846767
1463044938000000000     14.379525569777279
1463044943000000000     14.379525569777279
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The time value is epoch microseconds. For more information on the InfluxDB query language, &lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.12/query_language/data_exploration/&#34;&gt;see here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, we&amp;rsquo;ve instrumented Logstash configuration to generate and send the data, we&amp;rsquo;ve validated that InfluxDB is getting the data &amp;hellip; now let&amp;rsquo;s graph the data!&lt;/p&gt;

&lt;h3 id=&#34;charting-it-in-grafana&#34;&gt;Charting it in Grafana&lt;/h3&gt;

&lt;p&gt;In Grafana I&amp;rsquo;ve added a datasource pointing to my InfluxDB, and then headed over to my dashboard. When done in real life, this kind of chart makes a lot of sense alongside other &amp;ldquo;health check&amp;rdquo; visualisations, enabling you to see not only what the data coming into the system is telling you, but also the status of that data flow. There&amp;rsquo;s nothing worse than thinking &amp;ldquo;hey cool, no errors&amp;rdquo; when the reason there&amp;rsquo;s no errors is that all the errors are backed up in the pipeline and not even making it into your monitoring system &amp;hellip;&lt;/p&gt;

&lt;p&gt;So here&amp;rsquo;s the basic chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve added a title, and values to the legend. Other than that, dead simple.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s make it easier to see, at a glance, if things are bad (&lt;a href=&#34;https://www.youtube.com/watch?v=Uh7l8dx-h8M&#34;&gt;m&amp;rsquo;kay&lt;/a&gt;) or not:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir02-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here I&amp;rsquo;ve added a Singlestat panel. A very important thing to change from the default option if you&amp;rsquo;re using it in this way, to show the current value - is to make sure you set it to that - current:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir03.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t do this, you get the average across all values, which typically of less use.&lt;/p&gt;

&lt;p&gt;The Singlestat panel also supports thresholds, so you can be alerted visually if the ingest rate is less than you&amp;rsquo;d want. Here it&amp;rsquo;s up to you to know what rate you would expect. In this screenshot it&amp;rsquo;s going to show green above 10, amber above 5, and red below 5:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir04.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In actuality, my ingest rate is pretty modest, at around 0.5 per second, so I&amp;rsquo;ve set my thresholds at 0.1 and 0.5. Anything below 0.5 I want to be aware of, anything below 0.1 and it suggests there&amp;rsquo;s a problem. Let&amp;rsquo;s see how that pans out.&lt;/p&gt;

&lt;p&gt;To start with, everything&amp;rsquo;s good. Rate is above 0.5, and we&amp;rsquo;re ticking along nicely:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir05.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For some reason, the ingest rate slows - could be my source, could be the pipeline - but I want to be aware. The Singlestat colour highlights this for me, since it&amp;rsquo;s below the threshold of 0.5 that I set:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir06.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s cut the pipeline and see what happens. We should get a nice big red alert background.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir07.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Oh. Not what we wanted. Even though the chart clearly shows there&amp;rsquo;s been no data for ten minutes, the Singlestat is showing a current ingest rate of 0.4 (and in amber, not red), and if you look closely the &amp;ldquo;Current&amp;rdquo; value on the legend shows the same.&lt;/p&gt;

&lt;p&gt;This is where we need to get a bit deeper into Grafana. If you look closely at the Metrics configuration for both the Graph and Singlestat, you&amp;rsquo;ll see that by default &amp;ldquo;fill&amp;rdquo; is set to null.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir08.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a time series chart, where time moves on whether you like it or not &amp;ndash; and whether you have data or not. Grafana by default will &amp;lsquo;fill&amp;rsquo; any gaps with null. Null is most definitely &lt;strong&gt;not&lt;/strong&gt; zero &amp;ndash; it&amp;rsquo;s null, it&amp;rsquo;s an absence of data, it&amp;rsquo;s &amp;ldquo;we don&amp;rsquo;t know&amp;rdquo;. So when we ask Grafana to use &amp;ldquo;current&amp;rdquo; value (in the legend, in the singlestat), it ends up using the &amp;ldquo;last known&amp;rdquo; value of the data - which for our purposes is stale and basically wrong.&lt;/p&gt;

&lt;p&gt;So in this case, we&amp;rsquo;re going to deliberately conflate &amp;ldquo;no ingest rate from Logstash&amp;rdquo; with &amp;ldquo;Logstash isn&amp;rsquo;t ingesting data&amp;rdquo;. Technically, this could be untrue at times, but it&amp;rsquo;s close enough for me. So now we will tell Grafana to use &lt;strong&gt;zero&lt;/strong&gt; if it doesn&amp;rsquo;t find any data for a given time period.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir09.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll notice the graph&amp;rsquo;s rendering different now, because Grafana&amp;rsquo;s plotting it at a resolution higher than we&amp;rsquo;re sending data. Logstash emits the event data every five seconds or so, and Grafana&amp;rsquo;s plotting at every second - so it&amp;rsquo;s marking the chart as zero for every four of each five seconds. To solution to this is to set the time group by to &lt;strong&gt;at least five seconds&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Applying the same Metric configuration (fill=zero, group by &amp;gt;=5s) to the Singlestat panel gives us a much better result now. When there&amp;rsquo;s no data, we get a big fat red zero making it nice and clear that there&amp;rsquo;s a problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir11.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the final touch, let&amp;rsquo;s give an indication on the chart of the threshold levels we&amp;rsquo;re using for the Singlestat, using the &lt;strong&gt;Thresholds&lt;/strong&gt; option:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir12.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This shows itself on the chart as a coloured background for each threshold level used:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/05/lsir13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(From this we can see probably 0.5 is too high a threshold since the data seems to usually fall within that range - and there&amp;rsquo;s nothing worse than a permanent &amp;ldquo;warning&amp;rdquo; that just becomes background noise.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So there you go - nice monitoring of Logstash ingest rates, using InfluxDB and Grafana. Stick around to see how we can do a similar thing for monitoring Elasticsearch, and even the data within it too&amp;hellip;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Collection of Articles on How to Write a Good Conference Abstract</title>
			<link>https://rmoff.github.io/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/</link>
			<pubDate>Thu, 05 May 2016 09:57:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/</guid>
			<description>&lt;p&gt;Here&amp;rsquo;s a collection of useful articles that I&amp;rsquo;ve found over the years that give good advice on writing a good abstract, mistakes to avoid, etc:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sqlblog.com/blogs/adam_machanic/archive/2013/02/22/capturing-attention-writing-great-session-descriptions.aspx&#34;&gt;Adam Machanic - Capturing Attention: Writing Great Session Descriptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pythian.com/blog/concrete-advice-for-abstract-writers/&#34;&gt;Gwen Shapira - Concrete Advice for Abstract Writers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dbakevlar.com/2013/10/abstracts-reviews-and-conferences-oh-my/&#34;&gt;Kellyn Pot’Vin-Gorman - Abstracts, Reviews and Conferences, Oh My!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://alistapart.com/article/conference-proposals-that-dont-suck&#34;&gt;Russ Unger - Conference Proposals that Don’t Suck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mwidlake.wordpress.com/2015/04/17/tips-on-submitting-an-abstract-to-conference/&#34;&gt;Martin Widlake - Tips on Submitting an Abstract to Conference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;(post photo courtesy of &lt;a href=&#34;https://unsplash.com/@calum_mac&#34;&gt;Calum MacAulay&lt;/a&gt; on &lt;a href=&#34;https://unsplash.com&#34;&gt;https://unsplash.com&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Using R to Denormalise Data for Analysis in Kibana</title>
			<link>https://rmoff.github.io/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</link>
			<pubDate>Sun, 24 Apr 2016 12:22:12 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</guid>
			<description>&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/products/kibana&#34;&gt;Kibana&lt;/a&gt; is a tool from &lt;a href=&#34;https://www.elastic.co/&#34;&gt;Elastic&lt;/a&gt; that makes analysis of data held in &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt; really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a &lt;a href=&#34;http://www.rittmanmead.com/2015/04/using-the-elk-stack-to-analyse-donors-choose-data/&#34;&gt;blog post last year&lt;/a&gt;, taking a CSV file and loading it into Elasticsearch via Logstash.&lt;/p&gt;

&lt;p&gt;This is all great, but the one real sticking point with analytics in Elasticsearch/Kibana is that it needs the data to be &lt;strong&gt;denormalised&lt;/strong&gt;. That is, you can&amp;rsquo;t give it a bunch of sources of data and it perform the joins for you in Kibana - it just doesn&amp;rsquo;t work like that. If you&amp;rsquo;re using Elasticsearch alone for analytics, maybe with a bespoke application, &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/current/relations.html&#34;&gt;there are ways of approaching it&lt;/a&gt;, but not through Kibana. Now, depending on where the data is coming from, this may not be a problem. For example, if you use the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html&#34;&gt;JDBC Logstash input&lt;/a&gt; to pull from an RDBMS source you can specify a complex SQL query going across multiple tables, so that the data when it hits Elasticsearch is nice and denormalised and ready for fun in Kibana. But, source data doesn&amp;rsquo;t always come this way, and it&amp;rsquo;s useful to have a way to work with the data still when it is like this.&lt;/p&gt;

&lt;p&gt;I was playing around with some data recently (as one does, of course 8-) ) to try and load Elasticsearch so as to look at the &lt;a href=&#34;https://www.elastic.co/products/graph&#34;&gt;Graph&lt;/a&gt; function in more detail, but struggling because the data itself was mostly made of codes that were foreign keys to separate datasets. The data was a CSV from &lt;a href=&#34;https://data.gov.uk/dataset/road-accidents-safety-data&#34;&gt;data.gov.uk&lt;/a&gt;, detailing road accidents. Each field, such as the police force, was simply a code that then had to be looked up on an Excel document &lt;a href=&#34;https://data.gov.uk/data/resource_cache/ad/ad15bff1-9fec-4bac-befe-7005d104344e/Road-Accident-Safety-Data-Guide.xls&#34;&gt;available separately&lt;/a&gt;. Loading just the main CSV into Elasticsearch was easy enough (&lt;a href=&#34;https://github.com/rmoff/dft/blob/master/logstash-DfTRoadSafety_Accidents.conf&#34;&gt;see github for details&lt;/a&gt;), but of limited use once in Kibana:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/dft01.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Day_of_Week&lt;/code&gt; being from 1 to 7 I could probably hazard a guess at the lookup value myself, but &lt;code&gt;Junction_Detail&lt;/code&gt; of &amp;ldquo;8&amp;rdquo; &amp;hellip; not a clue. To find out, I need to match each foreign key with the corresponding lookup data, which is in a set of sheets in an Excel document:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/dft04.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point we are at the official start of &amp;ldquo;wrangling&amp;rdquo; the data. We don&amp;rsquo;t know what we can do with the data until we&amp;rsquo;ve got it in a structure that makes it useful, so we don&amp;rsquo;t want to front-load the time needed for discovery with some complex ETL if the data&amp;rsquo;s not going to turn out to yield much of interest. We &lt;em&gt;could&lt;/em&gt; use a data integration tool such as &lt;a href=&#34;http://www.oracle.com/technetwork/middleware/data-integrator/overview/index.html&#34;&gt;ODI&lt;/a&gt;, but talk about sledgehammer to crack a nut &amp;hellip; and that&amp;rsquo;s before we take into account license costs, infrastructure overheard, and so on. Surely we can do this more smartly, as a one-off or hacky-repeatable thing, until we&amp;rsquo;re sure it&amp;rsquo;s going to be of worth to &amp;lsquo;productionise&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;First up was trying to keep the toolset down to a minimum, and modifying the CSV file from bash itself. People sometimes forget that bash comes with a fantastically powerful set of data manipulation tools, in the form of &lt;code&gt;sed&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, &lt;code&gt;tr&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, and so on. One of these is also &lt;code&gt;join&lt;/code&gt;, which as the name says, let&amp;rsquo;s you take two files and join on a given column. However, after a few iterations (read: bouncing back and forth between the terminal and Google and nearly throwing my laptop at the wall) I wrote it off as unfeasible. Problems with character encoding of the datasets, and the fact that I had to manually extract the data from Excel first before I could run it - not to mention the fact there are nearly 20 dimensions to join - meant that I discounted this option.&lt;/p&gt;

&lt;p&gt;I then had one of those &amp;ldquo;ahhhhh&amp;rdquo; moments, remembering the excellent work that my colleague Jordan Meyer did at the Rittman Mead BI Forum last year in his &lt;a href=&#34;https://s3.amazonaws.com/rmc_docs/rm_bi_forum_mclass_2015_part1.pdf&#34;&gt;Data Discovery and Predictive Modelling&lt;/a&gt; masterclass. In it he extolled the many virtues of R for working with data. Not only is R a powerful statistics language, it&amp;rsquo;s also a damn useful (and, I would say, elegant) one for manipulating data, helped in large part by the &lt;a href=&#34;https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html&#34;&gt;dplyr package&lt;/a&gt;. It also has lots of libraries for doing useful stuff like reading CSV files based on the headers without having to declare the column, as well as reading natively from Excel files. So, off we go:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;### Load main accident facts from CSV
library(&#39;readr&#39;)
accidents &amp;lt;- read_csv(&amp;quot;/tmp/dft/DfTRoadSafety_Accidents_2014.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;N.B. If you&amp;rsquo;ve any libraries here not already installed, you can install using &lt;code&gt;install.packages(&#39;foo&#39;)&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the data loaded, let&amp;rsquo;s now set a proper timestamp column, since in the data there&amp;rsquo;s only Date and Time on their own. We can see some sample values with the &lt;code&gt;head&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; head(accidents$Date)
[1] &amp;quot;09/01/2014&amp;quot; &amp;quot;20/01/2014&amp;quot; &amp;quot;21/01/2014&amp;quot; &amp;quot;15/01/2014&amp;quot; &amp;quot;09/01/2014&amp;quot;
[6] &amp;quot;17/01/2014&amp;quot;
&amp;gt; head(accidents$Time)
[1] &amp;quot;13:21&amp;quot; &amp;quot;23:00&amp;quot; &amp;quot;10:40&amp;quot; &amp;quot;17:45&amp;quot; &amp;quot;08:50&amp;quot; &amp;quot;14:11&amp;quot;
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handling and parsing dates is a problem to be solved across all languages and technologies, and R&amp;rsquo;s &lt;code&gt;lubridate&lt;/code&gt; package is by far the best way I&amp;rsquo;ve ever seen. Using &lt;code&gt;lubridate&lt;/code&gt; you describe the &lt;strong&gt;order&lt;/strong&gt; that the date/time components appear in (e.g. &amp;ldquo;year month day&amp;rdquo;) but without needing to specify the exact format string usually needed, thus avoiding the usual monkeying around with letter symbols, counting out the right number of them and getting the case right (is it YYYYMMDD or YYYYmmDD or YYMD?). In the above data sample you can see that the date is in the form Day / Month / Year, and the Time is Hour / Minute. That&amp;rsquo;s all we need to know - forget having to check if &amp;ldquo;MM&amp;rdquo; is month or minute, whether to escape the separators and so on. Since it&amp;rsquo;s Day / Month / Year / Hour / Minute, we use the &lt;strong&gt;&lt;code&gt;dmy_hm&lt;/code&gt;&lt;/strong&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;### Populate a timestamp using the awesome lubridate package
library(&#39;lubridate&#39;)
### Using &amp;quot;dmy_hm&amp;quot; we&#39;re telling lubridate that the date is in the order
### Day / Month / Year / Hour / Minute - the actual format string and
### separators get figured out automagically
accidents$timestamp &amp;lt;- dmy_hm(paste(accidents$Date,accidents$Time))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The data includes the geo-spatial reference points, which &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html&#34;&gt;Elasticsearch can store&lt;/a&gt; enabling analysis of the data in tools including Kibana. To store this properly we define &lt;code&gt;location&lt;/code&gt; as a &lt;code&gt;geo_point&lt;/code&gt; in &lt;a href=&#34;https://github.com/rmoff/dft/blob/master/elasticsearch_mapping_template.sh&#34;&gt;the mapping&lt;/a&gt;, and concatenate the latitude and longitude:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;### Define the location as a string. 
accidents$location &amp;lt;- paste(accidents$Latitude,accidents$Longitude,sep=&#39;,&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point we have just our main &amp;ldquo;fact&amp;rdquo; dataset, including properly formatted and typed Timestamp column:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;&amp;gt; str(accidents)
Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:	146322 obs. of  33 variables:
 $ ﻿Accident_Index                             : chr  &amp;quot;201401BS70001&amp;quot; &amp;quot;201401BS70002&amp;quot; &amp;quot;201401BS70003&amp;quot; &amp;quot;201401BS70004&amp;quot; ...
 $ Location_Easting_OSGR                      : int  524600 525780 526880 525580 527040 524750 524950 523850 524500 526450 ...
 $ Location_Northing_OSGR                     : int  179020 178290 178430 179080 179030 178970 179240 181450 180260 179230 ...
 $ Longitude                                  : num  -0.206 -0.19 -0.174 -0.192 -0.171 ...
 $ Latitude                                   : num  51.5 51.5 51.5 51.5 51.5 ...
 $ Police_Force                               : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Accident_Severity                          : int  3 3 3 3 3 3 3 3 3 3 ...
&amp;gt; head(accidents$timestamp)
[1] &amp;quot;2014-01-09 13:21:00 UTC&amp;quot; &amp;quot;2014-01-20 23:00:00 UTC&amp;quot;
[3] &amp;quot;2014-01-21 10:40:00 UTC&amp;quot; &amp;quot;2014-01-15 17:45:00 UTC&amp;quot;
[5] &amp;quot;2014-01-09 08:50:00 UTC&amp;quot; &amp;quot;2014-01-17 14:11:00 UTC&amp;quot;
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s load a lookup dataset for one of the dimensions &amp;ndash; Police Force.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(&#39;readxl&#39;)
police &amp;lt;-
  read_excel(&amp;quot;/tmp/dft/Road-Accident-Safety-Data-Guide.xls&amp;quot;,sheet = 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can check the data:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;&amp;gt; library(&#39;tibble&#39;)
&amp;gt; as_data_frame(police)
Source: local data frame [51 x 2]

    code               label
   &amp;lt;dbl&amp;gt;               &amp;lt;chr&amp;gt;
1      1 Metropolitan Police
2      3             Cumbria
3      4          Lancashire
4      5          Merseyside
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but note the second column heading &amp;ndash; &lt;code&gt;label&lt;/code&gt;. When we come to do our joining we want the name of this descriptor column to reference the dimension to which it is attached. So, we rename it using a function in the dplyr library:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;&amp;gt; library(&#39;dplyr&#39;)
&amp;gt; police &amp;lt;-
   police %&amp;gt;%
   rename(police_force=label)
&amp;gt; as_data_frame(police)
Source: local data frame [51 x 2]

    code        police_force
   &amp;lt;dbl&amp;gt;               &amp;lt;chr&amp;gt;
1      1 Metropolitan Police
2      3             Cumbria
3      4          Lancashire
4      5          Merseyside
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better! Now to join this to the main dataset. For this we use dplyr again, with its &lt;strong&gt;join&lt;/strong&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;accidents &amp;lt;-
  left_join(accidents,police, by=c(&amp;quot;Police_Force&amp;quot;=&amp;quot;code&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;If you want to see more about working with R, there&amp;rsquo;s a great reference PDF here: &lt;strong&gt;&lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;Data Wrangling with dplyr and tidyr Cheat Sheet&lt;/a&gt;&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Check out the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;&amp;gt; accidents %&amp;gt;%
+ select(1,34)
Source: local data frame [146,322 x 2]

   ﻿Accident_Index        police_force
            &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;
1   201401BS70001 Metropolitan Police
2   201401BS70002 Metropolitan Police
3   201401BS70003 Metropolitan Police
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the joining is done it&amp;rsquo;s off to another R library, this time &lt;code&gt;elastic&lt;/code&gt;, enabling us to write the denormalised data frame directly into Elasticsearch. We&amp;rsquo;re manually defining &lt;code&gt;Accident_Index&lt;/code&gt; as the unique key for the record, so that if re-run Elasticsearch won&amp;rsquo;t accept duplicate entries.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(&amp;quot;elastic&amp;quot;)
### connect() assumes Elasticsearch is running locally on port 9200
connect()
docs_bulk(accidents,doc_ids = accidents$Accident_Index,index=&amp;quot;dftroadsafetyaccidents02&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Heading over to Kibana we now have the basis from which to start usefully exploring the data &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/dft02.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip; as well as knocking out aggregate visualisations with ease:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/dft03.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And since we stored the data using the geo-spatial reference format we can also map it out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/dft05.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Stay tuned for more details of the actual Graph analysis that I did with the data once it was loaded&amp;hellip;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can find &lt;a href=&#34;https://github.com/rmoff/dft&#34;&gt;the full R code on github here&lt;/a&gt;, including joins to all 18 lookup data sets. There&amp;rsquo;s also the code for loading the data into Elasticsearch directly via Logstash from the CSV, and the necessary Elasticsearch mapping template.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE security patches, and FINAL 11.1.1.7 patchset release</title>
			<link>https://rmoff.github.io/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/</link>
			<pubDate>Mon, 18 Apr 2016 15:36:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/</guid>
			<description>&lt;p&gt;Two vulns for OBIEE in the latest critical patch update (CPU): &lt;a href=&#34;http://www.oracle.com/technetwork/security-advisory/cpuapr2016v3-2985753.html?elq_mid=45463&amp;amp;sh=91225181314122121267715271910&amp;amp;cmid=WWMK10067711MPP001C140&#34;&gt;http://www.oracle.com/technetwork/security-advisory/cpuapr2016v3-2985753.html?elq_mid=45463&amp;amp;sh=91225181314122121267715271910&amp;amp;cmid=WWMK10067711MPP001C140&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Patches is bundle patch &lt;code&gt;.160419&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;12.2.1: &lt;a href=&#34;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22734181&#34;&gt;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22734181&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;11.1.1.9: &lt;a href=&#34;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22393988&#34;&gt;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22393988&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;11.1.1.7: &lt;a href=&#34;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22225110&#34;&gt;https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22225110&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that April 2016 is the &lt;strong&gt;last regular patchset&lt;/strong&gt; for 11.1.1.7, ref: &lt;a href=&#34;https://support.oracle.com/epmos/faces/DocumentDisplay?id=2102148.1#mozTocId410847&#34;&gt;https://support.oracle.com/epmos/faces/DocumentDisplay?id=2102148.1#mozTocId410847&lt;/a&gt;. If you&amp;rsquo;re still on it, or earlier, time to upgrade!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(Photo credit: &lt;a href=&#34;https://unsplash.com/@jenlittlebirdie&#34;&gt;https://unsplash.com/@jenlittlebirdie&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
			<link>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
			<pubDate>Thu, 14 Apr 2016 22:51:43 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
			<description>

&lt;p&gt;Recently added to the &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;oracledi project over at java.net&lt;/a&gt; is &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;an adaptor&lt;/a&gt; enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;over at the Elastic blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (&lt;a href=&#34;http://rmoff.net/tag/elasticsearch/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;www.rittmanmead.com/tag/elasticsearch&#34;&gt;here&lt;/a&gt; for archives), as well as &lt;a href=&#34;https://speakerdeck.com/rmoff/data-discovery-and-systems-diagnostics-with-the-elk-stack&#34;&gt;spoken about&lt;/a&gt; - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with. So, being able to combine that with my &amp;ldquo;day job&amp;rdquo; focus of Oracle is fun. Let&amp;rsquo;s get started!&lt;/p&gt;

&lt;p&gt;From the &lt;a href=&#34;https://java.net/projects/oracledi/downloads/directory/GoldenGate/Oracle%20GoldenGate%20Adapter%20for%20ElasticSearch&#34;&gt;adaptor page&lt;/a&gt;, download the zip to your machine. I&amp;rsquo;m using Oracle&amp;rsquo;s &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;BigDataLite VM&lt;/a&gt; which already has GoldenGate installed and configured, and which I&amp;rsquo;ve also got Elasticsearch already on following on from &lt;a href=&#34;http://rmoff.net/2016/03/16/oracle-goldengate-kafka-hive-on-bigdatalite-4-4/&#34;&gt;this earlier post&lt;/a&gt;. If you&amp;rsquo;ve not got Elasticsearch already, head over to &lt;a href=&#34;https://www.elastic.co/downloads/elasticsearch&#34;&gt;elastic.co&lt;/a&gt; to download it. I&amp;rsquo;m using version 2.3.1, installed in &lt;code&gt;/opt/elasticsearch-2.3.1&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;ready&#34;&gt;Ready &amp;hellip;&lt;/h3&gt;

&lt;p&gt;Once you&amp;rsquo;ve got the OGG adaptor zip, you&amp;rsquo;ll want to unzip it &amp;ndash; a word of advice here, specify the destination folder as there&amp;rsquo;s no containing root within the archive so you&amp;rsquo;ll end up with a mess of folder and files in amongst your download folder otherwise:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;unzip OGG_elasticsearch_v1.0.zip -d /u01/OGG_elasticsearch_v1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy the provided &lt;code&gt;.prm&lt;/code&gt; and &lt;code&gt;.props&lt;/code&gt; files to your OGG &lt;code&gt;dirprm&lt;/code&gt; folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp /u01/OGG_elasticsearch_v1.0/dirprm/elasticsearch.props /u01/ogg-bd/dirprm/
cp /u01/OGG_elasticsearch_v1.0/dirprm/res.prm /u01/ogg-bd/dirprm/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the &lt;code&gt;elasticsearch.props&lt;/code&gt; (e.g. &lt;code&gt;/u01/ogg/dirprm/elasticsearch.props&lt;/code&gt;) file to set:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;gg.classpath&lt;/strong&gt;, to pick up both the Elasticsearch jars and the OGG adaptor jar. On my installation this is :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.classpath=/opt/elasticsearch-2.3.1/lib/*:/u01/OGG_elasticsearch_v1.0/bin/ogg-elasticsearch-adapter-1.0.jar:
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;gg.handler.elasticsearch.clusterName&lt;/strong&gt;, which is the name of your elasticsearch cluster - if you don&amp;rsquo;t know it you can check with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ~]$ curl -s localhost:9200|grep cluster_name
&amp;quot;cluster_name&amp;quot; : &amp;quot;elasticsearch&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So mine is the default - &lt;strong&gt;elasticsearch&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.elasticsearch.clusterName=elasticsearch
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For &lt;strong&gt;gg.handler.elasticsearch.host&lt;/strong&gt; and &lt;strong&gt;gg.handler.elasticsearch.port&lt;/strong&gt; I left the defaults (localhost / 9300) unchanged - update these for your Elasticsearch instance as required. Note that &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/current/_talking_to_elasticsearch.html&#34;&gt;Elasticsearch listens&lt;/a&gt; on two ports, with 9200 by default for HTTP traffic, and 9300 for Java clients which is what we&amp;rsquo;re using here.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;steady&#34;&gt;Steady &amp;hellip;&lt;/h3&gt;

&lt;p&gt;Run &lt;code&gt;ggsci&lt;/code&gt; to add and start the replicat using the provided &lt;code&gt;res&lt;/code&gt; configuration (&lt;strong&gt;res&lt;/strong&gt; = &lt;strong&gt;R&lt;/strong&gt;eplicat, &lt;strong&gt;E&lt;/strong&gt;lastic&lt;strong&gt;S&lt;/strong&gt;earch, I&amp;rsquo;m guessing) and sample trail file (i.e. we don&amp;rsquo;t need a live extract running to try this thing out):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd /u01/ogg-bd
$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter
Version 12.2.0.1.0 OGGCORE_12.2.0.1.0_PLATFORMS_151101.1925.2
Linux, x64, 64bit (optimized), Generic on Nov 10 2015 16:18:12
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2015, Oracle and/or its affiliates. All rights reserved.



GGSCI (bigdatalite.localdomain) 1&amp;gt; start mgr
Manager started.


GGSCI (bigdatalite.localdomain) 2&amp;gt; add replicat res, exttrail AdapterExamples/trail/tr
REPLICAT added.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;go&#34;&gt;Go!&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GGSCI (bigdatalite.localdomain) 3&amp;gt; start res

Sending START request to MANAGER ...
REPLICAT RES starting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yay!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GGSCI (bigdatalite.localdomain) 5&amp;gt; info res

REPLICAT   RES       Initialized   2016-04-14 22:03   Status STOPPED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;STOPPED&lt;/code&gt;? Oh &amp;hellip;&lt;/p&gt;

&lt;p&gt;Time for debug. Open up &lt;code&gt;/u01/ogg-bd/ggserr.log&lt;/code&gt;, and the error (&lt;em&gt;`Error loading shared library ggjava.dll&lt;/em&gt;) is nice and clear to see:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016-04-14 22:04:25  INFO    OGG-00987  Oracle GoldenGate Command Interpreter:  GGSCI command (oracle): start res.
2016-04-14 22:04:25  INFO    OGG-00963  Oracle GoldenGate Manager, mgr.prm:  Command received from GGSCI on host [127.0.0.1]:13379 (START REPLICAT RES ).
2016-04-14 22:04:25  INFO    OGG-00960  Oracle GoldenGate Manager, mgr.prm:  Access granted (rule #6).
2016-04-14 22:04:25  INFO    OGG-00975  Oracle GoldenGate Manager, mgr.prm:  REPLICAT RES starting.
2016-04-14 22:04:25  INFO    OGG-00995  Oracle GoldenGate Delivery, res.prm:  REPLICAT RES starting.
2016-04-14 22:04:25  INFO    OGG-03059  Oracle GoldenGate Delivery, res.prm:  Operating system character set identified as UTF-8.
2016-04-14 22:04:25  INFO    OGG-02695  Oracle GoldenGate Delivery, res.prm:  ANSI SQL parameter syntax is used for parameter parsing.
2016-04-14 22:04:25  ERROR   OGG-02554  Oracle GoldenGate Delivery, res.prm:  Error loading shared library ggjava.dll: 2 No such file or directory.
2016-04-14 22:04:25  ERROR   OGG-01668  Oracle GoldenGate Delivery, res.prm:  PROCESS ABENDING.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But hang on &amp;hellip; &lt;strong&gt;&lt;code&gt;ggjava.dll&lt;/code&gt;&lt;/strong&gt; ? &lt;em&gt;&lt;strong&gt;&lt;code&gt;dll&lt;/code&gt;&lt;/strong&gt;&lt;/em&gt;? This is Linux, not Windows.&lt;/p&gt;

&lt;p&gt;So, a quick change to the &lt;code&gt;prm&lt;/code&gt; is in order, switching &lt;code&gt;.dll&lt;/code&gt; for &lt;code&gt;.so&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ogg-bd]$ diff dirprm/res.prm dirprm/res.prm.bak
5c5
&amp;lt; TARGETDB LIBFILE libggjava.so SET property=dirprm/elasticsearch.props
---
&amp;gt; TARGETDB LIBFILE ggjava.dll SET property=dirprm/elasticsearch.props
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;second-time-lucky&#34;&gt;Second time lucky?&lt;/h3&gt;

&lt;p&gt;Redefine the replicat:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 7&amp;gt; delete res
Deleted REPLICAT RES.


GGSCI (bigdatalite.localdomain) 8&amp;gt; add replicat res, exttrail AdapterExamples/trail/tr
REPLICAT added.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And start it again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 9&amp;gt; start res

Sending START request to MANAGER ...
REPLICAT RES starting
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it looks better:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 14&amp;gt; info res

REPLICAT   RES       Last Started 2016-04-14 22:10   Status RUNNING
Checkpoint Lag       00:00:00 (updated 00:00:02 ago)
Process ID           15101
Log Read Checkpoint  File AdapterExamples/trail/tr000000000
                     2015-11-05 18:45:39.000000  RBA 5660
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;result&#34;&gt;Result!&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s check out what&amp;rsquo;s happened in Elasticsearch. The console log looks promising, showing that an index with two mappings has been created:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[2016-04-14 22:10:08,709][INFO ][cluster.metadata         ] [Abner Jenkins] [qasource] creating index, cause [auto(bulk api)], templates [], shards [5]/[1], mappings [tcustmer, tcustord]
[2016-04-14 22:10:09,458][INFO ][cluster.routing.allocation] [Abner Jenkins] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[qasource][4]] ...]).
[2016-04-14 22:10:09,488][INFO ][cluster.metadata         ] [Abner Jenkins] [qasource] update_mapping [tcustmer]
[2016-04-14 22:10:09,658][INFO ][cluster.metadata         ] [Abner Jenkins] [qasource] update_mapping [tcustord]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can confirm that with the Elasticsearch REST API:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl --silent -XGET http://localhost:9200/_cat/indices?pretty=true
yellow open qasource 5 1 8 6 19.6kb 19.6kb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And see how many documents (&amp;ldquo;rows&amp;rdquo;) have been loaded (8):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -s -XGET &#39;http://localhost:9200/qasource/_search?search_type=count&amp;amp;pretty=true&#39;
{
  &amp;quot;took&amp;quot; : 1,
  &amp;quot;timed_out&amp;quot; : false,
  &amp;quot;_shards&amp;quot; : {
    &amp;quot;total&amp;quot; : 5,
    &amp;quot;successful&amp;quot; : 5,
    &amp;quot;failed&amp;quot; : 0
  },
  &amp;quot;hits&amp;quot; : {
    &amp;quot;total&amp;quot; : 8,
    &amp;quot;max_score&amp;quot; : 0.0,
    &amp;quot;hits&amp;quot; : [ ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can even see the mappings (&amp;ldquo;schema&amp;rdquo;) defined within each index:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -XGET &#39;http://localhost:9200/_mapping?pretty=true&#39;
{
  &amp;quot;.kibana&amp;quot; : {
    &amp;quot;mappings&amp;quot; : {
      &amp;quot;config&amp;quot; : {
        &amp;quot;properties&amp;quot; : {
          &amp;quot;buildNum&amp;quot; : {
            &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;,
            &amp;quot;index&amp;quot; : &amp;quot;not_analyzed&amp;quot;
          }
        }
      }
    }
  },
  &amp;quot;qasource&amp;quot; : {
    &amp;quot;mappings&amp;quot; : {
      &amp;quot;tcustord&amp;quot; : {
        &amp;quot;properties&amp;quot; : {
          &amp;quot;CUST_CODE&amp;quot; : {
            &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
          },
          &amp;quot;ORDER_DATE&amp;quot; : {
            &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
          },
          &amp;quot;ORDER_ID&amp;quot; : {
            &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All this faffing about with &lt;code&gt;curl&lt;/code&gt; is fine, but if you&amp;rsquo;re doing proper poking with Elasticsearch you may well find &lt;a href=&#34;https://github.com/lmenezes/elasticsearch-kopf&#34;&gt;kopf&lt;/a&gt; handy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/ogges01.png&#34; alt=&#34;ogges01&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s easy to install:  (modify the path if your Elasticsearch binary is in a different location):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/elasticsearch-2.3.1/bin/plugin install lmenezes/elasticsearch-kopf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After installation, restart Elasticsearch and then go to &lt;a href=&#34;http://localhost:9200/_plugin/kopf&#34;&gt;http://localhost:9200/_plugin/kopf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re using Elasticsearch, you may well be doing so for the whole Elastic experience, using Kibana to view the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/ogges02.png&#34; alt=&#34;ogges02&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and even start doing quick profiling:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/ogges03.png&#34; alt=&#34;ogges03&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One issue with the data that&amp;rsquo;s come through in this example is that it is &lt;em&gt;all&lt;/em&gt; string - even the dates and numerics (AMOUNT, PRICE), which makes instant-analysis in Kibana less possible.&lt;/p&gt;

&lt;h1 id=&#34;streaming-data-from-oracle-to-elasticsearch&#34;&gt;Streaming data from Oracle to Elasticsearch&lt;/h1&gt;

&lt;p&gt;Now that we&amp;rsquo;ve tested and proven the replicat load into Elasticsearch, let&amp;rsquo;s do the full end-to-end. I&amp;rsquo;m going to use the same Extract as the BigDataLite &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/tutorials/obe/fmw/odi/odi_12c/DI_BDL_Guide/BigDataIntegration_Demo.html?cid=10235&amp;amp;ssid=0&#34;&gt;Oracle by Example&lt;/a&gt; (you can see my notes on it &lt;a href=&#34;http://rmoff.net/2016/03/16/oracle-goldengate-kafka-hive-on-bigdatalite-4-4/&#34;&gt;here&lt;/a&gt; if you&amp;rsquo;re interested).&lt;/p&gt;

&lt;p&gt;Reset &amp;amp; recreate the Extract, in the first OGG instance (&lt;code&gt;/u01/ogg&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /u01/ogg/
$ rlwrap ./ggsci

GGSCI (bigdatalite.localdomain as system@cdb/CDB$ROOT) 1&amp;gt; obey dirprm/reset_bigdata.oby

[...]

GGSCI (bigdatalite.localdomain as system@cdb/CDB$ROOT) 2&amp;gt; info all

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING

GGSCI (bigdatalite.localdomain) 3&amp;gt; obey dirprm/bigdata.oby

[...]

GGSCI (bigdatalite.localdomain as system@cdb/CDB$ROOT) 9&amp;gt; info all

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING
EXTRACT     RUNNING     EMOV        00:00:03      00:00:00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now define a new replicat parameter file, over in the second OGG instance (that we used above for the &lt;code&gt;res&lt;/code&gt; test):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;gt; /u01/ogg-bd/dirprm/relastic.prm &amp;lt;&amp;lt;EOF

REPLICAT relastic
TARGETDB LIBFILE libggjava.so SET property=dirprm/elasticsearch.props
REPORTCOUNT EVERY 1 MINUTES, RATE
GROUPTRANSOPS 10000
MAP orcl.moviedemo.movie TARGET orcl.moviedemo.movie;

EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove the previous replicat (&lt;code&gt;res&lt;/code&gt;) just to keep things clear:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /u01/ogg-bd
$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter
Version 12.2.0.1.0 OGGCORE_12.2.0.1.0_PLATFORMS_151101.1925.2
Linux, x64, 64bit (optimized), Generic on Nov 10 2015 16:18:12
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2015, Oracle and/or its affiliates. All rights reserved.


GGSCI (bigdatalite.localdomain) 2&amp;gt; stop res

Sending STOP request to REPLICAT RES ...
Request processed.


GGSCI (bigdatalite.localdomain) 3&amp;gt; delete res
Deleted REPLICAT RES.


GGSCI (bigdatalite.localdomain) 4&amp;gt; info all

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the new one (&lt;code&gt;relastic&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 1&amp;gt; add replicat relastic, exttrail /u01/ogg/dirdat/tm
REPLICAT added.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And start it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 2&amp;gt; start relastic

Sending START request to MANAGER ...
REPLICAT RELASTIC starting

GGSCI (bigdatalite.localdomain) 4&amp;gt; info relastic

REPLICAT   RELASTIC  Last Started 2016-04-14 22:55   Status RUNNING
Checkpoint Lag       00:00:00 (updated 00:00:04 ago)
Process ID           17564
Log Read Checkpoint  File /u01/ogg/dirdat/tm000000000
                     First Record  RBA 1406
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we head over to the Elasticsearch, we&amp;rsquo;ll see that &amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  curl --silent -XGET http://localhost:9200/_cat/indices?pretty=true
yellow open qasource  5 1 8 6 19.6kb 19.6kb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; nothing&amp;rsquo;s changed! Because, of course, nothing&amp;rsquo;s changed on the source Oracle table that the Extract is set up against.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s rectify that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ rlwrap sqlplus system/welcome1@orcl

SQL*Plus: Release 12.1.0.2.0 Production on Thu Apr 14 23:01:57 2016

Copyright (c) 1982, 2014, Oracle.  All rights reserved.

Last Successful login time: Thu Apr 14 2016 22:48:35 +01:00

Connected to:
Oracle Database 12c Enterprise Edition Release 12.1.0.2.0 - 64bit Production
With the Partitioning, OLAP, Advanced Analytics and Real Application Testing options

SQL&amp;gt; INSERT INTO &amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot; (MOVIE_ID, TITLE, YEAR, BUDGET, GROSS, PLOT_SUMMARY) VALUES (&#39;42444&#39;, &#39;never gonna&#39;, &#39;2014&#39;, &#39;500000&#39;, &#39;20000000&#39;, &#39;give you up&#39;);

1 row created.

SQL&amp;gt; COMMIT;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check Elasticsearch again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  curl --silent -XGET http://localhost:9200/_cat/indices?pretty=true
yellow open qasource  5 1 8 6 19.6kb 19.6kb
yellow open moviedemo 5 1 1 0  4.5kb  4.5kb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better - a new index! We&amp;rsquo;ve got a new index because the replicat is handling a different schema this time - moviedemo, not qasource.&lt;/p&gt;

&lt;p&gt;We can look at the data in the index directly:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -XGET &#39;http://localhost:9200/moviedemo/_search?q=*&amp;amp;pretty=true&#39;
{
  &amp;quot;took&amp;quot; : 6,
  &amp;quot;timed_out&amp;quot; : false,
  &amp;quot;_shards&amp;quot; : {
    &amp;quot;total&amp;quot; : 5,
    &amp;quot;successful&amp;quot; : 5,
    &amp;quot;failed&amp;quot; : 0
  },
  &amp;quot;hits&amp;quot; : {
    &amp;quot;total&amp;quot; : 1,
    &amp;quot;max_score&amp;quot; : 1.0,
    &amp;quot;hits&amp;quot; : [ {
      &amp;quot;_index&amp;quot; : &amp;quot;moviedemo&amp;quot;,
      &amp;quot;_type&amp;quot; : &amp;quot;movie&amp;quot;,
      &amp;quot;_id&amp;quot; : &amp;quot;42444&amp;quot;,
      &amp;quot;_score&amp;quot; : 1.0,
      &amp;quot;_source&amp;quot; : {
        &amp;quot;PLOT_SUMMARY&amp;quot; : &amp;quot;give you up&amp;quot;,
        &amp;quot;YEAR&amp;quot; : &amp;quot;2014&amp;quot;,
        &amp;quot;MOVIE_ID&amp;quot; : &amp;quot;42444&amp;quot;,
        &amp;quot;BUDGET&amp;quot; : &amp;quot;500000&amp;quot;,
        &amp;quot;TITLE&amp;quot; : &amp;quot;never gonna&amp;quot;,
        &amp;quot;GROSS&amp;quot; : &amp;quot;20000000&amp;quot;
      }
    } ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll note that the primary key (&lt;code&gt;MOVIE_ID&lt;/code&gt;) has been correctly identied as the unique document &lt;code&gt;_id&lt;/code&gt; field. The &lt;code&gt;_id&lt;/code&gt; is now where things begin to get interesting, because this field enables the new OGG-Elasticsearch adaptor to apparently perform &amp;ldquo;UPSERT&amp;rdquo; on documents that already exist.&lt;/p&gt;

&lt;p&gt;To doublecheck this apparent method of handling of the data, I first wanted to validate what was coming through from OGG in terms of the data flowing through from the extract. To do this I hooked up a second replicat, to Kafka and on to Logstash into Elasticseach (&lt;a href=&#34;http://rmoff.net/2016/03/16/oracle-goldengate-kafka-hive-on-bigdatalite-4-4/&#34;&gt;using this method&lt;/a&gt;), and then compared the doc count in the two relevant indices (or strictly speaking, the mapping types, corresponding to each index).&lt;/p&gt;

&lt;p&gt;To start with, I deleted all my Elasticsearch data, as this shows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;localhost:9200/*/_search?search_type=count&amp;amp;pretty=true&amp;quot; -d &#39;{
    &amp;quot;aggs&amp;quot;: {
        &amp;quot;count_by_type&amp;quot;: {
            &amp;quot;terms&amp;quot;: {
                &amp;quot;field&amp;quot;: &amp;quot;_type&amp;quot;
            }
        }
    }
}&#39;
{
  &amp;quot;took&amp;quot; : 2,
  &amp;quot;timed_out&amp;quot; : false,
  &amp;quot;_shards&amp;quot; : {
    &amp;quot;total&amp;quot; : 1,
    &amp;quot;successful&amp;quot; : 1,
    &amp;quot;failed&amp;quot; : 0
  },
  &amp;quot;hits&amp;quot; : {
    &amp;quot;total&amp;quot; : 0,
    &amp;quot;max_score&amp;quot; : 0.0,
    &amp;quot;hits&amp;quot; : [ ]
  },
  &amp;quot;aggregations&amp;quot; : {
    &amp;quot;count_by_type&amp;quot; : {
      &amp;quot;doc_count_error_upper_bound&amp;quot; : 0,
      &amp;quot;sum_other_doc_count&amp;quot; : 0,
      &amp;quot;buckets&amp;quot; : [ ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I insert a row on &lt;code&gt;&amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot;&lt;/code&gt; in Oracle (having previously truncated it):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; INSERT INTO &amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot; (MOVIE_ID, TITLE, YEAR, BUDGET, GROSS, PLOT_SUMMARY) VALUES (&#39;1&#39;, &#39;never gonna&#39;, &#39;2014&#39;, &#39;500000&#39;, &#39;20000000&#39;, &#39;give you up&#39;);

1 row created.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and see it shows up in both Elasticsearch indices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;localhost:9200/*/_search?search_type=count&amp;amp;pretty=true&amp;quot; -d &#39;{
    &amp;quot;aggs&amp;quot;: {
        &amp;quot;count_by_type&amp;quot;: {
            &amp;quot;terms&amp;quot;: {
                &amp;quot;field&amp;quot;: &amp;quot;_type&amp;quot;
            }
        }
    }
}&#39;
[...]
  }, {
    &amp;quot;key&amp;quot; : &amp;quot;logs&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 1
  }, {
    &amp;quot;key&amp;quot; : &amp;quot;movie&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 1
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;logs&lt;/code&gt;&lt;/strong&gt; is the index mapping loaded through OGG &amp;ndash;&amp;gt; Kafka &amp;ndash;&amp;gt; Logstash &amp;ndash;&amp;gt; Elasticsearch&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;movie&lt;/code&gt;&lt;/strong&gt; is the index mapping loaded through the new adaptor, OGG &amp;ndash;&amp;gt; Elasticsearch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So far, so good. Now, let&amp;rsquo;s add a second row in Oracle:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; INSERT INTO &amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot; (MOVIE_ID, TITLE, YEAR, BUDGET, GROSS, PLOT_SUMMARY) VALUES (&#39;2&#39;, &#39;foo&#39;, &#39;2014&#39;, &#39;500000&#39;, &#39;20000000&#39;, &#39;bar&#39;);

1 row created.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Both indices match count:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &amp;quot;buckets&amp;quot; : [ {
    &amp;quot;key&amp;quot; : &amp;quot;logs&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 2
  }, {
    &amp;quot;key&amp;quot; : &amp;quot;movie&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 2
  }, {
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What about an update?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; UPDATE &amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot; SET TITLE =&#39;Foobar&#39; where movie_id = 1;

1 row updated.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hmmmm&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  &amp;quot;buckets&amp;quot; : [ {
    &amp;quot;key&amp;quot; : &amp;quot;logs&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 3
  }, {
    &amp;quot;key&amp;quot; : &amp;quot;movie&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 2
  }, {
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The index loaded from the OGG-Elasticsearch Adaptor has only two documents still, whilst the other route has three. If we look at what&amp;rsquo;s in the first of these (movie, loaded by OGG-Elasticsearch) for &lt;code&gt;movie_id=1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ogg-bd]$ curl -XGET &#39;http://localhost:9200/moviedemo/_search?q=_id=1&amp;amp;pretty=true&#39;
{
  &amp;quot;took&amp;quot; : 2,
  &amp;quot;timed_out&amp;quot; : false,
  &amp;quot;_shards&amp;quot; : {
    &amp;quot;total&amp;quot; : 5,
    &amp;quot;successful&amp;quot; : 5,
    &amp;quot;failed&amp;quot; : 0
  },
  &amp;quot;hits&amp;quot; : {
    &amp;quot;total&amp;quot; : 1,
    &amp;quot;max_score&amp;quot; : 0.014065012,
    &amp;quot;hits&amp;quot; : [ {
      &amp;quot;_index&amp;quot; : &amp;quot;moviedemo&amp;quot;,
      &amp;quot;_type&amp;quot; : &amp;quot;movie&amp;quot;,
      &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
      &amp;quot;_score&amp;quot; : 0.014065012,
      &amp;quot;_source&amp;quot; : {
        &amp;quot;PLOT_SUMMARY&amp;quot; : &amp;quot;give you up&amp;quot;,
        &amp;quot;YEAR&amp;quot; : &amp;quot;2014&amp;quot;,
        &amp;quot;MOVIE_ID&amp;quot; : &amp;quot;1&amp;quot;,
        &amp;quot;BUDGET&amp;quot; : &amp;quot;500000&amp;quot;,
        &amp;quot;TITLE&amp;quot; : &amp;quot;Foobar&amp;quot;,
        &amp;quot;GROSS&amp;quot; : &amp;quot;20000000&amp;quot;
      }
    } ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see it&amp;rsquo;s the latest version of the row (&lt;code&gt;TITLE=Foobar&lt;/code&gt;). In the second index, loaded from the change record sent to Kafka and then on through Logstash, there are &lt;em&gt;both&lt;/em&gt; the before and after record for this key:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;}
[oracle@bigdatalite ogg-bd]$ curl -XGET &#39;http://localhost:9200/logstash*/_search?q=*&amp;amp;pretty=true&#39;
[...]
      &amp;quot;_source&amp;quot; : {
        &amp;quot;table&amp;quot; : &amp;quot;ORCL.MOVIEDEMO.MOVIE&amp;quot;,
        &amp;quot;op_type&amp;quot; : &amp;quot;I&amp;quot;,
        &amp;quot;op_ts&amp;quot; : &amp;quot;2016-04-14 22:34:43.000000&amp;quot;,
        &amp;quot;current_ts&amp;quot; : &amp;quot;2016-04-14T23:34:45.131000&amp;quot;,
        &amp;quot;pos&amp;quot; : &amp;quot;00000000000000003514&amp;quot;,
        &amp;quot;primary_keys&amp;quot; : [ &amp;quot;MOVIE_ID&amp;quot; ],
        &amp;quot;tokens&amp;quot; : { },
        &amp;quot;before&amp;quot; : null,
        &amp;quot;after&amp;quot; : {
          &amp;quot;MOVIE_ID&amp;quot; : &amp;quot;1&amp;quot;,
          &amp;quot;MOVIE_ID_isMissing&amp;quot; : false,
          &amp;quot;TITLE&amp;quot; : &amp;quot;never gonna&amp;quot;,
          &amp;quot;TITLE_isMissing&amp;quot; : false,

[...]

        &amp;quot;_source&amp;quot; : {
        &amp;quot;table&amp;quot; : &amp;quot;ORCL.MOVIEDEMO.MOVIE&amp;quot;,
        &amp;quot;op_type&amp;quot; : &amp;quot;U&amp;quot;,
        &amp;quot;op_ts&amp;quot; : &amp;quot;2016-04-14 22:39:37.000000&amp;quot;,
        &amp;quot;current_ts&amp;quot; : &amp;quot;2016-04-14T23:39:39.583000&amp;quot;,
        &amp;quot;pos&amp;quot; : &amp;quot;00000000000000004097&amp;quot;,
        &amp;quot;primary_keys&amp;quot; : [ &amp;quot;MOVIE_ID&amp;quot; ],
        &amp;quot;tokens&amp;quot; : { },
        &amp;quot;before&amp;quot; : {
          [...]
          &amp;quot;TITLE&amp;quot; : &amp;quot;never gonna&amp;quot;,
          [...]
        },
        &amp;quot;after&amp;quot; : {
          [...]
          &amp;quot;TITLE&amp;quot; : &amp;quot;Foobar&amp;quot;,
          [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, if I delete a record in Oracle:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL&amp;gt; delete from &amp;quot;MOVIEDEMO&amp;quot;.&amp;quot;MOVIE&amp;quot; where MOVIE_ID = 1;

1 row deleted.

SQL&amp;gt; commit;

Commit complete.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;My document counts reflect what I&amp;rsquo;d expect &amp;ndash; the OGG-Elasticsearch adaptor deleted the record from Elasticsearch, whilst the Kafka route just recorded another change record, of &lt;code&gt;op_type=&#39;D&#39;&lt;/code&gt; this time.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   &amp;quot;key&amp;quot; : &amp;quot;logs&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 4
  }, {
    &amp;quot;key&amp;quot; : &amp;quot;movie&amp;quot;,
    &amp;quot;doc_count&amp;quot; : 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;This adaptor is a pretty smart way of mirroring a table&amp;rsquo;s contents from one of &lt;a href=&#34;http://www.oracle.com/technetwork/middleware/goldengate/certify-100402.html&#34;&gt;the many RDBMS that GoldenGate supports&lt;/a&gt; as an extract source, into Elasticsearch.&lt;/p&gt;

&lt;p&gt;If you want to retain history of changed records, then using &lt;a href=&#34;http://rmoff.net/2016/03/16/oracle-goldengate-kafka-hive-on-bigdatalite-4-4/&#34;&gt;OGG-&amp;gt;Kafka-&amp;gt;Logstash-&amp;gt;Elasticsearch&lt;/a&gt; is an option.&lt;/p&gt;

&lt;p&gt;And, if you don&amp;rsquo;t have the spare cash for OGG, you can use &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;Logstash&amp;rsquo;s JDBC input&lt;/a&gt; mechanism to pull data periodically from your RDBMS. This has the additional benefit of being able to specify custom SQL queries with joins etc - useful when pulling in denormalised datasets into Elasticsearch for analytics.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
			<link>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
			<pubDate>Tue, 12 Apr 2016 21:50:46 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
			<description>

&lt;p&gt;I&amp;rsquo;ve recently been playing around with the ELK stack (&lt;a href=&#34;https://www.elastic.co/blog/heya-elastic-stack-and-x-pack&#34;&gt;now officially known as the Elastic stack&lt;/a&gt;) collecting data from &lt;a href=&#34;http://rmoff.net/2016/03/03/obihackers-irc-channel/&#34;&gt;an IRC channel&lt;/a&gt; with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and &lt;a href=&#34;http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;analysing it with Kibana&lt;/a&gt;. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.&lt;/p&gt;

&lt;p&gt;As I &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;wrote about last year&lt;/a&gt;, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;. Today I&amp;rsquo;m writing up a short real-world example of this in practice. There are three elements to the flexibility that I really want to highlight:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Decoupling the consumption of data from its production at the previous stage&lt;/li&gt;
&lt;li&gt;Because the consumer is decoupled, being able to stop and start it and have it continue ingesting data from the point at which it previously stopped&lt;/li&gt;
&lt;li&gt;The ability to replay the ingest phase of a pipeline repeatedly into multiple consumers, with no change required to the configuration from source&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The simplest form of the pipeline I was using looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/kd01.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A logstash configuration (&lt;a href=&#34;https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-01-logstash-irc-conf&#34;&gt;&lt;code&gt;logstash-irc.conf&lt;/code&gt;&lt;/a&gt;) gets Logstash to connect to the IRC server and send messages received to Elasticsearch. From here they can be displayed and analysed within Kibana. &lt;a href=&#34;http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;Read more about the details here&lt;/a&gt; if you&amp;rsquo;re interested.&lt;/p&gt;

&lt;p&gt;From a &amp;ldquo;pipeline&amp;rdquo; point of view this is a pretty typical pattern. A tool (Logstash here, but could be ODI, Informatica, etc) runs with a set of &amp;ldquo;code&amp;rdquo; (a very simple &lt;code&gt;.conf&lt;/code&gt; here, elsewhere it could be mappings and load plans), reading data from a source. Obviously in full-blown system there&amp;rsquo;s a dozen more moving parts than this simple example, but the point stands.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s think a bit more about what a pipeline does, as this will give us the basis for understanding why and how Kafka fits in so nicely. Overlaying some labels onto the above diagram shows all the processing that we&amp;rsquo;re doing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/kd01a-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If any of this needs reconfiguring, restarting, or rerunning, it&amp;rsquo;s an all-or-nothing job. Given that we&amp;rsquo;re streaming data in near-real-time (or conceptually, designing something that &lt;em&gt;could&lt;/em&gt; if needed with minimal-to-no rework), shutting down the pipeline just to change one of these bits is a problem because we&amp;rsquo;ll lose the data that the source system is spitting out whether we&amp;rsquo;re there to gather it or not.&lt;/p&gt;

&lt;p&gt;Why would we need to change the pipeline configuration? Consider:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Reconfiguring - Adding in additional enrichment functionality (eg GeoIP lookup), or filtering out duff records, or fixing a bug in the logic, or a dozen other easy examples - in all these cases it&amp;rsquo;s great if we can reprocess the existing backlog of processed data and then continue processing data as it&amp;rsquo;s available from the source system.&lt;/li&gt;
&lt;li&gt;Restarting - if the load fails, ideally we don&amp;rsquo;t want to be hitting the source system again for our data if we&amp;rsquo;ve extracted it once already. Similarly if the load process needs to be stopped, maybe for maintenance of the target load system, it&amp;rsquo;s useful to be able to restart the processing exactly from where it left off.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I decoupled the source extract from any subsequent processing with a very simple Logstash configuration (&lt;a href=&#34;https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-02-logstash-irc-kafka-conf&#34;&gt;&lt;code&gt;logstash-irc-kafka.conf&lt;/code&gt;&lt;/a&gt;) that pulls the data from IRC as before and &lt;strong&gt;just&lt;/strong&gt; sends it straight to Kafka:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/kd02a.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The data lands in Kafka, which becomes our &amp;lsquo;staging&amp;rsquo; area in effect, taking advantage of Kafka&amp;rsquo;s &amp;ldquo;durable buffer&amp;rdquo; concept. The data extracted is ideally as raw as possible - because we don&amp;rsquo;t know what subsequent processing we want to do with it, maybe now, or at some future date. Kafka can be configured to retain data based on age or volume - since the data I was working with was low volume I set the topic to retain it for 90 days:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./kafka-topics.sh --zookeeper localhost:2181 --topic irc --alter --config retention.ms=7776000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the data streaming into Kafka and building up there we can then set up one or more consumers of that data. &lt;em&gt;Note that I&amp;rsquo;m using consumers in the logical sense, not the Kafka &amp;ldquo;Consumer&amp;rdquo; specific terminology&lt;/em&gt;. My consumer here is Logstash using &lt;a href=&#34;https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-03-logstash-kafka-es-conf&#34;&gt;&lt;code&gt;logstash-kafka-es.conf&lt;/code&gt;&lt;/a&gt;, which is a variant of the original configuration, this time pulling from Kafka instead of the live IRC feed. And since Kafka is so low-latency, a side-benefit of this setup is that I can both catch up on and replay past records, as well as stream live ones in near-real-time. Result!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/kd03a.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point I&amp;rsquo;m &lt;a href=&#34;http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;where I was before&lt;/a&gt;; streaming IRC content in near-real-time to Elasticsearch and analysing it with Kibana. The only difference is that I&amp;rsquo;ve added in Kafka as a buffer, decoupling the reading messages from IRC with the processing and subsequent storage of them.&lt;/p&gt;

&lt;p&gt;Now here&amp;rsquo;s the money shot &amp;ndash; I can add new consumers of this data that&amp;rsquo;s in Kafka, whenever I want, without needing to know about them at the time that I extracted the source data. I can pick up from the end of the feed, or I can reprocess the whole lot, &lt;em&gt;per consumer&lt;/em&gt;. I&amp;rsquo;ve used this in a couple of instances recently:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add and refine a GeoIP lookup step to the Logstash processing (see &lt;a href=&#34;https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-04-logstash-kafka-es-02-conf&#34;&gt;example config&lt;/a&gt;), &lt;em&gt;without affecting the existing Logstash-&amp;gt;Elasticsearch-&amp;gt;Kibana flow&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Testing the &lt;a href=&#34;https://www.elastic.co/blog/elasticsearch-5-0-0-alpha1-released&#34;&gt;Elastic stack v5 alpha release&lt;/a&gt; by processing the &lt;strong&gt;same source data again&lt;/strong&gt; but with a different version of the downstream tools, enabling a proper like-for-like comparison of the pipeline. This is similar in concept to an idea that &lt;a href=&#34;https://twitter.com/gwenshap&#34;&gt;Gwen Shapira&lt;/a&gt; wrote about &lt;a href=&#34;http://radar.oreilly.com/2015/05/validating-data-models-with-kafka-based-pipelines.html&#34;&gt;in an article in 2015&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both of these cases &lt;strong&gt;the existing original consumer remains running and untouched&lt;/strong&gt;. This kind of concurrent running is a great way to work with a single feed from the source system, keep the data pipeline running for subsequent analytics, whilst also developing and validating new functionality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/kd05a.png&#34; alt=&#34;kd05a&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;consumer-groups-and-offsets&#34;&gt;Consumer Groups and Offsets&lt;/h3&gt;

&lt;p&gt;One of the key concepts in all of this is that of the Kafka &lt;strong&gt;consumer group&lt;/strong&gt;, which is a unique identifier for a given consumer (or group of consumers for the same logical entity if you want to parallelise the consumption). In Kafka 0.8 Zookeeper is used by default to keep track off the &lt;strong&gt;offset&lt;/strong&gt; of the last record that a given consumer group received. So in my development environment I can look on my Kafka server at Zookeeper and see for each consumer group the latest offset: (&lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/System+Tools#SystemTools-ExportZookeeperOffsets&#34;&gt;reference&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/kafka-run-class.sh kafka.tools.ExportZkOffsets --zkconnect localhost:2181 --output-file &amp;gt;(cat)

/consumers/console-consumer-32467/offsets/irc/0:4145
/consumers/kafka-ubuntu03/offsets/irc/0:1035
/consumers/logstash/offsets/irc/0:4145
/consumers/logstash-5-testing/offsets/irc/0:4143
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;A brief note on the command above - I&amp;rsquo;m using &lt;a href=&#34;http://tldp.org/LDP/abs/html/process-sub.html&#34;&gt;bash process substitution&lt;/a&gt; to send the output to stdout (via &lt;code&gt;cat&lt;/code&gt;) instead of the asked-for output file.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From the above output you can see that there are four consumer groups. Two are at the same offset (4145) which happens to be the latest, and therefore have consumed all the available messages. A third (&lt;code&gt;logstash-5-testing&lt;/code&gt;) is almost caught up (4143, vs 4145), and the final one (&lt;code&gt;kafka-ubuntu03&lt;/code&gt;) is way behind at 1035. By running the command periodically you can see if a consumer is actually reading records, or just offline (or maybe stuck).&lt;/p&gt;

&lt;p&gt;To see more information about a given consumer, including the lag (current vs maximum offset) use &lt;code&gt;ConsumerOffsetChecker&lt;/code&gt; and specify the consumer group:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group logstash-5-testing
Group           Topic                          Pid Offset          logSize         Lag             Owner
logstash-5-testing irc                            0   4143            4148            5               none
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Building a successful data pipeline requires that it is flexible to changing requirements, as well as unknown future ones. This is as true for little local PoCs such as this one as it is for large-scale implementations. The pipeline needs to be able to have minimal impact on source systems whilst being able to satisfy multiple destinations, some or all of which may want to batch process instead of stream the data. In addition, being able to re-stream the raw data repeatedly and on-demand into adhoc applications without affecting the primary &amp;lsquo;productionised&amp;rsquo; consumers is a powerful enabler of the &amp;lsquo;data discovery lab&amp;rsquo; concept, which I write about &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;in more detail here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kafka enables the above, summarised in the following benefits:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Stream or batch the data from source &lt;strong&gt;once&lt;/strong&gt;, consume by multiple hetrogenous applications &lt;strong&gt;many&lt;/strong&gt; times.&lt;/li&gt;
&lt;li&gt;Offset tracking distinct for each consuming application&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Processing can be re-run, which is useful for:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Development process - iterative improvements / bug fixing against the same streamed data set&lt;/li&gt;
&lt;li&gt;Production data - data discovery/advanced analytics&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can read more about this in detail &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;over here&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Food Pr0n 02 - Devon &amp; Dorset</title>
			<link>https://rmoff.github.io/2016/04/11/food-pr0n-02-devon-dorset/</link>
			<pubDate>Mon, 11 Apr 2016 19:00:56 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/04/11/food-pr0n-02-devon-dorset/</guid>
			<description>

&lt;p&gt;On a family holiday in South Devon last week I had some good food experiences. Top of the pile was a &lt;strong&gt;&lt;a href=&#34;https://en.m.wikipedia.org/wiki/Lardy_cake&#34;&gt;Lardy Cake&lt;/a&gt;&lt;/strong&gt;, a delicacy new to me but which I&amp;rsquo;ll be sure to be searching out again. It reminded me of an Eccles cake, but bigger and lardier!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/thumb_IMG_6953_1024.jpg&#34; alt=&#34;Lardy Cake&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I got mine from &lt;a href=&#34;https://maps.google.com/?q=Vinnicombes%20Ltd%2C%2060%20High%20St%2C%20Sidmouth%2C%20Devon%20EX10%208EH&amp;amp;ftid=0x486d9ce356de16b5:0xd613601c4557c408&amp;amp;hl=en-GB&amp;amp;gl=uk&#34;&gt;Vinnicombes&lt;/a&gt; on the High Street in Sidmouth.&lt;/p&gt;

&lt;p&gt;Just down the coast from Sidmouth is a village called &lt;a href=&#34;https://maps.google.com/?q=Beer%2C%20Devon&amp;amp;ftid=0x486d8141cbe68345:0xc5a6d16d3c454ca3&amp;amp;hl=en-GB&amp;amp;gl=uk&#34;&gt;Beer&lt;/a&gt;. I didn&amp;rsquo;t have any food of note there, but it&amp;rsquo;s called Beer which is certainly worth recording :)&lt;/p&gt;

&lt;p&gt;A key attribute of any proper British holiday is rain, preferably from previously-clear skies so as to lull one into a false sense of security. Such was the start of a walk from [Lower Eype]() up to [Down House Farm]() where one of the finest cream teas to be had is on offer. I&amp;rsquo;d highly recommend this to anyone in the area (choose the fruit scones over plain, they&amp;rsquo;re the best), but I definitely wouldn&amp;rsquo;t drive it. There&amp;rsquo;s a road, but I use that word in the loosest definition of the term - the potholes are deep and nasty.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6631-1.jpg&#34; alt=&#34;Cream Tea&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Prior to the cream tea we visited the &lt;a href=&#34;http://www.hivebeachcafe.co.uk/&#34;&gt;Hive Beach Cafe&lt;/a&gt; and enjoyed a whole brown crab (the implements and effort required to eat it adding to the enjoyment) and mussels in a curry-type sauce. It&amp;rsquo;s under plastic awning and with the sun coming through was like eating sat in a greenhouse; pro-tip (1) don&amp;rsquo;t sit in the sun, however cold the wind may be outside (2) don&amp;rsquo;t order a crab that you have to dissect yourself if your two and four year old children are going to have finished their food in five minutes and want to leave…&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_1036.JPG&#34; alt=&#34;Crab&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At the beginning of the holiday we went via Bournemouth and had a pretty good fry up at &lt;a href=&#34;http://www.urbanreef.com/&#34;&gt;Urban Reef&lt;/a&gt;. The bacon and sausage were decent, but with no black pudding lost it points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6562.jpg&#34; alt=&#34;Fryup&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On the last day of holiday an overnight stop at Southhampton&amp;rsquo;s Premier Inn North gave the chain chance to redeem itself after the &lt;a href=&#34;http://rmoff.net/2016/03/19/food-pr0n-01/&#34;&gt;poor fry-up showing in Manchester recently&lt;/a&gt;, and redeem itself it did. Decent black pudding, good sausages, good egg, only let down by oddly-chewy bacon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_7375-1.jpg&#34; alt=&#34;Premier Inn Fryup&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;beer-discoveries&#34;&gt;Beer Discoveries&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://gyle59.co.uk/&#34;&gt;Gyle 59&lt;/a&gt;, Toujours. I&amp;rsquo;ve never quite &amp;ldquo;got&amp;rdquo; saison-style beer, having had a great one (Ilkley&amp;rsquo;s &lt;a href=&#34;http://www.ilkleybrewery.co.uk/our-beers/item/siberia&#34;&gt;Siberia&lt;/a&gt;) and a less good one (for a farmhouse ale it was certainly agricultural, it smelt more like the muck heap than anything good) &amp;ndash; but this convinced me that done we it&amp;rsquo;s a nice beer. Hoppy and tasty.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6808.jpg&#34; alt=&#34;Gyle 59, Toujours&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://palmersbrewery.com/ales/cask-ale/best-bitter/&#34;&gt;Palmers Best Bitter&lt;/a&gt;. Pint of this in the &lt;a href=&#34;http://www.theanchorinnseatown.co.uk/&#34;&gt;Anchor Inn at Seatown&lt;/a&gt; overlooking the sea. Most recommended. A good pint of best is as good as any of the gazillions of &amp;ldquo;IPA&amp;rdquo;-labelled blandness out there nowadays, and this was a great pint of best.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6692.jpg&#34; alt=&#34;Palmers Best Bitter&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;footnote-01-a-geek-s-worst-nightmare&#34;&gt;Footnote 01 - A geek&amp;rsquo;s worst nightmare&lt;/h3&gt;

&lt;p&gt;GPRS. All week. Anywhere in South Devon/Dorset. Better would be &amp;ldquo;No Signal&amp;rdquo;, then there&amp;rsquo;s no tantalising suggestion that it might just work…&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6807.jpg&#34; alt=&#34;GPRS&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;footnote-02-a-great-tech-tool-discovery&#34;&gt;Footnote 02 - A great tech tool discovery&lt;/h3&gt;

&lt;p&gt;A colleague of mine recently turned up to a meeting with nothing but his iPhone, and a Bluetooth keyboard. The portability and simplicity of the solution looked great and I soon bought myself an &lt;a href=&#34;http://www.amazon.co.uk/Bluetooth-Ultra-Slim-Aluminum-Keyboard-Windows-Android/dp/B00BKW2410&#34;&gt;Anker T320&lt;/a&gt;. I brought it away on holiday with me and coupled with an iPad mini it&amp;rsquo;s super for writing long passages of text (here using &lt;a href=&#34;http://omz-software.com/editorial/&#34;&gt;Editorial&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/04/IMG_6966.jpg&#34; alt=&#34;iPad with Bluetooth keyboard&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Experiments with Kibana Timelion</title>
			<link>https://rmoff.github.io/2016/03/29/experiments-with-kibana-timelion/</link>
			<pubDate>Tue, 29 Mar 2016 21:07:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/29/experiments-with-kibana-timelion/</guid>
			<description>&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/timelion-timeline&#34;&gt;Timelion&lt;/a&gt; was released in November 2015 and with the 4.4.2 release of &lt;a href=&#34;https://www.elastic.co/products/kibana&#34;&gt;Kibana&lt;/a&gt; is available as a native visualisation once installed. It adds some powerful capabilities to Kibana as an timeseries analysis tool, using its own data manipulation language.&lt;/p&gt;

&lt;p&gt;Installing Timelion is a piece of cake:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./bin/kibana plugin -i kibana/timelion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After restarting Kibana, you&amp;rsquo;ll see it as an option from the application picker&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-13-49.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a bit of a learning curve with Timelion, but it&amp;rsquo;s worth it. &lt;a href=&#34;https://www.elastic.co/blog/timelion-timeline&#34;&gt;The blog&lt;/a&gt; gives some basics, and the built-in help is really good too:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-12-28-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Best of all is the built-in context completion when you&amp;rsquo;re building up your expression:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-16-18.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The default expression, &lt;code&gt;.es(*)&lt;/code&gt; shows a count of everything in Elasticsearch:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-31-40.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can access metrics and perform aggregations on them. In this example I&amp;rsquo;m using OS data collected from &lt;a href=&#34;https://www.elastic.co/products/beats/topbeat&#34;&gt;Topbeat&lt;/a&gt;, and showing the system 1-minute load average:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(metric=&#39;max:load.load1&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-37-23.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can specify multiple metrics to plot, by comma-separating each expression:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(metric=&#39;max:load.load1&#39;), .es(metric=&#39;max:load.load5&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-39-53.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It starts to get really cool when you consider the chaining you can do with the Timelion functions within each expression. For example, adding a moving average to the data is as simple as including the function on the end of the expression:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-*).movingaverage(12)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-43-44.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As well as data manipulation you can do formatting and labelling too, for example here with the &lt;code&gt;.label&lt;/code&gt; function, and combined with the second time series to plot the actual and the moving average:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.es(index=logstash-*),.es(index=logstash-*).movingaverage(12).label(&#39;12hr moving average&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this syntax you can also see how you restrict the timeseries to a given set of Elasticsearch indices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-29_23-45-18.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So that&amp;rsquo;s data from Elasticsearch - but Timelion does more than that. It has support for pulling in data from other sources, including &lt;a href=&#34;https://www.quandl.com/&#34;&gt;Quandl&lt;/a&gt;. This is useful as it provides a great way to access complex datasets for experimenting with Timelion.&lt;/p&gt;

&lt;p&gt;Quandl gives free access to a whole bunch of time series data, including financial, economic and commodity prices. There&amp;rsquo;s a paid option too for the most recent or intra-day data, but plenty of free stuff to play with. To get started simply sign up, which gives you access to your API key. Take this, and add it to the Timelion configuration file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd my-kibana-install-folder
cd installedPlugins/timelion/
cp timelion.json timelion.json.bak
vi timelion.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add your API key into the &lt;code&gt;quandl&lt;/code&gt; part of the configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;quandl&amp;quot;: {
    &amp;quot;key&amp;quot;: &amp;quot;nevergonnagiveyouup&amp;quot;
  },
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart Kibana, and you&amp;rsquo;re good to go. To use Quandl find the timeseries of interest on the Quandl website. In this example we&amp;rsquo;ve got the stock price of &lt;a href=&#34;https://www.quandl.com/data/EURONEXT/HEIA&#34;&gt;Heineken&lt;/a&gt;. In the top-right you&amp;rsquo;ll see the Quandl API code (in this example, it&amp;rsquo;s &lt;code&gt;EURONEXT/HEIA&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-00-13.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Head over to Timelion and create a new timeseries expression, using the Quandl API code you just got and the Timelion &lt;code&gt;.quandl&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.quandl(&#39;EURONEXT/HEIA&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-03-25.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As before, you can plot multiple series on the same chart (note the use of the &lt;code&gt;.yaxis&lt;/code&gt; function here to put the second series on the right-hand y-axis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.quandl(&#39;EURONEXT/HEIA&#39;), .quandl(&#39;DY2/I3020000060&#39;).yaxis(2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-07-36.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Multiple charts can be included on the same sheet, using the &amp;ldquo;Add Chart&amp;rdquo; button available from the options menu on the top-right of the page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-08-28-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With multiple charts note that the hover-over cursor is mirrored on all charts for aiding comprehension:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/timelion01.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The Timelion plugin as seen above is a good place to start for trying it out, but as of 4.4.2 release of Kibana once you&amp;rsquo;ve installed Timelion it&amp;rsquo;s available within Kibana itself:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-22-33.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can build up the visualisation using the same syntax as before:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.quandl(&#39;EURONEXT/HEIA&#39;),.quandl(&#39;GOOG/NASDAQ_MLHR&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-23-18-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since it&amp;rsquo;s a native visualisation object in Kibana, this means that you can include Timelion in your Kibana dashboards too:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-30_00-26-39-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;Just in case you ever want to show CPU performance against the stock price of a beer company&amp;hellip;&lt;/em&gt;)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So I think Timelion is pretty damned neat. It&amp;rsquo;s worth taking the time to figure out the syntax, as its potential is great - and its close integration with Kibana bodes well for its future in the product.&lt;/p&gt;

&lt;p&gt;What are you waiting for? Even if you&amp;rsquo;ve not got any data in Elasticsearch, you can use Quandl to start exploring the potential for Timelion. &lt;a href=&#34;https://www.elastic.co/blog/timelion-timeline&#34;&gt;Go get it now&lt;/a&gt;!&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Connecting to OBIEE via JDBC - with jisql</title>
			<link>https://rmoff.github.io/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/</link>
			<pubDate>Mon, 28 Mar 2016 21:01:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/</guid>
			<description>&lt;p&gt;OBIEE supports JDBC as a connection protocol, using the driver available on all installations of OBIEE, &lt;a href=&#34;https://docs.oracle.com/middleware/11119/biee/BIEIT/odbc_data_source.htm#BIEIT1738&#34;&gt;bijdbc.jar&lt;/a&gt;. This makes connecting to OBIEE from custom or third-party applications very easy. Once connected, you issue &amp;ldquo;Logical SQL&amp;rdquo; against the &amp;ldquo;tables&amp;rdquo; of the Presentation Layer. An example of logical SQL is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT &amp;quot;Time&amp;quot;.&amp;quot;T05 Per Name Year&amp;quot; saw_0 FROM &amp;quot;A - Sample Sales&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To find more Logical SQL simply inspect your nqquery.log (obis-query.log in 12c), or Usage Tracking.&lt;/p&gt;

&lt;p&gt;You can use JDBC from the command line with &lt;a href=&#34;http://www.xigole.com/software/jisql/jisql.jsp&#34;&gt;&lt;code&gt;jisql&lt;/code&gt;&lt;/a&gt;, which is a useful command-line JDBC client. This is a handy way to explore and validate the JDBC connectivity of OBIEE.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll find the OBIEE JDBC driver in &lt;code&gt;$FMW_HOME/Oracle_BI1/bifoundation/jdbc&lt;/code&gt; (11g) or &lt;code&gt;$ORACLE_HOME/bi/bifoundation/jdbc/&lt;/code&gt; (12c)&lt;/p&gt;

&lt;p&gt;Invoke &lt;code&gt;jisql&lt;/code&gt; under java, passing &lt;code&gt;jisql&lt;/code&gt; and &lt;code&gt;bijdbc&lt;/code&gt; in the &lt;code&gt;classpath&lt;/code&gt;, followed by the library name, and then the OBIEE server connection details:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;java \
-classpath jisql-2.0.11.jar:jopt-simple-3.2.jar:lib/javacsv.jar:bijdbc.jar \
com.xigole.util.sql.Jisql \
-driver oracle.bi.jdbc.AnaJdbcDriver \
-cstring jdbc:oraclebi://obieesampleapp:9703/
-user Prodney \
-password Admin123 \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To find out the port that the BI Server is listening on one quick method is with &lt;code&gt;netstat&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@demo biee]$ netstat -plnt|grep nqsserver
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 :::7792                     :::*                        LISTEN      14740/nqsserver
tcp        0      0 :::7793                     :::*                        LISTEN      14740/nqsserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an example from SampleApp v511 (OBIEE 12c) - the BI Server listens on two ports, one for ODBC/JDBC inbound, the other for Cluster Controller &amp;lsquo;heartbeats&amp;rsquo;. In this case it&amp;rsquo;s 7792 and 7793 respectively, and you&amp;rsquo;d be able to confirm for definite by checking the config/log files.&lt;/p&gt;

&lt;p&gt;So back to the connection example, using an older version of SampleApp, in which the BI Server is listening on port 9703. Here&amp;rsquo;s an example of connecting, and by using &lt;code&gt;-driverinfo&lt;/code&gt; as well it show&amp;rsquo;s some additional information about the connection before exiting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@obieesampleapp jisql-2.0.11]$ ~/obiee/Oracle_BI1/jdk/bin/java -classpath lib/jisql-2.0.11.jar:lib/jopt-simple-3.2.jar:lib/javacsv.jar:/home/oracle/obiee/Oracle_BI1/bifoundation/jdbc/bijdbc.jar com.xigole.util.sql.Jisql -user Prodney -password Admin123 -driver oracle.bi.jdbc.AnaJdbcDriver -driverinfo -debug -cstring  jdbc:oraclebi://obieesampleapp:9703/
Feb 6, 2013 10:10:39 PM oracle.bi.jdbc.AnaJdbcDriver connect
INFO: connect to SECONDARYCCSPORT=9706;MAXRECONNECTATTEMPTS=3;TRUSTANYSERVER=true;PRIMARYCCSPORT=9706;MAXRPCCLIENTCREATEATTEMPTS=3;USER=Prodney;HEARTBEATINTERVAL=60;MAXHEARTBEATATTEMPTS=3;MAXRPCCLIENTCOUNT=100;SSL=false;TRUSTSTOREPASSWORD=***;PASSWORD=***;SECONDARYCCS=;PORT=9703;CATALOG=;HOST=obieesampleapp;PRIMARYCCS=;SSLKEYSTOREPASSWORD=***;RPCCLIENTEXPIRATIONTIME=60;
driver.getMajorVersion() is 1
driver.getMinorVersion() is 0
driver is not JDBC compliant
metaData.getDatabaseProductName(): &amp;quot;Oracle Business Intelligence&amp;quot;
metaData.getDatabaseProductVersion(): &amp;quot;11.1.1.6.2&amp;quot;
metaData.getDriverName(): &amp;quot;Oracle Business Intelligence&amp;quot;
metaData.getDriverVersion(): &amp;quot;11.1.1.6.2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the same as above, but without &lt;code&gt;-driverinfo&lt;/code&gt; to get a query prompt. To issue a command in &lt;code&gt;jisql&lt;/code&gt;, use &lt;code&gt;go&lt;/code&gt; as command terminator:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Enter a query:
1 &amp;gt; SELECT 0 s_0, &amp;quot;Airlines Traffic&amp;quot;.&amp;quot;Month&amp;quot;.&amp;quot;Month of Year&amp;quot; s_1,   &amp;quot;Airlines Traffic&amp;quot;.&amp;quot;Route&amp;quot;.&amp;quot;Route&amp;quot; s_2, &amp;quot;Airlines Traffic&amp;quot;.&amp;quot;Traffic Facts&amp;quot;.&amp;quot;# Depts Performed&amp;quot; s_3 FROM &amp;quot;Airlines Traffic&amp;quot; ORDER BY 1, 2 ASC NULLS LAST, 3 ASC NULLS LAST fetch first 5 rows only
2 &amp;gt; go
         s_0 |          s_1 |      s_2 |                    s_3 | 
-------------+--------------+----------+------------------------|
           0 |            1 |  ABE-ATL |                   51.0 | 
           0 |            1 |  ABE-AVP |                   22.0 | 
           0 |            1 |  ABE-CLE |                   42.0 | 
           0 |            1 |  ABE-CLT |                   85.0 | 
           0 |            1 |  ABE-DTW |                   65.0 | 

Enter a query:
1 &amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also pass an input file that holds the commands to run. Remember the command terminator - by default it&amp;rsquo;s &lt;code&gt;go&lt;/code&gt; so that needs to be in your input file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@obieesampleapp jisql-2.0.11]$ time ~/obiee/Oracle_BI1/jdk/bin/java -classpath lib/jisql-2.0.11.jar:lib/jopt-simple-3.2.jar:lib/javacsv.jar:/home/oracle/obiee/Oracle_BI1/bifoundation/jdbc/bijdbc.jar com.xigole.util.sql.Jisql -user Prodney -password Admin123 -driver oracle.bi.jdbc.AnaJdbcDriver -cstring  jdbc:oraclebi://obieesampleapp:9703/ -input ~/test_report.lsql | tail
Feb 6, 2013 10:16:46 PM oracle.bi.jdbc.AnaJdbcDriver connect
INFO: connect to SECONDARYCCSPORT=9706;MAXRECONNECTATTEMPTS=3;TRUSTANYSERVER=true;PRIMARYCCSPORT=9706;MAXRPCCLIENTCREATEATTEMPTS=3;USER=Prodney;HEARTBEATINTERVAL=60;MAXHEARTBEATATTEMPTS=3;MAXRPCCLIENTCOUNT=100;SSL=false;TRUSTSTOREPASSWORD=***;PASSWORD=***;SECONDARYCCS=;PORT=9703;CATALOG=;HOST=obieesampleapp;PRIMARYCCS=;SSLKEYSTOREPASSWORD=***;RPCCLIENTEXPIRATIONTIME=60;
           0 |           12 |  XNA-LAX |                    9.0 | 
           0 |           12 |  XNA-LEX |                    1.0 | 
           0 |           12 |  XNA-LGA |                   54.0 | 
           0 |           12 |  XNA-MEM |                   85.0 | 
           0 |           12 |  XNA-MSP |                   52.0 | 
           0 |           12 |  XNA-OKC |                    1.0 | 
           0 |           12 |  XNA-ORD |                  186.0 | 
           0 |           12 |  YUM-IPL |                   31.0 | 
           0 |           12 |  YUM-LAX |                  116.0 | 
           0 |           12 |  YUM-PHX |                  186.0 | 

real    0m5.732s
user    0m0.849s
sys     0m2.761s
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>My latest IRC client : Kibana</title>
			<link>https://rmoff.github.io/2016/03/24/my-latest-irc-client-kibana/</link>
			<pubDate>Thu, 24 Mar 2016 21:38:02 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/24/my-latest-irc-client-kibana/</guid>
			<description>&lt;p&gt;OK, maybe that&amp;rsquo;s not entirely true. But my &lt;em&gt;read-only&lt;/em&gt; client, certainly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-24_21-15-30.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I was perusing the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/input-plugins.html&#34;&gt;Logstash input plugins&lt;/a&gt; recently when I noticed that there was one for &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-inputs-irc.html&#34;&gt;IRC&lt;/a&gt;. Being a fan of IRC and a regular on the &lt;a href=&#34;http://rmoff.net/2016/03/03/obihackers-irc-channel/&#34;&gt;#obihackers&lt;/a&gt; channel, I thought this could be fun and yet another great example of how easy &lt;a href=&#34;http://elastic.co&#34;&gt;the Elastic stack&lt;/a&gt; is to work with.&lt;/p&gt;

&lt;p&gt;Installation is a piece of cake:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip
wget https://download.elastic.co/logstash/logstash/logstash-2.2.2.zip
wget https://download.elastic.co/kibana/kibana/kibana-4.4.2-linux-x64.tar.gz
unzip \*.zip
tar -xf kibana-4.4.2-linux-x64.tar.gz
sudo mv elasticsearch-2.2.1 logstash-2.2.2 kibana-4.4.2-linux-x64 /opt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(you&amp;rsquo;ll also need Oracle JDK installed if not already, &lt;a href=&#34;http://www.jamescoyle.net/how-to/1897-download-oracle-java-from-the-terminal-with-wget&#34;&gt;here&amp;rsquo;s a handy way to get it from the CLI&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Start up Elasticsearch and Kibana:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/opt/elasticsearch-2.2.1/bin/elasticsearch
/opt/kibana-4.4.2-linux-x64/bin/kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Use &lt;a href=&#34;https://en.wikipedia.org/wiki/GNU_Screen&#34;&gt;screen&lt;/a&gt;, cos it&amp;rsquo;s awesome, to run these in parallel on the same SSH connection.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-24_21-00-03.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now create a file (e.g. &lt;code&gt;logtash-irc.conf&lt;/code&gt;) to hold the Logstash configuration. It&amp;rsquo;s very simple - connect to the IRC server, on a given channel, then add geographical attributes to each message based on the host of the user, and then dump the whole lot to both &lt;code&gt;stdout&lt;/code&gt; and Elasticsearch:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# @rmoff / March 24, 2016
input {
    irc {
        channels =&amp;gt; &amp;quot;#obihackers&amp;quot;
        host =&amp;gt; &amp;quot;chat.freenode.net&amp;quot;
    }
}

filter {
    geoip {
        source =&amp;gt; &amp;quot;host&amp;quot;
    }
}

output {
    stdout {
        codec =&amp;gt; &amp;quot;rubydebug&amp;quot;
    }
    elasticsearch {
        hosts =&amp;gt; &amp;quot;localhost&amp;quot;
        index =&amp;gt; &amp;quot;logstash-irc-%{+YYYY.MM.dd}&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now set Logstash running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/opt/logstash-2.2.2/bin/logstash -f logstash-irc.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now any message to the channel will get picked up by the bot, sent to Elasticsearch, and echoed to &lt;code&gt;stdout&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;{
       &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;ChristianBerg: LOL, never thought that before&amp;quot;,
      &amp;quot;@version&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;,
    &amp;quot;@timestamp&amp;quot; =&amp;gt; &amp;quot;2016-03-24T15:52:47.616Z&amp;quot;,
          &amp;quot;user&amp;quot; =&amp;gt; &amp;quot;rmoff!~rmoff@12345&amp;quot;,
       &amp;quot;command&amp;quot; =&amp;gt; &amp;quot;PRIVMSG&amp;quot;,
       &amp;quot;channel&amp;quot; =&amp;gt; &amp;quot;#obihackers&amp;quot;,
          &amp;quot;nick&amp;quot; =&amp;gt; &amp;quot;rmoff&amp;quot;,
        &amp;quot;server&amp;quot; =&amp;gt; &amp;quot;chat.freenode.net:6667&amp;quot;,
          &amp;quot;host&amp;quot; =&amp;gt; &amp;quot;host-12345&amp;quot;,
         &amp;quot;geoip&amp;quot; =&amp;gt; {
                      &amp;quot;ip&amp;quot; =&amp;gt; &amp;quot;1.2.3.4&amp;quot;,
           &amp;quot;country_code2&amp;quot; =&amp;gt; &amp;quot;GB&amp;quot;,
           &amp;quot;country_code3&amp;quot; =&amp;gt; &amp;quot;GBR&amp;quot;,
            &amp;quot;country_name&amp;quot; =&amp;gt; &amp;quot;United Kingdom&amp;quot;,
          &amp;quot;continent_code&amp;quot; =&amp;gt; &amp;quot;EU&amp;quot;,
             &amp;quot;region_name&amp;quot; =&amp;gt; &amp;quot;B4&amp;quot;,
               &amp;quot;city_name&amp;quot; =&amp;gt; &amp;quot;Shipley&amp;quot;,
                &amp;quot;latitude&amp;quot; =&amp;gt; 53.83330000000001,
               &amp;quot;longitude&amp;quot; =&amp;gt; -1.766699999999986,
                &amp;quot;timezone&amp;quot; =&amp;gt; &amp;quot;Europe/London&amp;quot;,
        &amp;quot;real_region_name&amp;quot; =&amp;gt; &amp;quot;Bradford&amp;quot;,
                &amp;quot;location&amp;quot; =&amp;gt; [
            [0] -1.766699999999986,
            [1] 53.83330000000001
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can quickly check that the data&amp;rsquo;s making it into Elasticsearch by running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -XGET &#39;http://localhost:9200/logstash-irc-*/_search?pretty&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get something like this back:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;took&amp;quot; : 6,
  &amp;quot;timed_out&amp;quot; : false,
  &amp;quot;_shards&amp;quot; : {
    &amp;quot;total&amp;quot; : 5,
    &amp;quot;successful&amp;quot; : 5,
    &amp;quot;failed&amp;quot; : 0
  },
  &amp;quot;hits&amp;quot; : {
    &amp;quot;total&amp;quot; : 278,
    &amp;quot;max_score&amp;quot; : 1.0,
    &amp;quot;hits&amp;quot; : [ {
      &amp;quot;_index&amp;quot; : &amp;quot;logstash-irc-2016.03.24&amp;quot;,
      &amp;quot;_type&amp;quot; : &amp;quot;logs&amp;quot;,
      &amp;quot;_id&amp;quot; : &amp;quot;AVOpXg1lfUfBfaUyS5CU&amp;quot;,
      &amp;quot;_score&amp;quot; : 1.0,
      &amp;quot;_source&amp;quot; : {
        &amp;quot;message&amp;quot; : &amp;quot;rmoff: I can&#39;t even get an IP from hugh_jass&amp;quot;,
        &amp;quot;@version&amp;quot; : &amp;quot;1&amp;quot;,
        &amp;quot;@timestamp&amp;quot; : &amp;quot;2016-03-24T11:58:57.401Z&amp;quot;,
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the data&amp;rsquo;s in Elasticsearch, it&amp;rsquo;s a piece of cake to knock up a quick dashboard in Kibana with auto-refresh switched on, showing the current channel activity and some key stats for the day:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-24_21-13-16.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;ve not built a Kibana dashboard before, check out &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;other&lt;/a&gt; &lt;a href=&#34;http://www.rittmanmead.com/2015/04/using-the-elk-stack-to-analyse-donors-choose-data/&#34;&gt;articles&lt;/a&gt; I&amp;rsquo;ve written which walk through the process.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Food Pr0n - 01</title>
			<link>https://rmoff.github.io/2016/03/19/food-pr0n-01/</link>
			<pubDate>Sat, 19 Mar 2016 21:18:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/19/food-pr0n-01/</guid>
			<description>&lt;p&gt;One of the perks of my job is that I get to travel to some pretty nice places (hi, San Francisco, Bergen, Åland Islands), and get to eat some pretty good food too. If you&amp;rsquo;re looking for some techie content, then move along and go and read about &lt;a href=&#34;http://rmoff.net/2016/03/16/oracle-goldengate-kafka-hive-on-bigdatalite-4-4/&#34;&gt;Kafka&lt;/a&gt;, but if you enjoy food pr0n then stay put.&lt;/p&gt;

&lt;p&gt;I was working for a client in the centre of Manchester this week, staying as usual at a Premier Inn. I&amp;rsquo;m a big fan of my &lt;a href=&#34;http://rmoff.net/2016/02/26/fullenglish/&#34;&gt;fried breakfasts&lt;/a&gt;, and whilst never anything to write home about (or indeed write a blog about) Premier Inn breakfasts are generally fine for what they are &amp;ndash; but this one was a real disappointment. Even with the excitement of seeing &lt;a href=&#34;https://en.wikipedia.org/wiki/Bubble_and_squeak&#34;&gt;bubble and squeak&lt;/a&gt;, it was a real let down; luke warm, and the sausages in particular tasted really artificially flavoured.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3694--1--2.JPG&#34; alt=&#34;Premier Inn Fryup - hopefully just a blip on their record of &#39;not bad&#39; usually&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fortunately my eating the rest of the week more than made up for having to settle for a bowl of bran flakes for breakfast. At lunchtime my colleague and I were really spoilt by the nearby venue &lt;strong&gt;&lt;a href=&#34;http://thekitchensleftbank.com/&#34;&gt;The Kitchens&lt;/a&gt;&lt;/strong&gt;. Within it there&amp;rsquo;s separate little businesses each making a different type of food. It was so good we went back each day to try something different:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pick of the bunch was definitely &lt;strong&gt;&lt;a href=&#34;https://twitter.com/thehiphopchippy&#34;&gt;The Hip Hop Chip Shop&lt;/a&gt;&lt;/strong&gt;. Their &amp;ldquo;&lt;strong&gt;Meat Junkie Meal&lt;/strong&gt;&amp;rdquo; was fabulously awesome. Picture this : a really good sausage. Not the kind of crap that Premier Inn are served up (see above), but a really well made sausage. Now wrap it in good quality bacon. Now batter it, and deep fry it. As if that wasn&amp;rsquo;t enough, serve it with some of the best onion rings I&amp;rsquo;ve ever tasted, really good chips &amp;ndash; and a fantastic chip shop gravy with a cheeky bit of chilli in it.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3698.jpg&#34; alt=&#34;Meat Junkie Meal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no way you&amp;rsquo;d eat this more than once in a while given the amount of fried material in one serving, but my god, it&amp;rsquo;s worth the risk of coronary disease (and 3rd degree burns from the battered sausage)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Next up was &lt;strong&gt;&lt;a href=&#34;https://twitter.com/bangersandbacon&#34;&gt;Bangers and Bacon&lt;/a&gt;&lt;/strong&gt;. I&amp;rsquo;d never even heard of a Cubano sandwich before let alone tasted one, but after this gem I&amp;rsquo;ll be sure to look out for them. Pork, ham, cheese, mustard, and pickles - what&amp;rsquo;s not to be liked in that! I&amp;rsquo;m a bit of a sucker for anything with a good pickle in it to be honest.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Last was &lt;strong&gt;&lt;a href=&#34;https://twitter.com/wellhungkitchen&#34;&gt;Well Hung&lt;/a&gt;&lt;/strong&gt;. My first ever Philly Cheesesteak Sandwich wasn&amp;rsquo;t quite the revelation of the Cubano above, but still a pretty decent sandwich.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Neck-and-neck with the Meat Junkie Meal for accolade of &amp;ldquo;meal of the week&amp;rdquo; was the magnificence of both the starter and main I had for an evening meal at &lt;a href=&#34;http://www.beefandpudding.co.uk/&#34;&gt;Beef and Pudding&lt;/a&gt;. The &amp;ldquo;Bury Bomb&amp;rdquo; was Black Pudding with Lancashire cheese wrapped in potato, and astoundingly good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3687.jpg&#34; alt=&#34;Bury Bomb&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Something rarely seen on restaurant menus, and for shame really, is the suet pudding. This one was filled with chunks of beef and served with mash, mushy peas, and gravy - and was superb.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3689.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Elsewhere my evening meals took me to &lt;a href=&#34;http://thehawksmoor.com/locations/manchester/&#34;&gt;Hawksmoor&lt;/a&gt; for a very good burger (make sure you get the shortribs with it):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3692.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and to &lt;a href=&#34;http://www.mowglistreetfood.com/&#34;&gt;Mowgli&lt;/a&gt; for some interesting Indian &amp;ldquo;Street Food&amp;rdquo;, the highlight of which was a ginger and rhubarb dhal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/IMG_3706.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>OBIEE 11.1.1.9 installation - JPS-06514: Opening of file based keystore failed</title>
			<link>https://rmoff.github.io/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/</link>
			<pubDate>Fri, 18 Mar 2016 18:04:07 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/</guid>
			<description>&lt;p&gt;I got this lovely failure &lt;strong&gt;during a fresh install&lt;/strong&gt; of OBIEE 11.1.1.9. I emphasise that it was during the install because there&amp;rsquo;s other causes for this error &lt;strong&gt;on an existing system&lt;/strong&gt; to do with corrupted credential stores etc &amp;ndash; not the case here.&lt;/p&gt;

&lt;p&gt;The install had copied in the binaries and was in the process of building the domain. During the early stages of this where it starts configuring and restarting the AdminServer it failed, with the AdminServer.log showing the following: (I&amp;rsquo;ve extracted the salient errors from the log)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;BEA-090892&amp;gt; &amp;lt;The loading of OPSS java security policy provider failed due to exception, see the exception stack trace or the server log file for root cause. If still see no obvious cause, enable the debug flag -Djava.security.debug=jpspolicy to get more information. Error message: JPS-06514: Opening of file based keystore failed.&amp;gt;

&amp;lt;BEA-000386&amp;gt; &amp;lt;Server subsystem failed. Reason: weblogic.security.SecurityInitializationException: The loading of OPSS java security policy provider failed due to exception, see the exception stack trace or the server log file for root cause. If still see no obvious cause, enable the debug flag -Djava.security.debug=jpspolicy to get more information. Error message: JPS-06514: Opening of file based keystore failed.
weblogic.security.SecurityInitializationException: The loading of OPSS java security policy provider failed due to exception, see the exception stack trace or the server log file for root cause. If still see no obvious cause, enable the debug flag -Djava.security.debug=jpspolicy to get more information. Error message: JPS-06514: Opening of file based keystore failed.

Caused By: oracle.security.jps.service.keystore.KeyStoreServiceException: JPS-06519: Failed to get/set credential with map fks and key null in bootstrap credstore. Reason oracle.security.jps.service.keystore.KeyStoreServiceException: JPS-06519: Failed to get/set credential with map fks and key current.key in bootstrap credstore. Reason null

Caused By: oracle.security.jps.service.keystore.KeyStoreServiceException: JPS-06519: Failed to get/set credential with map fks and key current.key in bootstrap credstore. Reason null

Caused By: oracle.security.jps.service.credstore.CredStoreException: JPS-01061: Access to bootstrap credential store denied to application code.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The cause? An old JDK and/or set of environment variables. The machine I was installing on already had an existing legacy 11.1.1.6 install, and I was doing a side-by-side out-of-place upgrade (patch migration). For various reasons I was using the same OS user as the existing install, and in the &lt;code&gt;.bash_profile&lt;/code&gt; of this user there was a number of environment variables set pointing to the existing installation.&lt;/p&gt;

&lt;p&gt;I wasn&amp;rsquo;t sure if the install &amp;amp; domain build process spawns additional shells that mightn&amp;rsquo;t inherit the environment variables of the launching session (and instead use those defined in the &lt;code&gt;.bash_profile&lt;/code&gt;). So instead of simply unsetting the environment variables prior to launching &lt;code&gt;runInstaller.sh&lt;/code&gt;, or running it with a &lt;code&gt;env -i&lt;/code&gt; prefix (thanks, &lt;a href=&#34;https://twitter.com/sudoed&#34;&gt;etcSudoers&lt;/a&gt;!), I amended the &lt;code&gt;.bash_profile&lt;/code&gt; (having backed it up first, of course) to remove all of the environment variables that it was setting. After relaunching my session, the installation ran through with no problem.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
			<link>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
			<pubDate>Wed, 16 Mar 2016 22:01:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
			<description>

&lt;p&gt;The Oracle by Example (ObE) &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/tutorials/obe/fmw/odi/odi_12c/DI_BDL_Guide/BigDataIntegration_Demo.html?cid=10235&amp;amp;ssid=0&#34;&gt;here&lt;/a&gt; demonstrating how to use &lt;a href=&#34;[https://docs.oracle.com/goldengate/bd1221/gg-bd/GBDIN/intro_adapter.htm#GBDIN101](http://)&#34;&gt;Goldengate to replicate transactions big data targets&lt;/a&gt; such as HDFS is written for the BigDataLite &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite421-2843803.html&#34;&gt;4.2.1&lt;/a&gt;, and for me didn&amp;rsquo;t work on the current latest version, &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;4.4.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The OBE (and similar &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/odi/ODI_BigData_HOL.pdf&#34;&gt;Hands On Lab&lt;/a&gt; PDF) assume the presence of &lt;code&gt;pmov.prm&lt;/code&gt; and &lt;code&gt;pmov.properties&lt;/code&gt; in &lt;code&gt;/u01/ogg/dirprm/&lt;/code&gt;. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, &lt;code&gt;emov&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.4, with bells on because it includes &lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; too. And, who doesn&amp;rsquo;t like a bit of Kafka nowadays?&lt;/p&gt;

&lt;h3 id=&#34;getting-it-running&#34;&gt;Getting it running&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Set the Oracle extract running (as per the OBE/HOL instructions).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I&amp;rsquo;m using the ever-awesome &lt;code&gt;rlwrap&lt;/code&gt; so that if I mistype stuff in &lt;code&gt;ggsci&lt;/code&gt; I can just arrow up/down to cycle through command history.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ~]$ cd /u01/ogg
[oracle@bigdatalite ogg]$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter for Oracle
Version 12.2.0.1.0 OGGCORE_12.2.0.1.0_PLATFORMS_151101.1925.2_FBO
Linux, x64, 64bit (optimized), Oracle 12c on Nov 11 2015 03:53:23
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2015, Oracle and/or its affiliates. All rights reserved.



GGSCI (bigdatalite.localdomain) 1&amp;gt; obey dirprm/bigdata.oby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the output differs from the OBE/HOL screenshot, with only the &lt;code&gt;emov&lt;/code&gt; extract listed now:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain as system@cdb/CDB$ROOT) 9&amp;gt; info all

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING
EXTRACT     RUNNING     EMOV        04:00:10      00:00:01
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;strong&gt;Ctrl-D&lt;/strong&gt; to exit &lt;code&gt;ggsci&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Launch &lt;code&gt;ggsci&lt;/code&gt; again, but from the &lt;code&gt;/u01/ogg-bd&lt;/code&gt; folder this time. Run the same-named bigdata obey file, but note that it&amp;rsquo;s a different set of instructions (because we&amp;rsquo;re now in &lt;code&gt;/u01/ogg-bd&lt;/code&gt;, rather than &lt;code&gt;/u01/ogg&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ogg]$ cd /u01/ogg-bd
[oracle@bigdatalite ogg-bd]$ rlwrap ./ggsci

Oracle GoldenGate Command Interpreter
Version 12.2.0.1.0 OGGCORE_12.2.0.1.0_PLATFORMS_151101.1925.2
Linux, x64, 64bit (optimized), Generic on Nov 10 2015 16:18:12
Operating system character set identified as UTF-8.

Copyright (C) 1995, 2015, Oracle and/or its affiliates. All rights reserved.

GGSCI (bigdatalite.localdomain) 1&amp;gt; obey dirprm/bigdata.oby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looking at what&amp;rsquo;s running we can see two replicats:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GGSCI (bigdatalite.localdomain) 8&amp;gt; INFO ALL

Program     Status      Group       Lag at Chkpt  Time Since Chkpt

MANAGER     RUNNING
REPLICAT    RUNNING     RKAFKA      00:00:00      00:00:01
REPLICAT    RUNNING     RMOV        00:00:00      00:00:02
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Poking around the Kafka parameters we can see the configured topic for the transactions and schema. For full details about the Kafka handler &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/kafka_handler.htm#GADBD449&#34;&gt;see the documentation&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ogg-bd]$ cat dirprm/kafka.props

gg.handlerlist = kafkahandler
gg.handler.kafkahandler.type = kafka
gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties
gg.handler.kafkahandler.TopicName =oggtopic
gg.handler.kafkahandler.format =avro_op
gg.handler.kafkahandler.SchemaTopicName=mySchemaTopic
gg.handler.kafkahandler.BlockingSend =false
gg.handler.kafkahandler.includeTokens=false

gg.handler.kafkahandler.mode =tx
#gg.handler.kafkahandler.maxGroupSize =100, 1Mb
#gg.handler.kafkahandler.minGroupSize =50, 500Kb
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;testing-it-out&#34;&gt;Testing it out&lt;/h3&gt;

&lt;p&gt;Using the Kafka console shell we can observe what Oracle GoldenGate sends to Kafka:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite dirprm]$ kafka-console-consumer --zookeeper localhost:2181 --topic oggtopic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a separate session (or even better, in the same session but using &lt;code&gt;screen&lt;/code&gt; as in the demo below) modify data in the &lt;code&gt;MOVIEDEMO.MOVIE&lt;/code&gt; table on Oracle. You should see the change come through to Kafka after a few moments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/ogg-kafka.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;always-rtfm&#34;&gt;Always RTFM…&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;The manual? That thing that explains how things work, and what problems to watch out for? Yeah… about that…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So I got the Kafka bit working above and was happy, it worked, it was neat. But, for the life of me I couldn&amp;rsquo;t get the transactions to appear in Hive. They appeared in the HDFS file when I &lt;code&gt;hadoop fs -cat&lt;/code&gt;&amp;rsquo;d it, they showed up in the Hue data browser &amp;hellip; but not in Hive. Was this some &lt;a href=&#34;http://marcelkrcah.net/blog/how-newline-can-ruin-your-hive/&#34;&gt;weird bug/issue&lt;/a&gt; involving new lines? What was going on?&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what I saw in HDFS. Note the last line, 22:32:21:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite ogg]$ sudo su - hdfs -c &amp;quot;hadoop fs -cat /user/odi/hive/orcl.moviedemo.movie/*&amp;quot;
D2016-03-16 22:14:47.0000001Foo201450000020000000give you up
D2016-03-16 22:14:47.0000002never gonna201450000020000000give you up
D2016-03-16 22:14:47.0000003never gonna201450000020000000give you up
D2016-03-16 22:14:47.0000004never gonna201450000020000000give you up
I2016-03-16 22:27:18.0000002Sharknadozz201450000020000000Flying sharks attack city
I2016-03-16 22:32:21.0000004242never gonna201450000020000000give you up
[oracle@bigdatalite ogg]$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is what I saw in Hive - only five of the six rows of data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hive&amp;gt; select * from movie_updates;
OK
D       2016-03-16 22:14:47             1       Foo     2014    500000  20000000        give you up
D       2016-03-16 22:14:47             2       never gonna     2014    500000  20000000        give you up
D       2016-03-16 22:14:47             3       never gonna     2014    500000  20000000        give you up
D       2016-03-16 22:14:47             4       never gonna     2014    500000  20000000        give you up
I       2016-03-16 22:27:18             2       Sharknadozz     2014    500000  20000000        Flying sharks attack city
Time taken: 0.087 seconds, Fetched: 5 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;​
Turns out &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/hdfs_handler.htm#GADBD395&#34;&gt;the manual&lt;/a&gt; spells this out pretty darn clearly in a section cunningly named &lt;strong&gt;Common Pitfalls&lt;/strong&gt; it notes that&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;HDFS blocks under construction are not always visible to analytic tools.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And since I&amp;rsquo;m noodling around with a few rows of data here and there (nowhere near the 128MB HDFS block size), this was the very cause of my issue. A workaround to prove that I wasn&amp;rsquo;t going mad? Restart the GoldenGate replicat with the &lt;code&gt;rmov.properties&lt;/code&gt; file changed to close the HDFS file periodically:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gg.handler.hdfs.fileRollInterval=30s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Obviously this has performance implications in a real-life implementation, but for proving out functionality, it saved me from complete insanity :-)&lt;/p&gt;

&lt;h3 id=&#34;sidenote-error-in-reset-bigdata-oby&#34;&gt;Sidenote - error in reset_bigdata.oby?&lt;/h3&gt;

&lt;p&gt;I might be missing something here, but it looks like there&amp;rsquo;s a minor fubar in &lt;code&gt;/u01/ogg-bd/dirprm/reset_bigdata.oby&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;start mgr
stop rmov
stop rkafka
shell sleep 5
delete rmov
stop rkafka
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That second &lt;code&gt;stop rkafka&lt;/code&gt; I&amp;rsquo;m guessing should be &lt;code&gt;delete rkafka&lt;/code&gt;?&lt;/p&gt;

&lt;h3 id=&#34;logstash&#34;&gt;Logstash&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m a long-time fan of the &lt;a href=&#34;http://elastic.co&#34;&gt;Elastic stack&lt;/a&gt;, and Logstash has an input plugin for Kafka, so let&amp;rsquo;s see if that can fit the jigsaw here too.&lt;/p&gt;

&lt;p&gt;The data from GoldenGate is serialised using &lt;a href=&#34;https://avro.apache.org/&#34;&gt;Avro&lt;/a&gt;. The schema is put by Goldengate onto a separate Kafka topic, &lt;code&gt;mySchemaTopic&lt;/code&gt;. There&amp;rsquo;s probably a more proper way to get it but I dumped it to file thus:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kafka-console-consumer --zookeeper localhost:2181 --topic mySchemaTopic --from-beginning &amp;gt; ~/schema.avsc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a snippet of what it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;type&amp;quot; : &amp;quot;record&amp;quot;,
  &amp;quot;name&amp;quot; : &amp;quot;MOVIE&amp;quot;,
  &amp;quot;namespace&amp;quot; : &amp;quot;ORCL.MOVIEDEMO&amp;quot;,
  &amp;quot;fields&amp;quot; : [ {
    &amp;quot;name&amp;quot; : &amp;quot;table&amp;quot;,
    &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
  }, {
    &amp;quot;name&amp;quot; : &amp;quot;op_type&amp;quot;,
    &amp;quot;type&amp;quot; : &amp;quot;string&amp;quot;
  }, {
[...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Logstash can use the Avro &lt;strong&gt;codec&lt;/strong&gt; to deserialise the data it&amp;rsquo;s going to be pulling from Kafka. It isn&amp;rsquo;t part of the standard distribution, so needs installing thus:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/opt/logstash-2.2.0/bin/plugin install logstash-codec-avro
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can build our Logstash config file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt; input {
     kafka {
         zk_connect =&amp;gt; &#39;bigdatalite:2181&#39;
         topic_id =&amp;gt; &#39;oggtopic&#39;
         codec =&amp;gt;
             avro {
                 schema_uri =&amp;gt; &amp;quot;/home/oracle/schema.avsc&amp;quot;
             }
         # These next two options will force logstash to pull
         # the entire contents of the topic.
         reset_beginning =&amp;gt; &#39;true&#39;
         auto_offset_reset =&amp;gt; &#39;smallest&#39;
     }
 }

 output {
     stdout {
         codec =&amp;gt; rubydebug
     }
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the syntax for referencing the Avro codec - if you follow the syntax in the docs it will fail with the error &lt;code&gt;undefined method `decode&#39; for [&amp;quot;avro&amp;quot;&lt;/code&gt;. Thanks to &lt;a href=&#34;http://stackoverflow.com/a/33211940/350613&#34;&gt;this Stackoverflow post&lt;/a&gt; for help on fixing that problem.&lt;/p&gt;

&lt;p&gt;Because we&amp;rsquo;ve told Logstash to use the &lt;strong&gt;stdout&lt;/strong&gt; output plugin we can see everything that it&amp;rsquo;s reading from Kafka, and the &lt;strong&gt;rubydebug&lt;/strong&gt; codec ensures that field names etc are nicely formatted. You can see from this the point of the Avro schema - it supports the idea of records deletions, as in this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[oracle@bigdatalite logstash-2.2.0]$ /opt/logstash-2.2.0/bin/logstash -f ~/logstash-kafka-stdout.conf
Settings: Default pipeline workers: 4
Logstash startup completed
{
           &amp;quot;table&amp;quot; =&amp;gt; &amp;quot;ORCL.MOVIEDEMO.MOVIE&amp;quot;,
         &amp;quot;op_type&amp;quot; =&amp;gt; &amp;quot;D&amp;quot;,
           &amp;quot;op_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16 22:14:47.000000&amp;quot;,
      &amp;quot;current_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16T22:26:29.515000&amp;quot;,
             &amp;quot;pos&amp;quot; =&amp;gt; &amp;quot;00000000010000002172&amp;quot;,
    &amp;quot;primary_keys&amp;quot; =&amp;gt; [
        [0] &amp;quot;MOVIE_ID&amp;quot;
    ],
          &amp;quot;tokens&amp;quot; =&amp;gt; {},
          &amp;quot;before&amp;quot; =&amp;gt; {
                      &amp;quot;MOVIE_ID&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;,
            &amp;quot;MOVIE_ID_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;TITLE&amp;quot; =&amp;gt; &amp;quot;Foo&amp;quot;,
               &amp;quot;TITLE_isMissing&amp;quot; =&amp;gt; false,
                          &amp;quot;YEAR&amp;quot; =&amp;gt; &amp;quot;2014&amp;quot;,
                &amp;quot;YEAR_isMissing&amp;quot; =&amp;gt; false,
                        &amp;quot;BUDGET&amp;quot; =&amp;gt; &amp;quot;500000&amp;quot;,
              &amp;quot;BUDGET_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;GROSS&amp;quot; =&amp;gt; &amp;quot;20000000&amp;quot;,
               &amp;quot;GROSS_isMissing&amp;quot; =&amp;gt; false,
                  &amp;quot;PLOT_SUMMARY&amp;quot; =&amp;gt; &amp;quot;give you up&amp;quot;,
        &amp;quot;PLOT_SUMMARY_isMissing&amp;quot; =&amp;gt; false
    },
           &amp;quot;after&amp;quot; =&amp;gt; nil,
        &amp;quot;@version&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;,
      &amp;quot;@timestamp&amp;quot; =&amp;gt; &amp;quot;2016-03-16T22:53:48.675Z&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;as well as inserts and updates:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
           &amp;quot;table&amp;quot; =&amp;gt; &amp;quot;ORCL.MOVIEDEMO.MOVIE&amp;quot;,
         &amp;quot;op_type&amp;quot; =&amp;gt; &amp;quot;I&amp;quot;,
           &amp;quot;op_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16 22:32:21.000000&amp;quot;,
      &amp;quot;current_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16T22:32:22.941000&amp;quot;,
             &amp;quot;pos&amp;quot; =&amp;gt; &amp;quot;00000000010000003090&amp;quot;,
    &amp;quot;primary_keys&amp;quot; =&amp;gt; [
        [0] &amp;quot;MOVIE_ID&amp;quot;
    ],
          &amp;quot;tokens&amp;quot; =&amp;gt; {},
          &amp;quot;before&amp;quot; =&amp;gt; nil,
           &amp;quot;after&amp;quot; =&amp;gt; {
                      &amp;quot;MOVIE_ID&amp;quot; =&amp;gt; &amp;quot;4242&amp;quot;,
            &amp;quot;MOVIE_ID_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;TITLE&amp;quot; =&amp;gt; &amp;quot;never gonna&amp;quot;,
               &amp;quot;TITLE_isMissing&amp;quot; =&amp;gt; false,
                          &amp;quot;YEAR&amp;quot; =&amp;gt; &amp;quot;2014&amp;quot;,
                &amp;quot;YEAR_isMissing&amp;quot; =&amp;gt; false,
                        &amp;quot;BUDGET&amp;quot; =&amp;gt; &amp;quot;500000&amp;quot;,
              &amp;quot;BUDGET_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;GROSS&amp;quot; =&amp;gt; &amp;quot;20000000&amp;quot;,
               &amp;quot;GROSS_isMissing&amp;quot; =&amp;gt; false,
                  &amp;quot;PLOT_SUMMARY&amp;quot; =&amp;gt; &amp;quot;give you up&amp;quot;,
        &amp;quot;PLOT_SUMMARY_isMissing&amp;quot; =&amp;gt; false
    },
        &amp;quot;@version&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;,
      &amp;quot;@timestamp&amp;quot; =&amp;gt; &amp;quot;2016-03-16T23:08:58.804Z&amp;quot;
}
{
           &amp;quot;table&amp;quot; =&amp;gt; &amp;quot;ORCL.MOVIEDEMO.MOVIE&amp;quot;,
         &amp;quot;op_type&amp;quot; =&amp;gt; &amp;quot;U&amp;quot;,
           &amp;quot;op_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16 23:09:58.000000&amp;quot;,
      &amp;quot;current_ts&amp;quot; =&amp;gt; &amp;quot;2016-03-16T23:10:01.023000&amp;quot;,
             &amp;quot;pos&amp;quot; =&amp;gt; &amp;quot;00000000010000003700&amp;quot;,
    &amp;quot;primary_keys&amp;quot; =&amp;gt; [
        [0] &amp;quot;MOVIE_ID&amp;quot;
    ],
          &amp;quot;tokens&amp;quot; =&amp;gt; {},
          &amp;quot;before&amp;quot; =&amp;gt; {
                      &amp;quot;MOVIE_ID&amp;quot; =&amp;gt; &amp;quot;4242&amp;quot;,
            &amp;quot;MOVIE_ID_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;TITLE&amp;quot; =&amp;gt; &amp;quot;never gonna&amp;quot;,
               &amp;quot;TITLE_isMissing&amp;quot; =&amp;gt; false,
                          &amp;quot;YEAR&amp;quot; =&amp;gt; &amp;quot;2014&amp;quot;,
                &amp;quot;YEAR_isMissing&amp;quot; =&amp;gt; false,
                        &amp;quot;BUDGET&amp;quot; =&amp;gt; &amp;quot;500000&amp;quot;,
              &amp;quot;BUDGET_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;GROSS&amp;quot; =&amp;gt; &amp;quot;20000000&amp;quot;,
               &amp;quot;GROSS_isMissing&amp;quot; =&amp;gt; false,
                  &amp;quot;PLOT_SUMMARY&amp;quot; =&amp;gt; &amp;quot;give you up&amp;quot;,
        &amp;quot;PLOT_SUMMARY_isMissing&amp;quot; =&amp;gt; false
    },
           &amp;quot;after&amp;quot; =&amp;gt; {
                      &amp;quot;MOVIE_ID&amp;quot; =&amp;gt; &amp;quot;4242&amp;quot;,
            &amp;quot;MOVIE_ID_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;TITLE&amp;quot; =&amp;gt; &amp;quot;Foobar&amp;quot;,
               &amp;quot;TITLE_isMissing&amp;quot; =&amp;gt; false,
                          &amp;quot;YEAR&amp;quot; =&amp;gt; &amp;quot;2014&amp;quot;,
                &amp;quot;YEAR_isMissing&amp;quot; =&amp;gt; false,
                        &amp;quot;BUDGET&amp;quot; =&amp;gt; &amp;quot;500000&amp;quot;,
              &amp;quot;BUDGET_isMissing&amp;quot; =&amp;gt; false,
                         &amp;quot;GROSS&amp;quot; =&amp;gt; &amp;quot;20000000&amp;quot;,
               &amp;quot;GROSS_isMissing&amp;quot; =&amp;gt; false,
                  &amp;quot;PLOT_SUMMARY&amp;quot; =&amp;gt; &amp;quot;give you up&amp;quot;,
        &amp;quot;PLOT_SUMMARY_isMissing&amp;quot; =&amp;gt; false
    },
        &amp;quot;@version&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;,
      &amp;quot;@timestamp&amp;quot; =&amp;gt; &amp;quot;2016-03-16T23:10:11.043Z&amp;quot;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to get this into Elasticsearch you can send it there from Logstash, just by adding&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; elasticsearch { hosts =&amp;gt; &amp;quot;localhost&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to the &lt;code&gt;output&lt;/code&gt; stanza of the logstash configuration file. I hit the error&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SSLConnectionSocketFactory not found [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;when I ran Logstash with Elasticsearch output option, to which a quick Google produced the answer; run &lt;code&gt;unset CLASSPATH&lt;/code&gt; first.&lt;/p&gt;

&lt;p&gt;With the data in Elasticsearch it&amp;rsquo;s a matter of moments to get set up in Kibana and to start poking around it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-17_21-49-15.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So what&amp;rsquo;s the point of all this? Well, I mentioned it partly above - jigsaws. It&amp;rsquo;s fun seeing what fits together with what 8-) But more usefully, Kafka has a vital role to play in &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;flexible data pipelines&lt;/a&gt;, and Logstash is just an easy example of one of the many consumers that can take advantage of data persisted in the buffer that Kafka provides. Logstash itself gives a bunch of integration permutations, if the desired target itself doesn&amp;rsquo;t have a direct Kafka consumer (which something like Elasticsearch may have, with the advent of &lt;a href=&#34;http://docs.confluent.io/2.0.0/connect/&#34;&gt;Kafka Connect&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Pulling the GoldenGate data into Elasticsearch as seen above is cool (c.f. jigsaws, or maybe Lego is a better analogy), and for poking around the Kafka messages and deserialising the Avro messages it&amp;rsquo;s perfect, but given the CDC nature of it having update and delete transactions too it could be that &lt;a href=&#34;https://www.elastic.co/products/hadoop&#34;&gt;Elasticsearch-Hadoop&lt;/a&gt; is a better route if you want a consistent point-in-time view of your data. Done that way you&amp;rsquo;d have &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/hadoop/current/hive.html#_writing_data_to_elasticsearch_2&#34;&gt;Hive pushing a copy of the data to Elasticsearch&lt;/a&gt; thus:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;OGG -- [Kafka] --&amp;gt; HDFS/Hive --&amp;gt; Elasticsearch
&lt;/code&gt;&lt;/pre&gt;
</description>
		</item>
		
		<item>
			<title>Presentation Slides… bye-bye Slideshare, hello Speakerdeck</title>
			<link>https://rmoff.github.io/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/</link>
			<pubDate>Wed, 09 Mar 2016 09:43:30 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/</guid>
			<description>&lt;p&gt;I&amp;rsquo;ve always defaulted to &lt;a href=&#34;http://slideshare.net/themoff&#34;&gt;Slideshare&lt;/a&gt; for hosting slides from presentations that I&amp;rsquo;ve given, but it&amp;rsquo;s become more and more crap-infested. The UI is messy, and the UX sucks - for example, I want to download a slide deck, I most definitely 100% am not interested in &amp;ldquo;clipping&amp;rdquo; it&amp;hellip;even if you ask me every. damn. time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/03/2016-03-09_09-32-29-1.png&#34; alt=&#34;Nope!&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Looking around it seems the other popular option is &lt;a href=&#34;http://speakerdeck.com&#34;&gt;Speakerdeck&lt;/a&gt;. The UI is clean and simple, and I as both a user and uploader I feel like I&amp;rsquo;m there to read and share slides rather than be monetised as an eyeball on the site.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m still in the process of uploading all my past presentations, but you can see what&amp;rsquo;s there so far here at &lt;a href=&#34;https://speakerdeck.com/rmoff&#34;&gt;https://speakerdeck.com/rmoff&lt;/a&gt;.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>#obihackers IRC channel</title>
			<link>https://rmoff.github.io/2016/03/03/obihackers-irc-channel/</link>
			<pubDate>Thu, 03 Mar 2016 22:55:37 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/03/03/obihackers-irc-channel/</guid>
			<description>

&lt;h3 id=&#34;obihackers&#34;&gt;&lt;code&gt;#obihackers&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a &lt;code&gt;#obihackers&lt;/code&gt; IRC channel on freenode, where a dozen or so of us have hung out for several years now. Chat is usually OBIEE, Oracle, ODI, and general geek out.&lt;/p&gt;

&lt;p&gt;Bear in mind this is the equivalent of us hanging out in a bar; if you wanna shoot the shit with a geeky question about OBIEE go ahead, but if you&amp;rsquo;ve come to get help with your homework without even buying a round, you&amp;rsquo;ll probably get short shrift&amp;hellip; ;-)&lt;/p&gt;

&lt;p&gt;Interested? &lt;a href=&#34;http://webchat.freenode.net/?channels=%23obihackers&amp;amp;uio=d4&#34;&gt;Join in!&lt;/a&gt;. You&amp;rsquo;ll find me there most of the time with the handle &lt;code&gt;rmoff&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;what-s-irc&#34;&gt;What&amp;rsquo;s IRC?&lt;/h3&gt;

&lt;p&gt;For you kids out there, IRC is one of the old-school protocols on the internet, before WhatsApp et al existed, in fact, probably before the people who wrote WhatsApp et all were even born.&lt;/p&gt;

&lt;p&gt;IRC is a text-only chat protocol, with the idea of &amp;ldquo;channels&amp;rdquo;, as well as direct messaging. It&amp;rsquo;s lasted through the ages, proving its worth in its simplicity and accessibility.&lt;/p&gt;

&lt;p&gt;You can use a &lt;a href=&#34;http://webchat.freenode.net/?channels=%23obihackers&amp;amp;uio=d4&#34;&gt;web interface&lt;/a&gt;, or one of a multitude of IRC clients. I like &lt;a href=&#34;https://irssi.org/&#34;&gt;irssi&lt;/a&gt; from the command line, or &lt;a href=&#34;http://colloquy.info/&#34;&gt;Colloquy&lt;/a&gt; with a GUI on the Mac. If you really get into it, &lt;a href=&#34;http://wiki.znc.in/ZNC&#34;&gt;znc&lt;/a&gt; is a good &amp;ldquo;bouncer&amp;rdquo; which enables you to stay online in a channel and keep a track of conversations whilst you&amp;rsquo;re actually offline. If you come along to the #obihackers channel and see &lt;code&gt;zz_rmoff&lt;/code&gt; (rather than &lt;code&gt;rmoff&lt;/code&gt;) that&amp;rsquo;s my IRC bouncer keeping the tabs on the channel whilst I sleep :)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>Streaming data to InfluxDB from any bash command</title>
			<link>https://rmoff.github.io/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</link>
			<pubDate>Sat, 27 Feb 2016 21:05:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</guid>
			<description>&lt;p&gt;&lt;a href=&#34;https://influxdata.com/time-series-platform/influxdb/&#34;&gt;InfluxDB&lt;/a&gt; is a great time series database, that&amp;rsquo;s recently been rebranded as part of the &amp;ldquo;&lt;a href=&#34;https://influxdata.com/&#34;&gt;TICK&lt;/a&gt;&amp;rdquo; stack, including data collectors, visualisation, and ETL/Alerting. I&amp;rsquo;ve yet to really look at the other components, but InfluxDB alone works just great with my favourite visualisation/analysis tool for time series metrics, &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Getting data into InfluxDB is easy, with many tools supporting the native InfluxDB &lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.10/guides/writing_data/&#34;&gt;line input protocol&lt;/a&gt;, and those that don&amp;rsquo;t often supporting the &lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.10/write_protocols/graphite/&#34;&gt;carbon protocol&lt;/a&gt; (from Graphite), which InfluxDB also supports (&lt;a href=&#34;https://docs.influxdata.com/influxdb/v0.10/write_protocols/&#34;&gt;along with others&lt;/a&gt;). So for collecting broad ranges of OS stats, for example, &lt;a href=&#34;http://collectl.sourceforge.net/&#34;&gt;collectl&lt;/a&gt; via carbon and nmon via &lt;a href=&#34;https://github.com/adejoux/nmon2influxdb&#34;&gt;nmon2influxdb&lt;/a&gt; are both viable options.&lt;/p&gt;

&lt;p&gt;Using the power of *nix, we can set up a hacky, but effective, way of streaming &lt;strong&gt;additional&lt;/strong&gt; data into InfluxDB. For example, tracking the the amount of disk space used by a set of particular folders on disk can be really useful as part of system monitoring &amp;amp; troubleshooting. We can get the raw information easily enough at the commandline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-language-bash&#34;&gt;[oracle@demo ~]$ du -s /app/oracle/biee/user_projects/domains/bi/servers/*/tmp
417764	/app/oracle/biee/user_projects/domains/bi/servers/AdminServer/tmp
2061740	/app/oracle/biee/user_projects/domains/bi/servers/bi_server1/tmp
8	/app/oracle/biee/user_projects/domains/bi/servers/obiccs1/tmp
277484	/app/oracle/biee/user_projects/domains/bi/servers/obips1/tmp
636	/app/oracle/biee/user_projects/domains/bi/servers/obis1/tmp
12	/app/oracle/biee/user_projects/domains/bi/servers/obisch1/tmp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By using tools like &lt;code&gt;sed&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt; to reformat the data and construct the InfluxDB input message, and then send it over using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-language-bash&#34;&gt;while [ 1 -eq 1 ]; do
        du -s /app/oracle/biee/user_projects/domains/bi/servers/*/tmp| \
        sed &#39;s/\/app\/oracle\/biee\/user_projects\/domains\/bi\/servers\///g&#39;| \
        sed &#39;s/\/tmp//g&#39;| \
        awk &#39;{print &amp;quot;DiskTemp,component=&amp;quot;$2&amp;quot; value=&amp;quot;$1}&#39;| \
        curl -i -XPOST &#39;http://localhost:8086/write?db=obi&#39; --data-binary @-
        sleep 10
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the &lt;code&gt;\&lt;/code&gt; line continuation characters - the &lt;code&gt;du&lt;/code&gt; pipes to &lt;code&gt;sed&lt;/code&gt; (twice), then to &lt;code&gt;awk&lt;/code&gt; and finally to &lt;code&gt;curl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Heading over to InfluxDB&amp;rsquo;s admin interface we can see the data&amp;rsquo;s been received:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/02/2016-02-27_20-52-09.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And from there on into displaying it in Grafana:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/02/2016-02-27_20-55-10.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, for collecting &lt;code&gt;iotop&lt;/code&gt; data, this time as a one-liner:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-language-bash&#34;&gt;while [ 1 -eq 1 ]; do sudo iotop -n 1 -k -qqq -o|awk &#39;{print &amp;quot;io_read_kbs,pid=&amp;quot;$1&amp;quot;,process=&amp;quot;$12&amp;quot; value=&amp;quot;$4&amp;quot;\nio_write_kbs,pid=&amp;quot;$1&amp;quot;,process=&amp;quot;$12&amp;quot; value=&amp;quot;$6}&#39;|curl -i -XPOST &#39;http://localhost:8086/write?db=io&#39; --data-binary @-;sleep 1;done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/02/2016-02-27_21-01-43.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Even if proper metrics collection tools like &lt;code&gt;collectl&lt;/code&gt; can get this information, for point-in-time digging without needing to reconfigure and restart services, this is a handy trick to have up one&amp;rsquo;s sleeve.&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>#FullEnglish</title>
			<link>https://rmoff.github.io/2016/02/26/fullenglish/</link>
			<pubDate>Fri, 26 Feb 2016 18:02:31 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/02/26/fullenglish/</guid>
			<description>&lt;p&gt;Thanks to the power of twitter, I can look back on all the many and varied Full English breakfasts that I&amp;rsquo;ve (mostly) enjoyed: &lt;a href=&#34;https://twitter.com/search?q=rmoff%20%23fullenglish&amp;amp;src=typd&#34;&gt;https://twitter.com/search?q=rmoff%20%23fullenglish&amp;amp;src=typd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2016/02/fullenglish.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What makes a good Full English?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Good ingredients, cooked well. Nothing worse than a limp pink sausage, as it were.&lt;/li&gt;
&lt;li&gt;Sausage, standard pork or Cumberland at most. Definitely no daft apricot and guava bean with a hint of foie gras nonsense. Must be cooked right well, crispy skin, almost burnt.&lt;/li&gt;
&lt;li&gt;Bacon, starting to crisp (but not deep fried and crisp-like as the Americans do it - it should bend if you pick it up)&lt;/li&gt;
&lt;li&gt;Black Pudding, seriously. Grim idea when you think about it (blood sausage) but so very tasty, and binds the other components together&lt;/li&gt;
&lt;li&gt;Fried eggs with runny yolks&lt;/li&gt;
&lt;li&gt;Scrambled eggs are making an appearance more recently too, after 30 years of not really caring for them. The dodgy &amp;ldquo;scrambled egg&amp;rdquo; that you get in some hotels I think straight from a packet mix are crap though and best avoided.&lt;/li&gt;
&lt;li&gt;Hash browns. Controversial, and often a bit greasy - but I&amp;rsquo;d never pass one by&lt;/li&gt;
&lt;li&gt;Fried slice - sadly all too often missing. I&amp;rsquo;ll forego hash browns if there&amp;rsquo;s a fried slice&lt;/li&gt;
&lt;li&gt;Tomatoes, but only if there&amp;rsquo;s more tomato than stalk as often happens&lt;/li&gt;
&lt;li&gt;Mushrooms, thicky chopped so there&amp;rsquo;s something to it - and certainly not the wussy sliced things that you get sitting in a pool of black greasy water at some fryup buffets&lt;/li&gt;
&lt;li&gt;Baked beans&lt;/li&gt;
&lt;li&gt;Good white toast&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HP Sauce&lt;/strong&gt; - although I&amp;rsquo;ve realised recently that a really good fryup doesn&amp;rsquo;t actually need any sauce. But if it&amp;rsquo;s subpar and/or cheap ingrediants &amp;ndash; gotta have HP sauce (and definitely not that awful &amp;ldquo;brown sauce&amp;rdquo; that you get in sachets and tastes grim).&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Visualising OBIEE DMS Metrics over the years</title>
			<link>https://rmoff.github.io/2016/02/26/visualising-obiee-dms-metrics-over-the-years/</link>
			<pubDate>Fri, 26 Feb 2016 17:54:54 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/2016/02/26/visualising-obiee-dms-metrics-over-the-years/</guid>
			<description>&lt;p&gt;It struck me today when I was writing my most recent blog over at &lt;a href=&#34;http://ritt.md/obi-dms&#34;&gt;Rittman Mead&lt;/a&gt; that I&amp;rsquo;ve been playing with visualising OBIEE metrics for &lt;em&gt;years&lt;/em&gt; now.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Back in 2009 I wrote about using something called JManage to pull metrics out of OBIEE 10g via JMX:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rnm1978.files.wordpress.com/2009/07/jmanage08.png?w=900&amp;amp;h=760&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Still with OBIEE 10g in 2011, I was using rrdtool and some &lt;a href=&#34;https://rnm1978.wordpress.com/2010/12/06/collecting-obiee-systems-management-data-with-jmx/&#34;&gt;horrible-looking tcl hacking&lt;/a&gt; to get the metrics out through jmx :&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://rnm1978.files.wordpress.com/2011/03/graph.png?w=2048&amp;amp;h=542&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2014 brought with it DMS and my first forays with Graphite for storing &amp;amp; visualising data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.rittmanmead.com/wp-content/uploads/2014/03/2014-03-26_07-03-19.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;My current weapon of choice is &lt;a href=&#34;http://ritt.md/obi-dms&#34;&gt;OBIEE DMS metrics -&amp;gt; obi-metrics-agent -&amp;gt; InfluxDB -&amp;gt; Grafana&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.rittmanmead.com/wp-content/uploads/2016/02/metrics05.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why&amp;rsquo;s this interesting? I guess just that the numbers have always been there for the taking &amp;amp; use, but nowadays it is so easy to store them and interactively analyse them there is &lt;em&gt;no excuse&lt;/em&gt; not to be doing so!&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>About Me</title>
			<link>https://rmoff.github.io/about-me/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/about-me/</guid>
			<description>

&lt;p&gt;&lt;em&gt;My primary blog writing is &lt;a href=&#34;https://www.confluent.io/blog/author/robin/&#34;&gt;elsewhere&lt;/a&gt; (&lt;a href=&#34;http://ritt.md/rmoff&#34;&gt;previously&lt;/a&gt;). I write at &lt;a href=&#34;http://rmoff.net&#34;&gt;rmoff.net&lt;/a&gt; for random tech scribblings and notes, and pictures of fried breakfasts.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;whoami&#34;&gt;&lt;code&gt;whoami&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Robin is a Developer Advocate at Confluent, the company founded by the creators of Apache Kafka, as well as an Oracle Groundbreaker Ambassador and ACE Director (alumnus). His career has always involved data, from the old worlds of COBOL and DB2, through the worlds of Oracle and Hadoop, and into the current world with Kafka. His particular interests are analytics, systems architecture, performance testing and optimization. He blogs at &lt;a href=&#34;http://cnfl.io/rmoff&#34;&gt;http://cnfl.io/rmoff&lt;/a&gt; and &lt;a href=&#34;http://rmoff.net/&#34;&gt;http://rmoff.net/&lt;/a&gt; (and previously &lt;a href=&#34;http://ritt.md/rmoff&#34;&gt;http://ritt.md/rmoff&lt;/a&gt;) and can be found tweeting grumpy geek thoughts as @rmoff. Outside of work he enjoys drinking good beer and eating fried breakfasts, although generally not at the same time.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaking-experience&#34;&gt;Speaking experience&lt;/h2&gt;

&lt;p&gt;Speaker since 2009 at conferences including Devoxx, USENIX LISA, Kafka Summit, Oracle OpenWorld, JavaZone, Big Data LDN, UKOUG, Oracle CODE, PGConf, etc plus numerous meetups.&lt;/p&gt;

&lt;p&gt;Slides &amp;amp; Recordings: &lt;a href=&#34;http://rmoff.net/presentations/&#34;&gt;http://rmoff.net/presentations/&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;social&#34;&gt;Social&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Twitter &lt;a href=&#34;https://twitter.com/rmoff/&#34;&gt;@rmoff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;mailto:robin@rmoff.net&#34;&gt;Email&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/robinmoffatt&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://speakerdeck.com/rmoff&#34;&gt;SpeakerDeck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rmoff&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://t.me/rmoff&#34;&gt;Telegram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://keybase.io/rmoff/&#34;&gt;Keybase&lt;/a&gt; 🔑&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://untappd.com/user/rmoff&#34;&gt;Untappd&lt;/a&gt; 🍻&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.last.fm/user/themoff&#34;&gt;last.fm&lt;/a&gt; 🎵&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://soundcloud.com/rmoff&#34;&gt;Soundcloud&lt;/a&gt; 🎶&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mixcloud.com/rmoff/&#34;&gt;Mixcloud&lt;/a&gt; 🎶&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.strava.com/athletes/10250052/badge&#34;&gt;Strava&lt;/a&gt; 🏃&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://stackexchange.com/users/142729/robin-moffatt&#34;&gt;&lt;img src=&#34;https://stackexchange.com/users/flair/142729.png&#34; width=&#34;208&#34; height=&#34;58&#34; alt=&#34;profile for Robin Moffatt on Stack Exchange, a network of free, community-driven Q&amp;amp;A sites&#34; title=&#34;profile for Robin Moffatt on Stack Exchange, a network of free, community-driven Q&amp;amp;A sites&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;iframe height=&#39;160&#39; width=&#39;300&#39; frameborder=&#39;0&#39; allowtransparency=&#39;true&#39; scrolling=&#39;no&#39; src=&#39;https://www.strava.com/athletes/10250052/activity-summary/0fbe8f47b3fae6b562f6c9fba66b4d66492c0805&#39;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;mugshots&#34;&gt;Mugshots&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/05/ksldn18-01.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://rmoff.github.io/content/images/2018/05/robin-moffatt-600px.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>List of Presentations</title>
			<link>https://rmoff.github.io/list-of-presentations/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/list-of-presentations/</guid>
			<description>&lt;p&gt;See &lt;a href=&#34;https://speakerdeck.com/rmoff/&#34;&gt;SpeakerDeck&lt;/a&gt; for a list of all my presentations.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;(image by &lt;a href=&#34;https://unsplash.com/@rgrzybowski&#34;&gt;Radek Grzybowski&lt;/a&gt;, via &lt;a href=&#34;https://unsplash.com&#34;&gt;unsplash.com&lt;/a&gt;)&lt;/p&gt;
</description>
		</item>
		
		<item>
			<title>My Favourite Song</title>
			<link>https://rmoff.github.io/my-favourite-song/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/my-favourite-song/</guid>
			<description>&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/dQw4w9WgXcQ?autoplay=1&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
		</item>
		
		<item>
			<title>Presentations and Talks</title>
			<link>https://rmoff.github.io/presentations/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/presentations/</guid>
			<description>

&lt;p&gt;You can find most of my slide decks on &lt;a href=&#34;https://speakerdeck.com/rmoff/&#34;&gt;Speaker Deck&lt;/a&gt;. Below are links to particular talks I&amp;rsquo;ve done recently, along with recordings where available.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;embrace-the-anarchy-apache-kafka-s-role-in-modern-data-architectures&#34;&gt;Embrace the Anarchy: Apache Kafka&amp;rsquo;s Role in Modern Data Architectures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://rmoff.github.io/content/images/2018/06/2018-06-08_10-04-24.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;📽 Recording

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bapHWhtf6fE&#34;&gt;Devoxx Belgium&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Rated &lt;a href=&#34;https://twitter.com/Devoxx/status/1064452715034669056&#34;&gt;32&lt;/a&gt; out of all the conference talks&lt;/li&gt;
&lt;li&gt;Scored 4.&lt;sup&gt;31&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt;, 134 votes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Y5qeKmL5xMg&#34;&gt;Oracle CODE London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://speakerdeck.com/rmoff/embrace-the-anarchy-apache-kafkas-role-in-modern-data-architectures&#34;&gt;📖 Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;apache-kafka-and-ksql-in-action-let-s-build-a-streaming-data-pipeline&#34;&gt;Apache Kafka and KSQL in Action : Let’s Build a Streaming Data Pipeline!&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;📽 Recording

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RJtEacDX4Oc&#34;&gt;Devoxx Belgium&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Rated &lt;a href=&#34;https://twitter.com/Devoxx/status/1064452715034669056&#34;&gt;17&lt;/a&gt; out of all the conference talks&lt;/li&gt;
&lt;li&gt;Scored 4.&lt;sup&gt;55&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt;, 102 votes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FD2z3bdN1Jw&#34;&gt;USENIX LISA18&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hZE409e1tlg&amp;amp;feature=youtu.be&#34;&gt;Kraków meetup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SvaMFCT_6GI&#34;&gt;Madrid meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline&#34;&gt;📖 Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/mysql-debezium-ksql-elasticsearch/&#34;&gt;👾 Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;No More Silos: Integrating Databases and Apache Kafka&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;📖 Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/no-more-silos-mysql&#34;&gt;👾 Code (MySQL)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/no-more-silos-oracle&#34;&gt;👾 Code (Oracle)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka and KSQL&lt;/h2&gt;

&lt;p&gt;🥇 &lt;strong&gt;Top-rated session at Kafka Summit London 2018!&lt;/strong&gt; 🥇&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-london18/look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-london18/look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;📖 Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;steps-to-building-a-streaming-etl-pipeline-with-apache-kafka-and-ksql&#34;&gt;Steps to Building a Streaming ETL Pipeline with Apache Kafka and KSQL&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://videos.confluent.io/watch/4cVXUQ2jCLgJNmg4kjCRqo&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/ConfluentInc/steps-to-building-a-streaming-etl-pipeline-with-apache-kafka-and-ksql&#34;&gt;📖 Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;live-coding-a-ksql-application&#34;&gt;Live Coding a KSQL Application&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://videos.confluent.io/watch/m1LpiDwQo4Vvd4YHWKcszs&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/rmoff/7efa882dfd808dbab4eb7b8e6f9eda16&#34;&gt;📝 Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
		<item>
			<title>Talks</title>
			<link>https://rmoff.github.io/talks/</link>
			<pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate>
			<author>Robin Moffatt</author>
			<guid>https://rmoff.github.io/talks/</guid>
			<description>

&lt;p&gt;You can find most of my slide decks on &lt;a href=&#34;https://speakerdeck.com/rmoff/&#34;&gt;Speaker Deck&lt;/a&gt;. Below are links to particular talks I&amp;rsquo;ve done recently, along with recordings where available.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;atm-fraud-detection-with-kafka-and-ksql&#34;&gt;ATM Fraud detection with Kafka and KSQL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;📖 &lt;a href=&#34;https://speakerdeck.com/rmoff/atm-fraud-detection-with-kafka-and-ksql&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;👾 &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/ksql-atm-fraud-detection/ksql-atm-fraud-detection-README.adoc&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;embrace-the-anarchy-apache-kafka-s-role-in-modern-data-architectures&#34;&gt;Embrace the Anarchy: Apache Kafka&amp;rsquo;s Role in Modern Data Architectures&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;📽 Recording&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=bapHWhtf6fE&#34;&gt;Devoxx Belgium&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rated &lt;a href=&#34;https://twitter.com/Devoxx/status/1064452715034669056&#34;&gt;32&lt;/a&gt; out of all the conference talks&lt;/li&gt;
&lt;li&gt;Scored &lt;span class=&#34;math&#34;&gt;\( 4.31/5 \)&lt;/span&gt;, 134 votes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Y5qeKmL5xMg&#34;&gt;Oracle CODE London&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;📖 &lt;a href=&#34;https://speakerdeck.com/rmoff/embrace-the-anarchy-apache-kafkas-role-in-modern-data-architectures&#34;&gt;Slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;apache-kafka-and-ksql-in-action-let-s-build-a-streaming-data-pipeline&#34;&gt;Apache Kafka and KSQL in Action : Let’s Build a Streaming Data Pipeline!&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;📽 Recording

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=RJtEacDX4Oc&#34;&gt;Devoxx Belgium&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Rated &lt;a href=&#34;https://twitter.com/Devoxx/status/1064452715034669056&#34;&gt;17&lt;/a&gt; out of all the conference talks&lt;/li&gt;
&lt;li&gt;Scored 4.&lt;sup&gt;55&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt;, 102 votes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FD2z3bdN1Jw&#34;&gt;USENIX LISA18&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hZE409e1tlg&amp;amp;feature=youtu.be&#34;&gt;Kraków meetup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SvaMFCT_6GI&#34;&gt;Madrid meetup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;📖 &lt;a href=&#34;https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;👾 &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/mysql-debezium-ksql-elasticsearch/&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;No More Silos: Integrating Databases and Apache Kafka&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;📖 &lt;a href=&#34;https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;👾 &lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/no-more-silos-mysql&#34;&gt;Code (MySQL)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;👾 &lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/no-more-silos-oracle&#34;&gt;Code (Oracle)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka and KSQL&lt;/h3&gt;

&lt;p&gt;🥇 &lt;strong&gt;Top-rated session at Kafka Summit London 2018!&lt;/strong&gt; 🥇&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-london18/look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📖 &lt;a href=&#34;https://www.confluent.io/kafka-summit-london18/look-ma-no-code-building-streaming-data-pipelines-with-apache-kafka-and-ksql&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;steps-to-building-a-streaming-etl-pipeline-with-apache-kafka-and-ksql&#34;&gt;Steps to Building a Streaming ETL Pipeline with Apache Kafka and KSQL&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://videos.confluent.io/watch/4cVXUQ2jCLgJNmg4kjCRqo&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📖 &lt;a href=&#34;https://www.slideshare.net/ConfluentInc/steps-to-building-a-streaming-etl-pipeline-with-apache-kafka-and-ksql&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;live-coding-a-ksql-application&#34;&gt;Live Coding a KSQL Application&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://videos.confluent.io/watch/m1LpiDwQo4Vvd4YHWKcszs&#34;&gt;📽 Recording&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/rmoff/7efa882dfd808dbab4eb7b8e6f9eda16&#34;&gt;📝 Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
		</item>
		
	</channel>
</rss>
