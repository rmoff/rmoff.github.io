
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/</link>
    <description>Recent content on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>2019-11-29</lastBuildDate>
    
        <atom:link href="https://rmoff.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Connect - Request timed out</title>
      <link>https://rmoff.github.io/2019/11/29/kafka-connect-request-timed-out/</link>
      <pubDate>2019-11-29</pubDate>
      
      <guid>https://rmoff.github.io/2019/11/29/kafka-connect-request-timed-out/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/img/default-header-img.tn-500x500.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A short &amp;amp; sweet blog post to help people Googling for this error, and me next time I encounter it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The scenario: trying to create a connector in Kafka Connect (running in distributed mode, one worker) failed with the &lt;code&gt;curl&lt;/code&gt; response&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;HTTP/1.1 &lt;span style=&#34;color:#666&#34;&gt;500&lt;/span&gt; Internal Server Error
Date: Fri, &lt;span style=&#34;color:#666&#34;&gt;29&lt;/span&gt; Nov &lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;:33:53 GMT
Content-Type: application/json
Content-Length: &lt;span style=&#34;color:#666&#34;&gt;48&lt;/span&gt;
Server: Jetty&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;9&lt;/span&gt;.4.18.v20190429&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;error_code&amp;#34;&lt;/span&gt;:500,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Request timed out&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;But, no error in the Kafka Connect worker log (at &lt;code&gt;INFO&lt;/code&gt; level anyway). Most puzzling. After a lot of back &amp;amp; forth comparing my config with a working environment I tracked this down to a mis-configuration of my Kafka broker. Running a single broker, I had not specified an override value for the configuration &lt;code&gt;offsets.topic.replication.factor&lt;/code&gt;, which meant that it took the default of three. Three replicas, but only one broker…that&amp;#8217;s not going to be a good situation, and well it wasn&amp;#8217;t on checking my broker log which was full of continually repeating errors:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt;-11-29 &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;:40:46,841&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; ERROR &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;KafkaApi-1&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Number of alive brokers &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt; does not meet the required replication factor &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;for&lt;/span&gt; the offsets topic &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;configured via &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;offsets.topic.replication.factor&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;. This error can be ignored &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;if&lt;/span&gt; the cluster is starting up and not all brokers are up yet. &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;kafka.server.KafkaApis&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt;-11-29 &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;:40:46,945&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; ERROR &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;KafkaApi-1&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Number of alive brokers &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt; does not meet the required replication factor &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;for&lt;/span&gt; the offsets topic &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;configured via &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;offsets.topic.replication.factor&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;. This error can be ignored &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;if&lt;/span&gt; the cluster is starting up and not all brokers are up yet. &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;kafka.server.KafkaApis&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt;-11-29 &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;:40:47,048&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; ERROR &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;KafkaApi-1&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Number of alive brokers &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt; does not meet the required replication factor &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;for&lt;/span&gt; the offsets topic &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;configured via &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;offsets.topic.replication.factor&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;. This error can be ignored &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;if&lt;/span&gt; the cluster is starting up and not all brokers are up yet. &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;kafka.server.KafkaApis&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;…&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Presumably since Kafka Connect uses the offsets topic for its &lt;a href=&#34;https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/&#34;&gt;own internal load balancing&lt;/a&gt; the absence of it caused Kafka Connect to not be able to create a connector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The solution? Configure Kafka correctly :) Since I was using Docker Compose for my cluster I set:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;After bouncing the Kafka broker, Kafka Connect worked fine.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;Logged as &lt;a href=&#34;https://issues.apache.org/jira/browse/KAFKA-9252&#34;&gt;KAFKA-9252&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Using tcpdump With Docker</title>
      <link>https://rmoff.github.io/2019/11/29/using-tcpdump-with-docker/</link>
      <pubDate>2019-11-29</pubDate>
      
      <guid>https://rmoff.github.io/2019/11/29/using-tcpdump-with-docker/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/11/IMG_1337.jpeg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I was doing some troubleshooting between two services recently and wanting to poke around to see what was happening in the REST calls between them. Normally I&amp;#8217;d reach for &lt;code&gt;tcpdump&lt;/code&gt; to do this but imagine my horror when I saw:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;root@ksqldb-server:/# tcpdump
bash: tcpdump: &lt;span style=&#34;color:#008000&#34;&gt;command&lt;/span&gt; not found&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Of course, being Docker containers and being built with the correct philosophy of not including the kitchen sink, &lt;code&gt;tcpdump&lt;/code&gt; wasn&amp;#8217;t present.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;My &lt;a href=&#34;https://twitter.com/rmoff/status/1195382425536348165&#34;&gt;erstwhile companion on my IT career, Google&lt;/a&gt;, soon pointed me to the answer courtesy of &lt;a href=&#34;https://twitter.com/xxradar&#34;&gt;Philippe Bogaerts&lt;/a&gt; in his blog post &lt;a href=&#34;https://medium.com/@xxradar/how-to-tcpdump-effectively-in-docker-2ed0a09b5406&#34;&gt;How to TCPdump effectively in Docker&lt;/a&gt;. Here I&amp;#8217;ll shamelessly plagiarise the salient points and apply them to my Docker situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;First up, you can build Docker images using &lt;a href=&#34;http://tldp.org/LDP/abs/html/here-docs.html&#34;&gt;Here Documents&lt;/a&gt; which is pretty cool:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker build -t tcpdump - &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;FROM ubuntu 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y tcpdump 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CMD tcpdump -i eth0 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So that&amp;#8217;s built me a local image with &lt;code&gt;tcpdump&lt;/code&gt; on:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker images
REPOSITORY   TAG      IMAGE ID      CREATED             SIZE
tcpdump      latest   eebe12b8051f  &lt;span style=&#34;color:#666&#34;&gt;31&lt;/span&gt; minutes ago      &lt;span style=&#34;color:#666&#34;&gt;98&lt;/span&gt;.3MB&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;m interested in the communication between these two containers:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker ps
CONTAINER ID        IMAGE                                          COMMAND                   CREATED             STATUS                  PORTS                              NAMES
20a7bb264c82        confluentinc/ksqldb-server:0.6.0               &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;/usr/bin/docker/run&amp;#34;&lt;/span&gt;     &lt;span style=&#34;color:#666&#34;&gt;17&lt;/span&gt; hours ago        Up &lt;span style=&#34;color:#666&#34;&gt;42&lt;/span&gt; minutes           &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;.0.0.0:8088-&amp;gt;8088/tcp             ksqldb-server
df2be147f1ef        confluentinc/cp-kafka-connect:5.4.0-beta1      &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bash -c &amp;#39;echo \&amp;#34;Inst…&amp;#34;&lt;/span&gt;   &lt;span style=&#34;color:#666&#34;&gt;17&lt;/span&gt; hours ago        Up &lt;span style=&#34;color:#666&#34;&gt;17&lt;/span&gt; hours &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;healthy&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;   &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;.0.0.0:8083-&amp;gt;8083/tcp, &lt;span style=&#34;color:#666&#34;&gt;9092&lt;/span&gt;/tcp   kafka-connect-01&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Using Docker&amp;#8217;s ability to run a container that &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#network-container&#34;&gt;attaches to the network of another&lt;/a&gt; with the &lt;code&gt;--network=container:&amp;lt;container_name&amp;gt;&lt;/code&gt; option we can now run &lt;code&gt;tcpdump&lt;/code&gt; &#34;piggybacked&#34; on my container of interest:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker run --tty --net&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;container:ksqldb-server tcpdump&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;From this we can see all the network chatter going on:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker run --tty --net&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;container:ksqldb-server tcpdump
tcpdump: verbose output suppressed, use -v or -vv &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;for&lt;/span&gt; full protocol decode
listening on eth0, link-type EN10MB &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Ethernet&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;, capture size &lt;span style=&#34;color:#666&#34;&gt;262144&lt;/span&gt; bytes
&lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;:11:11.547672 IP kafka.ksqldb-twitter_default.29092 &amp;gt; ksqldb-server.59926: Flags &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;P.&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, seq &lt;span style=&#34;color:#666&#34;&gt;373631892&lt;/span&gt;:373631900, ack &lt;span style=&#34;color:#666&#34;&gt;22720794&lt;/span&gt;, win &lt;span style=&#34;color:#666&#34;&gt;15552&lt;/span&gt;, options &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;nop,nop,TS val &lt;span style=&#34;color:#666&#34;&gt;65125468&lt;/span&gt; ecr &lt;span style=&#34;color:#666&#34;&gt;65125418&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#666&#34;&gt;8&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;:11:11.547806 IP kafka.ksqldb-twitter_default.29092 &amp;gt; ksqldb-server.59926: Flags &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;P.&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, seq &lt;span style=&#34;color:#666&#34;&gt;8&lt;/span&gt;:22, ack &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, win &lt;span style=&#34;color:#666&#34;&gt;15552&lt;/span&gt;, options &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;nop,nop,TS val &lt;span style=&#34;color:#666&#34;&gt;65125468&lt;/span&gt; ecr &lt;span style=&#34;color:#666&#34;&gt;65125418&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;:11:11.547955 IP ksqldb-server.59926 &amp;gt; kafka.ksqldb-twitter_default.29092: Flags &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, ack &lt;span style=&#34;color:#666&#34;&gt;22&lt;/span&gt;, win &lt;span style=&#34;color:#666&#34;&gt;32044&lt;/span&gt;, options &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;nop,nop,TS val &lt;span style=&#34;color:#666&#34;&gt;65125468&lt;/span&gt; ecr &lt;span style=&#34;color:#666&#34;&gt;65125468&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;…&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What we can see here is our container (&lt;a href=&#34;https://ksqldb.io&#34;&gt;ksqlDB server&lt;/a&gt;) talking to the Kafka broker &lt;code&gt;kafka.ksqldb-twitter_default.29092&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;kafka&lt;/code&gt; is the broker&amp;#8217;s hostname&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ksqldb-twitter_default&lt;/code&gt; the name of the Docker network (that in this case Docker Compose has created)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;29092&lt;/code&gt; the &lt;a href=&#34;https://rmoff.net/2018/08/02/kafka-listeners-explained/&#34;&gt;Kafka broker&amp;#8217;s listener port&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;m not interested in this traffic, so instead of using the default runtime arguments for &lt;code&gt;tcpdump&lt;/code&gt; that were defined in the &lt;code&gt;CMD&lt;/code&gt; section when we built the Docker image above, we can override it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker run --tty --net&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;container:ksqldb-server tcpdump tcpdump -N -A &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;port 8083&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first &lt;code&gt;tcpdump&lt;/code&gt; is the name of the Docker image to run&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second &lt;code&gt;tcpdump&lt;/code&gt; overrides the command to execute (bypassing the &lt;code&gt;CMD&lt;/code&gt; default of the image), and calls &lt;code&gt;tcpdump&lt;/code&gt; with arguments:&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-N&lt;/code&gt; - Don&amp;#8217;t include the domain qualifications (in this case the &lt;code&gt;ksqldb-twitter_default&lt;/code&gt; network name)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-A&lt;/code&gt; - render in ASCII&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&#39;port 8083&#39;&lt;/code&gt; - Only show traffic on port 8083&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now we get to see the stuff we&amp;#8217;re interested in, like the ksqlDB server sending a REST call to create a connector&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;:15:02.394620 IP ksqldb-server.52086 &amp;gt; kafka-connect-01.8083: Flags &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;P.&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, seq &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;:898, ack &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, win &lt;span style=&#34;color:#666&#34;&gt;229&lt;/span&gt;, options &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;nop,nop,TS val &lt;span style=&#34;color:#666&#34;&gt;65148580&lt;/span&gt; ecr &lt;span style=&#34;color:#666&#34;&gt;65148580&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#666&#34;&gt;897&lt;/span&gt;
E.....@.@..9.........v...^..7..............
........POST /connectors HTTP/1.1
Content-Length: &lt;span style=&#34;color:#666&#34;&gt;662&lt;/span&gt;
Content-Type: application/json; &lt;span style=&#34;color:#19177c&#34;&gt;charset&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;UTF-8
Host: kafka-connect-01:8083
Connection: Keep-Alive
User-Agent: Apache-HttpClient/4.5.3 &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Java/1.8.0_222&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
Accept-Encoding: gzip,deflate

&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;SOURCE_TWITTER_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;config&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;com.github.jcustenborder.kafka.connect.twitter.TwitterSourceConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;twitter.oauth.accessToken&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;file&lt;/span&gt;:/data/credentials.pro
[…]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;My thanks to &lt;a href=&#34;https://twitter.com/xxradar&#34;&gt;Philippe Bogaerts&lt;/a&gt; for his excellent blog post &lt;a href=&#34;https://medium.com/@xxradar/how-to-tcpdump-effectively-in-docker-2ed0a09b5406&#34;&gt;How to TCPdump effectively in Docker&lt;/a&gt; on which this one is entirely based&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Common mistakes made when configuring multiple Kafka Connect workers</title>
      <link>https://rmoff.github.io/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/</link>
      <pubDate>2019-11-22</pubDate>
      
      <guid>https://rmoff.github.io/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/11/IMG_1281.jpeg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect can be deployed in two modes: &lt;strong&gt;Standalone&lt;/strong&gt; or &lt;strong&gt;Distributed&lt;/strong&gt;. You can learn more about them in my &lt;a href=&#34;http://rmoff.dev/ksldn19-kafka-connect&#34;&gt;Kafka Summit London 2019 talk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I usually recommend &lt;strong&gt;Distributed&lt;/strong&gt; for several reasons:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It can scale&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is fault-tolerant&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It can be run on a single node sandbox or a multi-node production environment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is the same configuration method however you run it&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I usually find that &lt;strong&gt;Standalone&lt;/strong&gt; is appropriate when:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You need to guarantee locality of task execution, such as picking up a log file from a folder on a specific machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don&amp;#8217;t care about scale or fault-tolerance ;-)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You like re-learning how to configure something when you realise that you &lt;em&gt;do&lt;/em&gt; care about scale or fault-tolerance X-D&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;My last snarky point on the list is why even if you&amp;#8217;re just playing around with Kafka Connect on a laptop, learning it in Distributed mode means you learn it once, and then you&amp;#8217;re all set. If you start with Standalone and its &lt;code&gt;.properties&lt;/code&gt; method of passing configuration files to the worker at startup, and then come to use Distributed you have to re-learn how to use the REST interface etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/connect.png&#34; alt=&#34;connect&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So anyway…a long lead into a short article pointing out some of the common mistakes that can be made when setting up multiple Kafka Connect workers in a cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;As always, refer to &lt;a href=&#34;https://docs.confluent.io/current/connect/concepts.html#distributed-workers&#34;&gt;the documentation&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_mistake_1_rest_advertised_host_name_set_to_localhost&#34;&gt;Mistake 1: &lt;code&gt;rest.advertised.host.name&lt;/code&gt; set to &lt;code&gt;localhost&lt;/code&gt;&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;code&gt;rest.advertised.host.name&lt;/code&gt; (or if you&amp;#8217;re using Docker, &lt;code&gt;CONNECT_REST_ADVERTISED_HOST_NAME&lt;/code&gt;) is how a Connect worker communicates with other workers in the cluster. If you set it to &lt;code&gt;localhost&lt;/code&gt; then each worker in the cluster will only ever be able to contact itself when you use the REST interface, e.g. to send configuration updates. If the worker happens to be the leader of the connect cluster then the command will work, but if it&amp;#8217;s not then you&amp;#8217;ll get this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;{&#34;error_code&#34;:409,&#34;message&#34;:&#34;Cannot complete request because of a conflicting operation (e.g. worker rebalance)&#34;}⏎&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you front your Kafka Connect workers with a load balancer with a random/round-robin policy then you&amp;#8217;ll see the above error &#34;randomly&#34;, since you&amp;#8217;ll only get it if you happen to be forwarded to a worker that is not the leader.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The second problem with doing this is that even though you might get connectors running successfully (if you send the config REST call to a worker that is the leader) the tasks that run across the cluster will all be identified as running on &lt;code&gt;localhost&lt;/code&gt;, which makes it impossible to determine which worker they&amp;#8217;re on. Here&amp;#8217;s an example of a connector running a six tasks across three workers:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;            jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;.&amp;#34;source-datagen-01&amp;#34;.status.tasks&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;state&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RUNNING&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;worker_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;localhost:8083&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;FIX&lt;/strong&gt;: make sure &lt;code&gt;rest.advertised.host.name&lt;/code&gt; / &lt;code&gt;CONNECT_REST_ADVERTISED_HOST_NAME&lt;/code&gt; is set to &lt;em&gt;the hostname of the worker that is resolvable to the other workers&lt;/em&gt;. If you&amp;#8217;ve got a private network (e.g. Docker, VPC, etc) then this is the &lt;strong&gt;internal&lt;/strong&gt; hostname/IP of the workers. It is nothing to do with the &lt;strong&gt;external&lt;/strong&gt; hostname that you might access it by through a load balancer etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_mistake_2_rest_advertised_host_name_set_to_something_not_resolvable_by_the_workers&#34;&gt;Mistake 2: &lt;code&gt;rest.advertised.host.name&lt;/code&gt; set to something not resolvable by the workers&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is a variation on the above problem. The &lt;code&gt;rest.advertised.host.name&lt;/code&gt; (or if you&amp;#8217;re using Docker, &lt;code&gt;CONNECT_REST_ADVERTISED_HOST_NAME&lt;/code&gt;) is how a Connect worker communicates with other workers in the cluster. It needs to be something that can be &lt;em&gt;resolved by the other workers&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A problem that can arise is if you set this to an address that may be resolvable outside the Kafka Connect cluster (e.g. an external DNS hostname) but which isn&amp;#8217;t within the cluster&amp;#8217;s network.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you do this then similarly to above, if you send the REST call to the worker that happens to be the leader of the cluster then things will work - but if it&amp;#8217;s not the leader you&amp;#8217;ll get&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;{&#34;error_code&#34;:500,&#34;message&#34;:&#34;IO Error trying to forward REST request: java.net.UnknownHostException: foobar2: Name or service not known&#34;}⏎&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(where &lt;code&gt;foobar2&lt;/code&gt; is the hostname of the leader worker of the cluster)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Since Kafka Connect uses Kafka topics to distribute configuration, if you &lt;em&gt;do&lt;/em&gt; send the REST call to the leader then it writes the config to the topic which the other workers then pick up - hence the connector will still execute.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;FIX&lt;/strong&gt;: make sure &lt;code&gt;rest.advertised.host.name&lt;/code&gt; / &lt;code&gt;CONNECT_REST_ADVERTISED_HOST_NAME&lt;/code&gt; is set to &lt;em&gt;the hostname of the worker that is resolvable to the other workers&lt;/em&gt;. If you&amp;#8217;ve got a private network (e.g. Docker, VPC, etc) then this is the &lt;strong&gt;internal&lt;/strong&gt; hostname/IP of the workers. It is nothing to do with the &lt;strong&gt;external&lt;/strong&gt; hostname that you might access it by through a load balancer etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_mistake_3_sharing_the_same_kafka_topics_for_different_kafka_connect_clusters&#34;&gt;Mistake 3: Sharing the same Kafka topics for different Kafka Connect clusters&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect uses Kafka topics to share and persist information about connector configuration, offsets, and the status of tasks. For each Kafka Connect cluster that you run, you need &lt;strong&gt;a unique set of three Kafka topics&lt;/strong&gt;. If you try to share them, even having set a different &lt;code&gt;group.id&lt;/code&gt; for your Kafka Connect workers, you&amp;#8217;ll find that each cluster will start running the other&amp;#8217;s connectors too.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can see why by examining the config topic; it doesn&amp;#8217;t include the &lt;code&gt;group.id&lt;/code&gt; in the key for the messages that share the configuration (&lt;em&gt;perhaps it should&lt;/em&gt;?), which means that any worker reading from this topic will assume that it&amp;#8217;s for it to run and share amongst its cluster&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafkacat -b localhost:9092 -t _kafka-connect-configs -o beginning -f &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;key: %k, payload: %s\n&amp;#39;&lt;/span&gt; -u -C

key: connector-source-datagen-01, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-0, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-1, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-2, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-3, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-4, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: task-source-datagen-01-5, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;properties&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;connector.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenConnector&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;quickstart&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;ratings&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;task.class&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.confluent.kafka.connect.datagen.DatagenTask&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks.max&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;6&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;source-datagen-01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;kafka.topic&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item_details_01&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;max.interval&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;250&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
key: commit-source-datagen-01, payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tasks&amp;#34;&lt;/span&gt;:6&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_interested_to_know_more&#34;&gt;Interested to know more?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can see my notes and try out the test rig on Docker Compose &lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/connect-cluster&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    <item>
      <title>Streaming data from SQL Server to Kafka to Snowflake ❄️ with Kafka Connect</title>
      <link>https://rmoff.github.io/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</link>
      <pubDate>2019-11-20</pubDate>
      
      <guid>https://rmoff.github.io/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/11/IMG_1318.jpeg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.snowflake.com/&#34;&gt;Snowflake&lt;/a&gt; is &lt;em&gt;the data warehouse built for the cloud&lt;/em&gt;, so let&amp;#8217;s get all ☁️ cloudy and stream some data from Kafka running in &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; to Snowflake!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What I&amp;#8217;m showing also works just as well for an on-premises Kafka cluster. I&amp;#8217;m using SQL Server as an example data source, with Debezium to capture and stream and changes from it into Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf01.png&#34; alt=&#34;sf01&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;m assuming that you&amp;#8217;ve signed up for &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; and &lt;a href=&#34;https://www.snowflake.com/try-the-data-warehouse-built-for-the-cloud/&#34;&gt;Snowflake&lt;/a&gt; and are the proud owner of credentials for both. I&amp;#8217;m going to use a demo rig based on Docker to provision SQL Server and a Kafka Connect worker, but you can use your own setup if you want.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;All the code shown here is based on &lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/pipeline-to-the-cloud&#34;&gt;this github repo&lt;/a&gt;. If you&amp;#8217;re following along then make sure you set up &lt;code&gt;.env&lt;/code&gt; (copy the template from &lt;code&gt;.env.example&lt;/code&gt;) with all of your cloud details. This &lt;code&gt;.env&lt;/code&gt; file gets mounted in the Docker container to &lt;code&gt;/data/credentials.properties&lt;/code&gt;, which is what&amp;#8217;s referenced in the connector configurations below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sql_server_️_kafka_with_debezium&#34;&gt;SQL Server ➡️ Kafka with Debezium&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SQL Server needs to be configured for CDC at a database level:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;USE [demo]
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;GO&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;EXEC&lt;/span&gt; sys.sp_cdc_enable_db
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;GO&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and then per table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;USE [demo]

&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;EXEC&lt;/span&gt; sys.sp_cdc_enable_table
&lt;span style=&#34;color:#666&#34;&gt;@&lt;/span&gt;source_schema &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; N&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;dbo&amp;#39;&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;@&lt;/span&gt;source_name   &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; N&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;ORDERS&amp;#39;&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;@&lt;/span&gt;role_name     &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;NULL&lt;/span&gt;,
&lt;span style=&#34;color:#666&#34;&gt;@&lt;/span&gt;supports_net_changes &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;GO&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Once that&amp;#8217;s done you can setup the connector. If you&amp;#8217;ve not installed it already then make sure you&amp;#8217;ve installed the Debezium SQL Server connector in your Kafka Connect worker and restarted it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;confluent-hub install --no-prompt debezium/debezium-connector-sqlserver:0.10.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check that the plugin has been loaded successfully:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s localhost:8083/connector-plugins|jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;.[].class&amp;#39;&lt;/span&gt;|grep debezium
&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;io.debezium.connector.sqlserver.SqlServerConnector&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Debezium will write to a topic with all of the data from SQL Server. Debezium also needs its own topic for tracking the DDL—and we need to pre-create both these topics (see &lt;a href=&#34;https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/&#34;&gt;this article&lt;/a&gt; for more details):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ ccloud kafka topic create --partitions &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; dbz_dbhistory.mssql.asgard-01
$ ccloud kafka topic create mssql-01-mssql.dbo.ORDERS
$ ccloud kafka topic list
                 Name
+-------------------------------------+
  dbz_dbhistory.mssql.asgard-01
  mssql-01-mssql.dbo.ORDERS&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now create the connector. It&amp;#8217;s a bit more verbose because we&amp;#8217;re using a secure Kafka cluster and Debezium needs the details passed directly to it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -i -X PUT -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    http://localhost:8083/connectors/source-debezium-mssql-01/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;connector.class&amp;#34;: &amp;#34;io.debezium.connector.sqlserver.SqlServerConnector&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.hostname&amp;#34;: &amp;#34;mssql&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.port&amp;#34;: &amp;#34;1433&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.user&amp;#34;: &amp;#34;sa&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.password&amp;#34;: &amp;#34;Admin123&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.dbname&amp;#34;: &amp;#34;demo&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.server.name&amp;#34;: &amp;#34;mssql&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;table.whitelist&amp;#34;:&amp;#34;dbo.orders&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.kafka.bootstrap.servers&amp;#34;: &amp;#34;${file:/data/credentials.properties:CCLOUD_BROKER_HOST}:9092&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.kafka.topic&amp;#34;: &amp;#34;dbz_dbhistory.mssql.asgard-01&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.security.protocol&amp;#34;: &amp;#34;SASL_SSL&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.ssl.endpoint.identification.algorithm&amp;#34;: &amp;#34;https&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.sasl.mechanism&amp;#34;: &amp;#34;PLAIN&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.sasl.jaas.config&amp;#34;: &amp;#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_KEY}\&amp;#34; password=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_SECRET}\&amp;#34;;&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.security.protocol&amp;#34;: &amp;#34;SASL_SSL&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.ssl.endpoint.identification.algorithm&amp;#34;: &amp;#34;https&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.sasl.mechanism&amp;#34;: &amp;#34;PLAIN&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.sasl.jaas.config&amp;#34;: &amp;#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_KEY}\&amp;#34; password=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_SECRET}\&amp;#34;;&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;decimal.handling.mode&amp;#34;:&amp;#34;double&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms&amp;#34;: &amp;#34;unwrap,addTopicPrefix&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.unwrap.type&amp;#34;: &amp;#34;io.debezium.transforms.ExtractNewRecordState&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.type&amp;#34;:&amp;#34;org.apache.kafka.connect.transforms.RegexRouter&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.regex&amp;#34;:&amp;#34;(.*)&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.replacement&amp;#34;:&amp;#34;mssql-01-$1&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;With that running we can then check the data from Kafka. Note that we&amp;#8217;re using Avro to serialise the data, with the Schema Registry running as part of Confluent Cloud.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ ccloud kafka topic consume --from-beginning mssql-04-mssql.dbo.ORDERS
Starting Kafka Consumer. ^C to &lt;span style=&#34;color:#008000&#34;&gt;exit&lt;/span&gt;
����������@Proper Job
���q&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
ףp�?Wainwright
��Ҝ333333@Proper Job
�ޜ��Q��@Galena&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Because it&amp;#8217;s Avro, it renders here as a bunch of &lt;em&gt;odd&lt;/em&gt; characters. We can use a tool such as &lt;code&gt;kafkacat&lt;/code&gt; if we want to deserialise it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;source&lt;/span&gt; .env
docker run --rm edenhill/kafkacat:1.5.0 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X security.protocol&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;SASL_SSL -X sasl.mechanisms&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;PLAIN &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X ssl.ca.location&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;./etc/ssl/cert.pem -X api.version.request&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008000&#34;&gt;true&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -b &lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;:9092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X sasl.username&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X sasl.password&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -r https://&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;@&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_HOST&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -s avro &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -t mssql-04-mssql.dbo.ORDERS &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -f &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;Topic %t[%p], offset: %o, Headers: %h, key: %k, payload: %S bytes: %s\n&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -C -o beginning -c5&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Topic mssql-04-mssql.dbo.ORDERS&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, offset: &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;, Headers: , key: , payload: &lt;span style=&#34;color:#666&#34;&gt;34&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;14&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;18233&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_total_usd&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;.8900000000000001&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Wainwright&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
Topic mssql-04-mssql.dbo.ORDERS&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, offset: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, Headers: , key: , payload: &lt;span style=&#34;color:#666&#34;&gt;30&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;18225&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_total_usd&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;.9100000000000001&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Galena&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
Topic mssql-04-mssql.dbo.ORDERS&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, offset: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;, Headers: , key: , payload: &lt;span style=&#34;color:#666&#34;&gt;32&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;7&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;19&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;18227&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_total_usd&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;.6900000000000004&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Landlord&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
Topic mssql-04-mssql.dbo.ORDERS&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, offset: &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;, Headers: , key: , payload: &lt;span style=&#34;color:#666&#34;&gt;34&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;8&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;18228&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_total_usd&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;.6699999999999999&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Proper Job&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
Topic mssql-04-mssql.dbo.ORDERS&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, offset: &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;, Headers: , key: , payload: &lt;span style=&#34;color:#666&#34;&gt;39&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;9&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;18229&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;order_total_usd&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;.2400000000000002&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;item&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Black Sheep Ale&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka_️_snowflake_️&#34;&gt;Kafka ➡️ Snowflake ❄️&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_setting_up_snowflake_account_and_key_pair&#34;&gt;Setting up Snowflake account and key pair&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To send data to Snowflake you first need to generate a private/public key pair that will be used for authentication. Generate the keys:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Create Private key - keep this safe, do not share!&lt;/span&gt;
openssl genrsa -out snowflake_key.pem &lt;span style=&#34;color:#666&#34;&gt;2048&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Generate public key from private key. You can share your public key.&lt;/span&gt; 
openssl rsa -in snowflake_key.pem  -pubout -out snowflake_key.pub&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You should now have two files:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ ls -l snowflake_key*
-rw-r--r--  &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; rmoff  staff  &lt;span style=&#34;color:#666&#34;&gt;1679&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;21&lt;/span&gt; Nov &lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:28 snowflake_key.pem
-rw-r--r--  &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; rmoff  staff   &lt;span style=&#34;color:#666&#34;&gt;451&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;21&lt;/span&gt; Nov &lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:28 snowflake_key.pub&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now you need to get your &lt;strong&gt;public&lt;/strong&gt; key:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ cat snowflake_key.pub
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAya/BRlyhsfdlJQnPqoRn
lJfxKxujoyionNBPIDFpVpGZ9C1ZE7Q1kGIrEoZfq1t2p6lT8cX6gIZkMDF10I/8
yqHGiCdSEQBuMYXwWpnl3C1sttFHNfxbsjiKSZDlMTbEmzwU5s5LpMt8YvFWp8Iu
3ilHK9Vwy0wbsMDCjDcrC6xCS6qp1n4oso+V24aaxKd/mUtpPy9toAx2NC5GMoDb
tehlbTyPkk/9qFl7GUsf46HbQMEGoGkRrY9VFm+3Z8wCwsFNpURIvLEBcrTFdnmn
IgDBa96+dKgaN8qV6RW3ZMheQOJH1tP3M0qXsLNbR00E7yAlCYjNQD3hXjGKL3Oc
5wIDAQAB
-----END PUBLIC KEY-----&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;But minus the header and footer and joined over a single line. You can do this manually, or automagically:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ grep -v &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;BEGIN PUBLIC&amp;#34;&lt;/span&gt; snowflake_key.pub | grep -v &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;END PUBLIC&amp;#34;&lt;/span&gt;|tr -d &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\n&lt;/span&gt;
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAya/BRlyhsfdlJQnPqoRnlJfxKxujoyionNBPIDFpVpGZ9C1ZE7Q1kGIrEoZfq1t2p6lT8cX6gIZkMDF10I/8yqHGiCdSEQBuMYXwWpnl3C1sttFHNfxbsjiKSZDlMTbEmzwU5s5LpMt8YvFWp8Iu3ilHK9Vwy0wbsMDCjDcrC6xCS6qp1n4oso+V24aaxKd/mUtpPy9toAx2NC5GMoDbtehlbTyPkk/9qFl7GUsf46HbQMEGoGkRrY9VFm+3Z8wCwsFNpURIvLEBcrTFdnmnIgDBa96+dKgaN8qV6RW3ZMheQOJH1tP3M0qXsLNbR00E7yAlCYjNQD3hXjGKL3Oc5wIDAQAB&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now head to Snowflake, where we need to create a user for loading the data. First up, switch to the &lt;code&gt;SECURITYADMIN&lt;/code&gt; role.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf02.png&#34; alt=&#34;sf02&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
Make sure you do this in the &lt;code&gt;Context&lt;/code&gt; section of the worksheet, not the top-right dropdown (otherwise you&amp;#8217;ll get &lt;code&gt;SQL access control error: Insufficient privileges to operate on account &#39;xyz&#39;&lt;/code&gt;).
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now create the user, here called &lt;code&gt;kafka&lt;/code&gt;. Because we&amp;#8217;re in demo-land we&amp;#8217;re also granting Kafka the keys to the kingdom (&lt;code&gt;SYSADMIN&lt;/code&gt;), just to make everything nice &#39;n easy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;CREATE USER kafka RSA_PUBLIC_KEY=&#39;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAya/BRlyhsfdlJQnPqoRnlJfxKxujoyionNBPIDFpVpGZ9C1ZE7Q1kGIrEoZfq1t2p6lT8cX6gIZkMDF10I/8yqHGiCdSEQBuMYXwWpnl3C1sttFHNfxbsjiKSZDlMTbEmzwU5s5LpMt8YvFWp8Iu3ilHK9Vwy0wbsMDCjDcrC6xCS6qp1n4oso+V24aaxKd/mUtpPy9toAx2NC5GMoDbtehlbTyPkk/9qFl7GUsf46HbQMEGoGkRrY9VFm+3Z8wCwsFNpURIvLEBcrTFdnmnIgDBa96+dKgaN8qV6RW3ZMheQOJH1tP3M0qXsLNbR00E7yAlCYjNQD3hXjGKL3Oc5wIDAQAB&#39;
GRANT ROLE SYSADMIN TO USER kafka;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf03.png&#34; alt=&#34;sf03&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now we need to extract the private key for the key pair, which is in the &lt;code&gt;.pem&lt;/code&gt; file that we created, minus the header and footer and on a single line:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf04.png&#34; alt=&#34;sf04&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
Your private key is &lt;strong&gt;private&lt;/strong&gt; - don&amp;#8217;t share it with anyone who shouldn&amp;#8217;t have access to the account, and definitely don&amp;#8217;t post it on the internet on a blog post!
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;As before you can extract the key automagically with:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;grep -v &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;BEGIN RSA PRIVATE&amp;#34;&lt;/span&gt; snowflake_key.pem | grep -v &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;END RSA PRIVATE&amp;#34;&lt;/span&gt;|tr -d &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\n&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Put this value, along with the URL of your Snowflake environment and the user that we created (&lt;code&gt;kafka&lt;/code&gt;) in the &lt;code&gt;.env&lt;/code&gt; file&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf05.png&#34; alt=&#34;sf05&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This &lt;code&gt;.env&lt;/code&gt; file gets mounted in the Docker container to &lt;code&gt;/data/credentials.properties&lt;/code&gt; which is what&amp;#8217;s referenced in the connector configuration below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_setting_up_the_snowflake_connector&#34;&gt;Setting up the Snowflake connector&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Install the connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;confluent-hub install --no-prompt snowflakeinc/snowflake-kafka-connector:0.5.5&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Restart the Kafka Connect connector and check that it&amp;#8217;s been loaded:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s localhost:8083/connector-plugins|jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;.[].class&amp;#39;&lt;/span&gt;|grep snowflake
&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;com.snowflake.kafka.connector.SnowflakeSinkConnector&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now set up your connector configuration. A few important settings of note:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;topics&lt;/code&gt; - A comma separated list of one or more topics that are to be streamed to Snowflake. You can optionally map topics to table names with &lt;code&gt;snowflake.topic2table.map&lt;/code&gt; but this is not mandatory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;value.converter&lt;/code&gt; - Snowflake provide their own converters. Use either:&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;com.snowflake.kafka.connector.records.SnowflakeAvroConverter&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;com.snowflake.kafka.connector.records.SnowflakeJsonConverter&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Authentication / sensitive information&lt;/strong&gt; I&amp;#8217;ve &lt;a href=&#34;https://rmoff.net/2019/05/24/putting-kafka-connect-passwords-in-a-separate-file-/-externalising-secrets/&#34;&gt;embedded these in a separate file&lt;/a&gt; (&lt;code&gt;.env&lt;/code&gt;) that&amp;#8217;s loaded by the connector directly:&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;snowflake.url.name&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;snowflake.user.name&lt;/code&gt; - we created the user &lt;code&gt;kafka&lt;/code&gt; for this above&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;snowflake.private.key&lt;/code&gt; - this is the key that we extracted in the step above&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can see all of the configuration options in &lt;a href=&#34;https://docs.snowflake.net/manuals/user-guide/kafka-connector-install.html#kafka-configuration-properties&#34;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Create the connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -i -X PUT -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    http://localhost:8083/connectors/sink_snowflake_01/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;connector.class&amp;#34;:&amp;#34;com.snowflake.kafka.connector.SnowflakeSinkConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;tasks.max&amp;#34;:1,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;topics&amp;#34;:&amp;#34;mssql-01-mssql.dbo.ORDERS&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.url.name&amp;#34;:&amp;#34;${file:/data/credentials.properties:SNOWFLAKE_HOST}&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.user.name&amp;#34;:&amp;#34;${file:/data/credentials.properties:SNOWFLAKE_USER}&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.user.role&amp;#34;:&amp;#34;SYSADMIN&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.private.key&amp;#34;:&amp;#34;${file:/data/credentials.properties:SNOWFLAKE_PRIVATE_KEY}&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.database.name&amp;#34;:&amp;#34;DEMO_DB&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;snowflake.schema.name&amp;#34;:&amp;#34;PUBLIC&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;key.converter&amp;#34;:&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;value.converter&amp;#34;:&amp;#34;com.snowflake.kafka.connector.records.SnowflakeAvroConverter&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;value.converter.schema.registry.url&amp;#34;:&amp;#34;https://${file:/data/credentials.properties:CCLOUD_SCHEMA_REGISTRY_HOST}&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;value.converter.basic.auth.credentials.source&amp;#34;:&amp;#34;USER_INFO&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        &amp;#34;value.converter.basic.auth.user.info&amp;#34;:&amp;#34;${file:/data/credentials.properties:CCLOUD_SCHEMA_REGISTRY_API_KEY}:${file:/data/credentials.properties:CCLOUD_SCHEMA_REGISTRY_API_SECRET}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check that it&amp;#8217;s running:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;           jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;           column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink    |  sink_snowflake_01         |  RUNNING  |  RUNNING  |  com.snowflake.kafka.connector.SnowflakeSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now head over to Snowflake and you&amp;#8217;ll see your table created and data loaded:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf06.png&#34; alt=&#34;sf06&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The connector writes the Kafka message payload to the &lt;code&gt;RECORD_CONTENT&lt;/code&gt; field and its metadata (partition, offset, etc) to &lt;code&gt;RECORD_METADATA&lt;/code&gt;. You can access the nested values using the colon as a seperator, e.g.:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;SELECT&lt;/span&gt; RECORD_CONTENT:customer_id &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;AS&lt;/span&gt; CUSTOMER_ID,
       RECORD_CONTENT:item &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;AS&lt;/span&gt; ITEM, 
       RECORD_CONTENT:order_total_usd &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;AS&lt;/span&gt; ORDER_TOTAL_USD
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FROM&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;DEMO_DB&amp;#34;&lt;/span&gt;.&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;PUBLIC&amp;#34;&lt;/span&gt;.&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;MSSQL_01_MSSQL_DBO_ORDERS_97237615&amp;#34;&lt;/span&gt;;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/sf07.png&#34; alt=&#34;sf07&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_footnote_a_few_gotchas&#34;&gt;Footnote : a few gotchas&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Gotcha 01 : The &lt;strong&gt;connector name&lt;/strong&gt; must be a valid Snowflake identifier. If it&amp;#8217;s not you&amp;#8217;ll get this error:&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; name is empty or invalid. It should match Snowflake object identifier syntax. Please see the documentation. &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;com.snowflake.kafka.connector.Utils:246&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In the example above, the connector name is &lt;code&gt;sink_snowflake_01&lt;/code&gt;. If I tried to name it &lt;code&gt;sink-snowflake-01&lt;/code&gt; (i.e. using &lt;code&gt;-&lt;/code&gt; instead of &lt;code&gt;_&lt;/code&gt;) then it would fail 🤷‍♂️&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;See &lt;a href=&#34;https://github.com/snowflakedb/snowflake-kafka-connector/issues/62&#34;&gt;this issue&lt;/a&gt; on the Snowflake connector repo.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: don&amp;#8217;t name your connector with characters that aren&amp;#8217;t &lt;a href=&#34;https://docs.snowflake.net/manuals/sql-reference/identifiers-syntax.html&#34;&gt;valid in a Snowflake object name&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You have to use Snowflake&amp;#8217;s own converters, or else you get:&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Exception: Invalid record data
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Error Code: &lt;span style=&#34;color:#666&#34;&gt;0019&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Detail: Unrecognizable record content, please use Snowflake Converters&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Depending on how your data is serialised, use &lt;code&gt;com.snowflake.kafka.connector.records.SnowflakeJsonConverter&lt;/code&gt; or &lt;code&gt;com.snowflake.kafka.connector.records.SnowflakeAvroConverter&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sometimes the connector will fail with an error and need restarting:&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Exception: Failed to put records
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Error Code: &lt;span style=&#34;color:#666&#34;&gt;5014&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;SF_KAFKA_CONNECTOR&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; Detail: SinkTask hasn&lt;span style=&#34;&#34;&gt;&amp;#39;&lt;/span&gt;t been initialized before calling PUT &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;function&lt;/span&gt;
  at com.snowflake.kafka.connector.internal.SnowflakeErrors.getException&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;SnowflakeErrors.java:362&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at com.snowflake.kafka.connector.internal.SnowflakeErrors.getException&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;SnowflakeErrors.java:321&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at com.snowflake.kafka.connector.SnowflakeSinkTask.getSink&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;SnowflakeSinkTask.java:94&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at com.snowflake.kafka.connector.SnowflakeSinkTask.put&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;SnowflakeSinkTask.java:195&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:538&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerSinkTask.poll&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:321&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:224&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerSinkTask.execute&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:192&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerTask.doRun&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerTask.java:177&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at org.apache.kafka.connect.runtime.WorkerTask.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerTask.java:227&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at java.util.concurrent.Executors&lt;span style=&#34;color:#19177c&#34;&gt;$RunnableAdapter&lt;/span&gt;.call&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Executors.java:511&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at java.util.concurrent.FutureTask.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;FutureTask.java:266&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at java.util.concurrent.ThreadPoolExecutor.runWorker&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;ThreadPoolExecutor.java:1149&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at java.util.concurrent.ThreadPoolExecutor&lt;span style=&#34;color:#19177c&#34;&gt;$Worker&lt;/span&gt;.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;ThreadPoolExecutor.java:624&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
  at java.lang.Thread.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Thread.java:748&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Restart the Connect task via the REST API. If your connector is called &lt;code&gt;sink_snowflake_01&lt;/code&gt; then you can run this to restart task &lt;code&gt;0&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://localhost:8083/connectors/sink_snowflake_01/tasks/0/restart&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    <item>
      <title>Running Dockerised Kafka Connect worker on GCP</title>
      <link>https://rmoff.github.io/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</link>
      <pubDate>2019-11-12</pubDate>
      
      <guid>https://rmoff.github.io/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/11/IMG_1070.jpeg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I &lt;a href=&#34;http://talks.rmoff.net/&#34;&gt;talk and write about Kafka and Confluent Platform&lt;/a&gt; a lot, and more and more of the demos that I&amp;#8217;m building are around &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt;. This means that I don&amp;#8217;t have to run or manage my own Kafka brokers, Zookeeper, Schema Registry, KSQL servers, etc which makes things a ton easier. Whilst there are managed connectors on Confluent Cloud (S3 etc), I need to run my own Kafka Connect worker for those connectors not yet provided. An example is the MQTT source connector that I use in &lt;a href=&#34;https://rmoff.dev/kssf19-ksql-video&#34;&gt;this demo&lt;/a&gt;. Up until now I&amp;#8217;d either run this worker locally, or manually build a cloud VM. Locally is fine, as it&amp;#8217;s all Docker, easily spun up in a single &lt;code&gt;docker-compose up -d&lt;/code&gt; command. I wanted something that would keep running whilst my laptop was off, but that was as close to my local build as possible—enter GCP and its functionality to run a container on a VM automagically.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;You can see &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/mqtt-tracker/launch-worker-container_gcloud.sh&#34;&gt;the full script here&lt;/a&gt;&lt;/strong&gt;. The rest of this article just walks through the how and why.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_the_script&#34;&gt;The script&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here&amp;#8217;s a walk through of what the script&amp;#8217;s doing and how to use it. First up, you need to create a &lt;code&gt;.env&lt;/code&gt; file with your secrets and config in:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CONFLUENTPLATFORM_VERSION&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;.4.0-beta1
#   
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
#
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_URL&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is then passed to &lt;code&gt;source&lt;/code&gt; to make the values available to the shell&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;source&lt;/span&gt; .env&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;m running Kafka Connect in distributed mode, which &lt;a href=&#34;http://rmoff.dev/ksldn19-kafka-connect&#34;&gt;I generally recommend in all instances - even on a single node&lt;/a&gt;. There&amp;#8217;s no reason not to, and it makes it easier to understand (and work with IMO) to learn a single deployment method instead of two. Since I&amp;#8217;m using distributed mode, all of the state for the worker is stored in Kafka itself. This is pretty cool, but it does mean that if you run multiple workers with the same persistence topics configured things will get funky. For that reason, I have a prefix for the worker and topics, which is based on the current timestamp—if you&amp;#8217;re using this script yourself you might want to vary this (or not; YMMV):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;$(&lt;/span&gt;date +%s&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Whilst Kafka Connect uses the Admin API to create its own internal topics (for state persistence) the topic(s) that the connector itself writes to need to be created manually. Here I use &lt;code&gt;kafka-topics&lt;/code&gt; to do that, through Docker running locally. I use Docker just for isolation and ease portability; if you want to use your own local install then you can. To use Confluent Cloud with &lt;code&gt;kafka-topics&lt;/code&gt; you need to have a local file with the necessary authentication details in, which is then passed with &lt;code&gt;--command-config&lt;/code&gt; in &lt;code&gt;kafka-topics&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat &amp;gt; /tmp/config-&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;.properties &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;ssl.endpoint.identification.algorithm=https
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;sasl.mechanism=PLAIN
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=&amp;#34;${CCLOUD_API_KEY}&amp;#34; password=&amp;#34;${CCLOUD_API_SECRET}&amp;#34;;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;security.protocol=SASL_SSL
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;request.timeout.ms=20000
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;retry.backoff.ms=500
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;

docker run --rm --volume /tmp/config-&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;.properties:/tmp/config.properties &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    confluentinc/cp-kafka:&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CONFLUENTPLATFORM_VERSION&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; /usr/bin/kafka-topics &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --command-config /tmp/config.properties &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --bootstrap-server &lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;:9092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --create &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --topic data_mqtt &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --partitions &lt;span style=&#34;color:#666&#34;&gt;6&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --replication-factor &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now that we&amp;#8217;ve got the topic created, we can spin up the worker itself. Or, almost. Because first we need to build a file with the necessary environment variables in for the worker. You &lt;em&gt;can&lt;/em&gt; pass environment variables directly in the &lt;code&gt;gcloud&lt;/code&gt; invocation but it&amp;#8217;s not a pretty sight&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/container_env.png&#34; alt=&#34;container env&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Even if it works, it&amp;#8217;s not particularly maintainable. Whilst you can externalise the environment variables into a file that you pass in with &lt;code&gt;container-env-file&lt;/code&gt; you can&amp;#8217;t interpolate secure values in that file which means that you end up with a file which is both config and authentication credentials, which is not ideal. Hence, the config is inline in this script and interpolated with credentials held in environment variables (via &lt;code&gt;.env&lt;/code&gt;) at runtime into a temporary file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#19177c&#34;&gt;PROPERTIES_FILE&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;/tmp/connect-worker-&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;_gcloud_env.properties

cat &amp;gt; &lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;PROPERTIES_FILE&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN=[%d] %p %X{connector.context}%m (%c:%L)%n
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CUB_KAFKA_TIMEOUT=300  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_BOOTSTRAP_SERVERS=${CCLOUD_BROKER_HOST}:9092
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect-01
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_REST_PORT=8083  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_GROUP_ID=kafka-connect-group-${epoch}
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONFIG_STORAGE_TOPIC=_kafka-connect-group-${epoch}-configs  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_OFFSET_STORAGE_TOPIC=_kafka-connect-group-${epoch}-offsets  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_STATUS_STORAGE_TOPIC=_kafka-connect-group-${epoch}-status  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_KEY_CONVERTER=io.confluent.connect.avro.AvroConverter  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=${CCLOUD_SCHEMA_REGISTRY_URL}
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_KEY_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE=USER_INFO
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO=${CCLOUD_SCHEMA_REGISTRY_API_KEY}:${CCLOUD_SCHEMA_REGISTRY_API_SECRET}
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_VALUE_CONVERTER=io.confluent.connect.avro.AvroConverter  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=${CCLOUD_SCHEMA_REGISTRY_URL}
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE=USER_INFO
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO=${CCLOUD_SCHEMA_REGISTRY_API_KEY}:${CCLOUD_SCHEMA_REGISTRY_API_SECRET}
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_LOG4J_ROOT_LOGLEVEL=INFO
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_LOG4J_LOGGERS=org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=3
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=3
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=3
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_PLUGIN_PATH=/usr/share/java,/usr/share/confluent-hub-components/
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM=https
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_SASL_MECHANISM=PLAIN
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_SECURITY_PROTOCOL=SASL_SSL
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_SASL_JAAS_CONFIG=org.apache.kafka.common.security.plain.PlainLoginModule required username=&amp;#34;${CCLOUD_API_KEY}&amp;#34; password=&amp;#34;${CCLOUD_API_SECRET}&amp;#34;;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONSUMER_SECURITY_PROTOCOL=SASL_SSL
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM=https
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONSUMER_SASL_MECHANISM=PLAIN
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_CONSUMER_SASL_JAAS_CONFIG=org.apache.kafka.common.security.plain.PlainLoginModule required username=&amp;#34;${CCLOUD_API_KEY}&amp;#34; password=&amp;#34;${CCLOUD_API_SECRET}&amp;#34;;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_PRODUCER_SECURITY_PROTOCOL=SASL_SSL
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM=https
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_PRODUCER_SASL_MECHANISM=PLAIN
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;CONNECT_PRODUCER_SASL_JAAS_CONFIG=org.apache.kafka.common.security.plain.PlainLoginModule required username=&amp;#34;${CCLOUD_API_KEY}&amp;#34; password=&amp;#34;${CCLOUD_API_SECRET}&amp;#34;;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now, finally, we can launch the container, using the &lt;code&gt;gcloud beta compute instances create-with-container&lt;/code&gt; option. The options I built out using the GCP web UI and then before launching clicked the magic button&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/11/equiv.png&#34; alt=&#34;equiv&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;It&amp;#8217;s fairly standard list of parameters, including &lt;code&gt;container-image&lt;/code&gt; to refer to the docker image, &lt;code&gt;container-env-file&lt;/code&gt; to point to the environment file that we built above—and then &lt;code&gt;container-command&lt;/code&gt; and &lt;code&gt;container-arg&lt;/code&gt; to run custom commands, which:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Install the connector plugin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch the Kafka Connect worker process&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wait for the worker to be ready&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Submit a connector configuration&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcloud beta compute &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--project&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;devx-testing instances create-with-container rmoff-connect-mqtt-&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--machine-type&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;n1-standard-1 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--subnet&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;default &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--metadata&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;google-logging-enabled&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008000&#34;&gt;true&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--maintenance-policy&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;MIGRATE &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;cos-stable-77-12371-114-0 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--image-project&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;cos-cloud &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --no-scopes &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --no-service-account &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--boot-disk-size&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;10GB &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--boot-disk-type&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;pd-standard &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--boot-disk-device-name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;rmoff-connect-mqtt-&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;epoch&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--container-restart-policy&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;always &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;container-vm&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;cos-stable-77-12371-114-0 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--container-image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;confluentinc/cp-kafka-connect:&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CONFLUENTPLATFORM_VERSION&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --container-env-file&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;PROPERTIES_FILE&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--container-command&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;bash &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--container-arg&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;-c &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;	--container-arg&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;set -x
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        echo &amp;#34;Installing connector plugins&amp;#34; 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        confluent-hub install --no-prompt confluentinc/kafka-connect-mqtt:1.2.3
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        #
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        echo &amp;#34;Launching Kafka Connect worker&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        /etc/confluent/docker/run &amp;amp; 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        #
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        echo &amp;#34;Waiting for Kafka Connect to start listening on localhost:8083 ⏳&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        while : ; do
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            curl_status=$(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors)
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            echo -e $(date) &amp;#34; Kafka Connect listener HTTP state: &amp;#34; $curl_status &amp;#34; (waiting for 200)&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            if [ $curl_status -eq 200 ] ; then
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            break
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            fi
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            sleep 5 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        done
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        #
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        echo -e &amp;#34;\n--\n+&amp;gt; Creating Kafka Connect MQTT source&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        curl -s -X PUT -H  &amp;#34;Content-Type:application/json&amp;#34; http://localhost:8083/connectors/source-mqtt/config \
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            -d &amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&amp;#39;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{  
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;connector.class&amp;#34; : &amp;#34;io.confluent.connect.mqtt.MqttSourceConnector&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;mqtt.server.uri&amp;#34; : &amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;MQTT_URL&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;mqtt.password&amp;#34; : &amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;MQTT_PASSWORD&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;mqtt.username&amp;#34; : &amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;MQTT_USERNAME&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;mqtt.topics&amp;#34; : &amp;#34;owntracks/#&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;kafka.topic&amp;#34; : &amp;#34;data_mqtt&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;key.converter&amp;#34;: &amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;value.converter&amp;#34;: &amp;#34;org.apache.kafka.connect.converters.ByteArrayConverter&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;tasks.max&amp;#34; : &amp;#34;1&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;confluent.topic.bootstrap.servers&amp;#34; : &amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;:9092&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;confluent.topic.sasl.jaas.config&amp;#34; : &amp;#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;\&amp;#34; password=\&amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;\&amp;#34;;&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;confluent.topic.security.protocol&amp;#34;: &amp;#34;SASL_SSL&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;confluent.topic.ssl.endpoint.identification.algorithm&amp;#34;: &amp;#34;https&amp;#34;, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;confluent.topic.sasl.mechanism&amp;#34;: &amp;#34;PLAIN&amp;#34; 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            }&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&amp;#39;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        #    
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        sleep infinity&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The bash script that&amp;#8217;s embedded as an argument to &lt;code&gt;bash -c&lt;/code&gt; is mostly as you&amp;#8217;d run it natively, except some funky quoting to deal with single quotes within the command (that enclose the &lt;code&gt;-d&lt;/code&gt; value of &lt;code&gt;curl&lt;/code&gt;)—these are done with &lt;code&gt;&#39;&#34;&#39;&#34;&#39;&lt;/code&gt; which breaks down to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&#39;&lt;/code&gt; close the string&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&#34;&#39;&#34;&lt;/code&gt; quote a single quote&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&#39;&lt;/code&gt; open the string&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_gotchas&#34;&gt;Gotchas&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;One problem that I hit a problem was where the VM was created but my container within was not. By looking at the serial port output from bootup using:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;gcloud compute instances get-serial-port-output rmoff-connect-mqtt-1573561087&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I could see the last entry was:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;[   12.759163] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Turns out I&amp;#8217;d set &lt;code&gt;--no-address&lt;/code&gt; when creating the VM and this caused the problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To fix it, I just omitted this configuration which meant that the default allocation of an ephemeral IP address happened, and Docker started up nicely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_my_question_to_you&#34;&gt;My question to you&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Is this an abomination? Am I struggling to do it in an elegant way because I&amp;#8217;m just using the wrong technology? All I want to do is spin up a Connect worker using config and settings that I&amp;#8217;ve built locally, following the philosophy of cattle-not-pets. Yes I can build a cloud VM and config Connect manually, but with all the context switching that I do I want something I can get working, check in to git, and come back to a month later and run without having to think about any of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Should I be learning k8s, or is that over-engineering it? My gut feel is that it would be because I don&amp;#8217;t need the orchestration and management bells and whistles of k8s—but perhaps they&amp;#8217;re just an added benefit and I should take the leap? What about other options? I gave Terraform a &lt;em&gt;very&lt;/em&gt; quick look but I&amp;#8217;d prefer something closer to my local Docker builds—and I&amp;#8217;m tied to Docker because it&amp;#8217;s the standard platform on which a lot of developers are accepting of for trying demos and new technology. The more non-standard pieces, the higher the friction—we&amp;#8217;ve all seen those demos that have a laundry list of pre-reqs to use, and we&amp;#8217;ve all thought…sod it ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So—tell me if I&amp;#8217;m wrong - &lt;em&gt;do&lt;/em&gt; &lt;code&gt;@&lt;/code&gt; me!
I&amp;#8217;m &lt;strong&gt;&lt;a href=&#34;https://twitter.com/rmoff/&#34;&gt;@rmoff&lt;/a&gt; on Twitter&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Debezium &amp; MySQL v8 : Public Key Retrieval Is Not Allowed</title>
      <link>https://rmoff.github.io/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/</link>
      <pubDate>2019-10-23</pubDate>
      
      <guid>https://rmoff.github.io/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/04/IMG_9090.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I started hitting problems when trying Debezium against MySQL v8. When creating the connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -i -X PUT -H &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Accept:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; http://localhost:8083/connectors/source-debezium-mysql-00/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;connector.class&amp;#34;: &amp;#34;io.debezium.connector.mysql.MySqlConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.hostname&amp;#34;: &amp;#34;mysql&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.port&amp;#34;: &amp;#34;3306&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.user&amp;#34;: &amp;#34;debezium&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.password&amp;#34;: &amp;#34;dbz&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.server.id&amp;#34;: &amp;#34;42&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.server.name&amp;#34;: &amp;#34;asgard&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;table.whitelist&amp;#34;: &amp;#34;demo.customers&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.history.kafka.bootstrap.servers&amp;#34;: &amp;#34;kafka:29092&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.history.kafka.topic&amp;#34;: &amp;#34;asgard.dbhistory.demo&amp;#34; ,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;include.schema.changes&amp;#34;: &amp;#34;true&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;d get the error&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;error_code&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;400&lt;/span&gt;,
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Connector configuration is invalid and contains the following 1 error(s):\nUnable to connect: Public Key Retrieval is not allowed\nYou can also find the above list of errors at the endpoint `/{connectorType}/config/validate`&amp;#34;&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The fix for this, courtesy of Jiri on the &lt;a href=&#34;https://gitter.im/debezium/user&#34;&gt;Debezium gitter.im chat room&lt;/a&gt;, is to add to the connector configuraton &lt;code&gt;database.allowPublicKeyRetrieval=true&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -i -X PUT -H &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Accept:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; http://localhost:8083/connectors/source-debezium-mysql-00/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;connector.class&amp;#34;: &amp;#34;io.debezium.connector.mysql.MySqlConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.hostname&amp;#34;: &amp;#34;mysql&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.port&amp;#34;: &amp;#34;3306&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.user&amp;#34;: &amp;#34;debezium&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.password&amp;#34;: &amp;#34;dbz&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.server.id&amp;#34;: &amp;#34;42&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.allowPublicKeyRetrieval&amp;#34;:&amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.server.name&amp;#34;: &amp;#34;asgard&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;table.whitelist&amp;#34;: &amp;#34;demo.customers&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.history.kafka.bootstrap.servers&amp;#34;: &amp;#34;kafka:29092&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;database.history.kafka.topic&amp;#34;: &amp;#34;asgard.dbhistory.demo&amp;#34; ,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;include.schema.changes&amp;#34;: &amp;#34;true&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;After this the connector was created successfully, but immediately &lt;code&gt;FAILED&lt;/code&gt; with the following error in the Kafka Connect worker log:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt; 
org.apache.kafka.connect.errors.ConnectException: Failed to authenticate to the MySQL database at mysql:3306 with user &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;debezium&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;…&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;
com.github.shyiko.mysql.binlog.network.AuthenticationException: Client does not support authentication protocol requested by server; consider upgrading MySQL client&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A bit of Googling threw up &lt;a href=&#34;https://github.com/shyiko/mysql-binlog-connector-java/issues/240&#34;&gt;this issue on GitHub&lt;/a&gt; with a solution that worked—add &lt;code&gt;WITH mysql_native_password&lt;/code&gt; to the authentication settings for the Debezium user:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;ALTER&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;USER&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;debezium&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;@&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;%&amp;#39;&lt;/span&gt; IDENTIFIED &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;WITH&lt;/span&gt; mysql_native_password &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;dbz&amp;#39;&lt;/span&gt;;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;After that, the connector ran successfully 👍&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Using Kafka Connect and Debezium with Confluent Cloud</title>
      <link>https://rmoff.github.io/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</link>
      <pubDate>2019-10-16</pubDate>
      
      <guid>https://rmoff.github.io/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/10/IMG_2849.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is based on using &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; to provide your managed Kafka and Schema Registry. All that you run yourself is the Kafka Connect worker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Optionally, you can use this &lt;a href=&#34;https://github.com/rmoff/debezium-ccloud/blob/master/docker-compose.yml&#34;&gt;Docker Compose&lt;/a&gt; to run the worker and a sample MySQL database.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_what_you_need&#34;&gt;What you need&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; account with a Kafka and Schema Registry API host names and keys. Write these to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_URL&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;
&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_install_debezium_connector&#34;&gt;Install Debezium connector&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This article assumes that you&amp;#8217;re running your own Kafka Connect worker with the &lt;a href=&#34;https://docs.confluent.io/current/cloud/connect/connect-cloud-config.html&#34;&gt;appropriate configuration&lt;/a&gt; done to hook it up to Confluent Cloud&amp;#8217;s brokers and Schema Registry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You need to install the Debezium connector on the Kafka Connect worker:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;confluent-hub install --no-prompt debezium/debezium-connector-mysql:0.10.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can also do this as part of your &lt;a href=&#34;https://github.com/rmoff/debezium-ccloud/blob/master/docker-compose.yml&#34;&gt;Docker Compose&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;    command: 
      - bash 
      - -c 
      - |
        &lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Installing connector plugins&amp;#34;&lt;/span&gt;
        confluent-hub install --no-prompt debezium/debezium-connector-mysql:0.10.0
        #
        &lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Launching Kafka Connect worker&amp;#34;&lt;/span&gt;
        /etc/confluent/docker/run &amp;amp; 
        #
        sleep infinity&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_pre_create_the_topics_to_which_youll_be_writing&#34;&gt;Pre-create the topics to which you&amp;#8217;ll be writing&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Make sure your &lt;code&gt;ccloud&lt;/code&gt; environment is using the correct Confluent Cloud cluster&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ ccloud kafka cluster list
      Id      |       Name        | Provider |  Region   | Durability | Status
+-------------+-------------------+----------+-----------+------------+--------+
    lkc-42p8m | pipeline-to-cloud | aws      | us-east-1 | HIGH       | UP
  * lkc-43xgj | race-mapper       | aws      | us-east-1 | LOW        | UP

$ ccloud kafka cluster use lkc-42p8m&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create the required topics:&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://debezium.io/documentation/reference/0.10/connectors/mysql.html#database-schema-history&#34;&gt;Database history&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Name is set in the configuration property &lt;code&gt;database.history.kafka.topic&lt;/code&gt;. Must not be partitioned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ccloud kafka topic create --partitions &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; dbz_dbhistory.asgard-01&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;If you don&amp;#8217;t create this topic in advance, Debezium will do so for you, but with a hardcoded timeout of 3 seconds which is often not long enough in a Cloud environment—hence it&amp;#8217;s best to create it in advance.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://debezium.io/documentation/reference/0.10/connectors/mysql.html#schema-change-topic&#34;&gt;Schema changes&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Enabled by default, set &lt;code&gt;include.schema.changes&lt;/code&gt; to false if not required.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Name is taken from the configuration property &lt;code&gt;database.server.name&lt;/code&gt; (&lt;code&gt;asgard&lt;/code&gt;). In this example I&amp;#8217;m using the &lt;code&gt;RegexRouter&lt;/code&gt; Single Message Transform which prepends a &lt;code&gt;mysql-01-&lt;/code&gt; prefix to the topic name too. This is optional.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;Note that this topic must not be partitioned - Thanks to Terry Franklin for this 👍&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ccloud kafka topic create --partitions &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; mysql-01-asgard&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One topic per table ingested. The topic name is made up by the &lt;code&gt;database.server.name&lt;/code&gt; (&lt;code&gt;asgard&lt;/code&gt;), the database name (&lt;code&gt;demo&lt;/code&gt;), and the table name.&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In this example I&amp;#8217;m using the &lt;code&gt;RegexRouter&lt;/code&gt; Single Message Transform which prepends a &lt;code&gt;mysql-01-&lt;/code&gt; prefix to the topic name too. This is optional.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ccloud kafka topic create mysql-01-asgard.demo.customers
ccloud kafka topic create mysql-01-asgard.demo.transactions&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you don&amp;#8217;t pre-create your topics, you&amp;#8217;ll get repeating errors in your Kafka Connect worker log:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Error &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;while&lt;/span&gt; fetching metadata with correlation id … : &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&amp;lt;…topic…&amp;gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;UNKNOWN_TOPIC_OR_PARTITION&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can create the topics afterwards if you forget, but it&amp;#8217;s easier up-front.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_create_the_connector&#34;&gt;Create the connector&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now create the connector itself, substituting your MySQL details below as indicated. The Confluent Cloud details and credentials will be picked up from the file &lt;code&gt;/data/credentials.properties&lt;/code&gt; local to the Kafka Connect worker—which if you&amp;#8217;re using Docker can be mapped from the same &lt;code&gt;.env&lt;/code&gt; file as above. Or, just hardcode the values if you&amp;#8217;d prefer 🤷‍.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The configuration is the same as a normal Debezium connector except the additional details for the connector to be able to connect to Confluent Cloud for writing and reading the &lt;a href=&#34;https://debezium.io/documentation/reference/0.10/connectors/mysql.html#database-schema-history&#34;&gt;database schema history topic&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -i -X PUT -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    http://localhost:8083/connectors/source-debezium-mysql-01/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;connector.class&amp;#34;: &amp;#34;io.debezium.connector.mysql.MySqlConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.hostname&amp;#34;: &amp;#34;mysql&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.port&amp;#34;: &amp;#34;3306&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.user&amp;#34;: &amp;#34;debezium&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.password&amp;#34;: &amp;#34;dbz&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.server.name&amp;#34;: &amp;#34;asgard&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.kafka.bootstrap.servers&amp;#34;: &amp;#34;${file:/data/credentials.properties:CCLOUD_BROKER_HOST}&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.kafka.topic&amp;#34;: &amp;#34;dbz_dbhistory.asgard-01&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.security.protocol&amp;#34;: &amp;#34;SASL_SSL&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.ssl.endpoint.identification.algorithm&amp;#34;: &amp;#34;https&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.sasl.mechanism&amp;#34;: &amp;#34;PLAIN&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.consumer.sasl.jaas.config&amp;#34;: &amp;#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_KEY}\&amp;#34; password=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_SECRET}\&amp;#34;;&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.security.protocol&amp;#34;: &amp;#34;SASL_SSL&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.ssl.endpoint.identification.algorithm&amp;#34;: &amp;#34;https&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.sasl.mechanism&amp;#34;: &amp;#34;PLAIN&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;database.history.producer.sasl.jaas.config&amp;#34;: &amp;#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_KEY}\&amp;#34; password=\&amp;#34;${file:/data/credentials.properties:CCLOUD_API_SECRET}\&amp;#34;;&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;table.whitelist&amp;#34;:&amp;#34;demo.transactions,demo.customers&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;decimal.handling.mode&amp;#34;:&amp;#34;double&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms&amp;#34;: &amp;#34;unwrap,addTopicPrefix&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.unwrap.type&amp;#34;: &amp;#34;io.debezium.transforms.ExtractNewRecordState&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.type&amp;#34;:&amp;#34;org.apache.kafka.connect.transforms.RegexRouter&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.regex&amp;#34;:&amp;#34;(.*)&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    &amp;#34;transforms.addTopicPrefix.replacement&amp;#34;:&amp;#34;mysql-01-$1&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check that the connector is running:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort

&lt;span style=&#34;color:#008000&#34;&gt;source&lt;/span&gt;  |  source-debezium-mysql-01  |  RUNNING  |  RUNNING  |  io.debezium.connector.mysql.MySqlConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consume_the_data&#34;&gt;Consume the data&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_confluent_cloud_gui&#34;&gt;Confluent Cloud GUI&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/10/ccloud-debezium-01.png&#34; alt=&#34;Confluent Cloud screenshot&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafkacat&#34;&gt;kafkacat&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Set the variables, either from this script or manually&lt;/span&gt;
&lt;span style=&#34;color:#008000&#34;&gt;source&lt;/span&gt; .env

&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;# Use kafkacat to pull Avro messages from Confluent Cloud&lt;/span&gt; 
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;#  deserialised using the Schema Registry hosted on Confluent Cloud&lt;/span&gt;

docker run --rm edenhill/kafkacat:1.5.0 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -X security.protocol&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;SASL_SSL -X sasl.mechanisms&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;PLAIN &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -X ssl.ca.location&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;./etc/ssl/cert.pem -X api.version.request&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008000&#34;&gt;true&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -b &lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_BROKER_HOST&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -X sasl.username&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -X sasl.password&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -r https://&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;@&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;CCLOUD_SCHEMA_REGISTRY_URL&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -s avro &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -t mysql-01-asgard.demo.transactions &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -C -o beginning

&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;996&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;69&lt;/span&gt;.819999999999993&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;CNY&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-04-10T10:23:41Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;997&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;74&lt;/span&gt;.170000000000002&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;PEN&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-11-19T15:29:14Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;998&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: -92.920000000000002&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;JPY&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-05-25T19:43:48Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;999&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;71&lt;/span&gt;.159999999999997&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;EUR&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-11-15T07:24:44Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;28&lt;/span&gt;.149999999999999&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;IRR&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-01-12T14:53:49Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;603&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;customer_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;amount&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;double&amp;#34;&lt;/span&gt;: -85.510000000000005&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;currency&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;CNY&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;txn_timestamp&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;2018-11-08T22:06:49Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Skipping bad records with the Kafka Connect JDBC sink connector</title>
      <link>https://rmoff.github.io/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</link>
      <pubDate>2019-10-15</pubDate>
      
      <guid>https://rmoff.github.io/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/10/IMG_0170.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The Kafka Connect framework provides generic &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;error handling and dead-letter queue capabilities&lt;/a&gt; which are available for problems with [de]serialisation and Single Message Transforms. When it comes to errors that a connector may encounter doing the actual &lt;code&gt;pull&lt;/code&gt; or &lt;code&gt;put&lt;/code&gt; of data from the source/target system, it&amp;#8217;s down to the connector itself to implement logic around that. For example, the Elasticsearch sink connector provides configuration (&lt;code&gt;behavior.on.malformed.documents&lt;/code&gt;) that can be set so that a single bad record won&amp;#8217;t halt the pipeline. Others, such as the JDBC Sink connector, don&amp;#8217;t provide this &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-jdbc/issues/721&#34;&gt;yet&lt;/a&gt;. That means that if you hit this problem, you need to manually unblock it yourself. One way is to manually move the offset of the consumer on past the bad message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; : You can use &lt;code&gt;kafka-consumer-groups --reset-offsets --to-offset &amp;lt;x&amp;gt;&lt;/code&gt; to manually move the connector past a bad message&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_try_it_out&#34;&gt;Try it out!&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Create the connector&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X PUT http://localhost:8083/connectors/sink_postgres_foo_00/config -H &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt; -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;connector.class&amp;#34;: &amp;#34;io.confluent.connect.jdbc.JdbcSinkConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;connection.url&amp;#34;: &amp;#34;jdbc:postgresql://postgres:5432/&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;connection.user&amp;#34;: &amp;#34;postgres&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;connection.password&amp;#34;: &amp;#34;postgres&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;value.converter&amp;#34;: &amp;#34;org.apache.kafka.connect.json.JsonConverter&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;value.converter.schemas.enable&amp;#34;: &amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;tasks.max&amp;#34;: &amp;#34;1&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;topics&amp;#34;: &amp;#34;foo&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;auto.create&amp;#34;: &amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;auto.evolve&amp;#34;:&amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;      &amp;#34;pk.mode&amp;#34;:&amp;#34;none&amp;#34;          
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;    }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Send a message to the topic&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b localhost:9092 -t foo -P &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;{ &amp;#34;schema&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;struct&amp;#34;, &amp;#34;fields&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;int32&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;field&amp;#34;: &amp;#34;c1&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;field&amp;#34;: &amp;#34;c2&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;create_ts&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;update_ts&amp;#34; }], &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;foobar&amp;#34; }, &amp;#34;payload&amp;#34;: { &amp;#34;c1&amp;#34;: 10000, &amp;#34;c2&amp;#34;: &amp;#34;bar&amp;#34;, &amp;#34;create_ts&amp;#34;: 1501834166000, &amp;#34;update_ts&amp;#34;: 1501834166000 } }
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Confirm that the data&amp;#8217;s on the topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafkacat -b localhost:9092 -t foo -C -f &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;Topic: %t\nPartition: %p\nOffset: %o\nKey: %k\nPayload: %S bytes: %s\n--\n&amp;#39;&lt;/span&gt;

Topic: foo
Partition: &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
Offset: &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
Key:
Payload: &lt;span style=&#34;color:#666&#34;&gt;543&lt;/span&gt; bytes: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;schema&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;struct&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;fields&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;[{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int32&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}]&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foobar&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;payload&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
--&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check the connector status:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink    |  sink_postgres_foo_00         |  RUNNING  |  RUNNING  |  io.confluent.connect.jdbc.JdbcSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Confirm there&amp;#8217;s data in the target DB:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;postgres&lt;span style=&#34;color:#666&#34;&gt;=#&lt;/span&gt; &lt;span style=&#34;&#34;&gt;\&lt;/span&gt;dt
        List &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;of&lt;/span&gt; relations
 &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Schema&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; Name &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Type&lt;/span&gt;  &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Owner&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;--------+------+-------+----------
&lt;/span&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; foo &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;table&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; postgres
(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;row&lt;/span&gt;)

postgres&lt;span style=&#34;color:#666&#34;&gt;-#&lt;/span&gt; &lt;span style=&#34;&#34;&gt;\&lt;/span&gt;d foo
                            &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Table&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;public.foo&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Column&lt;/span&gt;   &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;            &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Type&lt;/span&gt;             &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Collation&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Nullable&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Default&lt;/span&gt;
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;-----------+-----------------------------+-----------+----------+---------
&lt;/span&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;&lt;/span&gt; update_ts &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;timestamp&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;without&lt;/span&gt; time &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;zone&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
 create_ts &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;timestamp&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;without&lt;/span&gt; time &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;zone&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
 c1        &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000&#34;&gt;integer&lt;/span&gt;                     &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;
 c2        &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000&#34;&gt;text&lt;/span&gt;                        &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;null&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;

postgres&lt;span style=&#34;color:#666&#34;&gt;=#&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;select&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;;
      update_ts      &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;      create_ts      &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;  c1   &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; c2
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;---------------------+---------------------+-------+-----
&lt;/span&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; bar
(&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;row&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_here_comes_the_problem&#34;&gt;Here comes the problem&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Let&amp;#8217;s send another message to the topic, but omit one of the fields (&lt;code&gt;c2&lt;/code&gt;):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b localhost:9092 -t foo -P &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;{ &amp;#34;schema&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;struct&amp;#34;, &amp;#34;fields&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;int32&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;field&amp;#34;: &amp;#34;c1&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;create_ts&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;update_ts&amp;#34; }], &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;foobar&amp;#34; }, &amp;#34;payload&amp;#34;: { &amp;#34;c1&amp;#34;: 10000,  &amp;#34;create_ts&amp;#34;: 1501834166000, &amp;#34;update_ts&amp;#34;: 1501834166000 } }
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If we look at the status of the connector we can see that the task has &lt;code&gt;FAILED&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink  |  sink_postgres_foo_00  |  RUNNING  |  FAILED  |  io.confluent.connect.jdbc.JdbcSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;And the Kafka Connect worker log shows a problem:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt;-10-15 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:30:34,412&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; ERROR &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;sink_postgres_foo_00|task-0&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; WorkerSinkTask&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;sink_postgres_foo_00-0&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; Task threw an uncaught and unrecoverable exception &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;org.apache.kafka.connect.runtime.WorkerTask:179&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
   at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:560&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerSinkTask.poll&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:321&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:224&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerSinkTask.execute&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:192&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerTask.doRun&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerTask.java:177&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerTask.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerTask.java:227&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at java.util.concurrent.Executors&lt;span style=&#34;color:#19177c&#34;&gt;$RunnableAdapter&lt;/span&gt;.call&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Executors.java:511&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at java.util.concurrent.FutureTask.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;FutureTask.java:266&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at java.util.concurrent.ThreadPoolExecutor.runWorker&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;ThreadPoolExecutor.java:1149&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at java.util.concurrent.ThreadPoolExecutor&lt;span style=&#34;color:#19177c&#34;&gt;$Worker&lt;/span&gt;.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;ThreadPoolExecutor.java:624&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at java.lang.Thread.run&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;Thread.java:748&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
Caused by: org.apache.kafka.connect.errors.ConnectException: java.sql.SQLException: java.sql.BatchUpdateException: Batch entry &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt; INSERT INTO &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; VALUES&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;2017-08-04 08:09:26+00&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;2017-08-04 08:09:26+00&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; was aborted: ERROR: null value in column &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; violates not-null constraint
  Detail: Failing row contains &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;, null&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.  Call getNextException to see other errors in the batch.
org.postgresql.util.PSQLException: ERROR: null value in column &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; violates not-null constraint
  Detail: Failing row contains &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;, null&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.
org.postgresql.util.PSQLException: ERROR: null value in column &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; violates not-null constraint
  Detail: Failing row contains &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;-08-04 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:09:26, &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;, null&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;.
   at io.confluent.connect.jdbc.sink.JdbcSinkTask.put&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;JdbcSinkTask.java:87&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;WorkerSinkTask.java:538&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
   ... &lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt; more
&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt;-10-15 &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:30:34,413&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; ERROR &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;sink_postgres_foo_00|task-0&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; WorkerSinkTask&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;id&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;sink_postgres_foo_00-0&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; Task is being killed and will not recover &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;until&lt;/span&gt; manually restarted &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;org.apache.kafka.connect.runtime.WorkerTask:180&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Distilling this down gives us:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Batch entry &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt; INSERT INTO &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; VALUES&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;2017-08-04 08:09:26+00&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;2017-08-04 08:09:26+00&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt; was aborted: 
ERROR: null value in column &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; violates not-null constraint&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Because we omitted field &lt;code&gt;c2&lt;/code&gt; from our payload, and it&amp;#8217;s &lt;code&gt;NOT NULL&lt;/code&gt; in the target schema, the message cannot be written, and the Connect sink task aborts.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What about if we send another, valid, message to the topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b localhost:9092 -t foo -P &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;{ &amp;#34;schema&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;struct&amp;#34;, &amp;#34;fields&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;int32&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;field&amp;#34;: &amp;#34;c1&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;field&amp;#34;: &amp;#34;c2&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;create_ts&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;int64&amp;#34;, &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;, &amp;#34;version&amp;#34;: 1, &amp;#34;field&amp;#34;: &amp;#34;update_ts&amp;#34; }], &amp;#34;optional&amp;#34;: false, &amp;#34;name&amp;#34;: &amp;#34;foobar&amp;#34; }, &amp;#34;payload&amp;#34;: { &amp;#34;c1&amp;#34;: 10001, &amp;#34;c2&amp;#34;: &amp;#34;bar2&amp;#34;, &amp;#34;create_ts&amp;#34;: 1501834166000, &amp;#34;update_ts&amp;#34;: 1501834166000 } }
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;EOF&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Restart the connector&amp;#8217;s failed task:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://localhost:8083/connectors/sink_postgres_foo_00/tasks/0/restart&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;It&amp;#8217;s up…&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink  |  sink_postgres_foo_00  |  RUNNING  |  RUNNING  |  io.confluent.connect.jdbc.JdbcSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;but soon…it&amp;#8217;s down&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink  |  sink_postgres_foo_00  |  RUNNING  |  FAILED  |  io.confluent.connect.jdbc.JdbcSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The worker log shows the same error as before - &lt;code&gt;ERROR: null value in column &#34;c2&#34; violates not-null constraint&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Of the three messages on the topic, we&amp;#8217;ve got a &#39;poison pill&#39; which has broken our pipeline 😿. Each time we restart the connector, it will start from where it got to last time and so fall over again—regardless of how many &#39;good&#39; messages may come after it. The connector will only consider a message actually read once it has successfully written it to the target, which makes sense if you think about it from a data integrity point of view—but does land us with this problem here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_what_to_do&#34;&gt;What to do?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There are a few options:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;If we were using Avro then it would be harder to break things, because schema compatibility can be enforced and bad messages would be rejected when being produced &lt;em&gt;on&lt;/em&gt; to the topic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We could write a stream processing job to take the source topic &lt;code&gt;foo&lt;/code&gt; and write all valid messages from it to a new topic (e.g. &lt;code&gt;foo_good&lt;/code&gt;) and hook our JDBC sink up to that instead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the consumer group mechanism to just skip the bad message for the connector&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Which you use depends on how the problem arose. For example, one-off problems could be addressed by option #3, but it&amp;#8217;s very manual and could be error-prone if you&amp;#8217;re not careful. Option #2 is appropriate if you&amp;#8217;re dealing with third-parties and you have on-going data quality issues. #1, using Avro, is &lt;strong&gt;always&lt;/strong&gt; a good idea, regardless!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_manually_skipping_bad_messages&#34;&gt;Manually skipping bad messages&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Each sink connector in Kafka Connect has its own consumer group, with the offset persisted in Kafka itself (pretty clever, right). This is also why if you delete a connector and recreate it &lt;em&gt;with the same name&lt;/em&gt; you&amp;#8217;ll find it starts from where the previous instance got to.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can view consumer groups using the &lt;code&gt;kafka-consumer-groups&lt;/code&gt; command:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafka-consumer-groups &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --bootstrap-server kafka:29092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --list
connect-sink_postgres_00
_confluent-ksql-confluent_rmoff_01query_CSAS_JDBC_POSTGRES_TRANSACTIONS_GBP_2
_confluent-ksql-confluent_rmoff_01query_CSAS_JDBC_POSTGRES_TRANSACTIONS_NO_CUSTOMERID_1
connect-sink_postgres_foo_00
connect-SINK_ES_04
_confluent-ksql-confluent_rmoff_01transient_2925897355317205962_1571058964212
_confluent-controlcenter-5-4-0-1
connect-SINK_ES_03
_confluent-controlcenter-5-4-0-1-command
connect-SINK_ES_02
connect-SINK_ES_01&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There are various ones there, but we&amp;#8217;re interested in the one with a &lt;code&gt;connect-&lt;/code&gt; prefix that matches our connector name, &lt;code&gt;connect-sink_postgres_foo_00&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafka-consumer-groups &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --bootstrap-server kafka:29092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --describe &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --group connect-sink_postgres_foo_00

Consumer group &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;connect-sink_postgres_foo_00&amp;#39;&lt;/span&gt; has no active members.

GROUP                        TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
connect-sink_postgres_foo_00 foo             &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;          &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;               &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;               &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;               -               -               -&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can see from this that the current offset is 1, and there are two more messages to be read (one of which is the &#39;poison-pill&#39;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;code&gt;kafkacat&lt;/code&gt; is a fantastic tool for this kind of debugging, because we can directly relate offsets with the messages themselves:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafkacat -b localhost:9092 -t foo -C -f &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;Offset: %o\nPayload: %s\n--\n&amp;#39;&lt;/span&gt;
Offset: &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
Payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;schema&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;struct&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;fields&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;[{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int32&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}]&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foobar&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;payload&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
--
Offset: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;
Payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;schema&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;struct&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;fields&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;[{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int32&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}]&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foobar&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;payload&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt;,  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
--
Offset: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;
Payload: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;schema&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;struct&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;fields&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;[{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int32&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;int64&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.data.Timestamp&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;field&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}]&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;optional&amp;#34;&lt;/span&gt;: false, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foobar&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;payload&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c1&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;10001&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;c2&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bar2&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;create_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;update_ts&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1501834166000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
--
% Reached end of topic foo &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So at offset 0 is the good message which Connect read, thus the current offset is 1. When the connector restarts from its failure it will be at offset 1, which is the &#39;bad&#39; message. The end of the topic currently is offset 3, i.e. the position after the third message which is at offset 2 (zero-based offsets).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What we want to do is tell Kafka Connect to resume from the next-good message, which we can see from &lt;code&gt;kafkacat&lt;/code&gt; above is at offset 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafka-consumer-groups &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --bootstrap-server kafka:29092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --group connect-sink_postgres_foo_00 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --reset-offsets &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --topic foo &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --to-offset &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    --execute&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;GROUP                          TOPIC                          PARTITION  NEW-OFFSET
connect-sink_postgres_foo_00   foo                            &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;          &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now we can restart the failed task:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://localhost:8083/connectors/sink_postgres_foo_00/tasks/0/restart&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and this time the connector stays running:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl -s &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:8083/connectors?expand=info&amp;amp;expand=status&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             jq &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;. | to_entries[] | [ .value.info.type, .key, .value.status.connector.state,.value.status.tasks[].state,.value.info.config.&amp;#34;connector.class&amp;#34;]|join(&amp;#34;:|:&amp;#34;)&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;             column -s : -t| sed &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;s/\&amp;#34;//g&amp;#39;&lt;/span&gt;| sort
sink  |  sink_postgres_foo_00  |  RUNNING  |  RUNNING  |  io.confluent.connect.jdbc.JdbcSinkConnector&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and in Postgres we get the new rows of data (except for the bad one, which is lost to us):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;postgres&lt;span style=&#34;color:#666&#34;&gt;=#&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;select&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;;
      update_ts      &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;      create_ts      &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;  c1   &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt;  c2
&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;---------------------+---------------------+-------+------
&lt;/span&gt;&lt;span style=&#34;color:#408080;font-style:italic&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10000&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; bar
 &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;2017&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;04&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;08&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;09&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;26&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;10001&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;|&lt;/span&gt; bar2
(&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;rows&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Kafka Connect and Elasticsearch</title>
      <link>https://rmoff.github.io/2019/10/07/kafka-connect-and-elasticsearch/</link>
      <pubDate>2019-10-07</pubDate>
      
      <guid>https://rmoff.github.io/2019/10/07/kafka-connect-and-elasticsearch/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/10/IMG_0143.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I use the Elastic stack for a lot of my &lt;a href=&#34;https://talks.rmoff.net/&#34;&gt;talks&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/demo-scene/&#34;&gt;demos&lt;/a&gt; because it complements Kafka brilliantly. A few things have changed in recent releases and this blog is a quick note on some of the errors that you might hit and how to resolve them. It was inspired by a lot of the comments and discussion &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/314&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/342&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_tldr_how_do_i_stream_data_from_kafka_to_elasticsearch&#34;&gt;tl;dr How do I stream data from Kafka to Elasticsearch?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Use Kafka Connect! Not sure what Kafka Connect is or why you should use it instead of something like Logstash? Check out &lt;a href=&#34;http://rmoff.dev/ksldn19-kafka-connect&#34;&gt;the talk I did at Kafka Summit in London&lt;/a&gt; earlier this year.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect&amp;#8217;s Elasticsearch sink connector has been improved in 5.3.1 to fully support Elasticsearch 7.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To stream data from a Kafka topic to Elasticsearch create a connector using the Kafka Connect REST API. The parameters vary slightly between releases of Elasticsearch.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Some notes in general:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This assumes your data is serialised based on the defaults specified in your Kafka Connect workers (e.g. Avro). If it&amp;#8217;s not then you will need to add overrides—see &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained&#34;&gt;this article&lt;/a&gt; for a detailed explanation of why and how.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you&amp;#8217;re streaming data to Elasticsearch from KSQL you will need to set the Key converter to STRING since this is currently (October 2019 / 5.4.0-beta1) all that is supported for keys:&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;key.converter&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;org.apache.kafka.connect.storage.StringConverter&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The connector will automagically change upper-case topic names to lower-case index names in Elasticsearch; unlike in previous versions you don&amp;#8217;t need to manually map this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can use a regex to match multiple topics; just specify &lt;code&gt;topics.regex&lt;/code&gt; in place of &lt;code&gt;topics&lt;/code&gt; configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;For the full reference guide to the Kafka Connect Elasticsearch connector, including all its capabilities (including exactly-once) and configuration options &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/index.html&#34;&gt;see here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_to_schema_or_not_to_schema&#34;&gt;To schema or not to schema?&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;schema.ignore=true&lt;/code&gt;: You want to just chuck a JSON document at Elasticsearch and have it figure out the field mapping types automagically (which it does pretty well, or you can force using dynamic mapping templates). Also applicable if you &lt;em&gt;don&amp;#8217;t have an explicit schema in your data&lt;/em&gt; such as schema-less JSON (most JSON is schema-less) or CSV etc. If you are using Avro, you have a schema 🙌.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;schema.ignore=false&lt;/code&gt;: Your data has a schema (Avro, or JSON with embedded schema) and you want Kafka Connect to create the mapping explicitly in Elasticsearch when it pushes the data over&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Still not sure? Just wanna get data into Elasticsearch without really getting into the weeds of detail? Start off with &lt;code&gt;schema.ignore=true&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_to_elasticsearch_7&#34;&gt;Kafka to Elasticsearch 7&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This works with Kafka Connect Elasticsearch sink connector &amp;gt;=5.3.0&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -s -i -X PUT -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    http://localhost:8083/connectors/sink-elastic-01/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;connector.class&amp;#34;: &amp;#34;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;connection.url&amp;#34;: &amp;#34;http://elasticsearch7:9200&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;type.name&amp;#34;: &amp;#34;_doc&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;topics&amp;#34;: &amp;#34;sample_topic&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;key.ignore&amp;#34;: &amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;schema.ignore&amp;#34;: &amp;#34;true&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
&lt;strong&gt;The &lt;code&gt;type.name&lt;/code&gt; is &lt;code&gt;_doc&lt;/code&gt; - other values may cause problems in some configuration permutations. You can also leave it blank in some situations&lt;/strong&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_to_elasticsearch_6_and_earlier&#34;&gt;Kafka to Elasticsearch 6 and earlier&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The only difference from above is that you can specify any type name. Unless you&amp;#8217;re using a specific type in your target index by design then you can use any value here; but you can&amp;#8217;t leave it blank.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -s -i -X PUT -H  &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Content-Type:application/json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    http://localhost:8083/connectors/sink-elastic-01/config &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -d &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;connector.class&amp;#34;: &amp;#34;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;connection.url&amp;#34;: &amp;#34;http://elasticsearch6:9200&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;type.name&amp;#34;: &amp;#34;foobarwibble&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;topics&amp;#34;: &amp;#34;sample_topic&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;key.ignore&amp;#34;: &amp;#34;true&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;schema.ignore&amp;#34;: &amp;#34;true&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_templates_in_elasticsearch_7&#34;&gt;Templates in Elasticsearch 7&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Sometime you&amp;#8217;ll want to use templates with Elasticsearch for things such as defining the field types to be used in the document mapping. This has changed a bit in recent versions and caught me out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you copy and paste template definitions that you&amp;#8217;ve found lying around on t&amp;#8217;internet such as this one:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -XPUT &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:9200/_template/kafkaconnect/&amp;#34;&lt;/span&gt; -H &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt; -d&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        {
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;template&amp;#34;: &amp;#34;*&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 1, &amp;#34;number_of_replicas&amp;#34;: 0 }, 
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          &amp;#34;mappings&amp;#34;: { &amp;#34;_default_&amp;#34;: { &amp;#34;dynamic_templates&amp;#34;: [ { &amp;#34;dates&amp;#34;: { &amp;#34;match&amp;#34;: &amp;#34;*_TS&amp;#34;, &amp;#34;mapping&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; } } } ] } }
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;        }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You&amp;#8217;ll now get this error, which &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html&#34;&gt;is deliberate&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;error&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;root_cause&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;
            &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
                &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mapper_parsing_exception&amp;#34;&lt;/span&gt;,
                &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Root mapping definition has unsupported parameters:  [_default_ : {dynamic_templates=[{dates={mapping={type=date}, match=*_TS}}]}]&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
        &lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;,
        &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mapper_parsing_exception&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Failed to parse mapping [_doc]: Root mapping definition has unsupported parameters:  [_default_ : {dynamic_templates=[{dates={mapping={type=date}, match=*_TS}}]}]&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;caused_by&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;
            &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;mapper_parsing_exception&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Root mapping definition has unsupported parameters:  [_default_ : {dynamic_templates=[{dates={mapping={type=date}, match=*_TS}}]}]&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
    &lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;400&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To get this to work just remove the type name (&lt;code&gt;_default_&lt;/code&gt;) from the &lt;code&gt;mappings&lt;/code&gt; element entirely:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -XPUT &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;http://localhost:9200/_template/kafkaconnect/&amp;#34;&lt;/span&gt; -H &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt; -d&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          {
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;template&amp;#34;: &amp;#34;*&amp;#34;,
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;settings&amp;#34;: { &amp;#34;number_of_shards&amp;#34;: 1, &amp;#34;number_of_replicas&amp;#34;: 0 },
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;            &amp;#34;mappings&amp;#34;: { &amp;#34;dynamic_templates&amp;#34;: [ { &amp;#34;dates&amp;#34;: { &amp;#34;match&amp;#34;: &amp;#34;*_TS&amp;#34;, &amp;#34;mapping&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;date&amp;#34; } } } ]  }
&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;          }&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;HOWEVER this only works for Elasticsearch 7; on Elasticsearch 6 and earlier you will get &lt;code&gt;Malformed [mappings] section for type [dynamic_templates], should include an inner object describing the mapping&#34;}]&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;If you&amp;#8217;re using a template with Elasticsearch 7 then you &lt;strong&gt;must&lt;/strong&gt; specify &lt;code&gt;&#34;type.name&#34;: &#34;_doc&#34;&lt;/code&gt; in your &lt;em&gt;connector&lt;/em&gt; configuration. A blank or other value will cause the connector to fail.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_dealing_with_errors&#34;&gt;Dealing with errors&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Both the Elasticsearch sink connector, and Kafka Connect itself, have error handling support. See &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;this article&lt;/a&gt; for details of how Kafka Connect does it. By default the connector will abort as soon as it hits a problem, but you may not want this—to enable it in your connector you can set:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;errors.tolerance&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;all&amp;#34;&lt;/span&gt;,
&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;errors.log.enable&amp;#34;&lt;/span&gt;:true,
&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;errors.log.include.messages&amp;#34;&lt;/span&gt;:true,
&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;behavior.on.malformed.documents&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;warn&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is the most permissive configuration; &lt;code&gt;behavior.on.malformed.documents&lt;/code&gt; is a connector property which when set to &lt;code&gt;warn&lt;/code&gt; (or &lt;code&gt;ignore&lt;/code&gt;) will make the connector continue rather than abort, which is it&amp;#8217;s default setting. The remaining configuration items are for Kafka Connect itself and will deal with errors in the deserialisation process and any Single Message Transforms that have been configured.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_common_errors&#34;&gt;Common errors&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_field_is_a_metadata_field&#34;&gt;Field … is a metadata field&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.ConnectException: Bulk request failed: [{&#34;type&#34;:&#34;mapper_parsing_exception&#34;,&#34;reason&#34;:&#34;Field [_type] is a metadata field and cannot be added inside a document. Use the index API request parameters.&#34;}]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: You&amp;#8217;ve got a field called &lt;code&gt;_type&lt;/code&gt; in your Kafka message that you&amp;#8217;re sending to Elasticsearch&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Drop or rename the field e.g. with Single Message Transform or at source&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_rejecting_mapping_update_as_the_final_mapping_would_have_more_than_1_type&#34;&gt;Rejecting mapping update […] as the final mapping would have more than 1 type&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;WARN Encountered an illegal document error when executing batch 4 of 1 records. Ignoring and will not index record. Error was [{&#34;type&#34;:&#34;illegal_argument_exception&#34;,&#34;reason&#34;:&#34;Rejecting mapping update to [sample_topic] as the final mapping would have more than 1 type: [_doc, foo]&#34;}] (io.confluent.connect.elasticsearch.bulk.BulkProcessor)`&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause 1: Elasticsearch index already exists with a different type in the mapping&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause 2: Template with dynamic mapping exists and &lt;code&gt;type.name&lt;/code&gt; has been specified&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Unset &lt;code&gt;type.name&lt;/code&gt; (i.e. &lt;code&gt;`&#34;type.name&#34;: &#34;&#34;&lt;/code&gt;), or use the type that already exists (in the above example it&amp;#8217;s &lt;code&gt;_doc&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_validation_failed_type_is_missing&#34;&gt;Validation Failed […] type is missing&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.ConnectException: Bulk request failed: {&#34;root_cause&#34;:[{&#34;type&#34;:&#34;action_request_validation_exception&#34;,&#34;reason&#34;:&#34;Validation Failed: 1: type is missing;2: type is missing;3: type is missing;4: type is missing;5: type is missing;&#34;}],&#34;type&#34;:&#34;action_request_validation_exception&#34;,&#34;reason&#34;:&#34;Validation Failed: 1: type is missing;2: type is missing;3: type is missing;4: type is missing;5: type is missing;&#34;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause 1: Using a blank &lt;code&gt;type.name&lt;/code&gt; in the Kafka Connect connector configuration when indexing against Elasticsearch 7 with &lt;code&gt;schemas.ignore=false&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause 2: Using a blank &lt;code&gt;type.name&lt;/code&gt; in the Kafka Connect connector configuration when indexing against Elasticsearch versions prior to 7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Specify a non-blank &lt;code&gt;type.name&lt;/code&gt; in the Kafka Connect connector configuration&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_task_is_being_killed_and_will_not_recover_until_manually_restarted&#34;&gt;Task is being killed and will not recover until manually restarted&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Task threw an uncaught and unrecoverable exception
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
Task is being killed and will not recover until manually restarted&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: This is the Kafka Connect framework logging that a connector has failed&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Inspect the Kafka Connect worker log more closely to find the actual error logged by the connector task&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_java_io_charconversionexception_invalid_utf_32_character&#34;&gt;java.io.CharConversionException: Invalid UTF-32 character&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:
org.apache.kafka.common.errors.SerializationException: java.io.CharConversionException: Invalid UTF-32 character 0x1010443 (above 0x0010ffff) at char #
1, byte #7)
java.io.CharConversionException: Invalid UTF-32 character 0x1010443 (above 0x0010ffff) at char #1, byte #7)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: Using the JSON converter (&lt;code&gt;org.apache.kafka.connect.json.JsonConverter&lt;/code&gt;) to read Avro data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Use the Avro converter (&lt;code&gt;io.confluent.connect.avro.AvroConverter&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
Kafka Connect has &lt;strong&gt;two&lt;/strong&gt; deserialisers: the &lt;strong&gt;key&lt;/strong&gt; and the &lt;strong&gt;value&lt;/strong&gt;. It is not uncommon to have different serialisation formats used for each. For example, data from KSQL may have a String key and an Avro key.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_error_deserializing_avro_message_for_id_1_unknown_magic_byte&#34;&gt;Error deserializing Avro message for id -1 Unknown magic byte!&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.DataException: Failed to deserialize data for topic sample_topic to Avro:
org.apache.kafka.common.errors.SerializationException: Error deserializing Avro message for id -1
org.apache.kafka.common.errors.SerializationException: Unknown magic byte!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: Using the Avro converter (&lt;code&gt;io.confluent.connect.avro.AvroConverter&lt;/code&gt;) to read JSON data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Use the JSON converter (&lt;code&gt;org.apache.kafka.connect.json.JsonConverter&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
Kafka Connect has &lt;strong&gt;two&lt;/strong&gt; deserialisers: the &lt;strong&gt;key&lt;/strong&gt; and the &lt;strong&gt;value&lt;/strong&gt;. It is not uncommon to have different serialisation formats used for each. For example, data from KSQL may have a String key and an Avro key.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_cannot_infer_mapping_without_schema&#34;&gt;Cannot infer mapping without schema&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.DataException: Cannot infer mapping without schema.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: If you have set &lt;code&gt;schemas.ignore=false&lt;/code&gt; then the connector will create the mapping in the target index for you, based on the schema of your data. &lt;em&gt;BUT&lt;/em&gt;, for it to obtain the schema, there has to be a schema! Which means either using Avro, or using &lt;a href=&#34;https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/&#34;&gt;JSON with the schema-embedded&lt;/a&gt; and the connector&amp;#8217;s converter configured to expect it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Use Avro! It will save you tears and time and money. If you can&amp;#8217;t change how you produce the data, consider using KSQL to reserialise the topic into Avro. Or, write JSON in the &lt;a href=&#34;https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/&#34;&gt;required structure&lt;/a&gt; and set &lt;code&gt;value.converter.schemas.enable=true&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_jsonconverter_with_schemas_enable_requires_schema_and_payload_fields&#34;&gt;JsonConverter with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: You&amp;#8217;ve set &lt;code&gt;schemas.enable=true&lt;/code&gt; for your converter, but the JSON is not in the correct structure. See &lt;a href=&#34;https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/&#34;&gt;here for details&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: Depending on what you&amp;#8217;re trying to do either (a) use Avro, (b) produce your JSON with the schema/payload in the correct structure (c) set &lt;code&gt;value.converter.schemas.enable=false&lt;/code&gt; (if you don&amp;#8217;t care about the schema and want to set &lt;code&gt;schema.ignore=true&lt;/code&gt; for the Elasticsearch connector).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
&lt;code&gt;schemas.enable&lt;/code&gt; is a &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained&#34;&gt;converter&lt;/a&gt; configuration, so can be set for both &lt;code&gt;value.converter&lt;/code&gt; and &lt;code&gt;key.converter&lt;/code&gt;, and you can hit this error against both fields.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compressor_detection_can_only_be_called_on_some_xcontent_bytes&#34;&gt;Compressor detection can only be called on some xcontent bytes&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Bulk request failed: [{&#34;type&#34;:&#34;mapper_parsing_exception&#34;,&#34;reason&#34;:&#34;failed to parse&#34;,&#34;caused_by&#34;:{&#34;type&#34;:&#34;not_x_content_exception&#34;,&#34;reason&#34;:&#34;Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes&#34;}}] (io.confluent.connect.elasticsearch.bulk.BulkProcessor:393)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: This can come about if you try to read JSON data from a topic using the String converter (&lt;code&gt;org.apache.kafka.connect.storage.StringConverter&lt;/code&gt;) and have &lt;strong&gt;&lt;code&gt;&#34;schema.ignore&#34;: &#34;true&#34;&lt;/code&gt;&lt;/strong&gt;, because you end up with a single field of data. This in turn causes Elasticsearch to throw this error when Kafka Connect tries to index the data into it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: If it&amp;#8217;s JSON data in the topic, use the &lt;code&gt;org.apache.kafka.connect.json.JsonConverter&lt;/code&gt;, i.e.&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&#34;value.converter&#34;:&#34;org.apache.kafka.connect.json.JsonConverter&#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_root_mapping_definition_has_unsupported_parameters_type_text&#34;&gt;Root mapping definition has unsupported parameters:  [type : text]&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Error:&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;org.apache.kafka.connect.errors.ConnectException: Cannot create mapping {&#34;_doc&#34;:{&#34;type&#34;:&#34;text&#34;,&#34;fields&#34;:{&#34;keyword&#34;:{&#34;type&#34;:&#34;keyword&#34;,&#34;ignore_above&#34;:256}}}} -- {&#34;root_cause&#34;:[{&#34;type&#34;:&#34;mapper_parsing_exception&#34;,&#34;reason&#34;:&#34;Root mapping definition has unsupported parameters:  [type : text] [fields : {keyword={ignore_above=256, type=keyword}}]&#34;}],&#34;type&#34;:&#34;mapper_parsing_exception&#34;,&#34;reason&#34;:&#34;Root mapping definition has unsupported parameters:  [type : text] [fields : {keyword={ignore_above=256, type=keyword}}]&#34;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cause: This is an error from Elasticsearch and could be from various reasons. One is if you try to read JSON data from a topic using the String converter (&lt;code&gt;org.apache.kafka.connect.storage.StringConverter&lt;/code&gt;) and have &lt;strong&gt;&lt;code&gt;&#34;schema.ignore&#34;: &#34;false&#34;&lt;/code&gt;&lt;/strong&gt;, because you end up with a single field of data. This in turn causes Elasticsearch to throw this error when Kafka Connect tries to index the data into it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solution: If it&amp;#8217;s JSON data in the topic, use the &lt;code&gt;org.apache.kafka.connect.json.JsonConverter&lt;/code&gt;, i.e.&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&#34;value.converter&#34;:&#34;org.apache.kafka.connect.json.JsonConverter&#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_want_to_try_it_out_yourself&#34;&gt;Want to try it out yourself?&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can find my test rig &lt;a href=&#34;https://github.com/rmoff/kafka-elasticsearch&#34;&gt;on github here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Copying data between Kafka clusters with Kafkacat</title>
      <link>https://rmoff.github.io/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</link>
      <pubDate>2019-09-29</pubDate>
      
      <guid>https://rmoff.github.io/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</guid>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://rmoff.github.io/images/2019/09/IMG_2657.jpg" medium="image" type="image/jpg" width="100" height="100" />
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafkacat_gives_you_kafka_super_powers&#34;&gt;kafkacat gives you Kafka super powers 😎&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;ve &lt;a href=&#34;https://rmoff.net/categories/kafkacat/&#34;&gt;written before&lt;/a&gt; about &lt;a href=&#34;https://github.com/edenhill/kafkacat&#34;&gt;kafkacat&lt;/a&gt; and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in &lt;a href=&#34;https://talks.rmoff.net/8Oruwt/on-track-with-apache-kafka-building-a-streaming-etl-solution-with-rail-data#s9tMEWG&#34;&gt;a recent demo&lt;/a&gt; that I built in which data needed manipulating in a way that I couldn&amp;#8217;t easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it&amp;#8217;s getting data from &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; down to a local cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Why would I want to copy data from one Kafka cluster to another? In this instance it was because I had set up a pipeline streaming data into one cluster (on Confluent Cloud) and I needed that data locally so that I could work on building code around it whilst offline. Offline happens less and less often nowadays but it still happens—on many flights, and often at conferences/hotels with &#34;WiFi&#34; that isn&amp;#8217;t.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There are several proper ways to replicate data from one Kafka cluster to another, including MirrorMaker (part of Apache Kafka) and &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-replicator/index.html&#34;&gt;Replicator&lt;/a&gt; (part of Confluent Platform). If what I needed was a proper solution then obviously I&amp;#8217;d reach for Replicator—but here I just needed quick &amp;amp; dirty, didn&amp;#8217;t care about replicating consumer offsets etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Pipeline_(Unix)&#34;&gt;Unix pipelines&lt;/a&gt; are a beautiful thing, because they enable you to build fantastically powerful processing out of individual components that each focus on doing their own thing particularly well. kafkacat fully supports the pipelines concept, which means that you can stream data out of a Kafka topic (using kafkacat as a consumer) into any tool that accepts &lt;code&gt;stdin&lt;/code&gt;, and you can also take data from any tool that produces &lt;code&gt;stdout&lt;/code&gt; and write it to a Kafka topic (using kafkacat as a producer).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Guess what happens when you hook up two kafkacat instances, one consuming and one producing? A delightful thing happens; you read from one topic and write to another!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/09/kafkacat-copy-between-clusters.png&#34; alt=&#34;kafkacat copy between clusters&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
This is a hack—I have not tried it (and suspect it wouldn&amp;#8217;t work) with binary data; that&amp;#8217;s exactly what &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-replicator/index.html&#34;&gt;Replicator&lt;/a&gt; and its support for Byte arrays is for :)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;At a very simply level you can do this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b localhost:9092 -C -t source-topic -K: -e -o beginning | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;kafkacat -b localhost:9092 -P -t target-topic -K: &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;That will take every message on &lt;code&gt;source-topic&lt;/code&gt; and write it to &lt;code&gt;target-topic&lt;/code&gt; on the same cluster. Some flags:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-K&lt;/code&gt; output/parse keys separated by &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-e&lt;/code&gt; exit once at the end of the topic&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The above will run once, and you get a copy of that topic at the point in time at which the script runs. If you re-run it you&amp;#8217;ll get another full copy of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What about if you want to resume consumption after each execution, or even scale it out? kafkacat supports consumer groups!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b localhost:9092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X auto.offset.reset&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;earliest &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -K: &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -G cg01 source-topic | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;kafkacat -b localhost:9092 -t target-topic -K: -P&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The arguments are a bit different this time. We replace &lt;code&gt;-C&lt;/code&gt; (consumer) and specifying the topic with &lt;code&gt;-t&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;-C -t source-topic&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;with &lt;code&gt;-G&lt;/code&gt; to specify using the balanced consumer, its name, and which topic(s) to subscribe to&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;-G cg01 source-topic&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We also change &lt;code&gt;-o beginning&lt;/code&gt; for &lt;code&gt;-X auto.offset.reset=earliest&lt;/code&gt;. When this is run we see the partitions of the six-partition topic that have been assigned to it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;% Group cg01 rebalanced &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;memberid rdkafka-894c0f84-9464-42dd-b76a-885420b6c557&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;: assigned: source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;11&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;
% Reached end of topic source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and if we run a second instance of it with the same consumer group id both rebalance:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Instance 1&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;% Group cg01 rebalanced &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;memberid rdkafka-894c0f84-9464-42dd-b76a-885420b6c557&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;: assigned: source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instance 2&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;% Group cg01 rebalanced &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;memberid rdkafka-2416395f-7232-4fc0-8fbc-121d3bbc3758&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;: assigned: source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;, source-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_copying_data_from_confluent_cloud_to_a_local_kafka_cluster&#34;&gt;Copying data from Confluent Cloud to a local Kafka cluster&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now let&amp;#8217;s add Confluent Cloud into the mix—or any other secured Kafka cluster for that matter. We just need a few extra parameters in our call :&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kafkacat -b &lt;span style=&#34;color:#19177c&#34;&gt;$CCLOUD_BROKER_HOST&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X security.protocol&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;SASL_SSL -X sasl.mechanisms&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;PLAIN &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X sasl.username&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;$CCLOUD_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; -X sasl.password&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#19177c&#34;&gt;$CCLOUD_API_SECRET&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X ssl.ca.location&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;/usr/local/etc/openssl/cert.pem -X api.version.request&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#008000&#34;&gt;true&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -X auto.offset.reset&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;earliest &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -K: &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -G copy_to_local_00 source-topic  | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;kafkacat -b localhost:9092,localhost:19092,localhost:29092 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -t target-topic &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    -K: -P &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;And off we go. How can we check it&amp;#8217;s worked? kafkacat of course! We can sample some records:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafkacat -b localhost:9092 -t source-topic -o beginning -e -C -c &lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;batt&amp;#34;&lt;/span&gt;:97,,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;acc&amp;#34;&lt;/span&gt;:200,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;:98.689468383789062,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bs&amp;#34;&lt;/span&gt;:1,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vel&amp;#34;&lt;/span&gt;:0,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vac&amp;#34;&lt;/span&gt;:93,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;t&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;u&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conn&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tst&amp;#34;&lt;/span&gt;:1569316069,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;alt&amp;#34;&lt;/span&gt;:97,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;location&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;FF&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;cog&amp;#34;&lt;/span&gt;:193,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;batt&amp;#34;&lt;/span&gt;:45,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;acc&amp;#34;&lt;/span&gt;:16,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;:100.14543914794922,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bs&amp;#34;&lt;/span&gt;:1,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vel&amp;#34;&lt;/span&gt;:0,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vac&amp;#34;&lt;/span&gt;:3,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conn&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tst&amp;#34;&lt;/span&gt;:1569330854,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RM&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;location&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;alt&amp;#34;&lt;/span&gt;:104&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;batt&amp;#34;&lt;/span&gt;:97,,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;acc&amp;#34;&lt;/span&gt;:200,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;:98.689468383789062,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bs&amp;#34;&lt;/span&gt;:1,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vel&amp;#34;&lt;/span&gt;:0,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vac&amp;#34;&lt;/span&gt;:93,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;t&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;u&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conn&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tst&amp;#34;&lt;/span&gt;:1569316069,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;alt&amp;#34;&lt;/span&gt;:97,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;location&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;FF&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;cog&amp;#34;&lt;/span&gt;:193,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;batt&amp;#34;&lt;/span&gt;:45,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;acc&amp;#34;&lt;/span&gt;:16,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;:100.14543914794922,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bs&amp;#34;&lt;/span&gt;:1,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vel&amp;#34;&lt;/span&gt;:0,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vac&amp;#34;&lt;/span&gt;:3,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conn&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tst&amp;#34;&lt;/span&gt;:1569330854,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;RM&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;location&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;alt&amp;#34;&lt;/span&gt;:104&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;batt&amp;#34;&lt;/span&gt;:97,,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;acc&amp;#34;&lt;/span&gt;:200,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;:98.689468383789062,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;bs&amp;#34;&lt;/span&gt;:1,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vel&amp;#34;&lt;/span&gt;:0,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;vac&amp;#34;&lt;/span&gt;:93,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;t&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;u&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;conn&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tst&amp;#34;&lt;/span&gt;:1569316069,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;alt&amp;#34;&lt;/span&gt;:97,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;_type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;location&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;tid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;FF&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;and we can count how many are currently in the topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ kafkacat -b localhost:9092 -t target-topic -o beginning -e|wc -l
% Auto-selecting Consumer mode &lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;use -P or -C to override&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;
% Reached end of topic target-topic &lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; at offset &lt;span style=&#34;color:#666&#34;&gt;1668&lt;/span&gt;: exiting
    &lt;span style=&#34;color:#666&#34;&gt;1668&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We can see the consumer group in Confluent Cloud (with similar views in Confluent Control Center if you&amp;#8217;re running Apache Kafka for yourself), showing any lag in each consumer:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.github.io/images/2019/09/kafka–consumer-lag.png&#34; alt=&#34;kafka–consumer lag&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Like I said above, there are dozens of reasons why you would &lt;strong&gt;not&lt;/strong&gt; want to use this method for getting data from one Kafka cluster to another, including:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;No copying of offsets from source to target cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probably mangles binary data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn&amp;#8217;t copy topic properties (partition count, replication factor) from source to target&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Doesn&amp;#8217;t copy headers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hacky as hell!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>