<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>rmoff.net  | Streaming geopoint data from Kafka to Elasticsearch</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.49" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.e08a958ae3e530145318b6373195c765.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Streaming geopoint data from Kafka to Elasticsearch" />
<meta property="og:description" content="Using the Elasticsearch Kafka Connect connector to stream events from a Kafka topic to Elasticsearch.
curl -X &quot;POST&quot; &quot;http://kafka-connect:8083/connectors/&quot; \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{ &quot;name&quot;: &quot;es_sink_ATM_POSSIBLE_FRAUD&quot;, &quot;config&quot;: { &quot;topics&quot;: &quot;ATM_POSSIBLE_FRAUD&quot;, &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;, &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, &quot;value.converter.schemas.enable&quot;: false, &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;, &quot;key.ignore&quot;: &quot;true&quot;, &quot;schema.ignore&quot;: &quot;true&quot;, &quot;type.name&quot;: &quot;type.name=kafkaconnect&quot;, &quot;topic.index.map&quot;: &quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&quot;, &quot;connection.url&quot;: &quot;http://elasticsearch:9200&quot; } }&#39;  Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:
curl -XPUT &quot;http://elasticsearch:9200/_template/kafkaconnect/&quot; -H &#39;Content-Type: application/json&#39; -d&#39; { &quot;index_patterns&quot;: &quot;*&quot;, &quot;settings&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 }, &quot;mappings&quot;: { &quot;_default_&quot;: { &quot;dynamic_templates&quot;: [ { &quot;dates&quot;: { &quot;match&quot;: &quot;*TIMESTAMP&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;date&quot; } } }, { &quot;geopoint&quot;: { &quot;match&quot;: &quot;*LOCATION&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;geopoint2&quot;: { &quot;match&quot;: &quot;location&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;non_analysed_string_template&quot;: { &quot;match&quot;: &quot;account_id, atm, transaction_id&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } ] } } }&#39;  Sample JSON message from Kafka: (pretty-printed)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rmoff.github.io/posts/elastic-kafka-geopoint/" /><meta property="article:published_time" content="2018-10-05T17:29:42&#43;01:00"/>
<meta property="article:modified_time" content="2018-10-05T17:29:42&#43;01:00"/>

<meta itemprop="name" content="Streaming geopoint data from Kafka to Elasticsearch">
<meta itemprop="description" content="Using the Elasticsearch Kafka Connect connector to stream events from a Kafka topic to Elasticsearch.
curl -X &quot;POST&quot; &quot;http://kafka-connect:8083/connectors/&quot; \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{ &quot;name&quot;: &quot;es_sink_ATM_POSSIBLE_FRAUD&quot;, &quot;config&quot;: { &quot;topics&quot;: &quot;ATM_POSSIBLE_FRAUD&quot;, &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;, &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, &quot;value.converter.schemas.enable&quot;: false, &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;, &quot;key.ignore&quot;: &quot;true&quot;, &quot;schema.ignore&quot;: &quot;true&quot;, &quot;type.name&quot;: &quot;type.name=kafkaconnect&quot;, &quot;topic.index.map&quot;: &quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&quot;, &quot;connection.url&quot;: &quot;http://elasticsearch:9200&quot; } }&#39;  Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:
curl -XPUT &quot;http://elasticsearch:9200/_template/kafkaconnect/&quot; -H &#39;Content-Type: application/json&#39; -d&#39; { &quot;index_patterns&quot;: &quot;*&quot;, &quot;settings&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 }, &quot;mappings&quot;: { &quot;_default_&quot;: { &quot;dynamic_templates&quot;: [ { &quot;dates&quot;: { &quot;match&quot;: &quot;*TIMESTAMP&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;date&quot; } } }, { &quot;geopoint&quot;: { &quot;match&quot;: &quot;*LOCATION&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;geopoint2&quot;: { &quot;match&quot;: &quot;location&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;non_analysed_string_template&quot;: { &quot;match&quot;: &quot;account_id, atm, transaction_id&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } ] } } }&#39;  Sample JSON message from Kafka: (pretty-printed)">


<meta itemprop="datePublished" content="2018-10-05T17:29:42&#43;01:00" />
<meta itemprop="dateModified" content="2018-10-05T17:29:42&#43;01:00" />
<meta itemprop="wordCount" content="569">



<meta itemprop="keywords" content="kafka,elasticsearch,kafka connect,geopoint," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Streaming geopoint data from Kafka to Elasticsearch"/>
<meta name="twitter:description" content="Using the Elasticsearch Kafka Connect connector to stream events from a Kafka topic to Elasticsearch.
curl -X &quot;POST&quot; &quot;http://kafka-connect:8083/connectors/&quot; \ -H &quot;Content-Type: application/json&quot; \ -d &#39;{ &quot;name&quot;: &quot;es_sink_ATM_POSSIBLE_FRAUD&quot;, &quot;config&quot;: { &quot;topics&quot;: &quot;ATM_POSSIBLE_FRAUD&quot;, &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;, &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, &quot;value.converter.schemas.enable&quot;: false, &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;, &quot;key.ignore&quot;: &quot;true&quot;, &quot;schema.ignore&quot;: &quot;true&quot;, &quot;type.name&quot;: &quot;type.name=kafkaconnect&quot;, &quot;topic.index.map&quot;: &quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&quot;, &quot;connection.url&quot;: &quot;http://elasticsearch:9200&quot; } }&#39;  Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:
curl -XPUT &quot;http://elasticsearch:9200/_template/kafkaconnect/&quot; -H &#39;Content-Type: application/json&#39; -d&#39; { &quot;index_patterns&quot;: &quot;*&quot;, &quot;settings&quot;: { &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 }, &quot;mappings&quot;: { &quot;_default_&quot;: { &quot;dynamic_templates&quot;: [ { &quot;dates&quot;: { &quot;match&quot;: &quot;*TIMESTAMP&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;date&quot; } } }, { &quot;geopoint&quot;: { &quot;match&quot;: &quot;*LOCATION&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;geopoint2&quot;: { &quot;match&quot;: &quot;location&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;geo_point&quot; } } }, { &quot;non_analysed_string_template&quot;: { &quot;match&quot;: &quot;account_id, atm, transaction_id&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } ] } } }&#39;  Sample JSON message from Kafka: (pretty-printed)"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://rmoff.github.io" class="f3 fw2 hover-white no-underline white-90 dib">
      rmoff.net
    </a>
    <div class="flex-l items-center">
      
      









    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  <article class="flex-l flex-wrap justify-between mw8 center ph3 ph0-l">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">Streaming geopoint data from Kafka to Elasticsearch</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2018-10-05T17:29:42&#43;01:00">October 5, 2018</time>
    </header>

    <main class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p>Using the Elasticsearch Kafka Connect connector to stream events from a Kafka topic to Elasticsearch.</p>

<pre><code>curl -X &quot;POST&quot; &quot;http://kafka-connect:8083/connectors/&quot; \
     -H &quot;Content-Type: application/json&quot; \
     -d '{
  &quot;name&quot;: &quot;es_sink_ATM_POSSIBLE_FRAUD&quot;,
  &quot;config&quot;: {
    &quot;topics&quot;: &quot;ATM_POSSIBLE_FRAUD&quot;,
    &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,
    &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
    &quot;value.converter.schemas.enable&quot;: false,
    &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
    &quot;key.ignore&quot;: &quot;true&quot;,
    &quot;schema.ignore&quot;: &quot;true&quot;,
    &quot;type.name&quot;: &quot;type.name=kafkaconnect&quot;,
    &quot;topic.index.map&quot;: &quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&quot;,
    &quot;connection.url&quot;: &quot;http://elasticsearch:9200&quot;
  }
}'
</code></pre>

<p>Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:</p>

<pre><code>curl -XPUT &quot;http://elasticsearch:9200/_template/kafkaconnect/&quot; -H 'Content-Type: application/json' -d'
{
  &quot;index_patterns&quot;: &quot;*&quot;,
  &quot;settings&quot;: {
    &quot;number_of_shards&quot;: 1,
    &quot;number_of_replicas&quot;: 0
  },
  &quot;mappings&quot;: {
    &quot;_default_&quot;: {
      &quot;dynamic_templates&quot;: [
        {
          &quot;dates&quot;: {
            &quot;match&quot;: &quot;*TIMESTAMP&quot;,
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;date&quot;
            }
          }
        },
        {
          &quot;geopoint&quot;: {
            &quot;match&quot;: &quot;*LOCATION&quot;,
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;geo_point&quot;
            }
          }
        },
        {
          &quot;geopoint2&quot;: {
            &quot;match&quot;: &quot;location&quot;,
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;geo_point&quot;
            }
          }
        },
        {
          &quot;non_analysed_string_template&quot;: {
            &quot;match&quot;: &quot;account_id, atm, transaction_id&quot;,
            &quot;match_mapping_type&quot;: &quot;string&quot;,
            &quot;mapping&quot;: {
              &quot;type&quot;: &quot;keyword&quot;
            }
          }
        }
      ]
    }
  }
}'
</code></pre>

<p>Sample JSON message from Kafka: (pretty-printed)</p>

<pre><code>{
  &quot;ACCOUNT_ID&quot;: &quot;a898&quot;,
  &quot;TXN1_TIMESTAMP&quot;: 1538733229153,
  &quot;TXN2_TIMESTAMP&quot;: 1538733200285,
  &quot;TXN1_LOCATION&quot;: {
    &quot;LON&quot;: -122.4026113,
    &quot;LAT&quot;: 37.7911278
  },
  &quot;TXN2_LOCATION&quot;: {
    &quot;LON&quot;: -121.4943199,
    &quot;LAT&quot;: 38.5320738
  },
  &quot;TXN1_AMOUNT&quot;: 400,
  &quot;TXN2_AMOUNT&quot;: 50,
  &quot;DISTANCE_BETWEEN_TXNS&quot;: 114.42848872962888,
  &quot;MS_DIFFERENCE&quot;: -28868
}
</code></pre>

<p>Note that the case of all columns is uppercase which causes problems trying to stream this to Elasticsearch. Kafka Connect worker log shows:</p>

<pre><code>WARN Failed to execute batch 5560 of 19 records with attempt 2/6, will attempt retry after 111 ms. Failure reason: Bulk request failed: [{&quot;type&quot;:&quot;mapper_parsing_exception&quot;,&quot;reason&quot;:&quot;failed to parse&quot;,&quot;caused_by&quot;:{&quot;type&quot;:&quot;parse_exception&quot;,&quot;reason&quot;:&quot;field must be either [lat], [lon] or [geohash]&quot;}}
</code></pre>

<p>Taking the JSON message from the Kafka topic and manually sending it to Elasticsearch replicates the probem:</p>

<pre><code>curl -XPOST &quot;http://elasticsearch:9200/atm_possible_fraud/kafkaconnect&quot; -H 'Content-Type: application/json' -d'
{&quot;ACCOUNT_ID&quot;:&quot;a898&quot;,&quot;TXN1_TIMESTAMP&quot;:1538733229153,&quot;TXN2_TIMESTAMP&quot;:1538733200285,&quot;TXN1_LOCATION&quot;:{&quot;LON&quot;:-122.4026113,&quot;LAT&quot;:37.7911278},&quot;TXN2_LOCATION&quot;:{&quot;LON&quot;:-121.4943199,&quot;LAT&quot;:38.5320738},&quot;TXN1_AMOUNT&quot;:400,&quot;TXN2_AMOUNT&quot;:50,&quot;DISTANCE_BETWEEN_TXNS&quot;:114.42848872962888,&quot;MS_DIFFERENCE&quot;:-28868}'

{
&quot;error&quot;: {
    &quot;root_cause&quot;: [
    {
        &quot;type&quot;: &quot;parse_exception&quot;,
        &quot;reason&quot;: &quot;field must be either [lat], [lon] or [geohash]&quot;
    }
    ],
    &quot;type&quot;: &quot;mapper_parsing_exception&quot;,
    &quot;reason&quot;: &quot;failed to parse&quot;,
    &quot;caused_by&quot;: {
    &quot;type&quot;: &quot;parse_exception&quot;,
    &quot;reason&quot;: &quot;field must be either [lat], [lon] or [geohash]&quot;
    }
},
&quot;status&quot;: 400
}
</code></pre>

<p>So Elasticsearch is sensitive to the <em>case</em> of the <code>lat</code>, <code>lon</code> columns. The fix (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html[per">https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html[per</a> the examples here]) is to force the column names to lower case, or concatenate the lat/long into a single string):</p>

<pre><code>curl -XPOST &quot;http://elasticsearch:9200/atm_possible_fraud/kafkaconnect&quot; -H 'Content-Type: application/json' -d'
{&quot;ACCOUNT_ID&quot;:&quot;a898&quot;,
&quot;TXN1_TIMESTAMP&quot;:1538733229153,
&quot;TXN2_TIMESTAMP&quot;:1538733200285,
&quot;TXN1_LOCATION&quot;:&quot;37.7911278,-122.4026113&quot;,
&quot;TXN2_LOCATION&quot;:{&quot;lon&quot;:-121.4943199,&quot;lat&quot;:38.5320738},
&quot;TXN1_AMOUNT&quot;:400,
&quot;TXN2_AMOUNT&quot;:50,
&quot;DISTANCE_BETWEEN_TXNS&quot;:114.42848872962888,
&quot;MS_DIFFERENCE&quot;:-28868}'
</code></pre>

<p>To implement this, I used KSQL to wrangle the data from a <code>STRUCT</code> (with uppercase names) to a lat/long string:</p>

<pre><code>CREATE STREAM ATM_POSSIBLE_FRAUD_02 AS \
SELECT CAST(X.location-&gt;lat AS STRING) + ',' + CAST(X.location-&gt;lon AS STRING) AS TXN1_LOCATION
[...]
FROM   ATM_TXNS X 
</code></pre>

<p>Now the JSON messages look like this:</p>

<pre><code>{
  &quot;ACCOUNT_ID&quot;: &quot;a182&quot;,
  &quot;TXN1_TIMESTAMP&quot;: 1538735677504,
  &quot;TXN2_TIMESTAMP&quot;: 1538735677528,
  &quot;TXN1_LOCATION&quot;: &quot;37.8002247,-122.2160293&quot;,
  &quot;TXN2_LOCATION&quot;: &quot;37.764931,-122.4232384&quot;,
  &quot;TXN1_AMOUNT&quot;: 400,
  &quot;TXN2_AMOUNT&quot;: 20,
  &quot;DISTANCE_BETWEEN_TXNS&quot;: 18.628023818908343,
  &quot;MS_DIFFERENCE&quot;: 24
}
</code></pre>

<p>And the Kafka Connect -&gt; Elasticsearch pipeline works just great. Here&rsquo;s the resulting Elasticsearch index and sample document:</p>

<pre><code>{
  &quot;atm_possible_fraud&quot;: {
    &quot;aliases&quot;: {},
    &quot;mappings&quot;: {
[...]
      &quot;type.name=kafkaconnect&quot;: {
[...]
        &quot;properties&quot;: {
          &quot;ACCOUNT_ID&quot;: {
            &quot;type&quot;: &quot;text&quot;,
            &quot;fields&quot;: {
              &quot;keyword&quot;: {
                &quot;type&quot;: &quot;keyword&quot;,
                &quot;ignore_above&quot;: 256
              }
            }
          },
          &quot;DISTANCE_BETWEEN_TXNS&quot;: {
            &quot;type&quot;: &quot;float&quot;
          },
          &quot;MS_DIFFERENCE&quot;: {
            &quot;type&quot;: &quot;long&quot;
          },
          &quot;TXN1_AMOUNT&quot;: {
            &quot;type&quot;: &quot;long&quot;
          },
          &quot;TXN1_LOCATION&quot;: {
            &quot;type&quot;: &quot;geo_point&quot;
          },
          &quot;TXN1_TIMESTAMP&quot;: {
            &quot;type&quot;: &quot;date&quot;
          },
          &quot;TXN2_AMOUNT&quot;: {
            &quot;type&quot;: &quot;long&quot;
          },
          &quot;TXN2_LOCATION&quot;: {
            &quot;type&quot;: &quot;geo_point&quot;
          },
          &quot;TXN2_TIMESTAMP&quot;: {
            &quot;type&quot;: &quot;date&quot;
          }
        }
      }
    },
    &quot;settings&quot;: {
      &quot;index&quot;: {
        &quot;creation_date&quot;: &quot;1538735883573&quot;,
        &quot;number_of_shards&quot;: &quot;1&quot;,
        &quot;number_of_replicas&quot;: &quot;0&quot;,
        &quot;uuid&quot;: &quot;ppXU3hFvS-CU9kKlFaK-NA&quot;,
        &quot;version&quot;: {
          &quot;created&quot;: &quot;6040299&quot;
        },
        &quot;provided_name&quot;: &quot;atm_possible_fraud&quot;
      }
    }
  }
}
</code></pre>

<pre><code>  {
    &quot;_index&quot;: &quot;atm_possible_fraud&quot;,
    &quot;_type&quot;: &quot;type.name=kafkaconnect&quot;,
    &quot;_id&quot;: &quot;ATM_POSSIBLE_FRAUD2+0+7742&quot;,
    &quot;_score&quot;: 1,
    &quot;_source&quot;: {
      &quot;TXN2_TIMESTAMP&quot;: 1538735677573,
      &quot;TXN2_AMOUNT&quot;: 300,
      &quot;ACCOUNT_ID&quot;: &quot;a874&quot;,
      &quot;TXN1_TIMESTAMP&quot;: 1538735677515,
      &quot;MS_DIFFERENCE&quot;: 58,
      &quot;TXN1_AMOUNT&quot;: 300,
      &quot;DISTANCE_BETWEEN_TXNS&quot;: 57.33495049372549,
      &quot;TXN1_LOCATION&quot;: &quot;37.7923185,-122.3940464&quot;,
      &quot;TXN2_LOCATION&quot;: &quot;37.3540655,-122.0512763&quot;
    }
  }
</code></pre>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/kafka" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">kafka</a>
   </li>
  
   <li class="list">
     <a href="/tags/elasticsearch" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">elasticsearch</a>
   </li>
  
   <li class="list">
     <a href="/tags/kafka-connect" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">kafka connect</a>
   </li>
  
   <li class="list">
     <a href="/tags/geopoint" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">geopoint</a>
   </li>
  
</ul>
<div class="mt6">
        
      </div>
    </main>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://rmoff.github.io" >
    &copy; 2018 rmoff.net
  </a>
    <div>








</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
