<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.github.io/index.xml">
		<link rel="canonical" href="https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/">
		
		<link rel="shortcut icon" type="image/png" href="https://rmoff.github.io/apple-touch-icon-precomposed.png">
		
		
		<meta name="generator" content="Hugo 0.52" />

		
		<meta name="og:title" content="Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example" />
		<meta name="og:type" content="article" />
		<meta name="og:image" content="https://rmoff.github.io/images/2016/04/kd05a-1.png" />
		<meta name="og:description" content="" />
		<meta name="og:url" content="https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/" />
		<meta name="og:site_name" content="Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.github.io/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/descartes.css" />
		
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		

		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.github.io/js/story.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.github.io/images/2016/04/kd05a-1.png'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.github.io" title="Home">
						<img src="https://rmoff.github.io/img/logo.jpg" class="w2 h2 br-100" alt="rmoff&#39;s random ramblings" />
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="/about-me/">about</a>
						<a class="link f5 ml2 dim near-white" href="/talks/">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/rmoff/"><i class="fab fa-twitter-square"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.github.io/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.github.io/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2016-04-12T21:50:46Z">Apr 12, 2016</time>
								<span class="display-print">by Robin Moffatt</span>
								 in <a href="https://rmoff.github.io/categories//apache-kafka" class="no-underline category near-white dim">Apache Kafka</a>, <a href="https://rmoff.github.io/categories//kafka" class="no-underline category near-white dim">Kafka</a>, <a href="https://rmoff.github.io/categories//logstash" class="no-underline category near-white dim">Logstash</a>, <a href="https://rmoff.github.io/categories//elastic" class="no-underline category near-white dim">Elastic</a>, <a href="https://rmoff.github.io/categories//elasticsearch" class="no-underline category near-white dim">Elasticsearch</a>, <a href="https://rmoff.github.io/categories//kibana" class="no-underline category near-white dim">Kibana</a>, <a href="https://rmoff.github.io/categories//elastic-v5" class="no-underline category near-white dim">Elastic V5</a>, <a href="https://rmoff.github.io/categories//zookeeper" class="no-underline category near-white dim">Zookeeper</a>
								<span class="display-print">at https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</span>
							
						
					</h2>
				</div>

				
				
				
				

			</div>
		</header>
		
		<main role="main">
		
<article class="center bg-white br-3 pv1 ph4 nested-copy-line-height lh-copy f4 nested-links mw-100 measure-wide">
	

<p>I&rsquo;ve recently been playing around with the ELK stack (<a href="https://www.elastic.co/blog/heya-elastic-stack-and-x-pack">now officially known as the Elastic stack</a>) collecting data from <a href="http://rmoff.net/2016/03/03/obihackers-irc-channel/">an IRC channel</a> with Elastic&rsquo;s Logstash, storing it in Elasticsearch and <a href="http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/">analysing it with Kibana</a>. But, this isn&rsquo;t an &ldquo;ELK&rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.</p>

<p>As I <a href="http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/">wrote about last year</a>, Apache Kafka provides a handy way to build flexible &ldquo;pipelines&rdquo;. Today I&rsquo;m writing up a short real-world example of this in practice. There are three elements to the flexibility that I really want to highlight:</p>

<ol>
<li>Decoupling the consumption of data from its production at the previous stage</li>
<li>Because the consumer is decoupled, being able to stop and start it and have it continue ingesting data from the point at which it previously stopped</li>
<li>The ability to replay the ingest phase of a pipeline repeatedly into multiple consumers, with no change required to the configuration from source</li>
</ol>

<p>The simplest form of the pipeline I was using looks like this:</p>

<p><img src="/content/images/2016/04/kd01.png" alt="" /></p>

<p>A logstash configuration (<a href="https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-01-logstash-irc-conf"><code>logstash-irc.conf</code></a>) gets Logstash to connect to the IRC server and send messages received to Elasticsearch. From here they can be displayed and analysed within Kibana. <a href="http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/">Read more about the details here</a> if you&rsquo;re interested.</p>

<p>From a &ldquo;pipeline&rdquo; point of view this is a pretty typical pattern. A tool (Logstash here, but could be ODI, Informatica, etc) runs with a set of &ldquo;code&rdquo; (a very simple <code>.conf</code> here, elsewhere it could be mappings and load plans), reading data from a source. Obviously in full-blown system there&rsquo;s a dozen more moving parts than this simple example, but the point stands.</p>

<p>Let&rsquo;s think a bit more about what a pipeline does, as this will give us the basis for understanding why and how Kafka fits in so nicely. Overlaying some labels onto the above diagram shows all the processing that we&rsquo;re doing:</p>

<p><img src="/content/images/2016/04/kd01a-1.png" alt="" /></p>

<p>If any of this needs reconfiguring, restarting, or rerunning, it&rsquo;s an all-or-nothing job. Given that we&rsquo;re streaming data in near-real-time (or conceptually, designing something that <em>could</em> if needed with minimal-to-no rework), shutting down the pipeline just to change one of these bits is a problem because we&rsquo;ll lose the data that the source system is spitting out whether we&rsquo;re there to gather it or not.</p>

<p>Why would we need to change the pipeline configuration? Consider:</p>

<ul>
<li>Reconfiguring - Adding in additional enrichment functionality (eg GeoIP lookup), or filtering out duff records, or fixing a bug in the logic, or a dozen other easy examples - in all these cases it&rsquo;s great if we can reprocess the existing backlog of processed data and then continue processing data as it&rsquo;s available from the source system.</li>
<li>Restarting - if the load fails, ideally we don&rsquo;t want to be hitting the source system again for our data if we&rsquo;ve extracted it once already. Similarly if the load process needs to be stopped, maybe for maintenance of the target load system, it&rsquo;s useful to be able to restart the processing exactly from where it left off.</li>
</ul>

<p>So I decoupled the source extract from any subsequent processing with a very simple Logstash configuration (<a href="https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-02-logstash-irc-kafka-conf"><code>logstash-irc-kafka.conf</code></a>) that pulls the data from IRC as before and <strong>just</strong> sends it straight to Kafka:</p>

<p><img src="/content/images/2016/04/kd02a.png" alt="" /></p>

<p>The data lands in Kafka, which becomes our &lsquo;staging&rsquo; area in effect, taking advantage of Kafka&rsquo;s &ldquo;durable buffer&rdquo; concept. The data extracted is ideally as raw as possible - because we don&rsquo;t know what subsequent processing we want to do with it, maybe now, or at some future date. Kafka can be configured to retain data based on age or volume - since the data I was working with was low volume I set the topic to retain it for 90 days:</p>

<pre><code>./kafka-topics.sh --zookeeper localhost:2181 --topic irc --alter --config retention.ms=7776000000
</code></pre>

<p>With the data streaming into Kafka and building up there we can then set up one or more consumers of that data. <em>Note that I&rsquo;m using consumers in the logical sense, not the Kafka &ldquo;Consumer&rdquo; specific terminology</em>. My consumer here is Logstash using <a href="https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-03-logstash-kafka-es-conf"><code>logstash-kafka-es.conf</code></a>, which is a variant of the original configuration, this time pulling from Kafka instead of the live IRC feed. And since Kafka is so low-latency, a side-benefit of this setup is that I can both catch up on and replay past records, as well as stream live ones in near-real-time. Result!</p>

<p><img src="/content/images/2016/04/kd03a.png" alt="" /></p>

<p>At this point I&rsquo;m <a href="http://rmoff.net/2016/03/24/my-latest-irc-client-kibana/">where I was before</a>; streaming IRC content in near-real-time to Elasticsearch and analysing it with Kibana. The only difference is that I&rsquo;ve added in Kafka as a buffer, decoupling the reading messages from IRC with the processing and subsequent storage of them.</p>

<p>Now here&rsquo;s the money shot &ndash; I can add new consumers of this data that&rsquo;s in Kafka, whenever I want, without needing to know about them at the time that I extracted the source data. I can pick up from the end of the feed, or I can reprocess the whole lot, <em>per consumer</em>. I&rsquo;ve used this in a couple of instances recently:</p>

<ul>
<li>Add and refine a GeoIP lookup step to the Logstash processing (see <a href="https://gist.github.com/rmoff/862d0ceea223aa7283244b1b27594941#file-04-logstash-kafka-es-02-conf">example config</a>), <em>without affecting the existing Logstash-&gt;Elasticsearch-&gt;Kibana flow</em></li>
<li>Testing the <a href="https://www.elastic.co/blog/elasticsearch-5-0-0-alpha1-released">Elastic stack v5 alpha release</a> by processing the <strong>same source data again</strong> but with a different version of the downstream tools, enabling a proper like-for-like comparison of the pipeline. This is similar in concept to an idea that <a href="https://twitter.com/gwenshap">Gwen Shapira</a> wrote about <a href="http://radar.oreilly.com/2015/05/validating-data-models-with-kafka-based-pipelines.html">in an article in 2015</a>.</li>
</ul>

<p>In both of these cases <strong>the existing original consumer remains running and untouched</strong>. This kind of concurrent running is a great way to work with a single feed from the source system, keep the data pipeline running for subsequent analytics, whilst also developing and validating new functionality.</p>

<p><img src="/content/images/2016/04/kd05a.png" alt="kd05a" /></p>

<h3 id="consumer-groups-and-offsets">Consumer Groups and Offsets</h3>

<p>One of the key concepts in all of this is that of the Kafka <strong>consumer group</strong>, which is a unique identifier for a given consumer (or group of consumers for the same logical entity if you want to parallelise the consumption). In Kafka 0.8 Zookeeper is used by default to keep track off the <strong>offset</strong> of the last record that a given consumer group received. So in my development environment I can look on my Kafka server at Zookeeper and see for each consumer group the latest offset: (<a href="https://cwiki.apache.org/confluence/display/KAFKA/System+Tools#SystemTools-ExportZookeeperOffsets">reference</a>)</p>

<pre><code>$ bin/kafka-run-class.sh kafka.tools.ExportZkOffsets --zkconnect localhost:2181 --output-file &gt;(cat)

/consumers/console-consumer-32467/offsets/irc/0:4145
/consumers/kafka-ubuntu03/offsets/irc/0:1035
/consumers/logstash/offsets/irc/0:4145
/consumers/logstash-5-testing/offsets/irc/0:4143
</code></pre>

<p><em>A brief note on the command above - I&rsquo;m using <a href="http://tldp.org/LDP/abs/html/process-sub.html">bash process substitution</a> to send the output to stdout (via <code>cat</code>) instead of the asked-for output file.</em></p>

<p>From the above output you can see that there are four consumer groups. Two are at the same offset (4145) which happens to be the latest, and therefore have consumed all the available messages. A third (<code>logstash-5-testing</code>) is almost caught up (4143, vs 4145), and the final one (<code>kafka-ubuntu03</code>) is way behind at 1035. By running the command periodically you can see if a consumer is actually reading records, or just offline (or maybe stuck).</p>

<p>To see more information about a given consumer, including the lag (current vs maximum offset) use <code>ConsumerOffsetChecker</code> and specify the consumer group:</p>

<pre><code>$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group logstash-5-testing
Group           Topic                          Pid Offset          logSize         Lag             Owner
logstash-5-testing irc                            0   4143            4148            5               none
</code></pre>

<h3 id="summary">Summary</h3>

<p>Building a successful data pipeline requires that it is flexible to changing requirements, as well as unknown future ones. This is as true for little local PoCs such as this one as it is for large-scale implementations. The pipeline needs to be able to have minimal impact on source systems whilst being able to satisfy multiple destinations, some or all of which may want to batch process instead of stream the data. In addition, being able to re-stream the raw data repeatedly and on-demand into adhoc applications without affecting the primary &lsquo;productionised&rsquo; consumers is a powerful enabler of the &lsquo;data discovery lab&rsquo; concept, which I write about <a href="http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/">in more detail here</a>.</p>

<p>Kafka enables the above, summarised in the following benefits:</p>

<ol>
<li>Stream or batch the data from source <strong>once</strong>, consume by multiple hetrogenous applications <strong>many</strong> times.</li>
<li>Offset tracking distinct for each consuming application</li>

<li><p>Processing can be re-run, which is useful for:</p>

<ul>
<li>Development process - iterative improvements / bug fixing against the same streamed data set</li>
<li>Production data - data discovery/advanced analytics</li>
</ul></li>
</ol>

<p>You can read more about this in detail <a href="http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/">over here</a>.</p>

</article>

		</main>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr />

<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt" /></p>

<p>Robin Moffatt is a Developer Advocate at Confluent, and Oracle Groundbreaker Ambassador. He also likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</p>

				
			
		
		</div>
		
	</div>

		
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.github.io/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2019 Robin Moffatt
			</p>
		</footer>
		
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-75492960-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	</body>
</html>
