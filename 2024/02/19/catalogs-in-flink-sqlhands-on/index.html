<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Catalogs in Flink SQL‚ÄîHands On</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.net/index.xml">
		<link rel="canonical" href="https://rmoff.net/2024/02/19/catalogs-in-flink-sqlhands-on/">
		
		
		
		

		
		<meta property="og:title" content="Catalogs in Flink SQL‚ÄîHands On" />
		<meta property="og:type" content="article" />
		<meta property="og:image" content="https://rmoff.net/images/2024/02/flink-hands-on.webp" />
		<meta property="og:description" content="" />
		<meta property="og:url" content="https://rmoff.net/2024/02/19/catalogs-in-flink-sqlhands-on/" />
		<meta property="og:site_name" content="Catalogs in Flink SQL‚ÄîHands On" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.net/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/descartes.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/toc.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/retro-cards.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/custom.css" />
		
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		
		
		<script>
			!function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey getNextSurveyStep identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
            posthog.init('phc_93NEP79Ju4xqXYWXnoLbr4HMW0Iaepj1BGOVoEXYX6P',{api_host:'https://eu.i.posthog.com', person_profiles: 'identified_only' 
                })
		</script>
		
		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.net/js/story.js"></script>
		<script src="https://rmoff.net/js/toc.js"></script>
		<script src="https://rmoff.net/js/medium-mirror.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.net/images/2024/02/flink-hands-on.webp'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.net" title="Home">
						<link rel="preload" as="image" href="https://rmoff.net/img/repton.gif">
						<img
							src="https://rmoff.net/img/logo.jpg"
							class="w2 h2 br-100"
							alt="rmoff&#39;s random ramblings"
							onmouseover="this.src='https:\/\/rmoff.net\/img\/repton.gif';"
							onmouseout="this.src='https:\/\/rmoff.net\/img\/logo.jpg';"
						/>
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net/bio">about</a>
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://bsky.app/profile/rmoff.net"><i class="fa-brands fa-bluesky"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/rmoff/"><i class="fa-brands fa-x-twitter"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.youtube.com/c/rmoff"><i class="fab fa-youtube-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.net/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.net/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">
						<span class="terminal-title">Catalogs in Flink SQL‚ÄîHands On<span class="terminal-cursor"></span></span>
					</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2024-02-19T00:00:00Z">Feb 19, 2024</time>
								<span class="display-print">by </span>
								 in <a href="https://rmoff.net/categories/apache-flink" class="no-underline category near-white dim">Apache Flink</a>
								<span class="display-print">at https://rmoff.net/2024/02/19/catalogs-in-flink-sqlhands-on/</span>
							
						
					</h2>
				</div>

				
				
				
				<div class="w-100 cf hide-print">
					<a class="fr f6 ma0 pa2 link white-50 dim fas fa-camera" href="https://bsky.app/profile/rmoff.net" title="Photo Credit"></a>
				</div>
				
				

			</div>
		</header>
		
		<main role="main">
		
<div class="container-fluid docs">
  <div class="row">
    <main class="docs-content" role="main">

<article class="article">
	<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This post originally appeared on the <a href="https://www.decodable.co/blog/catalogs-in-flink-sql-hands-on">Decodable blog</a>.
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>In the  <a href="/2024/02/16/catalogs-in-flink-sqla-primer/">previous blog post</a>  I looked at the role of catalogs in Flink SQL, the different types, and some of the quirks around their configuration and use.
If you are new to Flink SQL and catalogs, I would recommend reading that post just to make sure you‚Äôre not making some of the same assumptions that I mistakenly did when looking at this for the first time.</p>
</div>
<div class="paragraph">
<p>In this article I am going to walk through the use of several different catalogs and go into a bit of depth with each to help you understand exactly what they‚Äôre doing‚Äîpartly out of general curiosity, but also as an important basis for troubleshooting.
I‚Äôll let you know now: some of this goes off on tangents somewhat, but I learnt lots during it and I want to share that with you!</p>
</div>
<div class="paragraph">
<p>Why three different metastores with the Iceberg catalog?
Because I found the documentation across the different projects difficult to reconcile into a consistent overall picture, so by examining multiple backends I got a proper grasp of what was going on.</p>
</div>
<div class="paragraph">
<p>Let‚Äôs get started with one of the most widely-supported and open-standard catalogs: Apache Hive, and specifically, its Metastore.</p>
</div>
<div class="sect1">
<h2 id="_using_the_hive_catalog_with_flink_sql">Using the Hive Catalog with Flink SQL&nbsp;<a class="headline-hash" href="#_using_the_hive_catalog_with_flink_sql">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/hive/hive_catalog/">Hive catalog</a>  is one of the three catalogs that are part of the Flink project._
_It uses the Hive Metastore to persist object definitions, so is one of the primary choices you‚Äôve got for a catalog to use with Flink.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_installation_and_configuration">Installation and Configuration&nbsp;<a class="headline-hash" href="#_installation_and_configuration">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>It‚Äôs important to note that whilst the Hive catalog is <em>part</em> of the Flink project, it‚Äôs not shipped with the binaries.
The docs describe the process of <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/hive/overview/#dependencies">installing the dependencies</a>  and necessary configuration, but to someone not super-familiar with Java and Hadoop I found myself stuck quite often.
In case you‚Äôre in the same boat, I‚Äôm going to detail here the steps I took to get it working.</p>
</div>
<div class="paragraph">
<p>This is all on my local machine; doing the same for a production-grade deployment of Flink would probably be different.
And if you‚Äôre using a <a href="https://decodable.co/">managed Flink service</a> , irrelevant üòÑ.</p>
</div>
<div class="paragraph">
<p>The first thing to do is to make sure you‚Äôve got a Hive Metastore.
Fortunately Chris Riccomini has built and shared a <a href="https://github.com/recap-build/hive-metastore-standalone">Docker image that provides just this</a> .
It uses an embedded <a href="https://db.apache.org/derby/">DerbyDB</a>  to store the metadata.
As mentioned; this is just for a local setup‚Äîif you decide to take this on to real use then you‚Äôll want to make sure you‚Äôre persisting the data in a proper RDBMS.</p>
</div>
<div class="paragraph">
<p>Run this to start the container which listens on port 9083:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-dockerfile" data-lang="dockerfile">docker run --rm --detach --name hms-standalone \
		--publish 9083:9083 \
		ghcr.io/recap-build/hive-metastore-standalone:latest</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we need to create a file that Flink is going to look for to tell it where to find the Hive Metastore.
This is <code>hive-site.xml</code> and needs to go in Flink‚Äôs <code>./conf</code> folder (by default):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">cat &gt; ./conf/hive-site.xml &lt;



    hive.metastore.local
    false



    hive.metastore.uris
    thrift://localhost:9083



EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that <code>localhost:9083</code> points to the Docker container we just started.
If you‚Äôre using a Hive Metastore on a different host/port then amend this as needed.</p>
</div>
<div class="paragraph">
<p>Now we get to the fun bit‚Äî <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/hive/overview/#dependencies">dependencies</a> !</p>
</div>
<div class="paragraph">
<p>The Hive dependency is actually straightforward: download <code>flink-sql-connector-hive-3.1.3</code> from <a href="https://central.sonatype.com/artifact/org.apache.flink/flink-sql-connector-hive-3.1.3_2.12/versions">Maven Central</a>  into a new subfolder under your <code>./lib</code> folder:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir -p ./lib/hive
curl https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.3_2.12/1.18.1/flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar \
	-o ./lib/hive/flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="_this_is_where_it_gets_fun_hadoop_dependency">This is where it gets &#34;fun&#34;: Hadoop Dependency&nbsp;<a class="headline-hash" href="#_this_is_where_it_gets_fun_hadoop_dependency">üîó</a> </h5>
<div class="paragraph">
<p>Unless you have a Hadoop distribution lying around on your hard drive you‚Äôre going to need to avail yourself of some JARs.
There‚Äôs the simple way, and the hacky way.
Let‚Äôs start with the hacky one.</p>
</div>
<div class="paragraph">
<p><strong>Option 1: Slightly Hacky but light-weight</strong></p>
</div>
<div class="paragraph">
<p>The alternative to a full Hadoop download which we‚Äôll see below (and resulting JAR clashes as seen with <a href="https://issues.apache.org/jira/browse/FLINK-33358">FLINK-33358</a> ) is to just download the JARs that Hive <em>seems</em> to want and make those available.
I‚Äôve identified these by trial-and-error because I was offended by needing such a heavy-weight download <code>¬Ø_(„ÉÑ)_/¬Ø</code>.
Download them directly into the <code>./lib/hive</code> folder that we created above:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir -p ./lib/hive
curl https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/5.3.0/woodstox-core-5.3.0.jar -o ./lib/hive/woodstox-core-5.3.0.jar
curl https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar -o ./lib/hive/commons-logging-1.1.3.jar
curl https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar -o ./lib/hive/commons-configuration2-2.1.1.jar
curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.3.2/hadoop-auth-3.3.2.jar -o ./lib/hive/hadoop-auth-3.3.2.jar
curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.2/hadoop-common-3.3.2.jar -o ./lib/hive/hadoop-common-3.3.2.jar
curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs-client/3.3.2/hadoop-hdfs-client-3.3.2.jar -o ./lib/hive/hadoop-hdfs-client-3.3.2.jar
curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.2/hadoop-mapreduce-client-core-3.3.2.jar -o ./lib/hive/hadoop-mapreduce-client-core-3.3.2.jar
curl https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar -o ./lib/hive/hadoop-shaded-guava-1.1.1.jar
curl https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar -o ./lib/hive/stax2-api-4.2.1.jar</code></pre>
</div>
</div>
<div class="sect5">
<h6 id="_option_2_the_proper_but_bloated_option">Option 2: The Proper (but bloated) Option&nbsp;<a class="headline-hash" href="#_option_2_the_proper_but_bloated_option">üîó</a> </h6>
<div class="paragraph">
<p>Download and extract 600MB Hadoop tar file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir ~/hadoop
cd ~/hadoop
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
tar xvf hadoop-3.3.2.tar.gz</code></pre>
</div>
</div>
<div class="paragraph">
<p>Set the <code>HADOOP_CLASSPATH</code>.
Something you might miss from <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/hive/overview/#dependencies">the docs</a>  (I did) is that what‚Äôs quoted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">export HADOOP_CLASSPATH=`hadoop classpath`</code></pre>
</div>
</div>
<div class="paragraph">
<p>This actually executes the <code>hadoop</code> binary with the <code>classpath</code> command, and sets the output as the environment variable <code>HADOOP_CLASSPATH</code>.
In effect it‚Äôs doing this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ cd hadoop/hadoop-3.3.2
$ ./bin/hadoop classpath
/Users/rmoff/hadoop/hadoop-3.3.2/etc/hadoop:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/common/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/common/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/mapreduce/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn/*</code></pre>
</div>
</div>
<div class="paragraph">
<p>and taking that output to set as the environment variable that the Hive code in Flink will use.
Unless you‚Äôve gone ahead and actually installed Hadoop, you‚Äôll need to specify the binary‚Äôs absolute path to use it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ export HADOOP_CLASSPATH=$(~/hadoop/hadoop-3.3.2/bin/hadoop classpath)
$ echo $HADOOP_CLASSPATH
/Users/rmoff/hadoop/hadoop-3.3.2/etc/hadoop:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/common/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/common/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/hdfs/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/mapreduce/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn/lib/*:/Users/rmoff/hadoop/hadoop-3.3.2/share/hadoop/yarn/*</code></pre>
</div>
</div>
<div class="paragraph">
<p><em>You‚Äôll notice I‚Äôm using $( ) instead of ` ` to enclose the hadoop call because to me it‚Äôs more readable and less ambiguous‚ÄîI read the docs as meaning you just had to put the Hadoop classpath in the place of hadoop classpath, not that it was an actual command to run.</em></p>
</div>
<div class="paragraph">
<p>If you‚Äôre using 1.18 then because of <a href="https://issues.apache.org/jira/browse/FLINK-33358">[FLINK-33358</a> Flink SQL Client fails to start in Flink on YARN - ASF JIRA]  you‚Äôll need to apply <a href="https://github.com/apache/flink/pull/23629#issuecomment-1917053501">this small PR</a>  to your <code>sql-client.sh</code> before running the SQL Client.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sql_client_with_the_hive_catalog">SQL Client with the Hive Catalog&nbsp;<a class="headline-hash" href="#_sql_client_with_the_hive_catalog">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>With our dependencies installed and configured, and a Hive Metastore instance running, we‚Äôre ready to go and use our Hive catalog.
Launch the SQL Client:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">./bin/sql-client.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p><em>If you‚Äôre using HADOOP_CLASSPATH make sure you set it in the context of the shell session that you launch the SQL Client in.</em></p>
</div>
<div class="paragraph">
<p>From the Flink SQL prompt you can create the catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_hive WITH (
      &#39;type&#39; = &#39;hive&#39;,
      &#39;hive-conf-dir&#39; = &#39;./conf/&#39;);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Set the catalog to the active one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">USE CATALOG c_hive;</code></pre>
</div>
</div>
<div class="paragraph">
<p>List databases:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SHOW DATABASES;
+---------------+
| database name |
+---------------+
|       default |
+---------------+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a new database &amp; use it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; CREATE DATABASE new_db;
[INFO] Execute statement succeed.

Flink SQL&gt; USE new_db;
[INFO] Execute statement succeed.</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>SHOW CURRENT</code> command is useful to orientate yourself in the session:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SHOW CURRENT CATALOG;
+----------------------+
| current catalog name |
+----------------------+
|               c_hive |
+----------------------+
1 row in set

Flink SQL&gt; SHOW CURRENT DATABASE;
+-----------------------+
| current database name |
+-----------------------+
|                new_db |
+-----------------------+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>To show that the persistence of the catalog metadata in Hive Metastore is working let‚Äôs go and create a table:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; CREATE TABLE foo (
			     c1 INT,
			     c2 STRING
			 ) WITH (
			   &#39;connector&#39; = &#39;datagen&#39;,
			   &#39;number-of-rows&#39; = &#39;8&#39;
			 );
[INFO] Execute statement succeed.

Flink SQL&gt; SHOW TABLES;
+------------+
| table name |
+------------+
|        foo |
+------------+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>We‚Äôll query it, just to make sure things are working:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SET &#39;sql-client.execution.result-mode&#39; = &#39;tableau&#39;;
[INFO] Execute statement succeed.

Flink SQL&gt; SET &#39;execution.runtime-mode&#39; = &#39;batch&#39;;
[INFO] Execute statement succeed.

Flink SQL&gt; SELECT * FROM foo;
+-------------+--------------------------------+
|          c1 |                             c2 |
+-------------+--------------------------------+
| -1661109571 | 5c6a9dc95b902e6f7fabb23d53e... |
|  1158331176 | 4256c5643eca73aaaa28a3609e0... |
| -2071639638 | 4b7b20b58a4ce9d4aa81d13a566... |
| -1586162357 | 9add50adef170e51cf22c99a150... |
|   358671098 | 4c938c5985783b36c8b1a90d819... |
| -2052452860 | 8a2a6328eba62aa160fa4dbc12c... |
| -1755663778 | 4395b96ceffcd46b5f9354d97ce... |
| -1454974054 | 38a87a1525daf1626b7c3c578e4... |
+-------------+--------------------------------+
8 rows in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now restart the session:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">Flink SQL&gt; EXIT;
[INFO] Exiting Flink SQL CLI Client...

Shutting down the session...
done.

‚ùØ ./bin/sql-client.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because we‚Äôre using the Hive catalog and not the in-memory one, we should see the database (<code>new_db</code>) and table (<code>foo</code>) still present:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SHOW TABLES IN `c_hive`.`new_db`;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Catalog c_hive does not exist</code></pre>
</div>
</div>
<div class="paragraph">
<p>Oh noes!
It didn‚Äôt work!
üôÄ Or did it?
üòº</p>
</div>
<div class="paragraph">
<p>I mentioned Catalog Stores in my  <a href="/2024/02/16/catalogs-in-flink-sqla-primer/">first blog post</a> , and I‚Äôve not defined one‚Äîmeaning that the catalog <em>definition</em> is not persisted between sessions.
If I define the catalog again:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; CREATE CATALOG c_hive WITH (
&gt;        &#39;type&#39; = &#39;hive&#39;,
&gt;        &#39;hive-conf-dir&#39; = &#39;./conf/&#39;);
[INFO] Execute statement succeed.</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then I find that the catalog‚Äôs metadata is still present, as it should be!</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SHOW TABLES IN `c_hive`.`new_db`;
+------------+
| table name |
+------------+
|        foo |
+------------+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this sense, when we create a catalog in Flink it‚Äôs more like creating a <em>connection</em>.
Once that connection is created, whatever metadata is stored the other side of it becomes available to Flink.</p>
</div>
<div class="paragraph">
<p>So that‚Äôs using the Hive catalog with Flink.
You can skip over the next section if you want, but if you‚Äôre like me and curious as to what‚Äôs happening behind the scenes then keep reading.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sidenote_digging_a_bit_deeper_into_the_hive_metastore">Sidenote: Digging a bit Deeper into the Hive Metastore&nbsp;<a class="headline-hash" href="#_sidenote_digging_a_bit_deeper_into_the_hive_metastore">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>Here‚Äôs what we‚Äôll see on successful connection from the SQL Client to the Hive Metastore in the logs (<code>flink-rmoff-sql-client-asgard08.log</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">org.apache.hadoop.hive.conf.HiveConf                 [] - Found configuration file file:/Users/rmoff/flink/flink-1.18.1/conf/hive-site.xml
org.apache.flink.table.catalog.hive.HiveCatalog      [] - Setting hive conf dir as ./conf/
org.apache.flink.table.catalog.hive.HiveCatalog      [] - Created HiveCatalog &#39;c_hive&#39;
org.apache.hadoop.hive.metastore.HiveMetaStoreClient [] - Trying to connect to metastore with URI thrift://localhost:9083
org.apache.hadoop.hive.metastore.HiveMetaStoreClient [] - Opened a connection to metastore, current connections: 1
org.apache.hadoop.hive.metastore.HiveMetaStoreClient [] - Connected to metastore.</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can inspect the network traffic between Flink and Hive using <code>tcpdump</code>.
Since the Hive Metastore is on Docker, we‚Äôll use another container to help here.
Create a <code>tcpdump</code> docker image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-dockerfile" data-lang="dockerfile">docker build -t tcpdump - &lt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>With this we can capture details of the communication between Flink and the Hive Metastore:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">docker run -v /tmp:/tmp/ \
           --rm \
           --tty \
           --net=container:hms-standalone tcpdump \
           tcpdump -w /tmp/flink-hms.pcap</code></pre>
</div>
</div>
<div class="paragraph">
<p>Hive metastore uses the <a href="https://thrift.apache.org/docs/concepts#protocol">Thrift protocol</a>  to communicate with clients, and by loading the resulting <code>pcap</code> file into Wireshark we can inspect this traffic in more detail.
Here we see the creation of a table called <code>foo_new2</code> in the <code>new_db</code> database:</p>
</div>
<div class="paragraph">
<p>+
Of course, none of this is actually <em>necessary</em> for simply using a catalog with Flink‚Äîbut I found it useful for mapping out in my mind what‚Äôs actually happening.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_does_the_hive_catalog_look_like_when_storing_parquet_data_in_s3_minio_from_flink">What does the Hive catalog look like when storing Parquet data in S3 (MinIO) from Flink?&nbsp;<a class="headline-hash" href="#_what_does_the_hive_catalog_look_like_when_storing_parquet_data_in_s3_minio_from_flink">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>OK, back to the main storyline.
We‚Äôve now got a Hive catalog working, persisting the metadata about a definition-only table.
What do I mean by a definition-only table?
Well it‚Äôs completely self-contained; there is no real data, just <code>datagen</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">CREATE TABLE foo (
     c1 INT,
     c2 STRING
 ) WITH (
   &#39;connector&#39; = &#39;datagen&#39;,
   &#39;number-of-rows&#39; = &#39;8&#39;
 );</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let‚Äôs now add in something more realistic, and understand how we can write data from Flink to a table whose data actually exists somewhere.
We‚Äôll store the data on <a href="https://min.io/">MinIO</a> , which is an S3-compatible object store that you can run locally, and write it in the widely-adopted <a href="https://parquet.apache.org/">Apache Parquet</a>  column-oriented file format.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfa82a396e17c57896f_6702c5d80cdc849848eddc30_65cee65ee28c340f206096a1_94MI5dKvnbsyglRSZ6xLYAJEK7weLu9GxvTxmehKEu6v__lNwi56IrLJ4qOEbtwOPBHqdglYZVEwXrJYULgb0GwMUmyZ_VkFiaR4HsXnRe6aZKkkDIXNVLLz6bF6EamR5unTT5t0DfGlMit0W2nBGNI.webp" alt="6717dbfa82a396e17c57896f 6702c5d80cdc849848eddc30 65cee65ee28c340f206096a1 94MI5dKvnbsyglRSZ6xLYAJEK7weLu9GxvTxmehKEu6v  lNwi56IrLJ4qOEbtwOPBHqdglYZVEwXrJYULgb0GwMUmyZ VkFiaR4HsXnRe6aZKkkDIXNVLLz6bF6EamR5unTT5t0DfGlMit0W2nBGNI"/>
</div>
</div>
<div class="sect4">
<h5 id="_setup">Setup&nbsp;<a class="headline-hash" href="#_setup">üîó</a> </h5>
<div class="paragraph">
<p>First we need to add the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/formats/parquet/">Parquet format</a>  to the available JARs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir -p lib/formats
curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-parquet/1.18.1/flink-sql-parquet-1.18.1.jar \
	-o ./lib/formats/flink-sql-parquet-1.18.1.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we‚Äôll set up the S3 bit, for which we‚Äôre using MinIO and will need Flink‚Äôs <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/filesystems/s3/">S3 support</a> .
Run MinIO using Docker:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">docker run --rm --detach \
           --name minio \
           -p 9001:9001 -p 9000:9000 \
           -e &#34;MINIO_ROOT_USER=admin&#34; \
           -e &#34;MINIO_ROOT_PASSWORD=password&#34; \
           minio/minio \
           server /data --console-address &#34;:9001&#34;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then provision a bucket:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">docker exec minio \
	mc config host add minio http://localhost:9000 admin password
docker exec minio \
	mc mb minio/warehouse</code></pre>
</div>
</div>
<div class="paragraph">
<p>Flink‚Äôs S3 plugin is included in the Flink distribution but needs to be added to the <code>./plugins</code> folder to be available for us:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir ./plugins/s3-fs-hadoop
cp ./opt/flink-s3-fs-hadoop-1.18.1.jar ./plugins/s3-fs-hadoop/</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, add the required configuration to <code>./conf/flink-conf.yaml</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">cat &gt;&gt; ./conf/flink-conf.yaml &lt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>[Re]start your Flink cluster, and launch the SQL Client.</p>
</div>
</div>
<div class="sect4">
<h5 id="_using_parquet_and_s3_from_flink">Using Parquet and S3 from Flink&nbsp;<a class="headline-hash" href="#_using_parquet_and_s3_from_flink">üîó</a> </h5>
<div class="paragraph">
<p>Declare the Hive catalog connection again, and create a new database within it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_hive WITH (
      &#39;type&#39; = &#39;hive&#39;,
      &#39;hive-conf-dir&#39; = &#39;./conf/&#39;);
USE CATALOG c_hive;

CREATE DATABASE db_rmoff;
USE db_rmoff;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we‚Äôll create a table that‚Äôs going to use filesystem persistence for its data, which will be written in Parquet format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE TABLE t_foo_fs (c1 varchar, c2 int)
WITH (
  &#39;connector&#39; = &#39;filesystem&#39;,
  &#39;path&#39; = &#39;s3://warehouse/t_foo_fs/&#39;,
  &#39;format&#39; = &#39;parquet&#39;
);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Add some data to the table:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; INSERT INTO t_foo_fs VALUES (&#39;a&#39;,42);
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 218ed3a025e219df7356bbb609cad5da</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using MinIO‚Äôs <code>mc</code> CLI tool we can see the table data written:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">‚ùØ docker exec minio mc ls -r minio/warehouse/

[2024-01-25 14:02:35 UTC]   428B STANDARD t_foo_fs/part-d79f78ef-510e-4fdc-b055-ee121f2be352-0-0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now let‚Äôs look at the catalog.
I‚Äôm using the same Hive Metastore container as we launched above, which stores the data in an DerbyDB.
We can copy this out of the container and onto our local machine for inspection using the <code>ij</code> tool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">‚ùØ docker cp hms-standalone:/tmp/metastore_db /tmp/hms_db
Successfully copied 7.11MB to /tmp/hms_db

‚ùØ rlwrap ij
ij version 10.17
ij&gt; connect &#39;jdbc:derby:/tmp/hms_db&#39;;

ij&gt; SHOW TABLE IN app;
TABLE_SCHEM         |TABLE_NAME                    |REMARKS
------------------------------------------------------------------------
APP                 |AUX_TABLE                     |
APP                 |BUCKETING_COLS                |
APP                 |CDS                           |
APP                 |COLUMNS                       |
APP                 |DBS.                          |
[‚Ä¶]

ij&gt; SELECT db_id, name FROM dbs;
DB_ID               |NAME
-----------------------------------------------------------------------------------------------------------------------------------------------------
1                   |default
2                   |db_rmoff
ij&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ij</code> is a bit clunky when it comes to pretty output (e.g.
rows are very wide and not helpfully formatted based on the width of the data) so let‚Äôs use <a href="https://dbeaver.io/">DBeaver</a>  to speed things up and look at the table we created.
Helpfully it can also infer the Entity-Relationship diagram automagically to aid our comprehension of the data that the metastore holds:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dc0182a396e17c578ab9_6702c5d80cdc849848eddc43_65cee65efbc3f51a61ae50e6_0MiiyG3mjYIs0lObpUXWeOSnTWgqBLGTBa4lGMe8mLmwgC3BEUNYbrrC4pwtBIMAGlNZPdL0aVwDpszYGgDMgyEo6wWOJgzyJqvxSLmtvD0xsODGnQhfiFcHtfnaY8ITI7QdCFmJJR_Bza83pa0Q7FM.webp" alt="An ERD of the Hive Metastore"/>
</div>
</div>
<div class="paragraph">
<p>Here‚Äôs the table that we created:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dc0082a396e17c578a18_6702c5d80cdc849848eddc47_65cee65da12d5e9cd28aef4d_lHPeDozFWKGDyO5j0goqw0SewjzNIC9efBPO7bXE0pRIABJ0e-CHbVqPeh0Mklq-rP78QBLbGaTE2nesA3B6_0q52oSvZKLvTWmtXuddmTbeC8Vt9aX-wYRB1Z9WeDAOewo_U_qiwc7fz5EVH2WOmMw.webp" alt="A row from the database showing metadata for the table created in Flink"/>
</div>
</div>
<div class="paragraph">
<p>+
I wonder where things like the warehouse path are stored?
Based on the above diagram we can see <code>TABLE_PARAMS</code> so let‚Äôs check that out:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dc0082a396e17c578a1b_6702c5d80cdc849848eddc40_65cee65d2bad840cf4c62c1c_1NGshdbo56ITjgNOWxOcpsbViTkunk4YknS6h8k0x7dl7TiktceHEKaTf7synud776jYQXFsQfd9tR95gKFkfIIhJ7m6B4V1f9Pcm3YfCm2p-tL6HnueGFhvOCqqprhmoDgP0_NEeayd8UzEyoGgzl8.webp" alt="Metadata for the table including location of the data on disk"/>
</div>
</div>
<div class="paragraph">
<p>Here‚Äôs all our metadata for the table, including the location of data on disk, its format, and so on.</p>
</div>
<div class="paragraph">
<p>Phew!
üòÖ That was the Hive Catalog.
There‚Äôs just one more catalog that‚Äôs provided with Flink before we get onto some of the other ones.
Without further ado, let‚Äôs look at the JDBC Catalog.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_flink_jdbc_catalog">The Flink JDBC Catalog&nbsp;<a class="headline-hash" href="#_the_flink_jdbc_catalog">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/jdbc/#jdbc-catalog">JDBC Catalog</a>  in Flink is a bit of an odd one if you‚Äôre coming to it expecting a catalog that holds object definitions in Flink <em>of your creation</em>.
What the JDBC catalog does is expose the <em>existing objects</em> and their data of a target database to Flink.
Which is pretty neat‚Äîit‚Äôs just not what you might assume it does.
With that in mind, let‚Äôs see how it works.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfa82a396e17c578967_6702c5d70cdc849848eddc21_65cee65dcf049d69d181d018_IRKfuQOJXyfJhr6p9lHUN4rMwQER9tjqkf_oVB73wTvK3w_lV4MbeX8tVQHGH5OHBTGPgjJ3B8QfyAnkd1vdL2gGwZ6ydLdwF4zZ6InJNG4UJMIzMmZAWUVGS_doNiPyBe9l6H1PtmxOX-Va9tnyQqk.webp" alt="6717dbfa82a396e17c578967 6702c5d70cdc849848eddc21 65cee65dcf049d69d181d018 IRKfuQOJXyfJhr6p9lHUN4rMwQER9tjqkf oVB73wTvK3w lV4MbeX8tVQHGH5OHBTGPgjJ3B8QfyAnkd1vdL2gGwZ6ydLdwF4zZ6InJNG4UJMIzMmZAWUVGS doNiPyBe9l6H1PtmxOX Va9tnyQqk"/>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_installation_and_configuration_2">Installation and Configuration&nbsp;<a class="headline-hash" href="#_installation_and_configuration_2">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>Fortunately, the dependencies for the JDBC catalog are a <em>lot</em> simpler than Hive‚Äôs.
As with the Hive connector you need to download the JDBC connector separately since it‚Äôs not bundled with the Flink distribution.
You also need the JDBC driver of the database to which you want to connect‚Äîthe docs have a useful reference to the <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/jdbc/#dependencies">download links</a>  for these.</p>
</div>
<div class="paragraph">
<p><em>As of the end of January 2024, Flink 1.18.1 has no released version of the JDBC connector, but with a</em> <a href="https://lists.apache.org/list.html?&lt;a href=" mailto:dev@flink.apache.org"="">dev@flink.apache.org</a>&#34;&gt;release vote underway <em>I‚Äôd expect that to change soon.</em>
<em>The example I‚Äôve done here is using</em> <a href="https://lists.apache.org/list?&lt;a href=" mailto:dev@flink.apache.org"="">dev@flink.apache.org</a>:lte=1M:jdbc&#34;&gt;the third release candidate (RC3) <em>of the JDBC connector.</em></p>
</div>
<div class="paragraph">
<p>So, let‚Äôs download both the required JARs into a new folder under <code>./lib</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">mkdir -p ./lib/jdbc
curl https://repository.apache.org/content/repositories/orgapacheflink-1706/org/apache/flink/flink-connector-jdbc/3.1.2-1.18/flink-connector-jdbc-3.1.2-1.18.jar \
	-o ./lib/jdbc/flink-connector-jdbc-3.1.2-1.18.jar
curl https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.1/postgresql-42.7.1.jar \
	-o ./lib/jdbc/postgresql-42.7.1.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>We also need a database to use.
I‚Äôm using a vanilla Postgres database in a Docker container:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">docker run --rm --name postgres \
           --publish 5432:5432 \
           -e POSTGRES_PASSWORD=postgres \
           -e POSTGRES_USER=postgres postgres \
           postgres</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let‚Äôs create a table with some data in it, with the <code>psql</code> CLI tool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">$ docker exec -it postgres psql --username=postgres
psql (16.0 (Debian 16.0-1.pgdg120+1))
Type &#34;help&#34; for help.

postgres=# CREATE TABLE t_foo (c1 varchar, c2 int);
CREATE TABLE
postgres=# INSERT INTO t_foo VALUES (&#39;a&#39;,42);
INSERT 0 1
postgres=#</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we‚Äôll hook this up to Flink.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_the_jdbc_catalog_in_flink">Using the JDBC Catalog in Flink&nbsp;<a class="headline-hash" href="#_using_the_jdbc_catalog_in_flink">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>With the Flink JDBC connector JAR and JDBC driver in place, we can launch the Flink cluster and SQL Client:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">./bin/start-cluster.sh
./bin/sql-client.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>From the SQL prompt let‚Äôs create the JDBC Catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_jdbc WITH (
   &#39;type&#39;             = &#39;jdbc&#39;,
   &#39;base-url&#39;         = &#39;jdbc:postgresql://localhost:5432&#39;,
   &#39;default-database&#39; = &#39;postgres&#39;,
   &#39;username&#39;         = &#39;postgres&#39;,
   &#39;password&#39;         = &#39;postgres&#39;
);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we can select the catalog as the current one and look at the tables that are defined in it.
These are the tables of the database to which we connected above.
Note that Flink doesn‚Äôt use the concept of schemas so <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/jdbc/#jdbc-catalog-for-postgresql">as noted in the docs</a>  the Postgres schema (<code>public</code> in this example) is prepended to the table name shown in Flink.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; USE CATALOG c_jdbc;
[INFO] Execute statement succeed.

Flink SQL&gt; SHOW TABLES;
+--------------+
|   table name |
+--------------+
| public.t_foo |
+--------------+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Querying the Postgres tables from Flink works as you‚Äôd expect.
Make sure you quote with backticks object names as needed (e.g.
the <code>public.</code> prefix on the Postgres table names):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SELECT * FROM `public.t_foo`;
+----+----+
| c1 | c2 |
+----+----+
|  a | 42 |
+----+----+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we were to change that data over in Postgres:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">postgres=# UPDATE t_foo SET c1=&#39;foo&#39; WHERE c2=42;
UPDATE 1
postgres=# SELECT * FROM t_foo ;
 c1  | c2
-----+----
 foo | 42
(1 row)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And run the same query again in Flink we can see it correctly shows the new data (as you would expect):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SELECT * FROM `public.t_foo`;
+-----+----+
|  c1 | c2 |
+-----+----+
| foo | 42 |
+-----+----+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>When it comes to writing from Flink to the JDBC catalog, we can only write data.
Per <a href="https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/jdbc/#jdbc-catalog">the documentation</a> , the creation of new objects (such as tables) isn‚Äôt supported:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; CREATE TABLE `public.t_new` (c1 varchar, c2 int);
[ERROR] Could not execute SQL statement. Reason:
java.lang.UnsupportedOperationException</code></pre>
</div>
</div>
<div class="paragraph">
<p>But what we <em>can</em> do is write data (as opposed to metadata) back to the database:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; INSERT INTO t_foo VALUES (&#39;Hello from Flink&#39;,42);
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 434d571da8e83976841649be7cdff69c</code></pre>
</div>
</div>
<div class="paragraph">
<p>Which we then see in Postgres:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">postgres=# SELECT * FROM t_foo ;
        c1        | c2
------------------+----
 foo              | 42
 Hello from Flink | 42
(2 rows)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So, there we have it.
Reading and writing from a database with Flink via the JDBC Connector and its JDBC Catalog!
This is going to be pretty handy, whether we want to analyse the data, or use it for joins with data coming from other sources, such as Apache Kafka or other streams of data.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_third_party_flink_catalogs_apache_iceberg">Third-Party Flink Catalogs: Apache Iceberg&nbsp;<a class="headline-hash" href="#_third_party_flink_catalogs_apache_iceberg">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>Flink can be used with many different technologies, including the open-table formats.
Each of these implement a Flink catalog so that you can access and use their objects from Flink directly.
Here I‚Äôll show you Apache Iceberg‚Äôs Flink catalog, with three different metastores, (or backing catalogs, however you like to think of it).
Why three?
Well, to get my head around what was Iceberg, what was Flink, and what was metastore, I needed to try multiple options to understand the pattern.</p>
</div>
<div class="paragraph">
<p>In all of these I‚Äôm using <a href="https://min.io/">MinIO</a>  for storage, which is an S3-compatible object store that can be run locally.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_flink_iceberg_and_hive_metastore">Flink, Iceberg, and Hive Metastore&nbsp;<a class="headline-hash" href="#_flink_iceberg_and_hive_metastore">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>This one was <a href="https://twitter.com/rmoff/status/1753177272045932927">a lot of fun</a>  to figure out.
You can perhaps put a hefty number of air-quotes around that innocently-italicised <code>fun</code>.
üòâ I‚Äôm going to dig into the deluge of dastardly debugging in a subsequent blog‚Äîfor now we‚Äôll just look at things when they go right.</p>
</div>
<div class="paragraph">
<p>Since the focus of my efforts is to understand how Flink SQL can be used by a non-Java person, I‚Äôm also making the assumption that they don‚Äôt have a Hadoop or Hive installation lying around and want to run as much of this standalone locally.
So as above‚Äîwhere we use the Hive Metastore as a Flink catalog directly‚ÄîI‚Äôm using the <a href="https://github.com/recap-build/hive-metastore-standalone">standalone Hive metastore Docker image</a> .
I‚Äôve bundled this up into <a href="https://github.com/decodableco/examples/tree/main/catalogs/flink-iceberg-hive">a GitHub repository with Flink and Iceberg</a>  if you want to try this out.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfc82a396e17c57899e_6702c5d80cdc849848eddc3d_65cee65d808c4f9dde063d3d_G5LL4M1N-CKp-JYquvL9uFSOGOreI41_0KpbG9oI7Gbny-V3dwpYbVnbbeN3MTQOC1ypu73i46VxlFckkMTmsCX9CeupddCWNrIGAmiHXeF-uPauHZbXclDTVYf9N4j7H_XDmRVsaqLmd4LGwdp-hEU.webp" alt="6717dbfc82a396e17c57899e 6702c5d80cdc849848eddc3d 65cee65d808c4f9dde063d3d G5LL4M1N CKp JYquvL9uFSOGOreI41 0KpbG9oI7Gbny V3dwpYbVnbbeN3MTQOC1ypu73i46VxlFckkMTmsCX9CeupddCWNrIGAmiHXeF uPauHZbXclDTVYf9N4j7H XDmRVsaqLmd4LGwdp hEU"/>
</div>
</div>
<div class="paragraph">
<p>The main thing to be aware of is that it‚Äôs not just your Flink instance that will write to MinIO (S3), but the Hive Metastore too (when you create a database, for example).
Therefore you need to add the S3 endpoint and authentication details to <a href="https://github.com/decodableco/examples/blob/main/catalogs/flink-iceberg-hive/hms-standalone-s3/conf/hive-site.xml">the hive-site.xml on the Hive Metastore too</a> ‚Äînot just Flink:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">   fs.s3a.access.key
   admin



   fs.s3a.secret.key
   password



   fs.s3a.endpoint
   http://minio:9000



   fs.s3a.path.style.access
   true</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <a href="https://github.com/decodableco/examples/blob/main/catalogs/flink-iceberg-hive/flink/conf/hive-site.xml">Flink hive-site.xml</a>  needs this too, along with the details of where the Hive Metastore can be found:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">  hive.metastore.local
  false



  hive.metastore.uris
  thrift://hms:9083</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the Hive configuration done, add <a href="https://github.com/decodableco/examples/blob/main/catalogs/flink-iceberg-hive/flink/Dockerfile#L21-L44">the necessary JAR files</a>  to your Flink <code>./lib</code> folder.
You can use subfolders if you want to make it easier to track these; the classpath will recurse through them.</p>
</div>
<div class="paragraph">
<p>Once you‚Äôve launched Flink, MinIO, and the Hive Metastore, you can go ahead and create the Iceberg catalog in Flink from the Flink SQL Client:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_iceberg_hive WITH (
        &#39;type&#39;          = &#39;iceberg&#39;,
        &#39;catalog-type&#39;  = &#39;hive&#39;,
        &#39;warehouse&#39;     = &#39;s3a://warehouse&#39;,
        &#39;hive-conf-dir&#39; = &#39;./conf&#39;);</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are a couple of important points to be aware of here.
Firstly, the <code>warehouse</code> path defines where both the table data <em>and metadata</em> is held.
That‚Äôs a storage choice made by the <a href="https://iceberg.apache.org/spec/">Iceberg format</a> , enhancing its portability and interoperability by not having its metadata tied into a particular backend.</p>
</div>
<div class="paragraph">
<p>The second thing to note in the catalog configuration is that it‚Äôs incomplete; we‚Äôre pointing to a second set of configuration held in the <code>hive-site.xml</code> file using the <code>hive-conf-dir</code> parameter.
This is where, as I mentioned above, the authentication and connection details for S3 are held.
We could even move <code>warehouse</code> into this and out of the <code>CREATE CATALOG</code> DDL, but I prefer it here for clarity.</p>
</div>
<div class="paragraph">
<p>Now we can create a database within this catalog, and tell Flink to use it for subsequent commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE DATABASE `c_iceberg_hive`.`db_rmoff`;
USE `c_iceberg_hive`.`db_rmoff`;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let‚Äôs go ahead and create an Iceberg table and add some data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE TABLE t_foo (c1 varchar, c2 int);
INSERT INTO t_foo VALUES (&#39;a&#39;, 42);</code></pre>
</div>
</div>
<div class="paragraph">
<p>To complete the end-to-end check, we can read the data back:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SELECT * FROM t_foo;
+----+----+
| c1 | c2 |
+----+----+
|  a | 42 |
+----+----+
1 row in set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let‚Äôs look at the data that‚Äôs been written to MinIO:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ docker exec minio mc ls -r minio/warehouse/
[2024-02-02 21:30:22 UTC]   608B STANDARD db_rmoff.db/t_foo/data/00000-0-41e6f635-3859-46ef-a57e-de5f774203fa-00001.parquet
[2024-02-02 21:30:08 UTC]   957B STANDARD db_rmoff.db/t_foo/metadata/00000-109580b8-77eb-45d5-b2a7-bd63bd239c99.metadata.json
[2024-02-02 21:30:23 UTC] 2.1KiB STANDARD db_rmoff.db/t_foo/metadata/00001-e5705f33-a446-4614-ba66-80a40e176318.metadata.json
[2024-02-02 21:30:23 UTC] 6.5KiB STANDARD db_rmoff.db/t_foo/metadata/3485210c-2c99-4c72-bb36-030c8e0a4a90-m0.avro
[2024-02-02 21:30:23 UTC] 4.2KiB STANDARD db_rmoff.db/t_foo/metadata/snap-125388589100921283-1-3485210c-2c99-4c72-bb36-030c8e0a4a90.avro</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can see here in practice how we have both <code>/data</code> <em>and</em> <code>/metadata</code>.
The metadata files hold, unsurprisingly, metadata:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ docker exec minio mc head minio/warehouse/db_rmoff.db/t_foo/metadata/00000-57d8f913-9e90-4446-a049-db084d17e49d.metadata.json
\\{
  &#34;format-version&#34; : 2,
  &#34;table-uuid&#34; : &#34;5bbf14cb-fbf8-4e10-9809-08854b1048a0&#34;,
  &#34;location&#34; : &#34;s3a://warehouse/db_rmoff.db/t_foo&#34;,
  &#34;last-sequence-number&#34; : 0,
  &#34;last-updated-ms&#34; : 1707132674956,
  &#34;last-column-id&#34; : 2,
  &#34;current-schema-id&#34; : 0,
  &#34;schemas&#34; : [ {
    &#34;type&#34; : &#34;struct&#34;,
    [‚Ä¶]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Whilst the data on disk itself is just a parquet file, which we can validate using DuckDB to read it once we‚Äôve fetched it from MinIO:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">$ docker exec minio mc \
   cat minio/warehouse/db_rmoff.db/t_foo/data/00000-0-e4327b65-69ac-40bc-8e90-aae40dc607c7-00001.parquet \
    &gt; /tmp/data.parquet &amp;&amp; \
    duckdb :memory: &#34;SELECT * FROM read_parquet(&#39;/tmp/data.parquet&#39;)&#34;
-- Loading resources from /Users/rmoff/.duckdbrc
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
|   c1    |  c2   |
| varchar | int32 |
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
| a       |    42 |
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
</div>
</div>
<div class="paragraph">
<p>How does Flink know to go to the bucket called <code>warehouse</code> and path <code>db_rmoff.db/t_foo/[‚Ä¶]</code> to find the data and metadata for the table?
That‚Äôs where the Catalog comes in.
The Hive metastore‚Äîin this case‚Äîholds the magical metadata of this relationship, which we can see if we query the embedded DerbyDB:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">ij&gt; SELECT DB.&#34;NAME&#34;,
	       DB.DB_LOCATION_URI,
	       TB.TBL_NAME,
	       TBP.PARAM_VALUE
	FROM   APP.DBS DB
	       INNER JOIN APP.TBLS TB
	               ON DB.DB_ID = TB.DB_ID
	       INNER JOIN APP.TABLE_PARAMS TBP
	               ON TB.TBL_ID = TBP.TBL_ID
	WHERE  TBP.PARAM_KEY = &#39;metadata_location&#39;;

NAME    |DB_LOCATION_URI            |TBL_NAME|PARAM_VALUE                                                                                         |
--------+---------------------------+--------+----------------------------------------------------------------------------------------------------+
db_rmoff|s3a://warehouse/db_rmoff.db|t_foo   |s3a://warehouse/t_foo/metadata/00000-5946940a-04fa-4a60-9bc9-b83db818560a.metadata.json</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_flink_iceberg_and_dynamodb_metastore">Flink, Iceberg, and DynamoDB Metastore&nbsp;<a class="headline-hash" href="#_flink_iceberg_and_dynamodb_metastore">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>This permutation is obviously‚Äîgiven the use of DynamoDB‚Äîdesigned for when you‚Äôre running Flink on AWS, perhaps with EMR.
My thanks to <a href="https://lazypro.medium.com/">Chunting Wu</a>  who published an <a href="https://lazypro.medium.com/getting-started-with-flink-sql-apache-iceberg-and-dynamodb-catalog-71b96817e3c3">article</a>  and corresponding <a href="https://github.com/wirelessr/flink-iceberg-playground">GitHub repo</a>  that shows how to get this up and running.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfe82a396e17c5789d2_6702c5d80cdc849848eddd18_65cee65d3ea703298e022bb1_Lj7PLCI5xr1LiHFwVLHFfvCU1xK_TaeIsil_EVBULoq3EEY6gy3J1OP-I9E8nwgTBT29_iUAPTRBBSTZi2Bx0LcrKFvvoFP1g0uEyCoKC6aSY-JXjNX7RUGmcqAETeXauXExTtgjryJG5oEg1KDKxIw.webp" alt="6717dbfe82a396e17c5789d2 6702c5d80cdc849848eddd18 65cee65d3ea703298e022bb1 Lj7PLCI5xr1LiHFwVLHFfvCU1xK TaeIsil EVBULoq3EEY6gy3J1OP I9E8nwgTBT29 iUAPTRBBSTZi2Bx0LcrKFvvoFP1g0uEyCoKC6aSY JXjNX7RUGmcqAETeXauXExTtgjryJG5oEg1KDKxIw"/>
</div>
</div>
<div class="paragraph">
<p>From the SQL Client, we create the Iceberg catalog with DynamoDB as the metastore.
Note the use of <code>catalog-impl</code> rather than <code>catalog-type</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_iceberg_dynamo WITH (
&#39;type&#39;                 = &#39;iceberg&#39;,
&#39;io-impl&#39;              = &#39;org.apache.iceberg.aws.s3.S3FileIO&#39;,
&#39;catalog-impl&#39;         = &#39;org.apache.iceberg.aws.dynamodb.DynamoDbCatalog&#39;,
&#39;dynamodb.table-name&#39;  = &#39;iceberg-catalog&#39;,
&#39;dynamodb.endpoint&#39;    = &#39;http://dynamodb-local:8000&#39;,
&#39;warehouse&#39;            = &#39;s3://warehouse&#39;,
&#39;s3.endpoint&#39;          = &#39;http://storage:9000&#39;,
&#39;s3.path-style-access&#39; = &#39;true&#39;);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now create a database in the new catalog and set it as the current one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE DATABASE c_iceberg_dynamo.db_rmoff;
USE c_iceberg_dynamo.db_rmoff;</code></pre>
</div>
</div>
<div class="paragraph">
<p>With that done we can create a table and some data in it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE TABLE t_foo (c1 varchar, c2 int);
INSERT INTO  t_foo VALUES (&#39;a&#39;, 42);</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check the data has been persisted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">Flink SQL&gt; SELECT * FROM t_foo;
+----+--------------------------------+-------------+
| op |                             c1 |          c2 |
+----+--------------------------------+-------------+
| +I |                              a |          42 |
+----+--------------------------------+-------------+
Received a total of 1 row</code></pre>
</div>
</div>
<div class="paragraph">
<p>This all looks good!
As you‚Äôd expect, the data and metadata written to disk is the same as above when we use the Hive Metastore‚Äîbecause all we‚Äôre changing out here is the metastore layer, everything else is the same.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ docker exec mc bash -c &#34;mc ls -r minio/warehouse/db_rmoff.db&#34;
[2024-01-25 17:38:45 UTC]   626B STANDARD t_foo/data/00000-0-048a9bc4-a071-4f5e-a583-f928fce83395-00001.parquet
[2024-01-25 17:38:36 UTC] 1.2KiB STANDARD t_foo/metadata/00000-b3eb6977-2a18-446a-8280-cbccdc61d13e.metadata.json
[2024-01-25 17:38:45 UTC] 2.3KiB STANDARD t_foo/metadata/00001-fa3a16bf-a8be-4d2a-81ec-171f3f4ef8e2.metadata.json
[2024-01-25 17:38:45 UTC] 5.6KiB STANDARD t_foo/metadata/ac3ed4d0-4b94-4666-994f-71ab6e5d0ea7-m0.avro
[2024-01-25 17:38:45 UTC] 3.7KiB STANDARD t_foo/metadata/snap-7271853754385708270-1-ac3ed4d0-4b94-4666-994f-71ab6e5d0ea7.avro</code></pre>
</div>
</div>
<div class="paragraph">
<p>Whilst the Hive metastore used a relational database to store metadata about the Iceberg table, we can see how the same set of data is stored in DynamoDB by using <a href="https://github.com/aaronshaf/dynamodb-admin">dynamodb-admin</a> :</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfc82a396e17c578998_6702c5d80cdc849848eddc33_65cee65d05934c679206d635_q_ogz5LMokJbXKe-x1QWYRU9CeV_ejjpgqSkw7kz8KSZMBoHkdwsaOIGiyVIPby1KPEcWJNCO6I7LrVwUrzFAukGVtpTlW7C3D8AQGyvpkVt2oxO3_6SHYTNv-Kn7L_OgQMZ90Vm9RPNZOirdkkDagI.webp" alt="CleanShot 2024-01-09 at 17.40.57.png"/>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dc0082a396e17c578a8c_6702c5d80cdc849848eddd15_65cee65dbe7c4a66f3a00500_sWXKyyjS37AS1vIBMRfANnz5x45s00wdnZoSK0igyxDJf3VehwgLMP1Ckcqd4wqYkcRrC9orksyJEPMMz8j1nXhBJ00CqF8o74XmtAHj2YlrR40b8Fr-qFqCgLVlVnjY2RU0W5sc3YLaApj4r9OHk20.webp" alt="CleanShot 2024-01-09 at 17.40.29.png"/>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_flink_iceberg_and_jdbc_metastore">Flink, Iceberg, and JDBC Metastore&nbsp;<a class="headline-hash" href="#_flink_iceberg_and_jdbc_metastore">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>Iceberg actually supports <a href="https://iceberg.apache.org/javadoc/latest/org/apache/iceberg/BaseMetastoreCatalog.html">9 catalog types</a> , but don‚Äôt worry‚ÄîI‚Äôm not going to go through each one üòÖ.
We‚Äôve already got a handle on the pattern here:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Flink tables are written with both metadata and data to storage (MinIO in our case).</p>
</li>
<li>
<p>Metadata about those tables is held in a Catalog metastore which is persisted somewhere specific to that metastore.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The JDBC Catalog uses a JDBC-compatible database - in the example below, Postgres.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/2024/02/6717dbfe82a396e17c5789cf_6702c5d80cdc849848eddc3a_65cee65dc7c1b1f5e5aac305__zudXNXyezBeowOCGRDy43v2TCz225r3Bvt8f7RiOlg_ogfVhQO3o6KDxnvjfGZv_6ZxgbpN4HfoT9xQEUDp_SnAu-DLlXh-Eo1hrzLLk5hM3a2S0ARH0zyGh4a2W_dCwETPnNv_qduAvnRJU1JgBY8.webp" alt="6717dbfe82a396e17c5789cf 6702c5d80cdc849848eddc3a 65cee65dc7c1b1f5e5aac305  zudXNXyezBeowOCGRDy43v2TCz225r3Bvt8f7RiOlg ogfVhQO3o6KDxnvjfGZv 6ZxgbpN4HfoT9xQEUDp SnAu DLlXh Eo1hrzLLk5hM3a2S0ARH0zyGh4a2W dCwETPnNv qduAvnRJU1JgBY8"/>
</div>
</div>
<div class="paragraph">
<p>In terms of <a href="https://github.com/decodableco/examples/blob/main/catalogs/flink-iceberg-jdbc/flink/Dockerfile#L13-L37">dependencies</a>  you need</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Flink S3 plugin</p>
</li>
<li>
<p>JDBC Driver for your database</p>
</li>
<li>
<p>Iceberg JARs</p>
</li>
<li>
<p>AWS S3 JARs</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can find the <a href="https://github.com/decodableco/examples/tree/main/catalogs/flink-iceberg-jdbc">full example on GitHub</a> .</p>
</div>
<div class="paragraph">
<p>I set the credentials for the S3 storage <a href="https://github.com/decodableco/examples/blob/main/catalogs/flink-iceberg-jdbc/">as environment variables</a> ‚Äîthere is probably a better way to do this.</p>
</div>
<div class="paragraph">
<p>Let‚Äôs go ahead and create the catalog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE CATALOG c_iceberg_jdbc WITH (
   &#39;type&#39;                 = &#39;iceberg&#39;,
   &#39;io-impl&#39;              = &#39;org.apache.iceberg.aws.s3.S3FileIO&#39;,
   &#39;warehouse&#39;            = &#39;s3://warehouse&#39;,
   &#39;s3.endpoint&#39;          = &#39;http://minio:9000&#39;,
   &#39;s3.path-style-access&#39; = &#39;true&#39;,
   &#39;catalog-impl&#39;         = &#39;org.apache.iceberg.jdbc.JdbcCatalog&#39;,
   &#39;uri&#39;                  =&#39;jdbc:postgresql://postgres:5432/?user=dba&amp;password=rules&#39;);</code></pre>
</div>
</div>
<div class="paragraph">
<p>You know the drill by now‚Äîcreate the database, set it as current, create the table and populate it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">CREATE DATABASE `c_iceberg_jdbc`.`db01`;
USE `c_iceberg_jdbc`.`db01`;
CREATE TABLE t_foo (c1 varchar, c2 int);
INSERT INTO t_foo VALUES (&#39;a&#39;,42);</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Iceberg table written to MinIO (S3) is as before - a mixture of <code>/data</code> and <code>/metadata</code>.
The difference this time round is where we‚Äôre storing the catalog.
Querying Postgres shows us the metastore tables:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">dba=# \dt
                   List of relations
 Schema |             Name             | Type  | Owner
--------+------------------------------+-------+-------
 public | iceberg_namespace_properties | table | dba
 public | iceberg_tables               | table | dba
(2 rows)

dba=# \d iceberg_tables
                             Table &#34;public.iceberg_tables&#34;
           Column           |          Type           | Collation | Nullable | Default
----------------------------+-------------------------+-----------+----------+---------
 catalog_name               | character varying(255)  |           | not null |
 table_namespace            | character varying(255)  |           | not null |
 table_name                 | character varying(255)  |           | not null |
 metadata_location          | character varying(1000) |           |          |
 previous_metadata_location | character varying(1000) |           |          |
Indexes:
    &#34;iceberg_tables_pkey&#34; PRIMARY KEY, btree (catalog_name, table_namespace, table_name)</code></pre>
</div>
</div>
<div class="paragraph">
<p>And the table‚Äôs metadata itself:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sql" data-lang="sql">dba=# SELECT * FROM iceberg_tables;

catalog_name    | table_namespace | table_name | metadata_location                                                                           | previous_metadata_location
----------------+-----------------+------------+---------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------
c_iceberg_jdbc  | db01            | t_foo      | s3://warehouse/db01/t_foo/metadata/00001-bdf5e336-36c1-4531-b6bf-9d90821bc94d.metadata.json | s3://warehouse/db01/t_foo/metadata/00000-a81cb608-6e46-42ab-a943-81230ad90b3d.metadata.json

(1 row)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_in_conclusion">In Conclusion‚Ä¶&nbsp;<a class="headline-hash" href="#_in_conclusion">üîó</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you‚Äôve stuck with me this far, well done!
üôÇ My aim was not to put you through the same pain as I had in traversing this, but to summarise the key constants and variables when using the different components and catalogs.</p>
</div>
<div class="paragraph">
<p>Stay tuned to this blog for my <a href="/2024/02/27/flink-sql-and-the-joy-of-jars/">next</a> <a href="/2024/08/06/troubleshooting-flink-sql-s3-problems/">posts</a> which will be a look at some of the troubleshooting techniques that can be useful when exploring Flink SQL.</p>
</div>
</div>
</div>
	<hr>
	<div style="background-color: rgba(204, 234, 255, 0.25); margin-bottom:50px;margin-top:50px;padding: 20px; border-width: 2px; border-style: solid; border-color: darkorange;">
	
		<script src="https://giscus.app/client.js"
				data-repo="rmoff/rmoff-blog"
				data-repo-id="MDEwOlJlcG9zaXRvcnkxNTE3NDg2MTE="
				data-category="Announcements"
				data-category-id="DIC_kwDOCQuAA84CvP5T"
				data-mapping="pathname"
				data-strict="1"
				data-reactions-enabled="1"
				data-emit-metadata="0"
				data-input-position="bottom"
				data-theme="light"
				data-lang="en"
				crossorigin="anonymous"
				async>
		</script>
		
	</div>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr>
<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt"></p>
<p><a href="https://bsky.app/profile/rmoff.net"><b class="fa-brands fa-bluesky"></b></a>  <em>Robin Moffatt works on the DevRel team at Confluent. He likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</em></p>

				
			
		
		</div>
		
	</div>

		

</article>
      </main>
    
      
      <div class="docs-toc">
        <ul class="nav toc-top">
          <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
        </ul>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#_using_the_hive_catalog_with_flink_sql">Using the Hive Catalog with Flink SQL</a></li>
    <li><a href="#_installation_and_configuration">Installation and Configuration</a>
      <ul>
        <li><a href="#_this_is_where_it_gets_fun_hadoop_dependency">This is where it gets &#34;fun&#34;: Hadoop Dependency</a></li>
      </ul>
    </li>
    <li><a href="#_sql_client_with_the_hive_catalog">SQL Client with the Hive Catalog</a></li>
    <li><a href="#_sidenote_digging_a_bit_deeper_into_the_hive_metastore">Sidenote: Digging a bit Deeper into the Hive Metastore</a></li>
    <li><a href="#_what_does_the_hive_catalog_look_like_when_storing_parquet_data_in_s3_minio_from_flink">What does the Hive catalog look like when storing Parquet data in S3 (MinIO) from Flink?</a>
      <ul>
        <li><a href="#_setup">Setup</a></li>
        <li><a href="#_using_parquet_and_s3_from_flink">Using Parquet and S3 from Flink</a></li>
      </ul>
    </li>
    <li><a href="#_the_flink_jdbc_catalog">The Flink JDBC Catalog</a></li>
    <li><a href="#_installation_and_configuration_2">Installation and Configuration</a></li>
    <li><a href="#_using_the_jdbc_catalog_in_flink">Using the JDBC Catalog in Flink</a></li>
    <li><a href="#_third_party_flink_catalogs_apache_iceberg">Third-Party Flink Catalogs: Apache Iceberg</a></li>
    <li><a href="#_flink_iceberg_and_hive_metastore">Flink, Iceberg, and Hive Metastore</a></li>
    <li><a href="#_flink_iceberg_and_dynamodb_metastore">Flink, Iceberg, and DynamoDB Metastore</a></li>
    <li><a href="#_flink_iceberg_and_jdbc_metastore">Flink, Iceberg, and JDBC Metastore</a></li>
    <li><a href="#_in_conclusion">In Conclusion‚Ä¶</a></li>
  </ul>
</nav>
      </div>
      
      <div class="toc-mobile-label">TABLE OF CONTENTS</div>
      
    
    </div>
  </div>
</div>


		</main>
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.net/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2025 
			</p>
		</footer>
		
	</body>
</html>
