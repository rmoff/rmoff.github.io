<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Using Delta from pySpark - &lt;code&gt;java.lang.ClassNotFoundException: delta.DefaultSource&lt;/code&gt;</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.net/index.xml">
		<link rel="canonical" href="https://rmoff.net/2023/04/05/using-delta-from-pyspark-java.lang.classnotfoundexception-delta.defaultsource/">
		
		
		
		
		
		<meta property="og:title" content="Using Delta from pySpark - java.lang.ClassNotFoundException: delta.DefaultSource" />
		<meta property="og:type" content="article" />
		<meta property="og:image" content="https://rmoff.net/images/2023/04/h_IMG_7944.jpeg" />
		<meta property="og:description" content="" />
		<meta property="og:url" content="https://rmoff.net/2023/04/05/using-delta-from-pyspark-java.lang.classnotfoundexception-delta.defaultsource/" />
		<meta property="og:site_name" content="Using Delta from pySpark - java.lang.ClassNotFoundException: delta.DefaultSource" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.net/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/descartes.css" />
		
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		

		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.net/js/story.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.net/images/2023/04/h_IMG_7944.jpeg'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.net" title="Home">
						<img src="https://rmoff.net/img/logo.jpg" class="w2 h2 br-100" alt="rmoff&#39;s random ramblings" />
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net/bio">about</a>
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://www.youtube.com/c/rmoff"><i class="fab fa-youtube-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://data-folks.masto.host/@rmoff"><i class="fab fa-mastodon"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/rmoff/"><i class="fab fa-twitter-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.net/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.net/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">Using Delta from pySpark - <code>java.lang.ClassNotFoundException: delta.DefaultSource</code></h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2023-04-05T15:51:41Z">Apr 5, 2023</time>
								<span class="display-print">by </span>
								 in <a href="https://rmoff.net/categories/pyspark" class="no-underline category near-white dim">PySpark</a>, <a href="https://rmoff.net/categories/delta-lake" class="no-underline category near-white dim">Delta Lake</a>
								<span class="display-print">at https://rmoff.net/2023/04/05/using-delta-from-pyspark-java.lang.classnotfoundexception-delta.defaultsource/</span>
							
						
					</h2>
				</div>

				
				
				
				<div class="w-100 cf hide-print">
					<a class="fr f6 ma0 pa2 link white-50 dim fas fa-camera" href="https://twitter.com/rmoff/" title="Photo Credit"></a>
				</div>
				
				

			</div>
		</header>
		
		<main role="main">
		
<article class="center bg-white br-3 pv1 ph4 lh-copy f5 nested-links mw8">
	<p>No great insights in this post, just something for folk who Google this error after me and don&rsquo;t want to waste three hours chasing their tailsâ€¦ ðŸ˜„</p>
<p>I wanted to use Delta Lake with <a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a> from within a Jupyter Notebook. Easy, right? Not if you&rsquo;re like me and perhaps are new to it and rely on copy and paste of snippets you find across the internet to start with.</p>
<p>Whatever I tried, I kept hitting this error:</p>
<pre tabindex="0"><code>Py4JJavaError: An error occurred while calling o45.save.
: java.lang.ClassNotFoundException: 
Failed to find data source: delta.
</code></pre><p><strong>In short, the problem was that I was creating both a <code>SparkSession</code> <em>and</em> a <code>SparkContext</code></strong>. I honestly don&rsquo;t understand enough about Spark to tell you why this causes the error, but through a lot of painful trial and error I can tell you that it does. <em>Someone more knowledgable than me can perhaps tell me (<a href="mailto:robin@rmoff.net">email</a> / <a href="https://twitter.com/rmoff/">twitter</a> / <a href="https://data-folks.masto.host/@rmoff">mastodon</a>) why this is and if what I&rsquo;ve ended up with is the right code</em>. <strong>UPDATE: Damon Cortesi explained it to me :) See <a href="#why-did-it-do-what-it-did">below</a> for details.</strong></p>
<p>Here&rsquo;re the salient points of the Jupyter notebook:</p>
<h2 id="versions-and-stuff">Versions and stuff&nbsp;<a class="headline-hash" href="#versions-and-stuff">ðŸ”—</a> </h2>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">sys</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;Kernel:&#34;</span>, sys<span style="color:#666">.</span>executable)
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;Python version:&#34;</span>, sys<span style="color:#666">.</span>version)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">pyspark</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;PySpark version:&#34;</span>, pyspark<span style="color:#666">.</span>__version__)
</span></span></code></pre></div><pre><code>Kernel: /opt/conda/bin/python
Python version: 3.9.7 | packaged by conda-forge | (default, Oct 10 2021, 15:08:54)
[GCC 9.4.0]
PySpark version: 3.2.0
</code></pre>
<h2 id="this-worked">This worked&nbsp;<a class="headline-hash" href="#this-worked">ðŸ”—</a> </h2>
<h3 id="initialise-spark-with-delta-lake-config">Initialise Spark with Delta Lake config&nbsp;<a class="headline-hash" href="#initialise-spark-with-delta-lake-config">ðŸ”—</a> </h3>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.context</span> <span style="color:#008000;font-weight:bold">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark</span> <span style="color:#008000;font-weight:bold">import</span> SparkFiles
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.sql.session</span> <span style="color:#008000;font-weight:bold">import</span> SparkSession
</span></span><span style="display:flex;"><span>spark <span style="color:#666">=</span> (
</span></span><span style="display:flex;"><span>    SparkSession<span style="color:#666">.</span>builder<span style="color:#666">.</span>master(<span style="color:#ba2121">&#34;local[*]&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.jars.packages&#34;</span>, <span style="color:#ba2121">&#34;io.delta:delta-core_2.12:2.0.0&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.extensions&#34;</span>, <span style="color:#ba2121">&#34;io.delta.sql.DeltaSparkSessionExtension&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.catalog.spark_catalog&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.catalog.DeltaCatalog&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.delta.logStore.class&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.storage.S3SingleDriverLogStore&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="test-delta">Test delta&nbsp;<a class="headline-hash" href="#test-delta">ðŸ”—</a> </h3>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="color:#666">=</span> spark<span style="color:#666">.</span>range(<span style="color:#666">0</span>, <span style="color:#666">5</span>)
</span></span><span style="display:flex;"><span>data<span style="color:#666">.</span>write<span style="color:#666">.</span>format(<span style="color:#ba2121">&#34;delta&#34;</span>)<span style="color:#666">.</span>save(<span style="color:#ba2121">&#34;/tmp/delta-table2&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#666">=</span> spark<span style="color:#666">.</span>read<span style="color:#666">.</span>format(<span style="color:#ba2121">&#34;delta&#34;</span>)<span style="color:#666">.</span>load(<span style="color:#ba2121">&#34;/tmp/delta-table2&#34;</span>)
</span></span><span style="display:flex;"><span>df<span style="color:#666">.</span>show()
</span></span></code></pre></div><pre><code>+---+
| id|
+---+
|  2|
|  1|
|  4|
|  3|
|  0|
+---+
</code></pre>
<h2 id="this-didnt-work">This didn&rsquo;t work&nbsp;<a class="headline-hash" href="#this-didnt-work">ðŸ”—</a> </h2>
<h3 id="initialise-spark-with-delta-lake-config-1">Initialise Spark with Delta Lake config&nbsp;<a class="headline-hash" href="#initialise-spark-with-delta-lake-config-1">ðŸ”—</a> </h3>
<p>(notice line 5 sets the <code>SparkContext</code>, unlike the example above)</p>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.context</span> <span style="color:#008000;font-weight:bold">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark</span> <span style="color:#008000;font-weight:bold">import</span> SparkFiles
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.sql.session</span> <span style="color:#008000;font-weight:bold">import</span> SparkSession
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sc <span style="color:#666">=</span> SparkContext(<span style="color:#ba2121">&#39;local[*]&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#666">=</span> (
</span></span><span style="display:flex;"><span>    SparkSession<span style="color:#666">.</span>builder<span style="color:#666">.</span>master(<span style="color:#ba2121">&#34;local[*]&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.jars.packages&#34;</span>, <span style="color:#ba2121">&#34;io.delta:delta-core_2.12:2.0.0&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.extensions&#34;</span>, <span style="color:#ba2121">&#34;io.delta.sql.DeltaSparkSessionExtension&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.catalog.spark_catalog&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.catalog.DeltaCatalog&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.delta.logStore.class&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.storage.S3SingleDriverLogStore&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>)        
</span></span></code></pre></div><h3 id="test-delta-1">Test delta&nbsp;<a class="headline-hash" href="#test-delta-1">ðŸ”—</a> </h3>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>data <span style="color:#666">=</span> spark<span style="color:#666">.</span>range(<span style="color:#666">0</span>, <span style="color:#666">5</span>)
</span></span><span style="display:flex;"><span>data<span style="color:#666">.</span>write<span style="color:#666">.</span>format(<span style="color:#ba2121">&#34;delta&#34;</span>)<span style="color:#666">.</span>save(<span style="color:#ba2121">&#34;/tmp/delta-table&#34;</span>)
</span></span></code></pre></div><pre><code>---------------------------------------------------------------------------

Py4JJavaError                             Traceback (most recent call last)

/tmp/ipykernel_983/939553335.py in &lt;module&gt;
      1 data = spark.range(0, 5)
----&gt; 2 data.write.format(&quot;delta&quot;).save(&quot;/tmp/delta-table&quot;)


/usr/local/spark/python/pyspark/sql/readwriter.py in save(self, path, format, mode, partitionBy, **options)
    738             self._jwrite.save()
    739         else:
--&gt; 740             self._jwrite.save(path)
    741 
    742     @since(1.4)


/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1307 
   1308         answer = self.gateway_client.send_command(command)
-&gt; 1309         return_value = get_return_value(
   1310             answer, self.gateway_client, self.target_id, self.name)
   1311 


/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
    109     def deco(*a, **kw):
    110         try:
--&gt; 111             return f(*a, **kw)
    112         except py4j.protocol.Py4JJavaError as e:
    113             converted = convert_exception(e.java_exception)


/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--&gt; 326                 raise Py4JJavaError(
    327                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    328                     format(target_id, &quot;.&quot;, name), value)


Py4JJavaError: An error occurred while calling o45.save.
: java.lang.ClassNotFoundException: 
Failed to find data source: delta. Please find packages at
http://spark.apache.org/third-party-projects.html
       
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:852)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)
	... 16 more
</code></pre>
<h2 id="notebook-log">Notebook Log&nbsp;<a class="headline-hash" href="#notebook-log">ðŸ”—</a> </h2>
<p>I did notice in the notebook that in the version I ran without setting <code>SparkContext</code> the Delta library was downloaded:</p>
<pre tabindex="0"><code>WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
:: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/jovyan/.ivy2/cache
The jars for the packages stored in: /home/jovyan/.ivy2/jars
io.delta#delta-core_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-86ca6813-f39f-472c-b6a2-dfe988ab0404;1.0
    confs: [default]
    found io.delta#delta-core_2.12;2.0.0 in central
    found io.delta#delta-storage;2.0.0 in central
    found org.antlr#antlr4-runtime;4.8 in central
    found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
:: resolution report :: resolve 94ms :: artifacts dl 4ms
    :: modules in use:
    io.delta#delta-core_2.12;2.0.0 from central in [default]
    io.delta#delta-storage;2.0.0 from central in [default]
    org.antlr#antlr4-runtime;4.8 from central in [default]
    org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
    ---------------------------------------------------------------------
    |                  |            modules            ||   artifacts   |
    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
    ---------------------------------------------------------------------
    |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
    ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-86ca6813-f39f-472c-b6a2-dfe988ab0404
    confs: [default]
    0 artifacts copied, 4 already retrieved (0kB/3ms)
23/04/05 16:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &#34;WARN&#34;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
</code></pre><p>whilst the version that did set <code>SparkContext</code> didn&rsquo;t.</p>
<pre tabindex="0"><code>WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &#34;WARN&#34;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/04/05 16:30:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/04/05 16:30:36 WARN Utils: Service &#39;SparkUI&#39; could not bind on port 4040. Attempting port 4041.
</code></pre><h2 id="why-did-it-do-what-it-did">Why Did It Do What It Did?&nbsp;<a class="headline-hash" href="#why-did-it-do-what-it-did">ðŸ”—</a> </h2>
<p>Courtesy of <a href="https://www.linkedin.com/feed/update/urn:li:activity:7049423288099319809?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7049423288099319809%2C7049433950406021120%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287049433950406021120%2Curn%3Ali%3Aactivity%3A7049423288099319809%29">Damon Cortesi</a>:</p>
<blockquote>
<p>In the example that doesn&rsquo;t work, you explicitly create a <code>SparkContext</code> first with <code>sc = SparkContext('local[*]')</code>.</p>
<p>When you use <code>SparkSession.builder</code>&hellip;<code>getOrCreate()</code>, it reuses the <code>SparkContext</code> you already created. You should be able to see this by running <code>spark.sparkContext</code>. That <code>SparkContext</code> unfortunately doesn&rsquo;t have the config variables you specified and, based on some reason I don&rsquo;t totally understand, the config variables you specify later are not updated. I&rsquo;m guessing this is because <code>SparkContext</code> spins up a JVM and some options (like <code>spark.jars.packages</code>) would need to be specified before you spin up the JVM.</p>
<p>In the example that works, it doesn&rsquo;t have a <code>SparkContext</code> to reuse, so it creates a one using the config you provided.</p>
<p>ðŸ˜… I love Spark! /s</p>
<p>This post does a pretty good job of explaining what&rsquo;s going on: <a href="https://medium.com/@achilleus/spark-session-10d0d66d1d24">A tale of Spark Session and Spark Context</a></p>
</blockquote>
<h2 id="proving-it-to-myself">Proving It To Myself&nbsp;<a class="headline-hash" href="#proving-it-to-myself">ðŸ”—</a> </h2>
<p>Damon&rsquo;s explanation and the linked blog were good, so to close the loop I wanted to prove to myself that I could reproduce this explanation locally. Here&rsquo;s <a href="https://gist.github.com/rmoff/1d86204b559f8ffce83be4b3206b1fa0">the notebook itself if you want to try it</a> and reproduced here too:</p>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">sys</span>
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">pyspark</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;Kernel:&#34;</span>, sys<span style="color:#666">.</span>executable)
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;Python version:&#34;</span>, sys<span style="color:#666">.</span>version)
</span></span><span style="display:flex;"><span><span style="color:#008000">print</span>(<span style="color:#ba2121">&#34;PySpark version:&#34;</span>, pyspark<span style="color:#666">.</span>__version__)
</span></span></code></pre></div><pre><code>Kernel: /opt/conda/bin/python
Python version: 3.9.7 | packaged by conda-forge | (default, Oct 10 2021, 15:08:54) 
[GCC 9.4.0]
PySpark version: 3.2.0
</code></pre>
<h2 id="spark-context-and-session---no-config-to-pick-up">Spark Context and Session - no config to pick up&nbsp;<a class="headline-hash" href="#spark-context-and-session---no-config-to-pick-up">ðŸ”—</a> </h2>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.context</span> <span style="color:#008000;font-weight:bold">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark</span> <span style="color:#008000;font-weight:bold">import</span> SparkFiles
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.sql.session</span> <span style="color:#008000;font-weight:bold">import</span> SparkSession
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sc <span style="color:#666">=</span> SparkContext(<span style="color:#ba2121">&#39;local&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#666">=</span> SparkSession(sc)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#666">.</span>sparkContext<span style="color:#666">.</span>getConf()<span style="color:#666">.</span>getAll()
</span></span></code></pre></div><pre><code>[('spark.master', 'local'),
 ('spark.app.startTime', '1680720996903'),
 ('spark.executor.id', 'driver'),
 ('spark.app.name', 'pyspark-shell'),
 ('spark.driver.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.driver.port', '33339'),
 ('spark.driver.host', '358d949974bd'),
 ('spark.rdd.compress', 'True'),
 ('spark.serializer.objectStreamReset', '100'),
 ('spark.app.id', 'local-1680720997412'),
 ('spark.submit.pyFiles', ''),
 ('spark.submit.deployMode', 'client'),
 ('spark.executor.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.ui.showConsoleProgress', 'true')]
</code></pre>
<p><em>Now restart the kernel</em></p>
<hr>
<h2 id="no-explicit-spark-context---picks-up-config-as-expected">No explicit Spark Context - picks up config as expected&nbsp;<a class="headline-hash" href="#no-explicit-spark-context---picks-up-config-as-expected">ðŸ”—</a> </h2>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.context</span> <span style="color:#008000;font-weight:bold">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark</span> <span style="color:#008000;font-weight:bold">import</span> SparkFiles
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.sql.session</span> <span style="color:#008000;font-weight:bold">import</span> SparkSession
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark <span style="color:#666">=</span> (
</span></span><span style="display:flex;"><span>    SparkSession<span style="color:#666">.</span>builder<span style="color:#666">.</span>master(<span style="color:#ba2121">&#34;local[*]&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.jars.packages&#34;</span>, <span style="color:#ba2121">&#34;io.delta:delta-core_2.12:2.2.0&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.extensions&#34;</span>, <span style="color:#ba2121">&#34;io.delta.sql.DeltaSparkSessionExtension&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.catalog.spark_catalog&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.catalog.DeltaCatalog&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>)        
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#666">.</span>sparkContext<span style="color:#666">.</span>getConf()<span style="color:#666">.</span>getAll()
</span></span></code></pre></div><pre><code>[('spark.repl.local.jars',
  'file:///home/jovyan/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),
 ('spark.app.id', 'local-1680721007128'),
 ('spark.app.startTime', '1680721006667'),
 ('spark.files',
  'file:///home/jovyan/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),
 ('spark.app.initial.file.urls',
  'file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-2.2.0.jar'),
 ('spark.executor.id', 'driver'),
 ('spark.app.name', 'pyspark-shell'),
 ('spark.driver.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.app.initial.jar.urls',
  'spark://358d949974bd:41145/jars/io.delta_delta-core_2.12-2.2.0.jar,spark://358d949974bd:41145/jars/io.delta_delta-storage-2.2.0.jar,spark://358d949974bd:41145/jars/org.antlr_antlr4-runtime-4.8.jar'),
 ('spark.jars.packages', 'io.delta:delta-core_2.12:2.2.0'),
 ('spark.driver.host', '358d949974bd'),
 ('spark.sql.warehouse.dir', 'file:/home/jovyan/spark-warehouse'),
 ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),
 ('spark.rdd.compress', 'True'),
 ('spark.submit.pyFiles',
  '/home/jovyan/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,/home/jovyan/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,/home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),
 ('spark.driver.port', '41145'),
 ('spark.jars',
  'file:///home/jovyan/.ivy2/jars/io.delta_delta-core_2.12-2.2.0.jar,file:///home/jovyan/.ivy2/jars/io.delta_delta-storage-2.2.0.jar,file:///home/jovyan/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar'),
 ('spark.serializer.objectStreamReset', '100'),
 ('spark.master', 'local[*]'),
 ('spark.submit.deployMode', 'client'),
 ('spark.executor.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.ui.showConsoleProgress', 'true'),
 ('spark.sql.catalog.spark_catalog',
  'org.apache.spark.sql.delta.catalog.DeltaCatalog')]
</code></pre>
<hr>
<p><em>Now restart the kernel</em></p>
<hr>
<h2 id="existing-spark-context-with-attempted-config-for-the-session-">Existing Spark Context with attempted config for the Session ðŸ’€&nbsp;<a class="headline-hash" href="#existing-spark-context-with-attempted-config-for-the-session-">ðŸ”—</a> </h2>
<p><em>SparkContext gets implictly reused by the Spark Session and so config is ignored</em></p>
<div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.context</span> <span style="color:#008000;font-weight:bold">import</span> SparkContext
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark</span> <span style="color:#008000;font-weight:bold">import</span> SparkFiles
</span></span><span style="display:flex;"><span><span style="color:#008000;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">pyspark.sql.session</span> <span style="color:#008000;font-weight:bold">import</span> SparkSession
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sc <span style="color:#666">=</span> SparkContext(<span style="color:#ba2121">&#39;local&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spark <span style="color:#666">=</span> (
</span></span><span style="display:flex;"><span>    SparkSession<span style="color:#666">.</span>builder<span style="color:#666">.</span>master(<span style="color:#ba2121">&#34;local[*]&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.jars.packages&#34;</span>, <span style="color:#ba2121">&#34;io.delta:delta-core_2.12:2.2.0&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.extensions&#34;</span>, <span style="color:#ba2121">&#34;io.delta.sql.DeltaSparkSessionExtension&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>config(<span style="color:#ba2121">&#34;spark.sql.catalog.spark_catalog&#34;</span>, <span style="color:#ba2121">&#34;org.apache.spark.sql.delta.catalog.DeltaCatalog&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#666">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>)        
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style=";-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#666">.</span>sparkContext<span style="color:#666">.</span>getConf()<span style="color:#666">.</span>getAll()
</span></span></code></pre></div><pre><code>[('spark.master', 'local'),
 ('spark.app.startTime', '1680721019537'),
 ('spark.executor.id', 'driver'),
 ('spark.app.name', 'pyspark-shell'),
 ('spark.app.id', 'local-1680721020036'),
 ('spark.driver.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.driver.host', '358d949974bd'),
 ('spark.sql.warehouse.dir', 'file:/home/jovyan/spark-warehouse'),
 ('spark.rdd.compress', 'True'),
 ('spark.serializer.objectStreamReset', '100'),
 ('spark.submit.pyFiles', ''),
 ('spark.driver.port', '46397'),
 ('spark.submit.deployMode', 'client'),
 ('spark.executor.extraJavaOptions',
  '-Dio.netty.tryReflectionSetAccessible=true'),
 ('spark.ui.showConsoleProgress', 'true')]
</code></pre>
</article>

		</main>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr>
<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt"></p>
<p><a href="https://www.youtube.com/c/rmoff"><b class="fab fa-youtube-square"></b></a> <a href="https://www.linkedin.com/in/robinmoffatt/"><b class="fab fa-linkedin"></b></a> <em><a rel="me" href="https://data-folks.masto.host/@rmoff">Robin Moffatt</a> is a Principal DevEx Engineer at <a href="https://decodable.co">Decodable</a>. He likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</em></p>

				
			
		
		</div>
		
	</div>

		
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.net/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2024 
			</p>
		</footer>
		
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-75492960-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	</body>
</html>
