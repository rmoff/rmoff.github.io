<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/post/</link>
    <description>Recent content in Posts on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Dec 2020 16:00:00 +0000</lastBuildDate><atom:link href="https://rmoff.net/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>üìå    üéÅ A collection of Kafka-related talks üíù</title>
      <link>https://rmoff.net/2020/09/23/a-collection-of-kafka-related-talks/</link>
      <pubDate>Wed, 23 Sep 2020 15:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/23/a-collection-of-kafka-related-talks/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here‚Äôs a collection of Kafka-related talks, &lt;em&gt;just for you.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Each one has üçøüé• a recording, üìî slides, and üëæ code to go and try out.¬†&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 9: Cast</title>
      <link>https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/</link>
      <pubDate>Fri, 18 Dec 2020 16:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/18/twelve-days-of-smt-day-9-cast/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/cast.html&#34;&gt;&lt;code&gt;Cast&lt;/code&gt;&lt;/a&gt; Single Message Transform lets you change the data type of fields in a Kafka message, supporting numerics, string, and boolean.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 8: TimestampConverter</title>
      <link>https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/</link>
      <pubDate>Thu, 17 Dec 2020 16:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/timestampconverter.html&#34;&gt;&lt;code&gt;TimestampConverter&lt;/code&gt;&lt;/a&gt; Single Message Transform lets you work with timestamp fields in Kafka messages. You can convert a string into a native &lt;a href=&#34;https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Timestamp.html&#34;&gt;Timestamp&lt;/a&gt; type (or &lt;a href=&#34;https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Date.html&#34;&gt;Date&lt;/a&gt; or &lt;a href=&#34;https://kafka.apache.org/26/javadoc/org/apache/kafka/connect/data/Time.html&#34;&gt;Time&lt;/a&gt;), as well as Unix epoch - and the same in reverse too.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is really useful to make sure that data ingested into Kafka is correctly stored as a Timestamp (if it is one), and also enables you to write a Timestamp out to a sink connector in a string format that you choose.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 7: TimestampRouter</title>
      <link>https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/</link>
      <pubDate>Wed, 16 Dec 2020 16:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/16/twelve-days-of-smt-day-7-timestamprouter/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Just like the &lt;a href=&#34;https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/&#34;&gt;&lt;code&gt;RegExRouter&lt;/code&gt;&lt;/a&gt;, the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/timestamprouter.html&#34;&gt;&lt;code&gt;TimeStampRouter&lt;/code&gt;&lt;/a&gt; can be used to modify the topic name of messages as they pass through Kafka Connect. Since the topic name is usually the basis for the naming of the object to which messages are written in a sink connector, this is a great way to achieve time-based partitioning of those objects if required. For example, instead of streaming messages from Kafka to an Elasticsearch index called &lt;code&gt;cars&lt;/code&gt;, they can be routed to monthly indices e.g. &lt;code&gt;cars_2020-10&lt;/code&gt;, &lt;code&gt;cars_2020-11&lt;/code&gt;, &lt;code&gt;cars_2020-12&lt;/code&gt;, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;code&gt;TimeStampRouter&lt;/code&gt; takes two arguments; the format of the final topic name to generate, and the format of the timestamp to put in the topic name (based on &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html&#34;&gt;&lt;code&gt;SimpleDateFormat&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                                     &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;addTimestampToTopic&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.addTimestampToTopic.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;            &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.TimestampRouter&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.addTimestampToTopic.topic.format&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;    &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;${topic}_${timestamp}&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.addTimestampToTopic.timestamp.format&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;YYYY-MM-dd&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 6: InsertField II</title>
      <link>https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/</link>
      <pubDate>Tue, 15 Dec 2020 16:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/15/twelve-days-of-smt-day-6-insertfield-ii/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We kicked off this series by seeing on &lt;a href=&#34;https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/&#34;&gt;day 1&lt;/a&gt; how to use &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/insertfield.html&#34;&gt;&lt;code&gt;InsertField&lt;/code&gt;&lt;/a&gt; to add in the timestamp to a message passing through the Kafka Connect sink connector. Today we‚Äôll see how to use the same Single Message Transform to add in a static field value, as well as the name of the Kafka topic, partition, and offset from which the message has been read.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                                &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;insertStaticField1&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertStaticField1.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;        &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.InsertField$Value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertStaticField1.static.field&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;sourceSystem&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertStaticField1.static.value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;NeverGonna&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 5: MaskField</title>
      <link>https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/</link>
      <pubDate>Mon, 14 Dec 2020 16:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/14/twelve-days-of-smt-day-5-maskfield/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you want to mask fields of data as you ingest from a source into Kafka, or write to a sink from Kafka with Kafka Connect, the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/maskfield.html&#34;&gt;&lt;code&gt;MaskField&lt;/code&gt;&lt;/a&gt; Single Message Transform is perfect for you. It retains the fields whilst replacing its value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the Single Message Transform you specify the field to mask, and its replacement value. To mask the contents of a field called &lt;code&gt;cc_num&lt;/code&gt; you would use:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                               &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;maskCC&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.maskCC.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                   &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.MaskField$Value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.maskCC.fields&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                 &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;cc_num&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.maskCC.replacement&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;            &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;****-****-****-****&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 4: RegExRouter</title>
      <link>https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/</link>
      <pubDate>Fri, 11 Dec 2020 16:40:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/11/twelve-days-of-smt-day-4-regexrouter/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you want to change the topic name to which a source connector writes, or object name that‚Äôs created on a target by a sink connector, the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/regexrouter.html&#34;&gt;&lt;code&gt;RegExRouter&lt;/code&gt;&lt;/a&gt; is exactly what you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the Single Message Transform you specify the pattern in the topic name to match, and its replacement. To drop a prefix of &lt;code&gt;test-&lt;/code&gt; from a topic you would use:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                             &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;dropTopicPrefix&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.dropTopicPrefix.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;        &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.RegexRouter&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.dropTopicPrefix.regex&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;       &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;test-(.*)&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.dropTopicPrefix.replacement&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;$1&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 3: Flatten</title>
      <link>https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/</link>
      <pubDate>Thu, 10 Dec 2020 16:25:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/10/twelve-days-of-smt-day-3-flatten/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/flatten.html&#34;&gt;&lt;code&gt;Flatten&lt;/code&gt;&lt;/a&gt; Single Message Transform (SMT) is useful when you need to collapse a nested message down to a flat structure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the Single Message Transform you only need to reference it; there‚Äôs no additional configuration required:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                    &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;flatten&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.flatten.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;       &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.Flatten$Value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 2: ValueToKey and ExtractField</title>
      <link>https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/</link>
      <pubDate>Wed, 09 Dec 2020 20:00:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Setting the key of a Kafka message is important as it ensures correct logical processing when consumed across multiple partitions, as well as being a requirement when joining to messages in other topics. When using Kafka Connect the connector may already set the key, which is great. If not, you can use these two Single Message Transforms (SMT) to set it as part of the pipeline based on a field in the value part of the message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/valuetokey.html&#34;&gt;&lt;code&gt;ValueToKey&lt;/code&gt;&lt;/a&gt; Single Message Transform specify the name of the field (&lt;code&gt;id&lt;/code&gt;) that you want to copy from the value to the key:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                    &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;copyIdToKey&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.copyIdToKey.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;   &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.ValueToKey&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.copyIdToKey.fields&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;id&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üéÑ Twelve Days of SMT üéÑ - Day 1: InsertField (timestamp)</title>
      <link>https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/</link>
      <pubDate>Tue, 08 Dec 2020 22:23:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can use the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/insertfield.html&#34;&gt;&lt;code&gt;InsertField&lt;/code&gt;&lt;/a&gt; Single Message Transform (SMT) to add the message timestamp into each message that Kafka Connect sends to a sink.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the Single Message Transform specify the name of the field (&lt;code&gt;timestamp.field&lt;/code&gt;) that you want to add to hold the message timestamp:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                         &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;insertTS&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertTS.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;           &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.InsertField$Value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertTS.timestamp.field&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;messageTS&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Life as a Developer Advocate, nine months into a pandemic</title>
      <link>https://rmoff.net/2020/12/03/life-as-a-developer-advocate-nine-months-into-a-pandemic/</link>
      <pubDate>Thu, 03 Dec 2020 22:15:59 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/12/03/life-as-a-developer-advocate-nine-months-into-a-pandemic/</guid>
      <description>Back in March 2020 the western world came to somewhat of a juddering halt, thanks to COVID-19. No-one knew then what would happen, but there was the impression that whilst the next few months were a write-off for sure, maybe things would pick up again later in the year.
 It‚Äôs now early December 2020, and nothing is picking up any time soon. Summer provided a respite from the high levels of infection and mortality (in the UK at least), but then numbers spiked again in many places around the world and what was punted down the river back in March is being firmly punted yet again now.</description>
    </item>
    
    <item>
      <title>My Workstation - 2020</title>
      <link>https://rmoff.net/2020/12/02/my-workstation-2020/</link>
      <pubDate>Wed, 02 Dec 2020 17:19:21 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/12/02/my-workstation-2020/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;Is a blog even a blog nowadays if it doesn‚Äôt include a &amp;#34;Here is my home office setup&amp;#34;?&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Thanks to conferences all being online, and thus my talks being delivered from my study‚Äîand &lt;a href=&#34;https://twitter.com/search?q=speakerselfie%20(from%3Armoff)&amp;amp;src=typed_query&amp;amp;f=live&#34;&gt;my habit of posting a #SpeakerSelfie&lt;/a&gt; each time I do a conference talk‚ÄîI often get questions about my setup. Plus, I‚Äôm kinda pleased with it so I want to show it off too ;-)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Keynote - Why is Replace Fonts greyed out?</title>
      <link>https://rmoff.net/2020/11/13/keynote-why-is-replace-fonts-greyed-out/</link>
      <pubDate>Fri, 13 Nov 2020 15:49:37 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/11/13/keynote-why-is-replace-fonts-greyed-out/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very short &amp;amp; sweet this post, but Google turned up nothing when I was stuck so hopefully I‚Äôll save someone else some head scratching by sharing this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.net/images/2020/11/keynote01.jpg&#34; alt=&#34;keynote01&#34;/&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Connect, ksqlDB, and Kafka Tombstone messages</title>
      <link>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</link>
      <pubDate>Tue, 03 Nov 2020 17:14:33 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;As you may already realise, Kafka is not just a fancy message bus, or a pipe for big data. It‚Äôs an event streaming platform! If this is news to you, I‚Äôll wait here whilst you &lt;a href=&#34;https://www.confluent.io/learn/kafka-tutorial/&#34;&gt;read this&lt;/a&gt; or &lt;a href=&#34;https://rmoff.dev/kafka101&#34;&gt;watch this&lt;/a&gt;‚Ä¶&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Streaming Geopoint data from Kafka to Elasticsearch</title>
      <link>https://rmoff.net/2020/11/03/streaming-geopoint-data-from-kafka-to-elasticsearch/</link>
      <pubDate>Tue, 03 Nov 2020 10:36:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/11/03/streaming-geopoint-data-from-kafka-to-elasticsearch/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Streaming data from Kafka to Elasticsearch is easy with Kafka Connect - you can see how in this &lt;a href=&#34;https://rmoff.dev/kafka-elasticsearch&#34;&gt;tutorial&lt;/a&gt; and &lt;a href=&#34;https://rmoff.dev/kafka-elasticsearch-video&#34;&gt;video&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;One of the things that sometimes causes issues though is how to get location data correctly indexed into Elasticsearch as &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html&#34;&gt;&lt;code&gt;geo_point&lt;/code&gt;&lt;/a&gt; fields to enable all that lovely location analysis. Unlike data types like dates and numerics, Elasticsearch‚Äôs &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-field-mapping.html&#34;&gt;Dynamic Field Mapping&lt;/a&gt; won‚Äôt automagically pick up &lt;code&gt;geo_point&lt;/code&gt; data, and so you have to do two things:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ksqlDB - How to model a variable number of fields in a nested value (`STRUCT`)</title>
      <link>https://rmoff.net/2020/10/07/ksqldb-how-to-model-a-variable-number-of-fields-in-a-nested-value-struct/</link>
      <pubDate>Wed, 07 Oct 2020 11:44:51 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/07/ksqldb-how-to-model-a-variable-number-of-fields-in-a-nested-value-struct/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There was a &lt;a href=&#34;https://stackoverflow.com/questions/64241285/kafka-topic-with-variable-nested-json-object-as-ksql-db-stream/64242383#64242383&#34;&gt;good question on StackOverflow&lt;/a&gt; recently in which someone was struggling to find the appropriate ksqlDB DDL to model a source topic in which there was a variable number of fields in a &lt;code&gt;STRUCT&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Streaming XML messages from IBM MQ into Kafka into MongoDB</title>
      <link>https://rmoff.net/2020/10/05/streaming-xml-messages-from-ibm-mq-into-kafka-into-mongodb/</link>
      <pubDate>Mon, 05 Oct 2020 10:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/05/streaming-xml-messages-from-ibm-mq-into-kafka-into-mongodb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Let‚Äôs imagine we have XML data on a queue in IBM MQ, and we want to ingest it into Kafka to then use downstream, perhaps in an application or maybe to stream to a NoSQL store like MongoDB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
This same pattern for ingesting XML will work with other connectors such as &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-jms&#34;&gt;JMS&lt;/a&gt; and &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-activemq&#34;&gt;ActiveMQ&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Ingesting XML data into Kafka - Option 3: Kafka Connect FilePulse connector</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-3-kafka-connect-filepulse-connector/</link>
      <pubDate>Thu, 01 Oct 2020 15:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-3-kafka-connect-filepulse-connector/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;em&gt;&lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/&#34;&gt;Ingesting XML data into Kafka - Introduction&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We saw in the &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/&#34;&gt;first post&lt;/a&gt; how to hack together an ingestion pipeline for XML into Kafka using a source such as &lt;code&gt;curl&lt;/code&gt; piped through &lt;code&gt;xq&lt;/code&gt; to wrangle the XML and stream it into Kafka using &lt;code&gt;kafkacat&lt;/code&gt;, optionally using ksqlDB to apply and register a schema for it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/&#34;&gt;second one&lt;/a&gt; showed the use of any Kafka Connect source connector plus the &lt;code&gt;kafka-connect-transform-xml&lt;/code&gt; Single Message Transformation. Now we‚Äôre going to take a look at a source connector from the community that can also be used to ingest XML data into Kafka.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Ingesting XML data into Kafka - Option 2: Kafka Connect plus Single Message Transform</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/</link>
      <pubDate>Thu, 01 Oct 2020 14:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We previously looked at the background to &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/&#34;&gt;getting XML into Kafka&lt;/a&gt;, and potentially &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/&#34;&gt;how [not] to do it&lt;/a&gt;. Now let‚Äôs look at the &lt;em&gt;proper&lt;/em&gt; way to build a streaming ingestion pipeline for XML into Kafka, using Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If you‚Äôre unfamiliar with Kafka Connect, check out this &lt;a href=&#34;https://rmoff.dev/what-is-kafka-connect&#34;&gt;quick intro to Kafka Connect here&lt;/a&gt;. Kafka Connect‚Äôs excellent plugable architecture means that we can pair any &lt;strong&gt;source connector&lt;/strong&gt; to read XML from wherever we have it (for example, a flat file, or a MQ, or anywhere else), with a &lt;strong&gt;Single Message Transform&lt;/strong&gt; to transform the XML into a payload with a schema, and finally a &lt;strong&gt;converter&lt;/strong&gt; to serialise the data in a form that we would like to use such as Avro or Protobuf.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Ingesting XML data into Kafka - Option 1: The Dirty Hack</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/</link>
      <pubDate>Thu, 01 Oct 2020 13:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;üëâ &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/&#34;&gt;Ingesting XML data into Kafka - Introduction&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;What would a blog post on &lt;code&gt;rmoff.net&lt;/code&gt; be if it didn‚Äôt include the dirty hack option? üòÅ&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;The secret to dirty hacks is that they are often rather effective and when needs must, they can suffice. If you‚Äôre prototyping and need to &lt;a href=&#34;https://www.urbandictionary.com/define.php?term=JFDI&#34;&gt;&lt;strong&gt;JFDI&lt;/strong&gt;&lt;/a&gt;, a dirty hack is just fine. If you‚Äôre looking for code to run in Production, then a dirty hack probably is not fine.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Ingesting XML data into Kafka - Introduction</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/</link>
      <pubDate>Thu, 01 Oct 2020 12:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/</guid>
      <description>XML has been around for 20+ years, and whilst other ways of serialising our data have gained popularity in more recent times (such as JSON, Avro, and Protobuf), XML is not going away soon. Part of that is down to technical reasons (clearly defined and documented schemas), and part of it is simply down to enterprise inertia - having adopted XML for systems in the last couple of decades, they‚Äôre not going to be changing now just for some short-term fad.</description>
    </item>
    
    <item>
      <title>`abcde` - Error trying to calculate disc ids without lead-out information</title>
      <link>https://rmoff.net/2020/10/01/abcde-error-trying-to-calculate-disc-ids-without-lead-out-information/</link>
      <pubDate>Thu, 01 Oct 2020 09:16:11 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/abcde-error-trying-to-calculate-disc-ids-without-lead-out-information/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Short &amp;amp; sweet to help out future Googlers. Trying to use &lt;code&gt;abcde&lt;/code&gt; I got the error:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;[WARNING] something went wrong while querying the CD... Maybe a DATA CD or the CD is not loaded?
[WARNING] Error trying to calculate disc ids without lead-out information.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>IBM MQ on Docker - Channel was blocked</title>
      <link>https://rmoff.net/2020/10/01/ibm-mq-on-docker-channel-was-blocked/</link>
      <pubDate>Thu, 01 Oct 2020 01:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ibm-mq-on-docker-channel-was-blocked/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Running IBM MQ in a Docker container and the client connecting to it was throwing repeated &lt;code&gt;Channel was blocked&lt;/code&gt; errors.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Setting key value when piping from `jq` to `kafkacat`</title>
      <link>https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/</link>
      <pubDate>Wed, 30 Sep 2020 20:54:09 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;One of my favourite hacks for getting data into Kafka is using kafkacat and &lt;code&gt;stdin&lt;/code&gt;, often from &lt;code&gt;jq&lt;/code&gt;. You can see this in action with &lt;a href=&#34;https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/&#34;&gt;Wi-Fi data&lt;/a&gt;, &lt;a href=&#34;https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/&#34;&gt;IoT data&lt;/a&gt;, and data from a &lt;a href=&#34;https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/&#34;&gt;REST endpoint&lt;/a&gt;. This is fine for getting values into a Kafka message - but Kafka messages are &lt;strong&gt;key&lt;/strong&gt;/value, and being able to specify a key is can often be important.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here‚Äôs a way to do that, using a separator and some &lt;code&gt;jq&lt;/code&gt; magic. Note that at the moment &lt;a href=&#34;https://github.com/edenhill/kafkacat/issues/140&#34;&gt;kafkacat only supports single byte separator characters&lt;/a&gt;, so you need to choose carefully. If you pick a separator that also appears in your data, it‚Äôs possibly going to have unintended consequences.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Some of my favourite public data sets</title>
      <link>https://rmoff.net/2020/09/25/some-of-my-favourite-public-data-sets/</link>
      <pubDate>Fri, 25 Sep 2020 12:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/25/some-of-my-favourite-public-data-sets/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Readers of a certain age and RDBMS background will probably remember &lt;code&gt;northwind&lt;/code&gt;, or &lt;code&gt;HR&lt;/code&gt;, or &lt;code&gt;OE&lt;/code&gt; databases - or quite possibly not just remember them but still be using them. Hardcoded sample data is fine, and it‚Äôs great for repeatable tutorials and examples - but it‚Äôs boring as heck if you want to build an example with something that isn‚Äôt using the same data set for the 100th time.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using the Debezium MS SQL connector with ksqlDB embedded Kafka Connect</title>
      <link>https://rmoff.net/2020/09/18/using-the-debezium-ms-sql-connector-with-ksqldb-embedded-kafka-connect/</link>
      <pubDate>Fri, 18 Sep 2020 10:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/18/using-the-debezium-ms-sql-connector-with-ksqldb-embedded-kafka-connect/</guid>
      <description>Prompted by a question on StackOverflow I thought I‚Äôd take a quick look at setting up ksqlDB to ingest CDC events from Microsoft SQL Server using Debezium. Some of this is based on my previous article, Streaming data from SQL Server to Kafka to Snowflake ‚ùÑÔ∏è with Kafka Connect.
 Setting up the Docker Compose I like standalone, repeatable, demo code. For that reason I love using Docker Compose and I embed everything in there - connector installation, the kitchen sink - the works.</description>
    </item>
    
    <item>
      <title>Including content from external links with Asciidoc in Hugo</title>
      <link>https://rmoff.net/2020/09/18/including-content-from-external-links-with-asciidoc-in-hugo/</link>
      <pubDate>Fri, 18 Sep 2020 09:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/18/including-content-from-external-links-with-asciidoc-in-hugo/</guid>
      <description>I use Hugo for my blog, hosted on GitHub pages. One of the reasons I‚Äôm really happy with it is that I can use Asciidoc to author my posts. I was writing a blog recently in which I wanted to include some code that‚Äôs hosted on GitHub. I could have copied &amp;amp; pasted it into the blog but that would be lame!
 With Asciidoc you can use the include:: directive to include both local files:</description>
    </item>
    
    <item>
      <title>What is Kafka Connect?</title>
      <link>https://rmoff.net/2020/09/11/what-is-kafka-connect/</link>
      <pubDate>Fri, 11 Sep 2020 16:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/11/what-is-kafka-connect/</guid>
      <description>&lt;p&gt;Kafka Connect is the integration API for Apache Kafka. Check out this video for an overview of what Kafka Connect enables you to do, and how to do it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Counting the number of messages in a Kafka topic</title>
      <link>https://rmoff.net/2020/09/08/counting-the-number-of-messages-in-a-kafka-topic/</link>
      <pubDate>Tue, 08 Sep 2020 10:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/08/counting-the-number-of-messages-in-a-kafka-topic/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There‚Äôs ways, and then there‚Äôs ways, to count the number of records/events/messages in a Kafka topic. Most of them are potentially inaccurate, or inefficient, or both. Here‚Äôs one that falls into the &lt;em&gt;potentially inefficient&lt;/em&gt; category, using &lt;code&gt;kafkacat&lt;/code&gt; to read all the messages and pipe to &lt;code&gt;wc&lt;/code&gt; which with the &lt;code&gt;-l&lt;/code&gt; will tell you how many lines there are, and since each message is a line, how many messages you have in the Kafka topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color: #008080&#34;&gt;$ &lt;/span&gt;kafkacat &lt;span style=&#34;color: #000080&#34;&gt;-b&lt;/span&gt; broker:29092 &lt;span style=&#34;color: #000080&#34;&gt;-t&lt;/span&gt; mytestopic &lt;span style=&#34;color: #000080&#34;&gt;-C&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-e&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-q&lt;/span&gt;| &lt;span style=&#34;color: #0086B3&#34;&gt;wc&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-l&lt;/span&gt;
       3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Poking around the search engines in Google Chrome</title>
      <link>https://rmoff.net/2020/09/07/poking-around-the-search-engines-in-google-chrome/</link>
      <pubDate>Mon, 07 Sep 2020 23:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/07/poking-around-the-search-engines-in-google-chrome/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Google Chrome automagically adds sites that you visit which support searching to a list of custom search engines. For each one you can set a keyword which activates it, so based on the above list if I want to search Amazon I can just type &lt;code&gt;a&lt;/code&gt; &lt;code&gt;&amp;lt;tab&amp;gt;&lt;/code&gt; and then my search term&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;https://rmoff.net/images/2020/09/searchengines02.gif&#34; alt=&#34;searchengines02&#34;/&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ü§ñBuilding a Telegram bot with Apache Kafka, Go, and ksqlDB</title>
      <link>https://rmoff.net/2020/08/20/building-a-telegram-bot-with-apache-kafka-go-and-ksqldb/</link>
      <pubDate>Thu, 20 Aug 2020 10:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/08/20/building-a-telegram-bot-with-apache-kafka-go-and-ksqldb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I had the pleasure of presenting at &lt;a href=&#34;https://dataengconf.com.au/&#34;&gt;DataEngBytes&lt;/a&gt; recently, and am delighted to share with you the &lt;strong&gt;üóíÔ∏è slides, üëæ code, and üé• recording&lt;/strong&gt; of my ‚ú®brand new talk‚ú®:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://rmoff.dev/carpark-telegram-bot&#34;&gt;ü§ñBuilding a Telegram bot with Apache Kafka, Go, and ksqlDB&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Telegram bot - BOT_COMMAND_INVALID</title>
      <link>https://rmoff.net/2020/07/23/telegram-bot-bot_command_invalid/</link>
      <pubDate>Thu, 23 Jul 2020 15:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/23/telegram-bot-bot_command_invalid/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A tiny snippet since I wasted 10 minutes going around the houses on this one‚Ä¶&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;tl;dr: If you try to create a command that is &lt;strong&gt;not in lower case&lt;/strong&gt; (e.g. &lt;code&gt;Alert&lt;/code&gt; not &lt;code&gt;alert&lt;/code&gt;) then the &lt;code&gt;setMyCommands&lt;/code&gt; API will return &lt;code&gt;BOT_COMMAND_INVALID&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E09 - Processing chunked responses before EOF is reached</title>
      <link>https://rmoff.net/2020/07/23/learning-golang-some-rough-notes-s02e09-processing-chunked-responses-before-eof-is-reached/</link>
      <pubDate>Thu, 23 Jul 2020 10:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/23/learning-golang-some-rough-notes-s02e09-processing-chunked-responses-before-eof-is-reached/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The server sends &lt;code&gt;Transfer-Encoding: chunked&lt;/code&gt; data, and you want to work with the data &lt;strong&gt;as you get it&lt;/strong&gt;, instead of waiting for the server to finish, the EOF to fire, and &lt;em&gt;then&lt;/em&gt; process the data?&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E08 - Checking Kafka advertised.listeners with Go</title>
      <link>https://rmoff.net/2020/07/17/learning-golang-some-rough-notes-s02e08-checking-kafka-advertised.listeners-with-go/</link>
      <pubDate>Fri, 17 Jul 2020 17:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/17/learning-golang-some-rough-notes-s02e08-checking-kafka-advertised.listeners-with-go/</guid>
      <description>At the beginning of all this my aim was to learn something new (Go), and use it to write a version of a utility that I‚Äôd previously hacked together in Python that checks your Apache Kafka broker configuration for possible problems with the infamous advertised.listeners setting. Check out a blog that I wrote which explains all about Apache Kafka and listener configuration.
     You can find the code at https://github.</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E07 - Splitting Go code into separate source files and building a binary executable</title>
      <link>https://rmoff.net/2020/07/16/learning-golang-some-rough-notes-s02e07-splitting-go-code-into-separate-source-files-and-building-a-binary-executable/</link>
      <pubDate>Thu, 16 Jul 2020 11:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/16/learning-golang-some-rough-notes-s02e07-splitting-go-code-into-separate-source-files-and-building-a-binary-executable/</guid>
      <description>So far I‚Äôve been running all my code either in the Go Tour sandbox, using Go Playground, or from a single file in VS Code. My explorations in the previous article ended up with a a source file that was starting to get a little bit unwieldily, so let‚Äôs take a look at how that can be improved.
 Within my most recent code, I have the main function and the doProduce function, which is fine when collapsed down:</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E06 - Putting the Producer in a function and handling errors in a Go routine</title>
      <link>https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e06-putting-the-producer-in-a-function-and-handling-errors-in-a-go-routine/</link>
      <pubDate>Wed, 15 Jul 2020 14:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e06-putting-the-producer-in-a-function-and-handling-errors-in-a-go-routine/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;When I set out to &lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/&#34;&gt;learn Go&lt;/a&gt; one of the aims I had in mind was to write a version of &lt;a href=&#34;https://github.com/rmoff/kafka-listeners/blob/master/python/python_kafka_test_client.py&#34;&gt;this little Python utility&lt;/a&gt; which accompanies a blog I wrote recently about &lt;a href=&#34;https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc&#34;&gt;understanding and diagnosing problems with Kafka advertised listeners&lt;/a&gt;. Having successfully got &lt;a href=&#34;https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/&#34;&gt;Producer&lt;/a&gt;, &lt;a href=&#34;https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e04-kafka-go-consumer-function-based/&#34;&gt;Consumer&lt;/a&gt;, and &lt;a href=&#34;https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e05-kafka-go-adminclient/&#34;&gt;AdminClient&lt;/a&gt; API examples working, it is now time to turn to that task.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E05 - Kafka Go AdminClient</title>
      <link>https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e05-kafka-go-adminclient/</link>
      <pubDate>Wed, 15 Jul 2020 11:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/15/learning-golang-some-rough-notes-s02e05-kafka-go-adminclient/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Having ticked off the basics with an Apache Kafka &lt;a href=&#34;https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/&#34;&gt;producer&lt;/a&gt; and &lt;a href=&#34;https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e03-kafka-go-consumer-channel-based/&#34;&gt;consumer&lt;/a&gt; in Go, let‚Äôs now check out the AdminClient. This is useful for checking out metadata about the cluster, creating topics, and stuff like that.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E04 - Kafka Go Consumer (Function-based)</title>
      <link>https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e04-kafka-go-consumer-function-based/</link>
      <pubDate>Tue, 14 Jul 2020 13:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e04-kafka-go-consumer-function-based/</guid>
      <description>Last time I looked at creating my first Apache Kafka consumer in Go, which used the now-deprecated channel-based consumer. Whilst idiomatic for Go, it has some issues which mean that the function-based consumer is recommended for use instead. So let‚Äôs go and use it!
 Instead of reading from the Events() channel of the consumer, we read events using the Poll() function with a timeout. The way we handle events (a switch based on their type) is the same:</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E03 - Kafka Go Consumer (Channel-based)</title>
      <link>https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e03-kafka-go-consumer-channel-based/</link>
      <pubDate>Tue, 14 Jul 2020 11:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/14/learning-golang-some-rough-notes-s02e03-kafka-go-consumer-channel-based/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Having written my first &lt;a href=&#34;https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e01-my-first-kafka-go-producer/&#34;&gt;Kafka producer in Go&lt;/a&gt;, and even &lt;a href=&#34;https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/&#34;&gt;added error handling to it&lt;/a&gt;, the next step was to write a consumer. It follows closely the pattern of &lt;a href=&#34;https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/&#34;&gt;Producer code I finished up with previously&lt;/a&gt;, using the channel-based approach for the &lt;a href=&#34;https://docs.confluent.io/current/clients/confluent-kafka-go/index.html#Consumer&#34;&gt;Consumer&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E02 - Adding error handling to the Producer</title>
      <link>https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/</link>
      <pubDate>Fri, 10 Jul 2020 10:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/10/learning-golang-some-rough-notes-s02e02-adding-error-handling-to-the-producer/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I looked &lt;a href=&#34;https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e01-my-first-kafka-go-producer/&#34;&gt;last time&lt;/a&gt; at the very bare basics of writing a Kafka producer using Go. It worked, but only with everything lined up and pointing the right way. There was no error handling of any sorts. Let‚Äôs see about fixing this now.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E01 - My First Kafka Go Producer</title>
      <link>https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e01-my-first-kafka-go-producer/</link>
      <pubDate>Wed, 08 Jul 2020 17:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e01-my-first-kafka-go-producer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S02E00 - Kafka and Go</title>
      <link>https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e00-kafka-and-go/</link>
      <pubDate>Wed, 08 Jul 2020 10:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/08/learning-golang-some-rough-notes-s02e00-kafka-and-go/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;With the first leg of my journey with Go &lt;a href=&#34;https://rmoff.net/2020/07/03/learning-golang-some-rough-notes-s01e10-concurrency-web-crawler/&#34;&gt;done&lt;/a&gt; (starting from a &lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/&#34;&gt;&lt;em&gt;very&lt;/em&gt; rudimentary base&lt;/a&gt;), the next step for me was to bring it into my current area of interest and work - Apache Kafka.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E10 - Concurrency (Web Crawler)</title>
      <link>https://rmoff.net/2020/07/03/learning-golang-some-rough-notes-s01e10-concurrency-web-crawler/</link>
      <pubDate>Fri, 03 Jul 2020 16:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/03/learning-golang-some-rough-notes-s01e10-concurrency-web-crawler/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/concurrency/9&#34;&gt;A Tour of Go : sync.Mutex&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In the &lt;a href=&#34;https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e09-concurrency-channels-goroutines/&#34;&gt;previous exercise&lt;/a&gt; I felt my &lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/&#34;&gt;absence of a formal CompSci background&lt;/a&gt; with the introduction of Binary Sorted Trees, and now I am concious of it again with learning about mutex. I‚Äôd &lt;em&gt;heard&lt;/em&gt; of them before, mostly when Oracle performance folk were talking about wait types - TIL it stands for &lt;code&gt;mutual exclusion&lt;/code&gt;!&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Why JSON isn&#39;t the same as JSON Schema in Kafka Connect converters (Viewing Kafka messages bytes as hex)</title>
      <link>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</link>
      <pubDate>Fri, 03 Jul 2020 08:16:36 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve been playing around with the new SerDes (serialisers/deserialisers) that shipped with Confluent Platform 5.5 - &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/serdes-develop/index.html&#34;&gt;Protobuf, and JSON Schema&lt;/a&gt; (these were added to the existing support for Avro). The serialisers (and associated &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/connect.html&#34;&gt;Kafka Connect converters&lt;/a&gt;) take a payload and serialise it into bytes for sending to Kafka, and I was interested in what those bytes look like. For that I used my favourite Kafka swiss-army knife: kafkacat.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E09 - Concurrency (Channels, Goroutines)</title>
      <link>https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e09-concurrency-channels-goroutines/</link>
      <pubDate>Thu, 02 Jul 2020 16:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e09-concurrency-channels-goroutines/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://tour.golang.org/concurrency/1&#34;&gt;A Tour of Go : Goroutines&lt;/a&gt; was OK but as with some previous material I headed over to &lt;a href=&#34;https://gobyexample.com/goroutines&#34;&gt;Go by example&lt;/a&gt; for clearer explanations.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E08 - Images</title>
      <link>https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e08-images/</link>
      <pubDate>Thu, 02 Jul 2020 14:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/02/learning-golang-some-rough-notes-s01e08-images/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/methods/25&#34;&gt;A Tour of Go : Exercise: Images&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is based on the Picture generator from the &lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e02-slices/&#34;&gt;Slices exercise&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E07 - Readers</title>
      <link>https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e07-readers/</link>
      <pubDate>Wed, 01 Jul 2020 15:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e07-readers/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/methods/21&#34;&gt;A Tour of Go : Readers&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôm not intending to pick holes in the Tour‚Ä¶but it‚Äôs not helping itself ;-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;For an introductory text, it makes a ton of assumptions about the user. Here it introduces Readers, and the explanation is good‚Äîbut the example code looks like this:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E06 - Errors</title>
      <link>https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e06-errors/</link>
      <pubDate>Wed, 01 Jul 2020 10:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/01/learning-golang-some-rough-notes-s01e06-errors/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/methods/20&#34;&gt;A Tour of Go : Exercise: Errors&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Like Interfaces, the Tour didn‚Äôt really do it for me on Errors either. Too absract, and not enough explanation of the code examples for my liking. It also doesn‚Äôt cover the &lt;a href=&#34;https://golang.org/pkg/errors/&#34;&gt;&lt;code&gt;errors&lt;/code&gt;&lt;/a&gt; package which other tutorial do. I‚Äôm not clear if that‚Äôs because the errors package isn‚Äôt used much, or the Tour focusses only on teaching the raw basics.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E05 - Interfaces</title>
      <link>https://rmoff.net/2020/06/30/learning-golang-some-rough-notes-s01e05-interfaces/</link>
      <pubDate>Tue, 30 Jun 2020 16:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/30/learning-golang-some-rough-notes-s01e05-interfaces/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/methods/9&#34;&gt;A Tour of Go : Interfaces&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This page really threw me, for several reasons:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The text notes that there‚Äôs an error (&lt;em&gt;so why don‚Äôt they fix it?&lt;/em&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The provided code doesn‚Äôt run (presumably because of the above error)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It‚Äôs not clear if this is a deliberate error to illustrate a point, or just a snafu&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E04 - Function Closures</title>
      <link>https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e04-function-closures/</link>
      <pubDate>Mon, 29 Jun 2020 14:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e04-function-closures/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/moretypes/25&#34;&gt;A Tour of Go : Function Closures&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;So far the Tour has been ü§î and üßê and even ü§® but function closures had me ü§Ø ‚Ä¶&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Each of the words on the page made sense but strung together in a sentence didn‚Äôt really make any sense to me.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E03 - Maps</title>
      <link>https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e03-maps/</link>
      <pubDate>Mon, 29 Jun 2020 13:59:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/29/learning-golang-some-rough-notes-s01e03-maps/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/moretypes/23&#34;&gt;A Tour of Go : Exercise - Maps&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Implement WordCount&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is probably bread-and-butter for any seasoned programmer, but I enjoyed the simple process and satisfaction of breaking the problem down into steps to solve using what the tutorial had just covered. Sketching out the logic in pseudo-code first, I figured that I wanted to do this:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E02 - Slices</title>
      <link>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e02-slices/</link>
      <pubDate>Thu, 25 Jun 2020 11:20:23 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e02-slices/</guid>
      <description>&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
&lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/&#34;&gt;Learning Go : Background&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/moretypes/7&#34;&gt;A Tour of Go : Slices&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Slices made sense, until I got to &lt;a href=&#34;https://tour.golang.org/moretypes/11&#34;&gt;&lt;em&gt;Slice length and capacity&lt;/em&gt;&lt;/a&gt;. Two bits puzzled me in this code:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E01 - Pointers</title>
      <link>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e01-pointers/</link>
      <pubDate>Thu, 25 Jun 2020 11:15:23 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e01-pointers/</guid>
      <description>&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
&lt;a href=&#34;https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/&#34;&gt;Learning Go : Background&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;üëâ &lt;a href=&#34;https://tour.golang.org/moretypes/1&#34;&gt;A Tour of Go : Pointers&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve never used pointers before. Found plenty of good resources about &lt;strong&gt;what&lt;/strong&gt; they are, e.g.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.callicoder.com/golang-pointers/&#34; class=&#34;bare&#34;&gt;https://www.callicoder.com/golang-pointers/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://dave.cheney.net/2017/04/26/understand-go-pointers-in-less-than-800-words-or-your-money-back&#34; class=&#34;bare&#34;&gt;https://dave.cheney.net/2017/04/26/understand-go-pointers-in-less-than-800-words-or-your-money-back&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;But &lt;strong&gt;why&lt;/strong&gt;? It‚Äôs like explaining patiently to someone that 2+2 = 4, without really explaining &lt;strong&gt;why&lt;/strong&gt; would we want to add two numbers together in the first place.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Learning Golang (some rough notes) - S01E00</title>
      <link>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/</link>
      <pubDate>Thu, 25 Jun 2020 11:13:23 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/25/learning-golang-some-rough-notes-s01e00/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;My background is not a traditional CompSci one. I studied Music at university, and managed to wangle my way into IT through various means, ending up doing what I do now with no formal training in coding, and a grab-bag of hacky programming attempts on my CV. My weapons of choice have been BBC Basic, VBA, ASP, and more recently some very unpythonic-Python. It‚Äôs got me by, but I figured recently I‚Äôd like to learn something new, and several people pointed to Go as a good option.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How to install connector plugins in Kafka Connect</title>
      <link>https://rmoff.net/2020/06/19/how-to-install-connector-plugins-in-kafka-connect/</link>
      <pubDate>Fri, 19 Jun 2020 17:28:09 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/19/how-to-install-connector-plugins-in-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect (which is part of Apache Kafka) supports pluggable connectors, enabling you to stream data between Kafka and numerous types of system, including to mention just a few:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Databases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Message Queues&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flat files&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Object stores&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The appropriate plugin for the technology which you want to integrate can be found on &lt;a href=&#34;https://www.confluent.io/hub/&#34;&gt;Confluent Hub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Loading CSV data into Kafka</title>
      <link>https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/</link>
      <pubDate>Wed, 17 Jun 2020 17:57:18 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;For whatever reason, CSV still exists as a ubiquitous data interchange format. It doesn‚Äôt get much simpler: chuck some plaintext with fields separated by commas into a file and stick &lt;code&gt;.csv&lt;/code&gt; on the end. If you‚Äôre feeling helpful you can include a header row with field names in.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;pre&gt;&lt;code class=&#34;language-csv&#34; data-lang=&#34;csv&#34;&gt;order_id,customer_id,order_total_usd,make,model,delivery_city,delivery_company,delivery_address
1,535,190899.73,Dodge,Ram Wagon B350,Sheffield,DuBuque LLC,2810 Northland Avenue
2,671,33245.53,Volkswagen,Cabriolet,Edinburgh,Bechtelar-VonRueden,1 Macpherson Crossing&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In this article we‚Äôll see how to load this CSV data into Kafka, without even needing to write any code&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How to list and create Kafka topics using the REST Proxy API</title>
      <link>https://rmoff.net/2020/06/05/how-to-list-and-create-kafka-topics-using-the-rest-proxy-api/</link>
      <pubDate>Fri, 05 Jun 2020 09:46:06 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/06/05/how-to-list-and-create-kafka-topics-using-the-rest-proxy-api/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;In v5.5 of Confluent Platform the REST Proxy added new Admin API capabilities, including functionality to list, and create, topics on your cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Check out the &lt;a href=&#34;https://docs.confluent.io/current/kafka-rest/api.html#crest-api-v3&#34;&gt;docs here&lt;/a&gt; and &lt;a href=&#34;https://www.confluent.io/download/#confluent-platform&#34;&gt;download Confluent Platform&lt;/a&gt; here. The REST proxy is &lt;a href=&#34;https://www.confluent.io/confluent-community-license-faq/&#34;&gt;Confluent Community Licenced&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Working with JSON nested arrays in ksqlDB - example</title>
      <link>https://rmoff.net/2020/05/26/working-with-json-nested-arrays-in-ksqldb-example/</link>
      <pubDate>Tue, 26 May 2020 10:02:48 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/05/26/working-with-json-nested-arrays-in-ksqldb-example/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Question from the Confluent Community Slack group:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;How can I access the data in object in an array like below using ksqlDB stream&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlight&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&amp;#34;Total&amp;#34;: [
        {
          &amp;#34;TotalType&amp;#34;: &amp;#34;Standard&amp;#34;,
          &amp;#34;TotalAmount&amp;#34;: 15.99
        },
{
          &amp;#34;TotalType&amp;#34;: &amp;#34;Old Standard&amp;#34;,
          &amp;#34;TotalAmount&amp;#34;: 16,
&amp;#34; STID&amp;#34;:56
        }
]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Searching Alfred&#39;s Clipboard history programatically</title>
      <link>https://rmoff.net/2020/05/18/searching-alfreds-clipboard-history-programatically/</link>
      <pubDate>Mon, 18 May 2020 12:46:02 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/05/18/searching-alfreds-clipboard-history-programatically/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.alfredapp.com/&#34;&gt;Alfred&lt;/a&gt; is one of my favourite productivity apps for the Mac. It‚Äôs a file indexer, a clipboard manager, a snippet expander - and that‚Äôs just scratching the surface really. I recently got a new machine without it installed and realised &lt;em&gt;just how much&lt;/em&gt; I rely on Alfred, particularly its clipboard manager.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Building a Telegram bot with Apache Kafka and ksqlDB</title>
      <link>https://rmoff.net/2020/05/18/building-a-telegram-bot-with-apache-kafka-and-ksqldb/</link>
      <pubDate>Mon, 18 May 2020 11:28:15 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/05/18/building-a-telegram-bot-with-apache-kafka-and-ksqldb/</guid>
      <description>Imagine you‚Äôve got a stream of data; it‚Äôs not ‚Äúbig data,‚Äù but it‚Äôs certainly a lot. Within the data, you‚Äôve got some bits you‚Äôre interested in, and of those bits, you‚Äôd like to be able to query information about them at any point. Sounds fun, right?
   What if you didn‚Äôt need any datastore other than Apache Kafka itself to be able to do this? What if you could ingest, filter, enrich, aggregate, and query data with just Kafka?</description>
    </item>
    
    <item>
      <title>Add Markers list from Screenflow to Youtube Table of Contents</title>
      <link>https://rmoff.net/2020/05/04/add-markers-list-from-screenflow-to-youtube-table-of-contents/</link>
      <pubDate>Mon, 04 May 2020 10:20:10 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/05/04/add-markers-list-from-screenflow-to-youtube-table-of-contents/</guid>
      <description>Screenflow has a useful Markers feature for adding notes to the timeline.
   You can use these to helpfully add a table of contents to your Youtube video, but unfortunately Screenflow doesn‚Äôt have the option to export them directly. Instead, use the free Subler program as an intermediary (download it from here).
  Export from Screenflow with a chapters track
    Open the file in Subler and export to text file</description>
    </item>
    
    <item>
      <title>Using Confluent Cloud when there is no Cloud (or internet)</title>
      <link>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</link>
      <pubDate>Mon, 20 Apr 2020 13:55:46 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</guid>
      <description>‚òÅÔ∏èConfluent Cloud is a great solution for a hosted and managed Apache Kafka service, with the additional benefits of Confluent Platform such as ksqlDB and managed Kafka Connect connectors. But as a developer, you won‚Äôt always have a reliable internet connection. Train, planes, and automobiles‚Äînot to mention crappy hotel or conference Wi-Fi. Wouldn‚Äôt it be useful if you could have a replica of your Cloud data on your local machine? That just pulled down new data automagically, without needing to be restarted each time you got back on the network?</description>
    </item>
    
    <item>
      <title>How to install kafkacat on Fedora</title>
      <link>https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/</link>
      <pubDate>Mon, 20 Apr 2020 10:25:32 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/</guid>
      <description>kafkacat is one of my go-to tools when working with Kafka. It‚Äôs a producer and consumer, but also a swiss-army knife of debugging and troubleshooting capabilities. So when I built a new Fedora server recently, I needed to get it installed. Unfortunately there‚Äôs no pre-packed install available on yum, so here‚Äôs how to do it manually.
 Pre-requisite installs We‚Äôll need some packages from the Confluent repo so set this up for yum first by creating /etc/yum.</description>
    </item>
    
    <item>
      <title>Converting from AsciiDoc to Google Docs and MS Word</title>
      <link>https://rmoff.net/2020/04/16/converting-from-asciidoc-to-google-docs-and-ms-word/</link>
      <pubDate>Thu, 16 Apr 2020 14:27:50 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/16/converting-from-asciidoc-to-google-docs-and-ms-word/</guid>
      <description>Updated 16 April 2020 to cover formatting tricks &amp;amp; add import to Google Docs info
 Short and sweet this one. I‚Äôve written in the past how I love Markdown but I‚Äôve actually moved on from that and now firmly throw my hat in the AsciiDoc ring. I‚Äôll write another post another time explaining why in more detail, but in short it‚Äôs just more powerful whilst still simple and readable without compilation.</description>
    </item>
    
    <item>
      <title>A quick and dirty way to monitor data arriving on Kafka</title>
      <link>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</link>
      <pubDate>Thu, 16 Apr 2020 00:51:18 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve been poking around recently with &lt;a href=&#34;https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/&#34;&gt;capturing Wi-Fi packet data&lt;/a&gt; and streaming it into Apache Kafka, from where I‚Äôm processing and analysing it. Kafka itself is rock-solid - because I‚Äôm using &lt;a href=&#34;https://confluent.cloud/signup&#34;&gt;‚òÅÔ∏èConfluent Cloud&lt;/a&gt; and someone else worries about provisioning it, scaling it, and keeping it running for me. But whilst Kafka works just great, my side of the setup‚Äî&lt;code&gt;tshark&lt;/code&gt; running on a Raspberry Pi‚Äîis less than stable. For whatever reason it sometimes stalls and I have to restart the Raspberry Pi and restart the capture process.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Are Tech Conferences Dead?</title>
      <link>https://rmoff.net/2020/03/13/are-tech-conferences-dead/</link>
      <pubDate>Fri, 13 Mar 2020 22:19:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/03/13/are-tech-conferences-dead/</guid>
      <description>ü¶†COVID-19 has well and truly hit the tech scene this week. As well as being full of &amp;#34;WFH tips&amp;#34; for all the tech workers suddenly banished from their offices, my particular Twitter bubble is full of DevRel folk musing and debating about what this interruption means to our profession. For sure, in the short term, the Spring conference season is screwed‚Äî all the conferences are cancelled (or postponed).
 But what about the future?</description>
    </item>
    
    <item>
      <title>Streaming Wi-Fi trace data from Raspberry Pi to Apache Kafka with Confluent Cloud</title>
      <link>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</link>
      <pubDate>Wed, 11 Mar 2020 11:58:13 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</guid>
      <description>Wi-fi is now ubiquitous in most populated areas, and the way the devices communicate leaves a lot of &amp;#39;digital exhaust&amp;#39;. Usually a computer will have a Wi-Fi device that‚Äôs configured to connect to a given network, but often these devices can be configured instead to pick up the background Wi-Fi chatter of surrounding devices.
 There are good reasons‚Äîand bad‚Äîfor doing this. Just like taking apart equipment to understand how it works teaches us things, so being able to dissect and examine protocol traffic lets us learn about this.</description>
    </item>
    
    <item>
      <title>Kafka Connect JDBC Sink - setting the key field name</title>
      <link>https://rmoff.net/2020/02/25/kafka-connect-jdbc-sink-setting-the-key-field-name/</link>
      <pubDate>Tue, 25 Feb 2020 14:37:12 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/02/25/kafka-connect-jdbc-sink-setting-the-key-field-name/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I wanted to get some data from a Kafka topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;ksql&lt;span style=&#34;color:#666&#34;&gt;&amp;gt;&lt;/span&gt; PRINT PERSON_STATS &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FROM&lt;/span&gt; BEGINNING;
&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Key&lt;/span&gt; format: KAFKA (STRING)
Value format: AVRO
rowtime: &lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;25&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;20&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;12&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;51&lt;/span&gt; PM UTC, &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;key&lt;/span&gt;: robin, value: &lt;span style=&#34;&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;PERSON&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;robin&amp;#34;&lt;/span&gt;,
 &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;LOCATION_CHANGES&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;UNIQUE_LOCATIONS&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;into Postgres, so did the easy thing and used Kafka Connect with the &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html&#34;&gt;JDBC Sink connector&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Adventures in the Cloud, Part 94: ECS</title>
      <link>https://rmoff.net/2020/02/13/adventures-in-the-cloud-part-94-ecs/</link>
      <pubDate>Thu, 13 Feb 2020 00:12:23 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/02/13/adventures-in-the-cloud-part-94-ecs/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;My name‚Äôs Robin, and I‚Äôm a Developer Advocate. What that means in part is that I build a ton of demos, and Docker Compose is my jam. I love using Docker Compose for the same reasons that many people do:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Spin up and tear down fully-functioning multi-component environments with ease. No bespoke builds, no cloning of VMs to preserve &amp;#34;that magic state where everything works&amp;#34;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repeatability. It‚Äôs the same each time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Portability. I can point someone at a &lt;code&gt;docker-compose.yml&lt;/code&gt; that I‚Äôve written and they can run the same on their machine with the same results almost guaranteed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Primitive Keys in ksqlDB</title>
      <link>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</link>
      <pubDate>Fri, 07 Feb 2020 10:58:06 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ksqlDB 0.7 will add support for message keys as primitive data types beyond just &lt;code&gt;STRING&lt;/code&gt; (which is all we‚Äôve had to date). That means that Kafka messages are going to be much easier to work with, and require less wrangling to get into the form in which you need them. Take an example of a database table that you‚Äôve ingested into a Kafka topic, and want to join to a stream of events. Previously you‚Äôd have had to take the Kafka topic into which the table had been ingested and run a ksqlDB processor to re-key the messages such that ksqlDB could join on them. &lt;em&gt;Friends, I am here to tell you that this is no longer needed!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Fantastical / Mac Calendar not showing Google Shared Calendar</title>
      <link>https://rmoff.net/2020/01/24/fantastical-/-mac-calendar-not-showing-google-shared-calendar/</link>
      <pubDate>Fri, 24 Jan 2020 11:50:01 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/24/fantastical-/-mac-calendar-not-showing-google-shared-calendar/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very simple to fix: go to &lt;a href=&#34;https://calendar.google.com/calendar/syncselect&#34; class=&#34;bare&#34;&gt;https://calendar.google.com/calendar/syncselect&lt;/a&gt; and select the calendars that you want. Click save.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Notes on getting data into InfluxDB from Kafka with Kafka Connect</title>
      <link>https://rmoff.net/2020/01/23/notes-on-getting-data-into-influxdb-from-kafka-with-kafka-connect/</link>
      <pubDate>Thu, 23 Jan 2020 12:01:35 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/23/notes-on-getting-data-into-influxdb-from-kafka-with-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can download the InfluxDB connector for Kafka Connect &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-influxdb&#34;&gt;here&lt;/a&gt;. Documentation for it is &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-influxdb/influx-db-sink-connector/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;When a message from your source Kafka topic is written to InfluxDB the InfluxDB values are set thus:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Timestamp&lt;/strong&gt; is taken from the Kafka message timestamp (which is either set by your producer, or the time at which it was received by the broker)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag(s)&lt;/strong&gt; are taken from the &lt;code&gt;tags&lt;/code&gt; field in the message. This field must be a &lt;code&gt;map&lt;/code&gt; type - see below&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt; fields are taken from the rest of the message, and must be numeric or boolean&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Measurement name&lt;/strong&gt; can be specified as a field of the message, or hardcoded in the connector config.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Connect and Schemas</title>
      <link>https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</link>
      <pubDate>Wed, 22 Jan 2020 00:26:03 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/22/kafka-connect-and-schemas/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here‚Äôs a fun one that Kafka Connect can sometimes throw out:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;java.lang.ClassCastException: 
java.util.HashMap cannot be cast to org.apache.kafka.connect.data.Struct&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;HashMap? Struct? HUH?&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Monitoring Sonos with ksqlDB, InfluxDB, and Grafana</title>
      <link>https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/</link>
      <pubDate>Tue, 21 Jan 2020 22:47:35 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/</guid>
      <description>I‚Äôm quite a fan of Sonos audio equipment but recently had some trouble with some of the devices glitching and even cutting out whilst playing. Under the covers Sonos stuff is running Linux (of course) and exposes some diagnostics through a rudimentary frontend that you can access at http://&amp;lt;sonos player IP&amp;gt;:1400/support/review:
   Whilst this gives you the current state, you can‚Äôt get historical data on it. It felt like the problems were happening &amp;#34;all the time&amp;#34;, but were they actually?</description>
    </item>
    
    <item>
      <title>UnsupportedClassVersionError: `&lt;x&gt;` has been compiled by a more recent version of the Java Runtime</title>
      <link>https://rmoff.net/2020/01/21/unsupportedclassversionerror-x-has-been-compiled-by-a-more-recent-version-of-the-java-runtime/</link>
      <pubDate>Tue, 21 Jan 2020 22:26:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/21/unsupportedclassversionerror-x-has-been-compiled-by-a-more-recent-version-of-the-java-runtime/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This article is just for Googlers and my future self encountering this error. Recently I was building a Docker image from the ksqlDB code base, and whilst it built successfully the ksqlDB server process in the Docker container when instantiated failed with a &lt;code&gt;UnsupportedClassVersionError&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Changing the Logging Level for Kafka Connect Dynamically</title>
      <link>https://rmoff.net/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/</link>
      <pubDate>Thu, 16 Jan 2020 22:50:45 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/</guid>
      <description>Logs are magical things. They tell us what an application is doing‚Äîor not doing. They help us debug problems. As it happens, they also underpin the entire philosophy of Apache Kafka, but that‚Äôs a story for another day. Today we‚Äôre talking about logs written by Kafka Connect, and how we can change the amount of detail written.
 By default, Kafka Connect will write logs at INFO and above. So when it starts up, the settings that it‚Äôs using, and any WARN or ERROR messages along the way - a missing configuration, a broken connector, and so on.</description>
    </item>
    
    <item>
      <title>How to win [or at least not suck] at the conference abstract submission game</title>
      <link>https://rmoff.net/2020/01/16/how-to-win-or-at-least-not-suck-at-the-conference-abstract-submission-game/</link>
      <pubDate>Thu, 16 Jan 2020 13:45:31 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/16/how-to-win-or-at-least-not-suck-at-the-conference-abstract-submission-game/</guid>
      <description>Just over a year ago, I put together the crudely-titled &amp;#34;Quick Thoughts on Not Writing a Crap Abstract&amp;#34; after reviewing a few dozen conference abstracts. This time around I‚Äôve had the honour of being on a conference programme committee and with it the pleasure of reading 250+ abstracts‚Äîfrom which I have some more snarky words of wisdom to impart on the matter.
 Remind me‚Ä¶how does this conference game work? Before we really get into it, let‚Äôs recap how this whole game works, because plenty of people are new to conference speaking.</description>
    </item>
    
    <item>
      <title>Exploring ksqlDB window start time</title>
      <link>https://rmoff.net/2020/01/09/exploring-ksqldb-window-start-time/</link>
      <pubDate>Thu, 09 Jan 2020 14:25:01 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/09/exploring-ksqldb-window-start-time/</guid>
      <description>Prompted by a question on StackOverflow I had a bit of a dig into how windows behave in ksqlDB, specifically with regards to their start time. This article shows also how to create test data in ksqlDB and create data to be handled with a timestamp in the past.
 For a general background to windowing in ksqlDB see the excellent docs.
 The nice thing about recent releases of ksqlDB/KSQL is that you can create and populate streams directly with CREATE STREAM and INSERT INTO respectively.</description>
    </item>
    
    <item>
      <title>Streaming messages from RabbitMQ into Kafka with Kafka Connect</title>
      <link>https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/</link>
      <pubDate>Wed, 08 Jan 2020 13:06:57 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/</guid>
      <description>This was prompted by a question on StackOverflow to which I thought the answer would be straightforward, but turned out not to be so. And then I got a bit carried away and ended up with a nice example of how you can handle schema-less data coming from a system such as RabbitMQ and apply a schema to it.
   Note  This same pattern for ingesting bytes and applying a schema will work with other connectors such as MQTT     What?</description>
    </item>
    
    <item>
      <title>Analysing network behaviour with ksqlDB and MongoDB</title>
      <link>https://rmoff.net/2019/12/20/analysing-network-behaviour-with-ksqldb-and-mongodb/</link>
      <pubDate>Fri, 20 Dec 2019 17:23:40 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/12/20/analysing-network-behaviour-with-ksqldb-and-mongodb/</guid>
      <description>In this post I want to build on my previous one and show another use of the Syslog data that I‚Äôm capturing. Instead of looking for SSH attacks, I‚Äôm going to analyse the behaviour of my networking components.
   Note  You can find all the code to run this on GitHub.     Getting Syslog data into Kafka As before, let‚Äôs create ourselves a syslog connector in ksqlDB:</description>
    </item>
    
    <item>
      <title>Detecting and Analysing SSH Attacks with ksqlDB</title>
      <link>https://rmoff.net/2019/12/18/detecting-and-analysing-ssh-attacks-with-ksqldb/</link>
      <pubDate>Wed, 18 Dec 2019 17:23:40 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/12/18/detecting-and-analysing-ssh-attacks-with-ksqldb/</guid>
      <description>I‚Äôve written previously about ingesting Syslog into Kafka and using KSQL to analyse it. I want to revisit the subject since it‚Äôs nearly two years since I wrote about it and some things have changed since then.
 ksqlDB now includes the ability to define connectors from within it, which makes setting things up loads easier.
 You can find the full rig to run this on GitHub.
 Create and configure the Syslog connector To start with, create a source connector:</description>
    </item>
    
    <item>
      <title>Copy MongoDB collections from remote to local instance</title>
      <link>https://rmoff.net/2019/12/17/copy-mongodb-collections-from-remote-to-local-instance/</link>
      <pubDate>Tue, 17 Dec 2019 20:23:49 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/12/17/copy-mongodb-collections-from-remote-to-local-instance/</guid>
      <description>This is revisiting the blog I wrote a while back, which showed using mongodump and mongorestore to copy a MongoDB database from one machine (a Unifi CloudKey) to another. This time instead of a manual lift and shift, I wanted a simple way to automate the update of the target with changes made on the source.
 The source is as before, Unifi‚Äôs CloudKey, which runs MongoDB to store its data about the network - devices, access points, events, and so on.</description>
    </item>
    
    <item>
      <title>Kafka Connect - Request timed out</title>
      <link>https://rmoff.net/2019/11/29/kafka-connect-request-timed-out/</link>
      <pubDate>Fri, 29 Nov 2019 14:37:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/29/kafka-connect-request-timed-out/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A short &amp;amp; sweet blog post to help people Googling for this error, and me next time I encounter it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The scenario: trying to create a connector in Kafka Connect (running in distributed mode, one worker) failed with the &lt;code&gt;curl&lt;/code&gt; response&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;HTTP/1.1 &lt;span style=&#34;color:#666&#34;&gt;500&lt;/span&gt; Internal Server Error
Date: Fri, &lt;span style=&#34;color:#666&#34;&gt;29&lt;/span&gt; Nov &lt;span style=&#34;color:#666&#34;&gt;2019&lt;/span&gt; 14:33:53 GMT
Content-Type: application/json
Content-Length: &lt;span style=&#34;color:#666&#34;&gt;48&lt;/span&gt;
Server: Jetty&lt;span style=&#34;color:#666&#34;&gt;(&lt;/span&gt;9.4.18.v20190429&lt;span style=&#34;color:#666&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color:#666&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;error_code&amp;#34;&lt;/span&gt;:500,&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;Request timed out&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using tcpdump With Docker</title>
      <link>https://rmoff.net/2019/11/29/using-tcpdump-with-docker/</link>
      <pubDate>Fri, 29 Nov 2019 11:17:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/29/using-tcpdump-with-docker/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I was doing some troubleshooting between two services recently and wanting to poke around to see what was happening in the REST calls between them. Normally I‚Äôd reach for &lt;code&gt;tcpdump&lt;/code&gt; to do this but imagine my horror when I saw:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;root@ksqldb-server:/# tcpdump
bash: tcpdump: &lt;span style=&#34;color:#008000&#34;&gt;command&lt;/span&gt; not found&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Common mistakes made when configuring multiple Kafka Connect workers</title>
      <link>https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/</link>
      <pubDate>Fri, 22 Nov 2019 11:33:48 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/</guid>
      <description>Kafka Connect can be deployed in two modes: Standalone or Distributed. You can learn more about them in my Kafka Summit London 2019 talk.
 I usually recommend Distributed for several reasons:
   It can scale
  It is fault-tolerant
  It can be run on a single node sandbox or a multi-node production environment
  It is the same configuration method however you run it</description>
    </item>
    
    <item>
      <title>Streaming data from SQL Server to Kafka to Snowflake ‚ùÑÔ∏è with Kafka Connect</title>
      <link>https://rmoff.net/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</link>
      <pubDate>Wed, 20 Nov 2019 17:59:50 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</guid>
      <description>Snowflake is the data warehouse built for the cloud, so let‚Äôs get all ‚òÅÔ∏è cloudy and stream some data from Kafka running in Confluent Cloud to Snowflake!
 What I‚Äôm showing also works just as well for an on-premises Kafka cluster. I‚Äôm using SQL Server as an example data source, with Debezium to capture and stream and changes from it into Kafka.
   I‚Äôm assuming that you‚Äôve signed up for Confluent Cloud and Snowflake and are the proud owner of credentials for both.</description>
    </item>
    
    <item>
      <title>Running Dockerised Kafka Connect worker on GCP</title>
      <link>https://rmoff.net/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</link>
      <pubDate>Tue, 12 Nov 2019 14:45:43 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I &lt;a href=&#34;http://talks.rmoff.net/&#34;&gt;talk and write about Kafka and Confluent Platform&lt;/a&gt; a lot, and more and more of the demos that I‚Äôm building are around &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt;. This means that I don‚Äôt have to run or manage my own Kafka brokers, Zookeeper, Schema Registry, KSQL servers, etc which makes things a ton easier. Whilst there are managed connectors on Confluent Cloud (S3 etc), I need to run my own Kafka Connect worker for those connectors not yet provided. An example is the MQTT source connector that I use in &lt;a href=&#34;https://rmoff.dev/kssf19-ksql-video&#34;&gt;this demo&lt;/a&gt;. Up until now I‚Äôd either run this worker locally, or manually build a cloud VM. Locally is fine, as it‚Äôs all Docker, easily spun up in a single &lt;code&gt;docker-compose up -d&lt;/code&gt; command. I wanted something that would keep running whilst my laptop was off, but that was as close to my local build as possible‚Äîenter GCP and its functionality to run a container on a VM automagically.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;You can see &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/mqtt-tracker/launch-worker-container_gcloud.sh&#34;&gt;the full script here&lt;/a&gt;&lt;/strong&gt;. The rest of this article just walks through the how and why.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Debezium &amp; MySQL v8 : Public Key Retrieval Is Not Allowed</title>
      <link>https://rmoff.net/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/</link>
      <pubDate>Wed, 23 Oct 2019 11:54:51 -0400</pubDate>
      
      <guid>https://rmoff.net/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I started hitting problems when trying Debezium against MySQL v8. When creating the connector:&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using Kafka Connect and Debezium with Confluent Cloud</title>
      <link>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</link>
      <pubDate>Wed, 16 Oct 2019 16:29:34 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is based on using &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; to provide your managed Kafka and Schema Registry. All that you run yourself is the Kafka Connect worker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Optionally, you can use this &lt;a href=&#34;https://github.com/rmoff/debezium-ccloud/blob/master/docker-compose.yml&#34;&gt;Docker Compose&lt;/a&gt; to run the worker and a sample MySQL database.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Skipping bad records with the Kafka Connect JDBC sink connector</title>
      <link>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</link>
      <pubDate>Tue, 15 Oct 2019 09:58:38 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</guid>
      <description>&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The Kafka Connect framework provides generic &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;error handling and dead-letter queue capabilities&lt;/a&gt; which are available for problems with [de]serialisation and Single Message Transforms. When it comes to errors that a connector may encounter doing the actual &lt;code&gt;pull&lt;/code&gt; or &lt;code&gt;put&lt;/code&gt; of data from the source/target system, it‚Äôs down to the connector itself to implement logic around that. For example, the Elasticsearch sink connector provides configuration (&lt;code&gt;behavior.on.malformed.documents&lt;/code&gt;) that can be set so that a single bad record won‚Äôt halt the pipeline. Others, such as the JDBC Sink connector, don‚Äôt provide this &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-jdbc/issues/721&#34;&gt;yet&lt;/a&gt;. That means that if you hit this problem, you need to manually unblock it yourself. One way is to manually move the offset of the consumer on past the bad message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; : You can use &lt;code&gt;kafka-consumer-groups --reset-offsets --to-offset &amp;lt;x&amp;gt;&lt;/code&gt; to manually move the connector past a bad message&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Connect and Elasticsearch</title>
      <link>https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/</link>
      <pubDate>Mon, 07 Oct 2019 15:44:59 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I use the Elastic stack for a lot of my &lt;a href=&#34;https://talks.rmoff.net/&#34;&gt;talks&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/demo-scene/&#34;&gt;demos&lt;/a&gt; because it complements Kafka brilliantly. A few things have changed in recent releases and this blog is a quick note on some of the errors that you might hit and how to resolve them. It was inspired by a lot of the comments and discussion &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/314&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/342&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Copying data between Kafka clusters with Kafkacat</title>
      <link>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</link>
      <pubDate>Sun, 29 Sep 2019 10:43:45 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafkacat_gives_you_kafka_super_powers&#34;&gt;kafkacat gives you Kafka super powers üòé&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve &lt;a href=&#34;https://rmoff.net/categories/kafkacat/&#34;&gt;written before&lt;/a&gt; about &lt;a href=&#34;https://github.com/edenhill/kafkacat&#34;&gt;kafkacat&lt;/a&gt; and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in &lt;a href=&#34;https://talks.rmoff.net/8Oruwt/on-track-with-apache-kafka-building-a-streaming-etl-solution-with-rail-data#s9tMEWG&#34;&gt;a recent demo&lt;/a&gt; that I built in which data needed manipulating in a way that I couldn‚Äôt easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it‚Äôs getting data from &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; down to a local cluster.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Summit GoldenGate bridge run/walk</title>
      <link>https://rmoff.net/2019/09/23/kafka-summit-goldengate-bridge-run/walk/</link>
      <pubDate>Mon, 23 Sep 2019 10:55:23 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/09/23/kafka-summit-goldengate-bridge-run/walk/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Coming to Kafka Summit in San Francisco next week? Inspired by &lt;a href=&#34;https://www.facebook.com/oraclesqldev/photos/gm.1401265536847886/1228813493825348/?type=3&amp;amp;theater&#34;&gt;similar events&lt;/a&gt; at Oracle OpenWorld in past years, I‚Äôm proposing an unofficial run (or walk) across the GoldenGate bridge on the morning of Tuesday 1st October. We should be up and out and back in plenty of time to still attend the morning keynotes. Some people will run, some may prefer to walk, it‚Äôs open to everyone :)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Staying sane on the road as a Developer Advocate</title>
      <link>https://rmoff.net/2019/09/19/staying-sane-on-the-road-as-a-developer-advocate/</link>
      <pubDate>Thu, 19 Sep 2019 23:38:42 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/09/19/staying-sane-on-the-road-as-a-developer-advocate/</guid>
      <description>I‚Äôve been a full-time Developer Advocate for nearly 1.5 years now, and have learnt lots along the way. The stuff I‚Äôve learnt about being an advocate I‚Äôve written about elsewhere (here/here/here); today I want to write about something that‚Äôs just as important: staying sane and looking after yourself whilst on the road. This is also tangentially related to another of my favourite posts that I‚Äôve written: Travelling for Work, with Kids at Home.</description>
    </item>
    
    <item>
      <title>Where I&#39;ll be on the road for the remainder of 2019</title>
      <link>https://rmoff.net/2019/09/02/where-ill-be-on-the-road-for-the-remainder-of-2019/</link>
      <pubDate>Mon, 02 Sep 2019 17:36:01 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/09/02/where-ill-be-on-the-road-for-the-remainder-of-2019/</guid>
      <description>I‚Äôve had a relaxing couple of weeks off work over the summer, and came back today to realise that I‚Äôve got a fair bit of conference and meetup travel to wrap my head around for the next few months :)
 If you‚Äôre interested in where I‚Äôll be and want to come and say hi, hear about Kafka‚Äîor just grab a coffee or beer, herewith my itinerary as it currently stands.</description>
    </item>
    
    <item>
      <title>Reset Kafka Connect Source Connector Offsets</title>
      <link>https://rmoff.net/2019/08/15/reset-kafka-connect-source-connector-offsets/</link>
      <pubDate>Thu, 15 Aug 2019 10:42:34 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/08/15/reset-kafka-connect-source-connector-offsets/</guid>
      <description>Kafka Connect in distributed mode uses Kafka itself to persist the offsets of any source connectors. This is a great way to do things as it means that you can easily add more workers, rebuild existing ones, etc without having to worry about where the state is persisted. I personally always recommend using distributed mode, even if just for a single worker instance - it just makes things easier, and more standard.</description>
    </item>
    
    <item>
      <title>Starting a Kafka Connect sink connector at the end of a topic</title>
      <link>https://rmoff.net/2019/08/09/starting-a-kafka-connect-sink-connector-at-the-end-of-a-topic/</link>
      <pubDate>Fri, 09 Aug 2019 17:11:06 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/08/09/starting-a-kafka-connect-sink-connector-at-the-end-of-a-topic/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;When you create a sink connector in Kafka Connect, by default it will start reading from the beginning of the topic and stream all of the existing‚Äîand new‚Äîdata to the target. The setting that controls this behaviour is &lt;code&gt;auto.offset.reset&lt;/code&gt;, and you can see its value in the worker log when the connector runs:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#666&#34;&gt;[&lt;/span&gt;2019-08-05 23:31:35,405&lt;span style=&#34;color:#666&#34;&gt;]&lt;/span&gt; INFO ConsumerConfig values:
        allow.auto.create.topics &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#008000&#34;&gt;true&lt;/span&gt;
        auto.commit.interval.ms &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;5000&lt;/span&gt;
        auto.offset.reset &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; earliest
‚Ä¶&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Resetting a Consumer Group in Kafka</title>
      <link>https://rmoff.net/2019/08/09/resetting-a-consumer-group-in-kafka/</link>
      <pubDate>Fri, 09 Aug 2019 16:32:46 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/08/09/resetting-a-consumer-group-in-kafka/</guid>
      <description>I‚Äôve been using Replicator as a powerful way to copy data from my Kafka rig at home onto my laptop‚Äôs Kafka environment. It means that when I‚Äôm on the road I can continue to work with the same set of data and develop pipelines etc. With a VPN back home I can even keep them in sync directly if I want to.
 I hit a problem the other day where Replicator was running, but I had no data in my target topics on my laptop.</description>
    </item>
    
    <item>
      <title>Migrating Alfred Clipboard to New Laptop</title>
      <link>https://rmoff.net/2019/08/07/migrating-alfred-clipboard-to-new-laptop/</link>
      <pubDate>Wed, 07 Aug 2019 14:23:33 -0700</pubDate>
      
      <guid>https://rmoff.net/2019/08/07/migrating-alfred-clipboard-to-new-laptop/</guid>
      <description>Alfred is one of my favourite productivity tools. One of its best features is the clipboard history, which when I moved laptops and it didn‚Äôt transfer I realised quite how much I rely on this functionality in my day-to-day work.
   Whilst Alfred has the options to syncronise its preferences across machines, it seems that it doesn‚Äôt synchronise the clipboard database. To get it to work I did the following:</description>
    </item>
    
    <item>
      <title>So how DO you make those cool diagrams? July 2019 update</title>
      <link>https://rmoff.net/2019/07/11/so-how-do-you-make-those-cool-diagrams-july-2019-update/</link>
      <pubDate>Thu, 11 Jul 2019 11:12:26 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/07/11/so-how-do-you-make-those-cool-diagrams-july-2019-update/</guid>
      <description>I write and speak lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:
  How do you make those cool diagrams?
   I wrote about this originally last year but since then have evolved my approach. I‚Äôve now pretty much ditched Paper, in favour of Concepts. It was recommended to me after I published the previous post.</description>
    </item>
    
    <item>
      <title>Taking the Vienna-Munich sleeper train</title>
      <link>https://rmoff.net/2019/07/03/taking-the-vienna-munich-sleeper-train/</link>
      <pubDate>Wed, 03 Jul 2019 07:17:12 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/07/03/taking-the-vienna-munich-sleeper-train/</guid>
      <description>This week I was scheduled in to a couple of meetups, in Vienna and Munich. Flying is an inevitable part of travel since I also happen to like being home seeing my family and airplanes are usually the quickest way to make this happen. I don‚Äôt particularly enjoy flying, and there‚Äôs the environmental impact of it too‚Äîso when I realised that Vienna and Munich are relatively close to each other I looked at getting the train.</description>
    </item>
    
    <item>
      <title>Manually delete a connector from Kafka Connect</title>
      <link>https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/</link>
      <pubDate>Sun, 23 Jun 2019 11:39:46 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect has as &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST API&lt;/a&gt; through which all config should be done, including removing connectors that have been created. Sometimes though, you might have reason to want to manually do this‚Äîand since Kafka Connect running in distributed mode uses Kafka as its persistent data store, you can achieve this by manually writing to the topic yourself.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Automatically restarting failed Kafka Connect tasks</title>
      <link>https://rmoff.net/2019/06/06/automatically-restarting-failed-kafka-connect-tasks/</link>
      <pubDate>Thu, 06 Jun 2019 17:51:44 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/06/06/automatically-restarting-failed-kafka-connect-tasks/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here‚Äôs a hacky way to automatically restart Kafka Connect connectors if they fail. Restarting automatically only makes sense if it‚Äôs a transient failure; if there‚Äôs a problem with your pipeline (e.g. bad records or a mis-configured server) then you don‚Äôt gain anything from this. You might want to check out &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;Kafka Connect‚Äôs error handling and dead letter queues&lt;/a&gt; too.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Putting Kafka Connect passwords in a separate file / externalising secrets</title>
      <link>https://rmoff.net/2019/05/24/putting-kafka-connect-passwords-in-a-separate-file-/-externalising-secrets/</link>
      <pubDate>Fri, 24 May 2019 17:30:57 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/24/putting-kafka-connect-passwords-in-a-separate-file-/-externalising-secrets/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect configuration is easy - you just write some JSON! But what if you‚Äôve got credentials that you need to pass? Embedding those in a config file is not always such a smart idea. Fortunately with &lt;a href=&#34;https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations&#34;&gt;KIP-297&lt;/a&gt; which was released in Apache Kafka 2.0 there is support for external secrets. It‚Äôs extendable to use your own &lt;code&gt;ConfigProvider&lt;/code&gt;, and ships with its own for just putting credentials in a file - which I‚Äôll show here. You can &lt;a href=&#34;https://docs.confluent.io/current/connect/security.html#externalizing-secrets&#34;&gt;read more here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Deleting a Connector in Kafka Connect without the REST API</title>
      <link>https://rmoff.net/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</link>
      <pubDate>Wed, 22 May 2019 10:32:10 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect exposes a &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST interface&lt;/a&gt; through which all config and monitoring operations can be done. You can create connectors, delete them, restart them, check their status, and so on. But, I found a situation recently in which I needed to delete a connector and couldn‚Äôt do so with the REST API. Here‚Äôs another way to do it, by amending the configuration Kafka topic that Kafka Connect in distributed mode uses to persist configuration information for connectors. Note that this is not a recommended way of working with Kafka Connect‚Äîthe REST API is there for a good reason :)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>A poor man&#39;s KSQL EXPLODE/UNNEST technique</title>
      <link>https://rmoff.net/2019/05/09/a-poor-mans-ksql-explode/unnest-technique/</link>
      <pubDate>Thu, 09 May 2019 10:01:50 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/09/a-poor-mans-ksql-explode/unnest-technique/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There is an &lt;a href=&#34;https://github.com/confluentinc/ksql/issues/527&#34;&gt;open issue for support of &lt;code&gt;EXPLODE&lt;/code&gt;/&lt;code&gt;UNNEST&lt;/code&gt; functionality in KSQL&lt;/a&gt;, and if you need it then do up-vote the issue. Here I detail a hacky, but effective, workaround for exploding arrays into multiple messages‚Äîso long as you know the upper-bound on your array.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>When a Kafka Connect converter is not a _converter_</title>
      <link>https://rmoff.net/2019/05/08/when-a-kafka-connect-converter-is-not-a-_converter_/</link>
      <pubDate>Wed, 08 May 2019 10:06:50 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/08/when-a-kafka-connect-converter-is-not-a-_converter_/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect is a API within Apache Kafka and its modular nature makes it powerful and flexible. Converters are part of the API but not always fully understood. I‚Äôve written previously about &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained&#34;&gt;Kafka Connect converters&lt;/a&gt;, and this post is just a hands-on example to show even further what they are‚Äîand are not‚Äîabout.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
To understand more about Kafka Connect in general, check out my talk from Kafka Summit London &lt;a href=&#34;https://talks.rmoff.net/QZ5nsS/from-zero-to-hero-with-kafka-connect&#34;&gt;&lt;em&gt;From Zero to Hero with Kafka Connect&lt;/em&gt;&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Reading Kafka Connect Offsets via the REST Proxy</title>
      <link>https://rmoff.net/2019/05/02/reading-kafka-connect-offsets-via-the-rest-proxy/</link>
      <pubDate>Thu, 02 May 2019 10:58:27 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/02/reading-kafka-connect-offsets-via-the-rest-proxy/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;When you run Kafka Connect in distributed mode it uses a Kafka topic to store the offset information for each connector. Because it‚Äôs just a Kafka topic, you can read that information using any consumer.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Pivoting Aggregates in Ksql</title>
      <link>https://rmoff.net/2019/04/17/pivoting-aggregates-in-ksql/</link>
      <pubDate>Wed, 17 Apr 2019 15:42:56 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/04/17/pivoting-aggregates-in-ksql/</guid>
      <description>&lt;p&gt;Prompted by &lt;a href=&#34;https://stackoverflow.com/questions/55680719/aggregating-by-multiple-fields-and-map-to-one-result&#34;&gt;a question on StackOverflow&lt;/a&gt;, the requirement is to take a series of events related to a common key and for each key output a series of aggregates derived from a changing value in the events. I&amp;rsquo;ll use the data from the question, based on ticket statuses. Each ticket can go through various stages, and the requirement was to show, per customer, how many tickets are currently at each stage.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Connecting KSQL to a Secured Schema Registry</title>
      <link>https://rmoff.net/2019/04/12/connecting-ksql-to-a-secured-schema-registry/</link>
      <pubDate>Fri, 12 Apr 2019 12:59:33 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/04/12/connecting-ksql-to-a-secured-schema-registry/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;See also : &lt;a href=&#34;https://docs.confluent.io/current/ksql/docs/installation/server-config/security.html#configuring-ksql-for-secured-sr-long&#34; class=&#34;bare&#34;&gt;https://docs.confluent.io/current/ksql/docs/installation/server-config/security.html#configuring-ksql-for-secured-sr-long&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Confluent Cloud now includes a secured Schema Registry, which you can use from external applications, including KSQL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To configure KSQL for it you need to set:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ksql.schema.registry.url&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;https://&amp;lt;Schema Registry endpoint&amp;gt;
ksql.schema.registry.basic.auth.credentials.source&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;USER_INFO
ksql.schema.registry.basic.auth.user.info&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&amp;lt;Schema Registry API Key&amp;gt;:&amp;lt;Schema Registry API Secret&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Exploring KSQL Stream-Stream Joins</title>
      <link>https://rmoff.net/2019/03/28/exploring-ksql-stream-stream-joins/</link>
      <pubDate>Thu, 28 Mar 2019 14:46:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/03/28/exploring-ksql-stream-stream-joins/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;What can you use stream-stream joins for? Can you use them to join between a stream of orders and stream of related shipments to do useful things? What‚Äôs not supported in KSQL, where are the cracks?&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Terminate All KSQL Queries</title>
      <link>https://rmoff.net/2019/03/25/terminate-all-ksql-queries/</link>
      <pubDate>Mon, 25 Mar 2019 16:45:40 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/03/25/terminate-all-ksql-queries/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Before you can drop a stream or table that‚Äôs populated by a query in KSQL, you have to terminate any queries upon which the object is dependent. Here‚Äôs a bit of &lt;code&gt;jq&lt;/code&gt; &amp;amp; &lt;code&gt;xargs&lt;/code&gt; magic to terminate &lt;strong&gt;all&lt;/strong&gt; queries that are currently running&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Quick Thoughts on Not Making a Crap Slide Deck</title>
      <link>https://rmoff.net/2019/03/19/quick-thoughts-on-not-making-a-crap-slide-deck/</link>
      <pubDate>Tue, 19 Mar 2019 10:10:34 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/03/19/quick-thoughts-on-not-making-a-crap-slide-deck/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This post is the companion to an earlier one that I wrote about &lt;a href=&#34;https://rmoff.net/2018/12/19/quick-thoughts-on-not-writing-a-crap-abstract/&#34;&gt;conference abstracts&lt;/a&gt;. In the same way that the last one was inspired by reviewing a ton of abstracts and noticing a recurring pattern in my suggestions, so this one comes from reviewing a bunch of slide decks for a forthcoming conference. They all look like good talks, but in several cases &lt;em&gt;these great talks are fighting to get out from underneath the deadening weight of slides&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Herewith follows my highly-opinionated, fairly-subjective, and extremely-terse advice and general suggestions for slide decks. You can also find relating ramblings in &lt;a href=&#34;https://rmoff.net/2019/03/01/preparing-a-new-talk/&#34;&gt;this recent post&lt;/a&gt; too. My friend and colleague Vik Gamov also wrote &lt;a href=&#34;https://gamov.io/posts/2019/03/15/quick-tips-on-designing-your-next-presentation.html&#34;&gt;a good post&lt;/a&gt; on this same topic, and linked to &lt;a href=&#34;https://player.oreilly.com/videos/9781491954980&#34;&gt;a good video&lt;/a&gt; that I‚Äôd recommend you watch.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using httpie with the Kafka REST Proxy</title>
      <link>https://rmoff.net/2019/03/08/using-httpie-with-the-kafka-rest-proxy/</link>
      <pubDate>Fri, 08 Mar 2019 15:37:42 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/03/08/using-httpie-with-the-kafka-rest-proxy/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This shows how to use &lt;a href=&#34;https://httpie.org/&#34;&gt;httpie&lt;/a&gt; with the &lt;a href=&#34;https://docs.confluent.io/current/kafka-rest/docs/index.html&#34;&gt;Confluent REST Proxy&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_send_data&#34;&gt;Send data&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#008000&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{&amp;#34;records&amp;#34;:[{&amp;#34;value&amp;#34;:{&amp;#34;foo&amp;#34;:&amp;#34;bar&amp;#34;}}]}&amp;#39;&lt;/span&gt; | &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;  http POST http://localhost:8082/topics/jsontest &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;  Content-Type:application/vnd.kafka.json.v2+json Accept:application/vnd.kafka.v2+json&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Preparing a New Talk</title>
      <link>https://rmoff.net/2019/03/01/preparing-a-new-talk/</link>
      <pubDate>Fri, 01 Mar 2019 11:00:26 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/03/01/preparing-a-new-talk/</guid>
      <description>I‚Äôve written quite a few talks over the years, but usually as a side-line to my day job. In my role as a Developer Advocate, talks are part of What I Do, and so I can dedicate more time to it. A lot of the talks I‚Äôve done previously have evolved through numerous iterations, and with a new talk to deliver for the &amp;#34;Spring Season&amp;#34; of conferences, I thought it would be interesting to track what it took from concept to actual delivery.</description>
    </item>
    
    <item>
      <title>Travelling for Work, with Kids at Home</title>
      <link>https://rmoff.net/2019/02/09/travelling-for-work-with-kids-at-home/</link>
      <pubDate>Sat, 09 Feb 2019 14:13:21 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/02/09/travelling-for-work-with-kids-at-home/</guid>
      <description>I began travelling for my job when my first child was three months old. But don‚Äôt mistake correlation for causation‚Ä¶it wasn‚Äôt the broken nights&amp;#39; sleep that forced me onto the road, but an excellent job opportunity that seemed worth the risk. Nearly eight years later and I‚Äôm in a different job but still with a bunch of travel involved. How much I travel has varied. It‚Äôs tended to average around 30%, but has peaked at way more than that.</description>
    </item>
    
    <item>
      <title>Kafka Connect Change Log Level and Write Log to File</title>
      <link>https://rmoff.net/2019/01/29/kafka-connect-change-log-level-and-write-log-to-file/</link>
      <pubDate>Tue, 29 Jan 2019 11:15:01 -0800</pubDate>
      
      <guid>https://rmoff.net/2019/01/29/kafka-connect-change-log-level-and-write-log-to-file/</guid>
      <description>By default Kafka Connect sends its output to stdout, so you‚Äôll see it on the console, Docker logs, or wherever. Sometimes you might want to route it to file, and you can do this by reconfiguring log4j. You can also change the configuration to get more (or less) detail in the logs by changing the log level.
 Finding the log configuration file The configuration file is called connect-log4j.properties and usually found in etc/kafka/connect-log4j.</description>
    </item>
    
    <item>
      <title>Replacing UTF8 non-breaking-space with bash/sed on the Mac</title>
      <link>https://rmoff.net/2019/01/21/replacing-utf8-non-breaking-space-with-bash/sed-on-the-mac/</link>
      <pubDate>Mon, 21 Jan 2019 14:01:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/21/replacing-utf8-non-breaking-space-with-bash/sed-on-the-mac/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A script I‚Äôd batch-run on my Markdown files had inserted a UTF-8 non-breaking-space between Markdown heading indicator and the text, which meant that &lt;code&gt;&lt;mark&gt;#&lt;/mark&gt; My title&lt;/code&gt; actually got rendered as that, instead of an H3 title.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Looking at the file contents, I could see it wasn‚Äôt just a space between the &lt;code&gt;#&lt;/code&gt; and the text, but a non-breaking space.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How KSQL handles case</title>
      <link>https://rmoff.net/2019/01/21/how-ksql-handles-case/</link>
      <pubDate>Mon, 21 Jan 2019 12:05:48 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/21/how-ksql-handles-case/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.confluent.io/ksql&#34;&gt;KSQL&lt;/a&gt; is generally case-sensitive. Very sensitive, at times ;-)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>KSQL REST API cheatsheet</title>
      <link>https://rmoff.net/2019/01/17/ksql-rest-api-cheatsheet/</link>
      <pubDate>Thu, 17 Jan 2019 12:12:11 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/17/ksql-rest-api-cheatsheet/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Full reference is &lt;a href=&#34;https://docs.confluent.io/current/ksql/docs/developer-guide/api.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Confluent Schema Registry REST API cheatsheet</title>
      <link>https://rmoff.net/2019/01/17/confluent-schema-registry-rest-api-cheatsheet/</link>
      <pubDate>Thu, 17 Jan 2019 11:25:40 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/17/confluent-schema-registry-rest-api-cheatsheet/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/docs/index.html&#34;&gt;Schema Registry&lt;/a&gt; support a &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/docs/api.html&#34;&gt;REST API&lt;/a&gt; for finding out information about the schemas within it. Here‚Äôs a quick cheatsheat with REST calls that I often use.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>What to Do When Docker on the Mac Runs Out of Space</title>
      <link>https://rmoff.net/2019/01/09/what-to-do-when-docker-on-the-mac-runs-out-of-space/</link>
      <pubDate>Wed, 09 Jan 2019 10:18:20 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/09/what-to-do-when-docker-on-the-mac-runs-out-of-space/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I use Docker and Docker Compose &lt;em&gt;a lot&lt;/em&gt;. Like, every day. It‚Äôs a fantastic way to build repeatable demos and examples, that can be torn down and spun up in a repeatable way. But‚Ä¶what happens when the demo that was working is spun up and then tail spins down in a blaze of flames?&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Quick Thoughts on Not Writing a Crap Abstract</title>
      <link>https://rmoff.net/2018/12/19/quick-thoughts-on-not-writing-a-crap-abstract/</link>
      <pubDate>Wed, 19 Dec 2018 22:26:04 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/19/quick-thoughts-on-not-writing-a-crap-abstract/</guid>
      <description>I‚Äôve reviewed a bunch of abstracts in the last couple of days, here are some common suggestions I made:
   No need to include your company name in the abstract text. Chances are I‚Äôve not heard of your company, and even if I have, what does it add to my comprehension of your abstract and what you‚Äôre going to talk about? Possible exception would be the &amp;#34;hot&amp;#34; tech companies where people will see a talk just because it‚Äôs Netflix etc</description>
    </item>
    
    <item>
      <title>Moving from Ghost to Hugo</title>
      <link>https://rmoff.net/2018/12/17/moving-from-ghost-to-hugo/</link>
      <pubDate>Mon, 17 Dec 2018 23:00:21 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/17/moving-from-ghost-to-hugo/</guid>
      <description>Why? I‚Äôve been blogging for quite a few years now, starting on Blogger, soon onto WordPress, and then to Ghost a couple of years ago. Blogger was fairly lame, WP yucky, but I really do like Ghost. It‚Äôs simple and powerful and was perfect for my needs. My needs being, an outlet for technical content that respected formatting, worked with a markup language (Markdown), and didn‚Äôt f**k things up in the way that WP often would in its WYSIWYG handling of content.</description>
    </item>
    
    <item>
      <title>Pull new version of multiple Docker images</title>
      <link>https://rmoff.net/2018/12/17/pull-new-version-of-multiple-docker-images/</link>
      <pubDate>Mon, 17 Dec 2018 17:44:02 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/17/pull-new-version-of-multiple-docker-images/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Tiny little snippet this one. Given a list of images:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker images|grep confluent
confluentinc/cp-enterprise-kafka                5.0.0               d0c5528d7f99        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        600MB
confluentinc/cp-kafka                           5.0.0               373a4e31e02e        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        558MB
confluentinc/cp-zookeeper                       5.0.0               3cab14034c43        &lt;span style=&#34;color:#666&#34;&gt;3&lt;/span&gt; months ago        558MB
confluentinc/cp-ksql-server                     5.0.0               691bc3c1991f        &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt; months ago        493MB
confluentinc/cp-ksql-cli                        5.0.0               e521f3e787d6        &lt;span style=&#34;color:#666&#34;&gt;4&lt;/span&gt; months ago        488MB
‚Ä¶&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Now there‚Äôs a new version available, and you want to pull down all the latest ones for it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker images|grep &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;^confluentinc&amp;#34;&lt;/span&gt;|awk &lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;|xargs -Ifoo docker pull foo:5.1.0&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Docker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka</title>
      <link>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</link>
      <pubDate>Sat, 15 Dec 2018 22:00:55 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</guid>
      <description>A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad‚Ä¶how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.</description>
    </item>
    
    <item>
      <title>Streaming data from Oracle into Kafka</title>
      <link>https://rmoff.net/2018/12/12/streaming-data-from-oracle-into-kafka/</link>
      <pubDate>Wed, 12 Dec 2018 09:49:04 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/12/streaming-data-from-oracle-into-kafka/</guid>
      <description>This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018 (refreshed June 2020). For a more detailed background to why and how at a broader level for all databases (not just Oracle) see this blog and this talk.
  What techniques &amp;amp; tools are there? Franck Pachot has written up an excellent analysis of the options available here.
As of June 2020, this is what the line-up looks like:</description>
    </item>
    
    <item>
      <title>Tools I Use: iPad Pro</title>
      <link>https://rmoff.net/2018/12/11/tools-i-use-ipad-pro/</link>
      <pubDate>Tue, 11 Dec 2018 15:12:15 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/11/tools-i-use-ipad-pro/</guid>
      <description>I&amp;rsquo;ve written recently about how I create the diagrams in my blog posts and talks, and from discussions around that, a couple of people were interested more broadly in how I use my iPad Pro. So, on the basis that if two people are interested maybe others are (and if no-one else is, I have a copy-and-paste answer to give to those two people) here we go.
Kit  iPad Pro 10.</description>
    </item>
    
    <item>
      <title>So how DO you make those cool diagrams?</title>
      <link>https://rmoff.net/2018/12/10/so-how-do-you-make-those-cool-diagrams/</link>
      <pubDate>Mon, 10 Dec 2018 12:38:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/10/so-how-do-you-make-those-cool-diagrams/</guid>
      <description>I write and speak lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:
 How do you make those cool diagrams?
 So here&amp;rsquo;s a short, and longer, answer!
 Update July 2019 I&amp;rsquo;ve moved away from Paper -&amp;gt; read more here
  tl;dr An iOS app called Paper, from a company called FiftyThree</description>
    </item>
    
    <item>
      <title>Get mtr working on the Mac</title>
      <link>https://rmoff.net/2018/12/08/get-mtr-working-on-the-mac/</link>
      <pubDate>Sat, 08 Dec 2018 12:45:40 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/08/get-mtr-working-on-the-mac/</guid>
      <description>Install Not sure why the brew doesn&amp;rsquo;t work as it used to, but here&amp;rsquo;s how to get it working:
brew install mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr /usr/local/bin/mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr-packet /usr/local/bin/mtr-packet  (If you don&amp;rsquo;t do the two symbolic links (ln) you&amp;rsquo;ll get mtr: command not found or mtr: Failure to start mtr-packet: Invalid argument)
Run sudo mtr google.com  </description>
    </item>
    
    <item>
      <title>Kafka Connect CLI tricks</title>
      <link>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</link>
      <pubDate>Mon, 03 Dec 2018 14:50:45 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</guid>
      <description>I do lots of work with Kafka Connect, almost entirely in Distributed mode‚Äîeven just with 1 node -&amp;gt; makes scaling out much easier when/if needed. Because I&amp;rsquo;m using Distributed mode, I use the Kafka Connect REST API to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.</description>
    </item>
    
    <item>
      <title>Logging in as root on Oracle Database Docker image</title>
      <link>https://rmoff.net/2018/11/30/logging-in-as-root-on-oracle-database-docker-image/</link>
      <pubDate>Fri, 30 Nov 2018 12:13:41 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/11/30/logging-in-as-root-on-oracle-database-docker-image/</guid>
      <description>&lt;p&gt;tl;dr:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;docker &lt;span style=&#34;color:#008000&#34;&gt;exec&lt;/span&gt; --interactive &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;            --tty &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;            --user root &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;            --workdir / &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;            oracle-container-name bash&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ERROR: Invalid interpolation format for &#34;command&#34; option in service‚Ä¶</title>
      <link>https://rmoff.net/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/</link>
      <pubDate>Tue, 20 Nov 2018 17:47:54 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/</guid>
      <description>&lt;p&gt;Doing some funky Docker Compose stuff, including:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Flatten CDC records in KSQL</title>
      <link>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</link>
      <pubDate>Thu, 11 Oct 2018 15:13:59 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</guid>
      <description>&lt;h3 id=&#34;the-problem---nested-messages-in-kafka&#34;&gt;The problem - nested messages in Kafka&lt;/h3&gt;
&lt;p&gt;Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exploring JMX with jmxterm</title>
      <link>https://rmoff.net/2018/09/19/exploring-jmx-with-jmxterm/</link>
      <pubDate>Wed, 19 Sep 2018 08:11:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/09/19/exploring-jmx-with-jmxterm/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://github.com/jiaqi/jmxterm/&#34;&gt;jmxterm repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Download jmxterm from &lt;a href=&#34;https://docs.cyclopsgroup.org/jmxterm&#34;&gt;https://docs.cyclopsgroup.org/jmxterm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Accessing Kafka Docker containers&#39; JMX from host</title>
      <link>https://rmoff.net/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</link>
      <pubDate>Mon, 17 Sep 2018 15:29:48 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</guid>
      <description>See also docs.
To help future Googlers‚Ä¶ with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables: &amp;lt;x&amp;gt;_JMX_HOSTNAME and &amp;lt;x&amp;gt;_JMX_PORT, prefixed by a component name.
  &amp;lt;x&amp;gt;_JMX_HOSTNAME - the hostname/IP of the JMX host machine, as accessible from the JMX Client.
This is used by the JMX client to connect back into JMX, so must be accessible from the host machine running the JMX client.</description>
    </item>
    
    <item>
      <title>Sending multiline messages to Kafka</title>
      <link>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</link>
      <pubDate>Tue, 04 Sep 2018 08:26:51 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</guid>
      <description>(SO answer repost)
You can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):
kafkacat -b kafka:29092 \ -t test_topic_01 \ -D/ \ -P &amp;lt;&amp;lt;EOF this is a string message with a line break/this is another message with two line breaks! EOF  Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140</description>
    </item>
    
    <item>
      <title>Window Timestamps in KSQL / Integration with Elasticsearch</title>
      <link>https://rmoff.net/2018/09/03/window-timestamps-in-ksql-/-integration-with-elasticsearch/</link>
      <pubDate>Mon, 03 Sep 2018 16:16:30 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/09/03/window-timestamps-in-ksql-/-integration-with-elasticsearch/</guid>
      <description>KSQL provides the ability to create windowed aggregations. For example, count the number of messages in a 1 minute window, grouped by a particular column:
CREATE TABLE RATINGS_BY_CLUB_STATUS AS \ SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \ FROM RATINGS_WITH_CUSTOMER_DATA \ WINDOW TUMBLING (SIZE 1 MINUTES) \ GROUP BY CLUB_STATUS; How KSQL, and Kafka Streams, stores the window timestamp associated with an aggregate, has recently changed. See #1497 for details.
Whereas previously the Kafka message timestamp (accessible through the KSQL ROWTIME system column) stored the start of the window for which the aggregate had been calculated, this changed in July 2018 to instead be the timestamp of the latest message to update that aggregate value.</description>
    </item>
    
    <item>
      <title>Where I&#39;m speaking in the rest of 2018</title>
      <link>https://rmoff.net/2018/08/21/where-im-speaking-in-the-rest-of-2018/</link>
      <pubDate>Tue, 21 Aug 2018 21:01:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/08/21/where-im-speaking-in-the-rest-of-2018/</guid>
      <description>There&amp;rsquo;s lots going on in the next few months :-)
I&amp;rsquo;m particularly excited to be speaking at several notable conferences for the first time, including JavaZone, USENIX LISA, and Devoxx.
As always, if you&amp;rsquo;re nearby then hope to see you there, and let me know if you want to meet for a coffee or beer!
 September üá™üá∏ Madrid, Spain  6th Sept: Madrid Kafka Meetup  üá≥üá¥ Oslo, Norway  10th Sept: Oslo Kafka Meetup 11th Sept: JavaZone  üáßüá™ Antwerp/Brussels, Belgium  25th Sept: Brussels Kafka Meetup  üá™üá∏ Barcelona, Spain  26th Sept: Barcelona Kafka Meetup   October üá¨üáß Leeds, UK  4th Oct: The JVM Thing meetup  üá∫üá∏ Nashville (TN), USA  31st Oct: LISA18   November üá©üá™ M√ºnich, Germany  7th Nov: W-JAX  üáßüá™ Antwerp, Belgium  13th Nov: Devoxx Belgium  üáµüá± Krakow, Poland  26th Nov: CoreDump   December üá¨üáß Liverpool, UK  4th Dec: UKOUG TECH 18  üá©üá™ Frankfurt, Germany  10th Dec: Apache Kafka Meetup 11th Dec: IT Days  </description>
    </item>
    
    <item>
      <title>Kafka Listeners - Explained</title>
      <link>https://rmoff.net/2018/08/02/kafka-listeners-explained/</link>
      <pubDate>Thu, 02 Aug 2018 19:38:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/08/02/kafka-listeners-explained/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This was cross-posted on the &lt;a href=&#34;https://www.confluent.io/blog/kafka-listeners-explained&#34;&gt;Confluent.io blog&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This question comes up on StackOverflow and such places a &lt;strong&gt;lot&lt;/strong&gt;, so here&amp;rsquo;s something to try and help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; : You need to set &lt;code&gt;advertised.listeners&lt;/code&gt; (or &lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address‚Äìand if that&amp;rsquo;s not reachable then problems ensue.&lt;/p&gt;
&lt;p&gt;Put another way, courtesy of Spencer Ruport:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;LISTENERS&lt;/code&gt; are what interfaces Kafka binds to. &lt;code&gt;ADVERTISED_LISTENERS&lt;/code&gt;  are how clients can connect.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Syntax highlighting code for presentation slides</title>
      <link>https://rmoff.net/2018/06/20/syntax-highlighting-code-for-presentation-slides/</link>
      <pubDate>Wed, 20 Jun 2018 18:32:10 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/06/20/syntax-highlighting-code-for-presentation-slides/</guid>
      <description>So you&amp;rsquo;ve got a code sample you want to share in a presentation, but whilst it looks beautiful in your text-editor with syntax highlighting, it&amp;rsquo;s fugly in Keynote? You could screenshot it and paste the image into your slide, but you just know that you&amp;rsquo;ll want to change that code, and end up re-snapshotting it‚Ä¶what a PITA.
Better to have a nicely syntax-highlighted code snippet that you can paste as formatted text into Keynote and amend from there as needed.</description>
    </item>
    
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana‚Äîand how KSQL again came into use for building some stream processing as a result of the discovery made.
The data came from my home Ubiquiti router, and took two forms:</description>
    </item>
    
    <item>
      <title>Compare and apply a diff / patch recursively</title>
      <link>https://rmoff.net/2018/06/07/compare-and-apply-a-diff-/-patch-recursively/</link>
      <pubDate>Thu, 07 Jun 2018 14:35:36 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/06/07/compare-and-apply-a-diff-/-patch-recursively/</guid>
      <description>Hacky way to keep config files in sync when there&amp;rsquo;s a new version of some software.
Caveat : probably completely wrong, may not pick up config entries added in the new version, etc. But, works for me right here right now ;-)
So let&amp;rsquo;s say we have two folders:
confluent-4.1.0 confluent-4.1.1  Same structures, different versions. 4.1.0 was set up with our local config in ./etc, that we want to preserve.</description>
    </item>
    
    <item>
      <title>Kafka Connect and Oracle data types</title>
      <link>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</link>
      <pubDate>Mon, 21 May 2018 08:59:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</guid>
      <description>The Kafka Connect JDBC Connector by default does not cope so well with:
 NUMBER columns with no defined precision/scale. You may end up with apparent junk (bytes) in the output, or just errors. TIMESTAMP WITH LOCAL TIME ZONE. Throws JDBC type -102 not currently supported warning in the log.  Read more about NUMBER data type in the Oracle docs.
tl;dr : How do I make it work? There are several options:</description>
    </item>
    
    <item>
      <title>Stream-Table Joins in KSQL: Stream events must be timestamped after the Table messages</title>
      <link>https://rmoff.net/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</link>
      <pubDate>Thu, 17 May 2018 10:16:43 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</guid>
      <description>(preserving this StackOverflow answer for posterity and future Googlers)
tl;dr When doing a stream-table join, your table messages must already exist (and must be timestamped) before the stream messages. If you re-emit your source stream messages, after the table topic is populated, the join will succeed.
Example data Use kafakcat to populate topics:
kafkacat -b localhost:9092 -P -t sessionDetails &amp;lt;&amp;lt;EOF {&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1} {&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2} EOF kafkacat -b localhost:9092 -P -t voipDetails &amp;lt;&amp;lt;EOF {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1a&amp;quot;} {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1b&amp;quot;} {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2,&amp;quot;Details&amp;quot;:&amp;quot;Bar2&amp;quot;} EOF  Validate topic contents:</description>
    </item>
    
    <item>
      <title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
      <link>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
      <pubDate>Thu, 10 May 2018 12:56:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
      <description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Use &lt;code&gt;curl&lt;/code&gt; to pull data from the Mockaroo REST endpoint, and pipe it into &lt;code&gt;kafkacat&lt;/code&gt;, thus:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \
kafkacat -b localhost:9092 -t purchases -P
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Tue, 27 Mar 2018 18:52:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong
MongoDB config - enabling replica sets For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:
Docs: Replication / Convert a Standalone to a Replica Set
Stop Mongo:
rmoff@proxmox01 ~&amp;gt; sudo service mongod stop Add replica set config to /etc/mongod.</description>
    </item>
    
    <item>
      <title>Cloning Ubiquiti&#39;s MongoDB instance to a separate server</title>
      <link>https://rmoff.net/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/</link>
      <pubDate>Tue, 27 Mar 2018 18:45:20 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/</guid>
      <description>DISCLAIMER: I am not a MongoDB person (even if it is Web Scale X-D) - below instructions may work for you, they may not. Use with care!
For some work I&amp;rsquo;ve been doing I wanted to access the data in Ubiquiti&amp;rsquo;s Unifi controller which it stores in MongoDB. Because I didn&amp;rsquo;t want to risk my actual Unifi device by changing local settings to enable remote access, and also because the version of MongoDB on it is older than ideal, I wanted to clone the data elsewhere.</description>
    </item>
    
    <item>
      <title>Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Sat, 24 Mar 2018 14:58:14 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>Debezium is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we&amp;rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.
The software versions used here are:
 Confluent Platform 4.</description>
    </item>
    
    <item>
      <title>KSQL: Topic ‚Ä¶ does not conform to the requirements</title>
      <link>https://rmoff.net/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/</link>
      <pubDate>Tue, 06 Mar 2018 23:08:11 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/</guid>
      <description>io.confluent.ksql.exception.KafkaTopicException: Topic &#39;KSQL_NOTIFY&#39; does not conform to the requirements Partitions:1 v 4. Replication: 1 v 1 Why? Because the topic KSQL creates to underpin a CREATE STREAM AS SELECT or CREATE TABLE AS SELECT already exists, and doesn&amp;rsquo;t match what it expects. By default it will create partitions &amp;amp; replicas based on the same values of the input topic.
Options:
  Use a different topic, via the WITH (KAFKA_TOPIC=&#39;FOO&#39;) syntax, e.</description>
    </item>
    
    <item>
      <title>Streaming data from Kafka into Elasticsearch</title>
      <link>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
      <pubDate>Tue, 06 Mar 2018 22:21:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
      <description>This article is part of a series exploring Streaming ETL in practice. You can read about setting up the ingest of realtime events from a standard Oracle platform, and building streaming ETL using KSQL.
 This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.</description>
    </item>
    
    <item>
      <title>Installing the Python Kafka library from Confluent - troubleshooting some silly errors‚Ä¶</title>
      <link>https://rmoff.net/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</guid>
      <description>System:
rmoff@proxmox01:~$ uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux rmoff@proxmox01:~$ head -n1 /etc/os-release PRETTY_NAME=&amp;quot;Debian GNU/Linux 8 (jessie)&amp;quot; rmoff@proxmox01:~$ python --version Python 2.7.9 Following:
 https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/ https://github.com/confluentinc/confluent-kafka-python  Install librdkafka, which is a pre-req for the Python library:
wget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add - sudo add-apt-repository &amp;quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main&amp;quot; sudo apt-get install librdkafka-dev python-dev  Setup virtualenv:
sudo apt-get install virtualenv virtualenv kafka_push_notify source .</description>
    </item>
    
    <item>
      <title>Why Do We Need Streaming ETL?</title>
      <link>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</guid>
      <description>(This is an expanded version of the intro to an article I posted over on the Confluent blog. Here I get to be as verbose as I like ;))
My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe.</description>
    </item>
    
    <item>
      <title>HOWTO: Oracle GoldenGate &#43; Apache Kafka &#43; Schema Registry &#43; Swingbench</title>
      <link>https://rmoff.net/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/</link>
      <pubDate>Thu, 01 Feb 2018 23:15:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/</guid>
      <description>This is the detailed step-by-step if you want to recreate the process I describe in the Confluent blog here
 I used Oracle&amp;rsquo;s Oracle Developer Days VM, which comes preinstalled with Oracle 12cR2. You can see the notes on how to do this here. These notes take you through installing and configuring:
 Swingbench, to create a sample &amp;ldquo;Order Entry&amp;rdquo; schema and simulate events on the Oracle database Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith)  I&amp;rsquo;m using Oracle GoldenGate 12.</description>
    </item>
    
    <item>
      <title>Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available</title>
      <link>https://rmoff.net/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</link>
      <pubDate>Wed, 03 Jan 2018 11:26:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</guid>
      <description>See also Kafka Listeners - Explained
 A short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:
WARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)  KSQL was throwing a similar error:
KSQL cannot initialize AdminCLient.  I had correctly set the machine&amp;rsquo;s hostname in my Kafka server.</description>
    </item>
    
    <item>
      <title>Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform</title>
      <link>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</link>
      <pubDate>Tue, 21 Nov 2017 17:31:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</guid>
      <description>Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!
 I used the Oracle Developer Days VM for this - it&amp;rsquo;s preinstalled with Oracle 12cR2. Big Data Lite is nice but currently has an older version of GoldenGate.
Login to the VM (oracle/oracle) and then install some useful things:
sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop sudo su - cd /etc/yum.</description>
    </item>
    
    <item>
      <title>Where will I be at OpenWorld / Oak Table World?</title>
      <link>https://rmoff.net/2017/09/29/where-will-i-be-at-openworld-/-oak-table-world/</link>
      <pubDate>Fri, 29 Sep 2017 19:02:55 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/29/where-will-i-be-at-openworld-/-oak-table-world/</guid>
      <description>Here&amp;rsquo;s where I&amp;rsquo;ll be!
If you use Google Calendar you can click on individual entries above and select copy to my calendar - which of course you&amp;rsquo;ll want to do for all the ones I&amp;rsquo;ve marked as [SPEAKING] :-)
Here&amp;rsquo;s a list of all the Apache Kafka talks at OpenWorld and JavaOne, most of which I&amp;rsquo;ll be trying to get to.</description>
    </item>
    
    <item>
      <title>Apache Kafka‚Ñ¢ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017</title>
      <link>https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</link>
      <pubDate>Wed, 20 Sep 2017 15:46:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</guid>
      <description>There&amp;rsquo;s an impressive 19 sessions that cover Apache Kafka‚Ñ¢ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for OOW, JavaOne, and Oak Table World. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!
Check out the writeup of my previous visit to OOW including useful tips here.</description>
    </item>
    
    <item>
      <title>Oracle GoldenGate / Kafka Connect Handler troubleshooting</title>
      <link>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</link>
      <pubDate>Tue, 12 Sep 2017 21:55:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</guid>
      <description>The Replicat was kapput:
GGSCI (localhost.localdomain) 3&amp;gt; info rkconnoe REPLICAT RKCONNOE Last Started 2017-09-12 17:06 Status ABENDED Checkpoint Lag 00:00:00 (updated 00:46:34 ago) Log Read Checkpoint File /u01/app/ogg/dirdat/oe000000 First Record RBA 0 So checking the OGG error log ggserr.log showed
2017-09-12T17:06:17.572-0400 ERROR OGG-15051 Oracle GoldenGate Delivery, rkconnoe.prm: Java or JNI exception: oracle.goldengate.util.GGException: Error detected handling operation added event. 2017-09-12T17:06:17.572-0400 ERROR OGG-01668 Oracle GoldenGate Delivery, rkconnoe.prm: PROCESS ABENDING. So checking the replicat log dirrpt/RKCONNOE_info_log4j.</description>
    </item>
    
    <item>
      <title>What is Markdown, and Why is it Awesome?</title>
      <link>https://rmoff.net/2017/09/12/what-is-markdown-and-why-is-it-awesome/</link>
      <pubDate>Tue, 12 Sep 2017 19:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/12/what-is-markdown-and-why-is-it-awesome/</guid>
      <description>Markdown is a plain-text formatting syntax. It enables you write documents in plain text, readable by others in plain text, and optionally rendered into nicely formatted PDF, HTML, DOCX etc.
It&amp;rsquo;s used widely in software documentation, particularly open-source, because it enables richer formatting than plain-text alone, but without constraining authors or readers to a given software platform.
Platforms such as github natively support Markdown rendering - so you write your README etc in markdown, and when viewed on github it is automagically rendered - without you needing to actually do anything.</description>
    </item>
    
    <item>
      <title>Conferences &amp; Meetups at which I&#39;ll be speaking - 2017</title>
      <link>https://rmoff.net/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/</link>
      <pubDate>Mon, 11 Sep 2017 06:45:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/</guid>
      <description>I&amp;rsquo;m excited to be speaking at several conferences and meetups over the next few months. Unsurprisingly, the topic will be Apache Kafka!
If you&amp;rsquo;re at any of these, please do come and say hi :)
Apache Kafka Meetup - London My first time talking at the London Apache Kafka Meetup - always a sold-out crowd, this will be fun!
 September 20th, 19:00 : Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka  Slides are available here    Oracle OpenWorld - San Francisco This will be my second time at OOW - I wrote up my previous trip here</description>
    </item>
    
    <item>
      <title>Kafka Connect - JsonDeserializer with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields</title>
      <link>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</link>
      <pubDate>Wed, 06 Sep 2017 12:00:25 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</guid>
      <description>An error that I see coming up frequently in the Kafka Connect community (e.g. mailing list, Slack group, StackOverflow) is:
JsonDeserializer with schemas.enable requires &amp;quot;schema&amp;quot; and &amp;quot;payload&amp;quot; fields and may not contain additional fields  or
No fields found using key and value schemas for table: foo-bar  You can see an explanation, and solution, for the issue in my StackOverflow answer here: https://stackoverflow.com/a/45940013/350613
If you&amp;rsquo;re using schemas.enable in the Connector configuration, you must have schema and payload as the root-level elements of your JSON message ( Which is pretty much verbatim what the error says üòÅ), like this:</description>
    </item>
    
    <item>
      <title>Simple export/import of Data Sources in Grafana</title>
      <link>https://rmoff.net/2017/08/08/simple-export/import-of-data-sources-in-grafana/</link>
      <pubDate>Tue, 08 Aug 2017 19:32:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/08/08/simple-export/import-of-data-sources-in-grafana/</guid>
      <description>Grafana API Reference
Export all Grafana data sources to data_sources folder mkdir -p data_sources &amp;amp;&amp;amp; curl -s &amp;quot;http://localhost:3000/api/datasources&amp;quot; -u admin:admin|jq -c -M &#39;.[]&#39;|split -l 1 - data_sources/  This exports each data source to a separate JSON file in the data_sources folder.
Load data sources back in from folder This submits every file that exists in the data_sources folder to Grafana as a new data source definition.
for i in data_sources/*; do \ curl -X &amp;quot;POST&amp;quot; &amp;quot;http://localhost:3000/api/datasources&amp;quot; \ -H &amp;quot;Content-Type: application/json&amp;quot; \ --user admin:admin \ --data-binary @$i done  </description>
    </item>
    
    <item>
      <title>Linux - USB disk connection problems - uas: probe failed with error -12</title>
      <link>https://rmoff.net/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/</link>
      <pubDate>Wed, 21 Jun 2017 06:14:45 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/</guid>
      <description>Usually connecting external disks in Linux is easy. Plug it in, run fdisk -l or lsblk | grep disk to identify the device ID, and then mount it.
Unfortunately in this instance, plugging in my Seagate 2TB wasn&amp;rsquo;t so simple. The server is running Proxmox:
# uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux  No device showed up on lsblk or fdisk -l.</description>
    </item>
    
    <item>
      <title>Configuring Kafka Connect to log REST HTTP messages to a separate file</title>
      <link>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</link>
      <pubDate>Mon, 12 Jun 2017 15:28:15 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</guid>
      <description>Kafka&amp;rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from the many available, it&amp;rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the Confluent Control Center, for both management and monitoring:
BUT &amp;hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured.</description>
    </item>
    
    <item>
      <title>kafka.common.KafkaException: No key found on line 1</title>
      <link>https://rmoff.net/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</link>
      <pubDate>Fri, 12 May 2017 00:52:41 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</guid>
      <description>A very silly PEBCAK problem this one, but Google hits weren&amp;rsquo;t so helpful so here goes.
Running a console producer, specifying keys:
kafka-console-producer \ --broker-list localhost:9092 \ --topic test_topic \ --property parse.key=true \ --property key.seperator=,  Failed when I entered a key/value:
1,foo kafka.common.KafkaException: No key found on line 1: 1,foo at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314) at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55) at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala) kafka.common.KafkaException: No key found on line &amp;hellip; but I specified the key, didn&amp;rsquo;t I?</description>
    </item>
    
    <item>
      <title>Keeping Up with the Deluge</title>
      <link>https://rmoff.net/2017/03/11/keeping-up-with-the-deluge/</link>
      <pubDate>Sat, 11 Mar 2017 15:30:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/03/11/keeping-up-with-the-deluge/</guid>
      <description>How do you try and stay current on technical affairs, given only 24 hours in a day and a job to do as well? Here&amp;rsquo;s my take on it&amp;hellip;
 One of the many things that has changed perceptibly since the beginning of this century when I started working in IT is the amount of information freely available, and being created all the time. Back then, printed books and manuals were still the primary source of definitive information about a piece of software.</description>
    </item>
    
    <item>
      <title>Install qemu on AWS EC2 Amazon Linux</title>
      <link>https://rmoff.net/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/</link>
      <pubDate>Sat, 11 Mar 2017 15:04:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/</guid>
      <description>Mucking about with virtual disks, I wanted to install qemu on a AWS EC2 instance in order to use qemu-img.
Not finding it in a yum repo, I built it from scratch:
$ uname -a Linux ip-10-0-1-238 4.4.41-36.55.amzn1.x86_64 #1 SMP Wed Jan 18 01:03:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  Steps:
sudo yum install -y ghc-glib-devel ghc-glib autoconf autogen intltool libtool wget http://download.qemu-project.org/qemu-2.8.0.tar.xz tar xvJf qemu-2.8.0.tar.xz cd qemu-2.8.0 ./configure make sudo make install   I hit a few errors, recorded here for passing Googlers:</description>
    </item>
    
    <item>
      <title>Mount VMDK/OVF/OVA on Amazon Web Services (AWS) EC2</title>
      <link>https://rmoff.net/2017/03/11/mount-vmdk/ovf/ova-on-amazon-web-services-aws-ec2/</link>
      <pubDate>Sat, 11 Mar 2017 14:21:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/03/11/mount-vmdk/ovf/ova-on-amazon-web-services-aws-ec2/</guid>
      <description>So you&amp;rsquo;ve got a Linux VM that you want to access the contents of in EC2 - how do you do it? Let&amp;rsquo;s see how. First up, convert the VMDK to raw image file. If you&amp;rsquo;ve got a ova/ovf then just untar it first (tar -xvf my_vm.ova), from which you should get the VMDK. With that, convert it using qemu-img:
$ time qemu-img convert -f vmdk -O raw SampleAppv607p-appliance-disk1.vmdk SampleAppv607p-appliance-disk1.raw real 16m36.</description>
    </item>
    
    <item>
      <title>Little Technology Wins</title>
      <link>https://rmoff.net/2017/03/11/little-technology-wins/</link>
      <pubDate>Sat, 11 Mar 2017 11:00:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/03/11/little-technology-wins/</guid>
      <description>Wireless Headset for VOIP With No 30-Minute Dalek Timebomb A lot of my work is done remotely, with colleagues and customers. Five years ago I bought a Microsoft LifeChat LX-3000 which plugged into the USB port on my Mac. It did the job kinda fine, with two gripes:
 it wasn&amp;rsquo;t wireless. I like to wander whilst I chat, and I didn&amp;rsquo;t like being tethered. But this in itself wasn&amp;rsquo;t a reason to ditch it After c.</description>
    </item>
    
    <item>
      <title>Time For a Change</title>
      <link>https://rmoff.net/2017/03/10/time-for-a-change/</link>
      <pubDate>Fri, 10 Mar 2017 17:30:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/03/10/time-for-a-change/</guid>
      <description>After 5 years at Rittman Mead, 126 blog posts, 16 conferences, four published OTN articles, an Oracle ACE award - not to mention, of course, a whole heap of interesting and challenging client work - I&amp;rsquo;ve decided that it&amp;rsquo;s time to do something different.
Later this month I&amp;rsquo;ll be joining Confluent as a Partner Technology Evangelist, helping spread the good word of Apache Kafka and the Confluent platform.
 As always you can find me on Twitter @rmoff, for beer tweets, fried breakfast pics - and lots of Apache Kafka!</description>
    </item>
    
    <item>
      <title>HBase crash after resuming suspended VM</title>
      <link>https://rmoff.net/2017/01/20/hbase-crash-after-resuming-suspended-vm/</link>
      <pubDate>Fri, 20 Jan 2017 09:36:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/01/20/hbase-crash-after-resuming-suspended-vm/</guid>
      <description>I use BigDataLite for a lot of my sandboxing work. This is a OVA provided by Oracle which can be run on VirtualBox, VMWare, etc and has the Cloudera Hadoop platform (CDH) along with all of Oracle&amp;rsquo;s Big Data goodies including Big Data Discovery and Big Data Spatial and Graph (BDSG).
Something that kept tripping me up during my work with BDSG was that HBase would become unavailable. Not being an HBase expert and simply using it as a data store for my property graph data, I wrote it off as mistakes on my part.</description>
    </item>
    
    <item>
      <title>Kibana Timelion - Anomaly Detection</title>
      <link>https://rmoff.net/2017/01/18/kibana-timelion-anomaly-detection/</link>
      <pubDate>Wed, 18 Jan 2017 19:53:10 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/01/18/kibana-timelion-anomaly-detection/</guid>
      <description>Using the holt function in Timelion to do anomaly detection on Metricbeat data in Kibana:
Expression:
$thres=0.02, .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).color(#eee).lines(10).label(&#39;Prediction&#39;), .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).color(#666).lines(1).label(Actual), .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;).lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).subtract(.es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;)).abs().if(lt, $thres, null, .es(index=&#39;metricbeat*&#39;,metric=&#39;max:system.cpu.user.pct&#39;)).points(10,3,0).color(#c66).label(&#39;Anomaly&#39;).title(&#39;max:system.cpu.user.pct / @rmoff&#39;)  References:
 https://twitter.com/rashidkpc/status/762754396111327232 https://github.com/elastic/timelion/issues/87 https://github.com/elastic/timelion/blob/master/FUNCTIONS.md  </description>
    </item>
    
    <item>
      <title>Streaming / Unbounded Data - Resources</title>
      <link>https://rmoff.net/2017/01/16/streaming-/-unbounded-data-resources/</link>
      <pubDate>Mon, 16 Jan 2017 11:10:38 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/01/16/streaming-/-unbounded-data-resources/</guid>
      <description>The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Data architectures for streaming applications SE-Radio Episode 272: Frances Perry on Apache Beam  (img credit)</description>
    </item>
    
    <item>
      <title>kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException</title>
      <link>https://rmoff.net/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-/-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</link>
      <pubDate>Fri, 02 Dec 2016 11:35:57 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-/-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</guid>
      <description>By default, the kafka-avro-console-producer will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 already!
[oracle@bigdatalite tmp]$ kafka-avro-console-producer \ &amp;gt; --broker-list localhost:9092 --topic kudu_test \ &amp;gt; --property value.schema=&#39;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}]}&#39; {&amp;quot;id&amp;quot;: 999, &amp;quot;random_field&amp;quot;: &amp;quot;foo&amp;quot;} org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;}]} Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character (&#39;&amp;lt;&#39; (code 60)): expected a valid value (number, String, array, object, &#39;true&#39;, &#39;false&#39; or &#39;null&#39;) at [Source: sun.</description>
    </item>
    
    <item>
      <title>Oracle GoldenGate -&gt; Kafka Connect - &#34;Failed to serialize Avro data&#34;</title>
      <link>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</link>
      <pubDate>Tue, 29 Nov 2016 22:04:38 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</guid>
      <description>tl;dr Make sure that key.converter.schema.registry.url and value.converter.schema.registry.url are specified, and that there are no trailing whitespaces.
 I&amp;rsquo;ve been building on previous work I&amp;rsquo;ve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the sample configuration gives.
Simply changing the Kafka Connect OGG configuration file (confluent.properties) from</description>
    </item>
    
    <item>
      <title>Kafka Connect - java.lang.IncompatibleClassChangeError</title>
      <link>https://rmoff.net/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</link>
      <pubDate>Thu, 24 Nov 2016 20:58:44 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</guid>
      <description>I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:
[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties [...] Exception in thread &amp;quot;main&amp;quot; java.lang.IncompatibleClassChangeError: Implementing class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) The fix was to unset the CLASSPATH first:
unset CLASSPATH  </description>
    </item>
    
    <item>
      <title>boto / S3 errors</title>
      <link>https://rmoff.net/2016/10/14/boto-/-s3-errors/</link>
      <pubDate>Fri, 14 Oct 2016 08:41:30 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/10/14/boto-/-s3-errors/</guid>
      <description>Presented without comment, warranty, or context - other than these might help a wandering code hacker.
When using SigV4, you must specify a &amp;lsquo;host&amp;rsquo; parameter boto.s3.connection.HostRequiredError: BotoClientError: When using SigV4, you must specify a &#39;host&#39; parameter.  To fix, switch
conn_s3 = boto.connect_s3() for
conn_s3 = boto.connect_s3(host=&amp;#39;s3.amazonaws.com&amp;#39;) You can see a list of endpoints here.
boto.exception.S3ResponseError: S3ResponseError: 400 Bad Request Make sure you&amp;rsquo;re specifying the correct hostname (see above) for the bucket&amp;rsquo;s region.</description>
    </item>
    
    <item>
      <title>OGG-15051 oracle.goldengate.util.GGException:  Class not found: &#34;kafkahandler&#34;</title>
      <link>https://rmoff.net/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</link>
      <pubDate>Fri, 29 Jul 2016 07:47:30 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</guid>
      <description>Similar to the previous issue, the sample config in the docs causes another snafu:
OGG-15051 Java or JNI exception: oracle.goldengate.util.GGException: Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler This time it&amp;rsquo;s in the kafka.props file:
gg.handler.kafkahandler.Type = kafka Should be
gg.handler.kafkahandler.type = kafka No capital T in Type!
 (Image credit: https://unsplash.com/@vanschneider)</description>
    </item>
    
    <item>
      <title>OGG -  Class not found: &#34;com.company.kafka.CustomProducerRecord&#34;</title>
      <link>https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</link>
      <pubDate>Thu, 28 Jul 2016 16:34:37 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</guid>
      <description>In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there&amp;rsquo;s a helpful sample configuration, which isn&amp;rsquo;t so helpful &amp;hellip;
[...] gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord [...] This value for gg.handler.kafkahandler.ProducerRecordClass will cause a failure when you start the replicat:
[...] Class not found: &amp;quot;com.company.kafka.CustomProducerRecord&amp;quot; [...]  If you comment this configuration item out, it&amp;rsquo;ll use the default (oracle.goldengate.handler.kafka.DefaultProducerRecord) and work swimingly!
 (Image credit: https://unsplash.com/@vanschneider)</description>
    </item>
    
    <item>
      <title>Kafka Connect JDBC - Oracle - Number of groups must be positive</title>
      <link>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</link>
      <pubDate>Wed, 27 Jul 2016 15:23:14 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</guid>
      <description>There are various reasons for this error, but the one I hit was that the table name is case sensitive, and returned from Oracle by the JDBC driver in uppercase.
If you specify the tablename in your connecter config in lowercase, it won&amp;rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit etc/kafka/connect-log4j.properties to set log4j.rootLogger=DEBUG, stdout), and observe: (I&amp;rsquo;ve truncated some of the output for legibility)</description>
    </item>
    
    <item>
      <title>Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required</title>
      <link>https://rmoff.net/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</link>
      <pubDate>Tue, 19 Jul 2016 14:36:52 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</guid>
      <description>I&amp;rsquo;ve been doing some noodling around with Confluent&amp;rsquo;s Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you&amp;rsquo;re not familiar with Kafka Connect this page gives a good idea of the thinking behind it.
One issue that I hit defeated my Google-fu so I&amp;rsquo;m recording it here to hopefully help out fellow n00bs.
The pipeline that I&amp;rsquo;d set up looked like this:
 Eneco&amp;rsquo;s Twitter Source streaming tweets to a Kafka topic Confluent&amp;rsquo;s HDFS Sink to stream tweets to HDFS and define Hive table automagically over them  It worked great, but only if I didn&amp;rsquo;t enable the Hive integration part.</description>
    </item>
    
    <item>
      <title>Configuring UPS/apcupsd</title>
      <link>https://rmoff.net/2016/07/18/configuring-ups/apcupsd/</link>
      <pubDate>Mon, 18 Jul 2016 07:59:51 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/18/configuring-ups/apcupsd/</guid>
      <description>With my new server I bought a UPS, partly just as a Good Thing, but also because I suspect a powercut fried the motherboard on a previous machine that I had, and this baby is too precious to lose ;)
The idea is that the UPS will smooth out the power supply to my server, protecting it from surges or temporarily blips in power loss. If there&amp;rsquo;s a proper power cut, the UPS is connected to my server and can initiate a graceful shutdown instead of system crash.</description>
    </item>
    
    <item>
      <title>Spark sqlContext.read.json - java.io.IOException: No input paths specified in job</title>
      <link>https://rmoff.net/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</link>
      <pubDate>Wed, 13 Jul 2016 04:50:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</guid>
      <description>Trying to use SparkSQL to read a JSON file, from either pyspark or spark-shell, I got this error:
java.io.IOException: No input paths specified in job  scala&amp;gt; sqlContext.read.json(&amp;quot;/u02/custom/twitter/twitter.json&amp;quot;) java.io.IOException: No input paths specified in job at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202) Despite the reference articles that I found using this local path syntax (/u02/custom/twitter/twitter.json), it turned out that I needed to prefix it with file://:
scala&amp;gt; sqlContext.read.json(&amp;quot;file:///u02/custom/twitter/twitter.json&amp;quot;) res3: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: string, created_at: string, entities: struct&amp;lt;hashtags:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,media:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string,id:bigint,id_str:string,indices:array&amp;lt;bigint&amp;gt;,media_url:string,media_url_https:string,sizes:struct&amp;lt;large:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,medium:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,small:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,thumb:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;&amp;gt;,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string&amp;gt;&amp;gt;,symbols:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,urls:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string.</description>
    </item>
    
    <item>
      <title>Proxmox 4 Containers - ssh - ssh_exchange_identification: read: Connection reset by peer</title>
      <link>https://rmoff.net/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/</link>
      <pubDate>Tue, 05 Jul 2016 15:20:37 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/</guid>
      <description>TL;DR When defining networking on Proxmox 4 LXC containers, use an appropriate CIDR suffix (e.g. 24) - don&amp;rsquo;t use 32!
 On my Proxmox 4 server I&amp;rsquo;m running a whole load of lovely LXC containers. Unfortunately, I had trouble connecting to them. From a client machine, I got the error
ssh_exchange_identification: read: Connection reset by peer  On the server I was connecting to (which I could get a console for through the Proxmox GUI, or a session on using pct enter from the Proxmox host) I ran a SSHD process with debug to see what was happening:</description>
    </item>
    
    <item>
      <title>Reset Hue password</title>
      <link>https://rmoff.net/2016/07/05/reset-hue-password/</link>
      <pubDate>Tue, 05 Jul 2016 13:27:06 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/05/reset-hue-password/</guid>
      <description>(Ref)
The bit that caught me out was this kept failing with
Error: Password not present	 and a Python stack trace that ended with
subprocess.CalledProcessError: Command &#39;/var/run/cloudera-scm-agent/process/78-hue-HUE_SERVER/altscript.sh sec-1-secret_key&#39; returned non-zero exit status 1  The answer (it seems) is to ensure that HUE_SECRET_KEY is set (to any value!)
Launch shell:
export HUE_SECRET_KEY=foobar /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/hue/build/env/bin/hue shell  Reset password for hue, activate account and make it superuser
from django.contrib.auth.models import User user = User.</description>
    </item>
    
    <item>
      <title>Apache Drill - conflicting jar problem - &#34;No current connection&#34;</title>
      <link>https://rmoff.net/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/</link>
      <pubDate>Mon, 20 Jun 2016 19:04:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/</guid>
      <description>Vanilla download of Apache Drill 1.6, attempting to follow the Followed the Drill in 10 Minutes tutorial - but kept just getting the error No current connection. Here&amp;rsquo;s an example:
[oracle@bigdatalite apache-drill-1.6.0]$ ./bin/drill-embedded Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 com.fasterxml.jackson.databind.JavaType.isReferenceType()Z apache drill 1.6.0 &amp;#34;the only truly happy people are children, the creative minority and drill users&amp;#34; 0: jdbc:drill:zk=local&amp;gt; SELECT version FROM sys.version; No current connection 0: jdbc:drill:zk=local&amp;gt; Whether SELECT version FROM sys.</description>
    </item>
    
    <item>
      <title>ClassNotFoundException with MongoDB-Hadoop in Hive</title>
      <link>https://rmoff.net/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/</link>
      <pubDate>Wed, 15 Jun 2016 17:58:19 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/</guid>
      <description>I wasted literally two hours on this one, so putting down a note to hopefully help future Googlers.
Symptom Here&amp;rsquo;s all the various errors that I got in the hive-server2.log during my attempts to get a CREATE EXTERNABLE TABLE to work against a MongoDB table in Hive:
Caused by: java.lang.ClassNotFoundException: com.mongodb.hadoop.io.BSONWritable Caused by: java.lang.ClassNotFoundException: com.mongodb.util.JSON Caused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson Caused by: java.lang.ClassNotFoundException: org.bson.io.OutputBuffer Whilst Hive would throw errors along the lines of:</description>
    </item>
    
    <item>
      <title>Erroneous SwapFree on LXC causes problems with CDH install</title>
      <link>https://rmoff.net/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/</link>
      <pubDate>Wed, 15 Jun 2016 17:52:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/</guid>
      <description>Installing CDH 5.7 on Linux Containers (LXC) hosted on Proxmox 4. Everything was going well until Cluster Setup, and which point it failed on Start YARN (MR2 included)
Completed only 0/1 steps. First failure: Failed to execute command Start on service YARN (MR2 Included)  Log /var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh57-01-node-02.moffatt.me.log.out showed:
org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.NumberFormatException: For input string: &amp;quot;18446744073709550364&amp;quot; java.lang.NumberFormatException: For input string: &amp;quot;18446744073709550364&amp;quot;  Looking down the stack trace, this came from org.</description>
    </item>
    
    <item>
      <title>Reviving a bricked EdgeRouter Lite (ERL) from a Mac</title>
      <link>https://rmoff.net/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/</link>
      <pubDate>Wed, 08 Jun 2016 15:58:30 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/</guid>
      <description>I&amp;rsquo;ve got an EdgeRouter LITE (ERL) which I used as my home router until a powercut fried it a while ago (looks like I&amp;rsquo;m not the only one to have this issue). The symptoms were it powering on but not giving any DHCP addresses, or after a factory reset responding on the default IP of 192.168.1.1. It was a real shame, because it had been a great bit of kit up until then.</description>
    </item>
    
    <item>
      <title>Running a Docker Container on Proxmox for BitTorrent Sync</title>
      <link>https://rmoff.net/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/</link>
      <pubDate>Tue, 07 Jun 2016 21:43:26 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/</guid>
      <description>(Previously, previously, previously)
Since Proxmox 4 has a recent Linux kernel and mainline one at that, it means that Docker can be run on it. I&amp;rsquo;ve yet to really dig into Docker and work out when it makes sense in place of Linux Containers (LXC), so this is going to be a learning experience for me.
To install Docker, add Backports repo to apt:
root@proxmox01:~# cat /etc/apt/sources.list.d/backports.list deb http://ftp.debian.org/debian jessie-backports main And then install:</description>
    </item>
    
    <item>
      <title>Importing VMWare and VirtualBox VMs to Proxmox</title>
      <link>https://rmoff.net/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/</link>
      <pubDate>Tue, 07 Jun 2016 21:14:26 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/</guid>
      <description>(Previously, previously)
I&amp;rsquo;ve got a bunch of existing VirtualBox and VMWare VMs that I want to run on Proxmox. Eventually I&amp;rsquo;ll migrate them to containers, but for the time being run them as &amp;ldquo;fat&amp;rdquo; VMs using Proxmox&amp;rsquo;s KVM virtualisation. After copying the OVA files that I had to the server, I uncompressed them:
root@proxmox01:/data04/vms/bdl44-biwa# cd ../bdl44 root@proxmox01:/data04/vms/bdl44# ll total 27249328 -rw------- 1 root root 27903306752 Jun 1 10:14 BigDataLite440.ova root@proxmox01:/data04/vms/bdl44# tar -xf BigDataLite440.</description>
    </item>
    
    <item>
      <title>Commissioning my Proxmox Server - OS and filesystems</title>
      <link>https://rmoff.net/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/</link>
      <pubDate>Tue, 07 Jun 2016 21:03:22 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/</guid>
      <description>(Previously)
With my server in place, I ran a memtest on it &amp;hellip; which with 128G took a while ;)
And then installed Proxmox 4, using a bootable USB that I&amp;rsquo;d created on my Mac from the ISO downloaded from Proxmox&amp;rsquo;s website. To create the bootable USB, create the img file:
hdiutil convert -format UDRW -o target.img source.iso and then burn it to USB:
sudo dd if=target.img of=/dev/rdiskN bs=1m Replace N with the correct device based on diskutil list output.</description>
    </item>
    
    <item>
      <title>A New Arrival</title>
      <link>https://rmoff.net/2016/06/07/a-new-arrival/</link>
      <pubDate>Tue, 07 Jun 2016 20:43:20 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/07/a-new-arrival/</guid>
      <description>After a long and painful delivery, I&amp;rsquo;m delighted to announce the arrival of a new addition to my household &amp;hellip; :
This custom-build from Scan 3XS is sat in my study quietly humming away. I&amp;rsquo;m going to use it for hosting VMs for R&amp;amp;D on OBIEE, Big Data Lite, Elastic, InfluxDB, Kafka, etc. I&amp;rsquo;ll blog various installations that I&amp;rsquo;ve done on it as a reference for myself, and anyone else interested.</description>
    </item>
    
    <item>
      <title>New version of BigDataLite VM from Oracle</title>
      <link>https://rmoff.net/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/</link>
      <pubDate>Mon, 06 Jun 2016 22:28:25 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/</guid>
      <description>Oracle&amp;rsquo;s excellent Big Data Lite VM has been updated, to version 4.5.
Download it here
Changes:
 CDH 5.5 -&amp;gt; 5.7 Big Data Spatial and Graph 1.1 -&amp;gt; 1.2 Big Data Discovery 1.1 -&amp;gt; 1.2 Oracle Big Data Connectors 4.4 -&amp;gt; 4.5 Oracle NoSQL 3.5 -&amp;gt; 4.0 GoldenGate 12.2.0.1 -&amp;gt; 12.2.0.1.1  </description>
    </item>
    
    <item>
      <title>OBIEE 12c blog posts</title>
      <link>https://rmoff.net/2016/06/01/obiee-12c-blog-posts/</link>
      <pubDate>Wed, 01 Jun 2016 22:30:14 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/01/obiee-12c-blog-posts/</guid>
      <description>I&amp;rsquo;ve been spending some interesting hours digging into OBIEE 12c recently, with some interesting blog posts to show for it. Some of it is just curiosities discovered along the way, but the real meaty stuff is the in the RESTful APIs - lots of potential here for cool integrations I think&amp;hellip;
 Lifting the Lid on OBIEE 12c Web Services - Part 1 Lifting the Lid on OBIEE 12c Web Services - Part 2 Extended Subject Areas (XSA) and the Data Set Service  Changes in BI Server Cache Behaviour in OBIEE 12c : OBIS_REFRESH_CACHE  Dynamic Naming of OBIEE 12c Service Instance Exports OBIEE 12c - &amp;ldquo;Add Data Source&amp;rdquo; in Answers   (Photo credit: https://unsplash.</description>
    </item>
    
    <item>
      <title>Presentation Services Logsources in OBIEE 12c</title>
      <link>https://rmoff.net/2016/06/01/presentation-services-logsources-in-obiee-12c/</link>
      <pubDate>Wed, 01 Jun 2016 11:03:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/06/01/presentation-services-logsources-in-obiee-12c/</guid>
      <description>Presentation Services can provide some very detailed logs, useful for troubleshooting, performance tracing, and general poking around. See here for details.
There&amp;rsquo;s no bi-init.sh in 12c, so need to set up the LD_LIBRARY_PATH ourselves:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/oracle/biee/bi/bifoundation/web/bin/:/app/oracle/biee/bi/lib/:/app/oracle/biee/lib/:/app/oracle/biee/bi/bifoundation/odbc/lib/ Run sawserver with flag to list all log sources
/app/oracle/biee/bi/bifoundation/web/bin/sawserver -logsources &amp;gt; saw_logsources_12.2.1.txt Full list: https://gist.github.com/rmoff/e3be9009da6130839c71181cb58509a0</description>
    </item>
    
    <item>
      <title>Lifting the Lid on OBIEE 12c Web Services - Part 2</title>
      <link>https://rmoff.net/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/</link>
      <pubDate>Sat, 28 May 2016 20:30:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/</guid>
      <description>In OBIEE 12c data-model-cmd is a wrapper for some java code which ultimately calls an internal RESTful web service in OBIEE 12c, bi-lcm. We saw in the previous post how these internal web services can be opened up slightly, and we&amp;rsquo;re going to do the same again here. Which means, time for the same caveat:
 None of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle.</description>
    </item>
    
    <item>
      <title>Dynamic Naming of OBIEE 12c Service Instance Exports</title>
      <link>https://rmoff.net/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/</link>
      <pubDate>Fri, 27 May 2016 09:13:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/</guid>
      <description>exportServiceInstance will export the RPD, Presentation Catalog, and Security model (application roles &amp;amp; policies etc &amp;ndash; but not WLS LDAP) into a single .bar file, from which they can be imported to another environment, or restored to the same one at a later date (e.g. for backup/restore).
To run exportServiceInstance you need to launch WLST first. The following demonstrates how to call it, and embeds the current timestamp &amp;amp; machine details in the backup (useful info, and also makes the backup name unique each time).</description>
    </item>
    
    <item>
      <title>OBIEE 12c - &#34;Add Data Source&#34; in Answers</title>
      <link>https://rmoff.net/2016/05/27/obiee-12c-add-data-source-in-answers/</link>
      <pubDate>Fri, 27 May 2016 08:44:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/27/obiee-12c-add-data-source-in-answers/</guid>
      <description>So this had me scratching my head for a good hour today. Comparing SampleApp v511 against a vanilla OBIEE 12c install I&amp;rsquo;d done, one had &amp;ldquo;Add Data Source&amp;rdquo; as an option in Answers, the other didn&amp;rsquo;t. The strange thing was that the option wasn&amp;rsquo;t there in SampleApp &amp;ndash; and usually that has all the bells and whistles enabled.
After checking and re-checking the Manage Privileges option, and even the Application Policy grants, and the manual, I hit MoS - and turned up Doc ID 2093886.</description>
    </item>
    
    <item>
      <title>York Fry Ups</title>
      <link>https://rmoff.net/2016/05/24/york-fry-ups/</link>
      <pubDate>Tue, 24 May 2016 21:45:09 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/24/york-fry-ups/</guid>
      <description>I had the pleasure of not one but two fry-ups in York, UK last weekend.
The first was courtesy of Bill&amp;rsquo;s Restaurant
Overall, pretty good, and I&amp;rsquo;ve had much worse. All the ingredients seemed decent. The black pudding was overcooked and almost biscuit-like, but that&amp;rsquo;s my only serious grumble. The bacon was cooked well. That black pudding, beans and the mashed/friend potato thing were each extra charges annoyed me. Particularly with a hangover, I just want to be able to order a full english, without playing Mastermind to work out what&amp;rsquo;s in or not.</description>
    </item>
    
    <item>
      <title>Lifting the Lid on OBIEE 12c Web Services - Part 1</title>
      <link>https://rmoff.net/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/</link>
      <pubDate>Tue, 24 May 2016 21:15:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/</guid>
      <description>Architecturally, OBIEE 12c is - on the surface - pretty similar to OBIEE 11g. Sure, we&amp;rsquo;ve lost OPMN in favour of Node Manager, but all the old favourites are there - WebLogic Servers, BI Server (nqsserver / OBIS), Presentation Services (sawserver / OBIPS), and so on.
But, scratch beneath the surface, or have a gander at slide decks such as this one from BIWA this year, and you realise that change is afoot.</description>
    </item>
    
    <item>
      <title>Kibana Timelion - Series Calculations - Difference from One Week Ago</title>
      <link>https://rmoff.net/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/</link>
      <pubDate>Mon, 23 May 2016 09:46:28 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/</guid>
      <description>I wrote recently about Kibana&amp;rsquo;s excellent Timelion feature, which brings time-series visualisations to Kibana. In the comments Ben Huang asked:
 do you know how to show whats the difference between this Friday and last Friday by Timelion?
 So I thought I&amp;rsquo;d answer properly here.
Timelion includes mathematical functions including add and subtract, as well as the ability to show data offset by an amount of time. So to answer Ben&amp;rsquo;s query, we combine the two.</description>
    </item>
    
    <item>
      <title>OBIEE 12c hangs at startup - Starting AdminServer ...</title>
      <link>https://rmoff.net/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../</link>
      <pubDate>Fri, 20 May 2016 14:22:21 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../</guid>
      <description>Running the OBIEE 12c startup on Windows:
C:\app\oracle\fmw\user_projects\domains\bi\bitools\bin\start.cmd  Just hangs at:
Starting AdminServer ...  No CPU being consumed, very odd. But then &amp;hellip; looking at DOMAIN_HOME\servers\AdminServer\logs\AdminServer.out shows the last log entry was:
Enter username to boot WebLogic server:  And that&amp;rsquo;s bad news, cos that&amp;rsquo;s an interactive prompt, but not echo&amp;rsquo;d to the console output of the startup command, and there&amp;rsquo;s no way to interact with it.
The start.</description>
    </item>
    
    <item>
      <title>oracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.</title>
      <link>https://rmoff.net/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./</link>
      <pubDate>Thu, 19 May 2016 10:06:03 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./</guid>
      <description>Another quick note on OBIEE 12c, this time on the importServiceInstance command. If you run it with a BAR file that doesn&amp;rsquo;t exist, it&amp;rsquo;ll fail (obviously), but the error at the end of the stack trace is slightly confusing:
oracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.  Scrolling back up the stack trace does show the error message:
SEVERE: Failed in reading bar file.</description>
    </item>
    
    <item>
      <title>OBIEE Baseline Validation Tool - Parameter &#39;directory&#39; is not a directory</title>
      <link>https://rmoff.net/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/</link>
      <pubDate>Wed, 18 May 2016 15:35:46 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/</guid>
      <description>Interesting quirk in running Baseline Validation Tool for OBIEE here. If you invoke obibvt from the bin folder, it errors with Parameter &amp;lsquo;directory&amp;rsquo; is not a directory
PS C:\OracleBI-BVT&amp;gt; cd bin PS C:\OracleBI-BVT\bin&amp;gt; .\obibvt -config C:\OracleBI-BVT\bin\bvt-config.xml -deployment current INFO: Result folder: Results\current Throwable: Parameter &#39;directory&#39; is not a directory Thread[main,5,main] SEVERE: Unhandled Exception SEVERE: java.lang.IllegalArgumentException: Parameter &#39;directory&#39; is not a directory at org.apache.commons.io.FileUtils.validateListFilesParameters(FileUtils.java:545) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:521) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:691) at com.oracle.biee.bvt.UpgradeTool.loadPlugins(UpgradeTool.java:537) at com.</description>
    </item>
    
    <item>
      <title>Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion</title>
      <link>https://rmoff.net/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</link>
      <pubDate>Fri, 13 May 2016 05:45:19 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</guid>
      <description>Yesterday I wrote about Monitoring Logstash Ingest Rates with InfluxDB and Grafana, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.
Mark Walkom reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it&amp;rsquo;ll integrate back into the Elastic stack:
Which then got me thinking &amp;ndash; why add in InfluxDB and Grafana, if you&amp;rsquo;re already using another datastore and front end (Elasticsearch and Kibana)?</description>
    </item>
    
    <item>
      <title>Monitoring Logstash Ingest Rates with InfluxDB and Grafana</title>
      <link>https://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/</link>
      <pubDate>Thu, 12 May 2016 20:56:38 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/</guid>
      <description>In this article I&amp;rsquo;m going to show you how to easily monitor the rate at which Logstash is ingesting data, as well as in future articles the rate at which Elasticsearch is indexing it. It&amp;rsquo;s a nice little touch to add to any project involving Logstash, and it&amp;rsquo;s easy to do.
Logstash is powerful tool for data ingest, processing, and distribution. It originated as simply the pipe to slurp at log files and put them into Elasticsearch, but has evolved into a whole bunch more.</description>
    </item>
    
    <item>
      <title>Collection of Articles on How to Write a Good Conference Abstract</title>
      <link>https://rmoff.net/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/</link>
      <pubDate>Thu, 05 May 2016 09:57:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/</guid>
      <description>Here&amp;rsquo;s a collection of useful articles that I&amp;rsquo;ve found over the years that give good advice on writing a good abstract, mistakes to avoid, etc:
 Adam Machanic - Capturing Attention: Writing Great Session Descriptions Gwen Shapira - Concrete Advice for Abstract Writers Kellyn Pot‚ÄôVin-Gorman - Abstracts, Reviews and Conferences, Oh My! Russ Unger - Conference Proposals that Don‚Äôt Suck Martin Widlake - Tips on Submitting an Abstract to Conference Bridget Kromhout - give actionable takeaways   (post photo courtesy of Calum MacAulay on https://unsplash.</description>
    </item>
    
    <item>
      <title>Using R to Denormalise Data for Analysis in Kibana</title>
      <link>https://rmoff.net/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</link>
      <pubDate>Sun, 24 Apr 2016 12:22:12 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</guid>
      <description>Kibana is a tool from Elastic that makes analysis of data held in Elasticsearch really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a blog post last year, taking a CSV file and loading it into Elasticsearch via Logstash.</description>
    </item>
    
    <item>
      <title>OBIEE security patches, and FINAL 11.1.1.7 patchset release</title>
      <link>https://rmoff.net/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/</link>
      <pubDate>Mon, 18 Apr 2016 15:36:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/</guid>
      <description>Two vulns for OBIEE in the latest critical patch update (CPU): http://www.oracle.com/technetwork/security-advisory/cpuapr2016v3-2985753.html?elq_mid=45463&amp;amp;sh=91225181314122121267715271910&amp;amp;cmid=WWMK10067711MPP001C140
Patches is bundle patch .160419:
 12.2.1: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22734181 11.1.1.9: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22393988 11.1.1.7: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT&amp;amp;sourceId=2102148.1&amp;amp;patchId=22225110  Note that April 2016 is the last regular patchset for 11.1.1.7, ref: https://support.oracle.com/epmos/faces/DocumentDisplay?id=2102148.1#mozTocId410847. If you&amp;rsquo;re still on it, or earlier, time to upgrade!
 (Photo credit: https://unsplash.com/@jenlittlebirdie)</description>
    </item>
    
    <item>
      <title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
      <link>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
      <pubDate>Thu, 14 Apr 2016 22:51:43 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
      <description>Recently added to the oracledi project over at java.net is an adaptor enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently over at the Elastic blog.
Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (here and here for archives), as well as spoken about - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with.</description>
    </item>
    
    <item>
      <title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
      <link>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
      <pubDate>Tue, 12 Apr 2016 21:50:46 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
      <description>I&amp;rsquo;ve recently been playing around with the ELK stack (now officially known as the Elastic stack) collecting data from an IRC channel with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and analysing it with Kibana. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.
As I wrote about last year, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Food Pr0n 02 - Devon &amp; Dorset</title>
      <link>https://rmoff.net/2016/04/11/food-pr0n-02-devon-dorset/</link>
      <pubDate>Mon, 11 Apr 2016 19:00:56 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/04/11/food-pr0n-02-devon-dorset/</guid>
      <description>On a family holiday in South Devon last week I had some good food experiences. Top of the pile was a Lardy Cake, a delicacy new to me but which I&amp;rsquo;ll be sure to be searching out again. It reminded me of an Eccles cake, but bigger and lardier!
I got mine from Vinnicombes on the High Street in Sidmouth.
Just down the coast from Sidmouth is a village called Beer.</description>
    </item>
    
    <item>
      <title>Experiments with Kibana Timelion</title>
      <link>https://rmoff.net/2016/03/29/experiments-with-kibana-timelion/</link>
      <pubDate>Tue, 29 Mar 2016 21:07:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/29/experiments-with-kibana-timelion/</guid>
      <description>Timelion was released in November 2015 and with the 4.4.2 release of Kibana is available as a native visualisation once installed. It adds some powerful capabilities to Kibana as an timeseries analysis tool, using its own data manipulation language.
Installing Timelion is a piece of cake:
./bin/kibana plugin -i kibana/timelion  After restarting Kibana, you&amp;rsquo;ll see it as an option from the application picker
There&amp;rsquo;s a bit of a learning curve with Timelion, but it&amp;rsquo;s worth it.</description>
    </item>
    
    <item>
      <title>Connecting to OBIEE via JDBC - with jisql</title>
      <link>https://rmoff.net/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/</link>
      <pubDate>Mon, 28 Mar 2016 21:01:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/</guid>
      <description>OBIEE supports JDBC as a connection protocol, using the driver available on all installations of OBIEE, bijdbc.jar. This makes connecting to OBIEE from custom or third-party applications very easy. Once connected, you issue &amp;ldquo;Logical SQL&amp;rdquo; against the &amp;ldquo;tables&amp;rdquo; of the Presentation Layer. An example of logical SQL is:
SELECT &amp;#34;Time&amp;#34;.&amp;#34;T05 Per Name Year&amp;#34; saw_0 FROM &amp;#34;A - Sample Sales&amp;#34; To find more Logical SQL simply inspect your nqquery.log (obis-query.log in 12c), or Usage Tracking.</description>
    </item>
    
    <item>
      <title>My latest IRC client : Kibana</title>
      <link>https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/</link>
      <pubDate>Thu, 24 Mar 2016 21:38:02 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/</guid>
      <description>OK, maybe that&amp;rsquo;s not entirely true. But my read-only client, certainly.
I was perusing the Logstash input plugins recently when I noticed that there was one for IRC. Being a fan of IRC and a regular on the #obihackers channel, I thought this could be fun and yet another great example of how easy the Elastic stack is to work with.
Installation is a piece of cake:
wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip wget https://download.</description>
    </item>
    
    <item>
      <title>Food Pr0n - 01</title>
      <link>https://rmoff.net/2016/03/19/food-pr0n-01/</link>
      <pubDate>Sat, 19 Mar 2016 21:18:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/19/food-pr0n-01/</guid>
      <description>One of the perks of my job is that I get to travel to some pretty nice places (hi, San Francisco, Bergen, √Öland Islands), and get to eat some pretty good food too. If you&amp;rsquo;re looking for some techie content, then move along and go and read about Kafka, but if you enjoy food pr0n then stay put.
I was working for a client in the centre of Manchester this week, staying as usual at a Premier Inn.</description>
    </item>
    
    <item>
      <title>OBIEE 11.1.1.9 installation - JPS-06514: Opening of file based keystore failed</title>
      <link>https://rmoff.net/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/</link>
      <pubDate>Fri, 18 Mar 2016 18:04:07 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/</guid>
      <description>I got this lovely failure during a fresh install of OBIEE 11.1.1.9. I emphasise that it was during the install because there&amp;rsquo;s other causes for this error on an existing system to do with corrupted credential stores etc &amp;ndash; not the case here.
The install had copied in the binaries and was in the process of building the domain. During the early stages of this where it starts configuring and restarting the AdminServer it failed, with the AdminServer.</description>
    </item>
    
    <item>
      <title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
      <link>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
      <pubDate>Wed, 16 Mar 2016 22:01:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
      <description>The Oracle by Example (ObE) here demonstrating how to use Goldengate to replicate transactions big data targets such as HDFS is written for the BigDataLite 4.2.1, and for me didn&amp;rsquo;t work on the current latest version, 4.4.0.
The OBE (and similar Hands On Lab PDF) assume the presence of pmov.prm and pmov.properties in /u01/ogg/dirprm/. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, emov.
Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.</description>
    </item>
    
    <item>
      <title>Presentation Slides‚Ä¶ bye-bye Slideshare, hello Speakerdeck</title>
      <link>https://rmoff.net/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/</link>
      <pubDate>Wed, 09 Mar 2016 09:43:30 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/</guid>
      <description>I&amp;rsquo;ve always defaulted to Slideshare for hosting slides from presentations that I&amp;rsquo;ve given, but it&amp;rsquo;s become more and more crap-infested. The UI is messy, and the UX sucks - for example, I want to download a slide deck, I most definitely 100% am not interested in &amp;ldquo;clipping&amp;rdquo; it&amp;hellip;even if you ask me every. damn. time:
Looking around it seems the other popular option is Speakerdeck. The UI is clean and simple, and I as both a user and uploader I feel like I&amp;rsquo;m there to read and share slides rather than be monetised as an eyeball on the site.</description>
    </item>
    
    <item>
      <title>#obihackers IRC channel</title>
      <link>https://rmoff.net/2016/03/03/#obihackers-irc-channel/</link>
      <pubDate>Thu, 03 Mar 2016 22:55:37 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/03/03/#obihackers-irc-channel/</guid>
      <description>#obihackers There&amp;rsquo;s a #obihackers IRC channel on freenode, where a dozen or so of us have hung out for several years now. Chat is usually OBIEE, Oracle, ODI, and general geek out.
Bear in mind this is the equivalent of us hanging out in a bar; if you wanna shoot the shit with a geeky question about OBIEE go ahead, but if you&amp;rsquo;ve come to get help with your homework without even buying a round, you&amp;rsquo;ll probably get short shrift&amp;hellip; ;-)</description>
    </item>
    
    <item>
      <title>Streaming data to InfluxDB from any bash command</title>
      <link>https://rmoff.net/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</link>
      <pubDate>Sat, 27 Feb 2016 21:05:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</guid>
      <description>InfluxDB is a great time series database, that&amp;rsquo;s recently been rebranded as part of the &amp;ldquo;TICK&amp;rdquo; stack, including data collectors, visualisation, and ETL/Alerting. I&amp;rsquo;ve yet to really look at the other components, but InfluxDB alone works just great with my favourite visualisation/analysis tool for time series metrics, Grafana.
Getting data into InfluxDB is easy, with many tools supporting the native InfluxDB line input protocol, and those that don&amp;rsquo;t often supporting the carbon protocol (from Graphite), which InfluxDB also supports (along with others).</description>
    </item>
    
    <item>
      <title>#FullEnglish</title>
      <link>https://rmoff.net/2016/02/26/#fullenglish/</link>
      <pubDate>Fri, 26 Feb 2016 18:02:31 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/02/26/#fullenglish/</guid>
      <description>Thanks to the power of twitter, I can look back on all the many and varied Full English breakfasts that I&amp;rsquo;ve (mostly) enjoyed: https://twitter.com/search?q=rmoff%20%23fullenglish&amp;amp;src=typd
What makes a good Full English?
 Good ingredients, cooked well. Nothing worse than a limp pink sausage, as it were. Sausage, standard pork or Cumberland at most. Definitely no daft apricot and guava bean with a hint of foie gras nonsense. Must be cooked right well, crispy skin, almost burnt.</description>
    </item>
    
    <item>
      <title>Visualising OBIEE DMS Metrics over the years</title>
      <link>https://rmoff.net/2016/02/26/visualising-obiee-dms-metrics-over-the-years/</link>
      <pubDate>Fri, 26 Feb 2016 17:54:54 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/02/26/visualising-obiee-dms-metrics-over-the-years/</guid>
      <description>It struck me today when I was writing my most recent blog over at Rittman Mead that I&amp;rsquo;ve been playing with visualising OBIEE metrics for years now.
  Back in 2009 I wrote about using something called JManage to pull metrics out of OBIEE 10g via JMX:
  Still with OBIEE 10g in 2011, I was using rrdtool and some horrible-looking tcl hacking to get the metrics out through jmx :</description>
    </item>
    
  </channel>
</rss>
