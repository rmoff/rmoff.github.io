<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.github.io/index.xml">
		<link rel="canonical" href="https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/">
		
		<link rel="shortcut icon" type="image/png" href="https://rmoff.github.io/apple-touch-icon-precomposed.png">
		
		
		<meta name="generator" content="Hugo 0.52" />

		
		<meta name="og:title" content="Streaming Data from MySQL into Kafka with Kafka Connect and Debezium" />
		<meta name="og:type" content="article" />
		<meta name="og:image" content="https://rmoff.github.io/images/2018/03/IMG_0699.jpg" />
		<meta name="og:description" content="" />
		<meta name="og:url" content="https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/" />
		<meta name="og:site_name" content="Streaming Data from MySQL into Kafka with Kafka Connect and Debezium" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.github.io/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/descartes.css" />
		
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		

		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.github.io/js/story.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.github.io/images/2018/03/IMG_0699.jpg'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.github.io" title="Home">
						<img src="https://rmoff.github.io/img/logo.jpg" class="w2 h2 br-100" alt="rmoff&#39;s random ramblings" />
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="/about-me/">about</a>
						<a class="link f5 ml2 dim near-white" href="/talks/">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/rmoff/"><i class="fab fa-twitter-square"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.github.io/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.github.io/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2018-03-24T14:58:14Z">Mar 24, 2018</time>
								<span class="display-print">by Robin Moffatt</span>
								 in <a href="https://rmoff.github.io/categories//debezium" class="no-underline category near-white dim">Debezium</a>, <a href="https://rmoff.github.io/categories//kafka" class="no-underline category near-white dim">Kafka</a>, <a href="https://rmoff.github.io/categories//kafka-connect" class="no-underline category near-white dim">Kafka Connect</a>, <a href="https://rmoff.github.io/categories//mysql" class="no-underline category near-white dim">Mysql</a>
								<span class="display-print">at https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</span>
							
						
					</h2>
				</div>

				
				
				
				

			</div>
		</header>
		
		<main role="main">
		
<article class="center bg-white br-3 pv1 ph4 nested-copy-line-height lh-copy f4 nested-links mw-100 measure-wide">
	

<p><a href="http://debezium.io/">Debezium</a> is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we&rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.</p>

<p>The software versions used here are:</p>

<ul>
<li>Confluent Platform 4.0</li>
<li>Debezium 0.7.2</li>
<li>MySQL 5.7.19 with <a href="https://dev.mysql.com/doc/sakila/en/sakila-installation.html">Sakila sample database</a> installed</li>
</ul>

<h3 id="install-debezium">Install Debezium</h3>

<p>To use it, you need the relevant JAR for the source system (e.g. MySQL), and make that JAR available to Kafka Connect. Here we&rsquo;ll set it up for MySQL.</p>

<p>Download <code>debezium-connector-mysql-0.7.2-plugin.tar.gz</code> jar from <a href="https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/">https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/</a></p>

<p>Unpack the <code>.tar.gz</code> into its own folder, for example <code>/u01/plugins</code> so that you have:</p>

<pre><code>/u01/plugins/debezium-connector-mysql/mysql-binlog-connector-java-0.13.0.jar
/u01/plugins/debezium-connector-mysql/debezium-core-0.7.2.jar
/u01/plugins/debezium-connector-mysql/mysql-binlog-connector-java-0.13.0.jar
/u01/plugins/debezium-connector-mysql/mysql-connector-java-5.1.40.jar
/u01/plugins/debezium-connector-mysql/debezium-connector-mysql-0.7.2.jar
</code></pre>

<p>Now configure Kafka Connect to pick up the Debezium plugin, by updating the Kafka Connect worker config.</p>

<p>Edit <code>./etc/kafka/connect-distributed.properties</code> and append to <code>plugin.path</code> the value for the <em>folder containing the Debezium JAR</em>. For example:</p>

<pre><code>plugin.path=share/java,/u01/plugins/
</code></pre>

<p><code>plugin.path</code> is based on this expected structure: <img src="/content/images/2018/03/KafkaConnect_pluginpath.png" alt="" /></p>

<h3 id="mysql-config">MySQL config</h3>

<p>Debezium uses MySQL&rsquo;s binlog facility to extract events, and you need to configure MySQL to enable it. Here is the bare-basics necessary to get this working - fine for demo purposes, but not a substitute for an actual MySQL DBA doing this properly :)</p>

<p>Doc: <a href="https://dev.mysql.com/doc/refman/5.7/en/server-configuration.html">Server Config reference</a></p>

<p>Check current state of binlog replication:</p>

<pre><code>$ mysqladmin variables -uroot|grep log_bin
| log_bin                                                  | OFF
[...]
</code></pre>

<p>Enable binlog <a href="http://debezium.io/docs/connectors/mysql/#enabling-the-binlog">per the doc</a>. On the Mac I&rsquo;d installed MySQL with homebrew, and enabled binlog by creating the following file at <code>/usr/local/opt/mysql/my.cnf</code></p>

<pre><code>[mysqld]
server-id         = 42
log_bin           = mysql-bin
binlog_format     = row
binlog_row_image  = full
expire_logs_days  = 10
</code></pre>

<p>I restarted <code>mysqld</code> with:</p>

<pre><code>brew services restart mysql
</code></pre>

<p>and verified that binlog was now enabled:</p>

<pre><code>$ mysqladmin variables -uroot|grep log_bin
| log_bin                                                  | ON
[...]
</code></pre>

<p>Create user with required permissions;</p>

<pre><code>$ mysql -uroot

mysql&gt; GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'debezium' IDENTIFIED BY 'dbz';
</code></pre>

<h3 id="kafka-connect-setup">Kafka Connect setup</h3>

<p>Load the connector configuration into Kafka Connect using the REST API:</p>

<pre><code>curl -i -X POST -H &quot;Accept:application/json&quot; \
    -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
    -d '{
      &quot;name&quot;: &quot;mysql-connector&quot;,
      &quot;config&quot;: {
            &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
            &quot;database.hostname&quot;: &quot;localhost&quot;,
            &quot;database.port&quot;: &quot;3306&quot;,
            &quot;database.user&quot;: &quot;debezium&quot;,
            &quot;database.password&quot;: &quot;dbz&quot;,
            &quot;database.server.id&quot;: &quot;42&quot;,
            &quot;database.server.name&quot;: &quot;demo&quot;,
            &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;,
            &quot;database.history.kafka.topic&quot;: &quot;dbhistory.demo&quot; ,
            &quot;include.schema.changes&quot;: &quot;true&quot;
       }
    }'
</code></pre>

<p>Now check that the connector is running successfully:</p>

<pre><code>curl -s &quot;http://localhost:8083/connectors&quot; | jq '.[]' | \
xargs -I{connector_name} curl -s &quot;http://localhost:8083/connectors/&quot;{connector_name}&quot;/status&quot; | \
jq -c -M '[.name,.connector.state,.tasks[].state] | \
join(&quot;:|:&quot;)'| column -s : -t| sed 's/\&quot;//g'| sort

mysql-connector  |  RUNNING  |  RUNNING
</code></pre>

<p>If it&rsquo;s <code>FAILED</code> then check the Connect Worker log for errors - often this will be down to mistakes with the plugin&rsquo;s JAR path or availability, so check that carefully.</p>

<p>Assuming it&rsquo;s <code>RUNNING</code>, you should see in the Connect Worker logs something like this, indicating that Debezium has successfully pulled data from MySQL:</p>

<pre><code>[2018-02-09 15:27:40,268] INFO Starting snapshot for jdbc:mysql://localhost:3306/?useInformationSchema=true&amp;nullCatalogMeansCurrent=false&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;characterSetResults=UTF-8&amp;zeroDateTimeBehavior=convertToNull with user 'debezium' (io.debezium.connector.mysql.SnapshotReader:220)
[...]
[2018-02-09 15:27:57,297] INFO Step 8: scanned 97354 rows in 24 tables in 00:00:15.617 (io.debezium.connector.mysql.SnapshotReader:579)
[2018-02-09 15:27:57,297] INFO Step 9: committing transaction (io.debezium.connector.mysql.SnapshotReader:611)
[2018-02-09 15:27:57,299] INFO Completed snapshot in 00:00:17.032 (io.debezium.connector.mysql.SnapshotReader:661)
</code></pre>

<h3 id="inspect-the-mysql-data-in-kafka">Inspect the MySQL data in Kafka</h3>

<p>Use <code>kafka-topics</code> to see all the topics created by Debezium:</p>

<pre><code>kafka-topics --zookeeper localhost:2181 --list
</code></pre>

<p>Each <strong>table</strong> in the database becomes one <strong>topic</strong> in Kafka. You&rsquo;ll see that the topic name is in the format of <code>database.schema.table</code>:</p>

<pre><code>fullfillment.sakila.actor
fullfillment.sakila.address
fullfillment.sakila.category
[...]
</code></pre>

<p>Now let&rsquo;s look at the messages. Each <strong>table row</strong> becomes a <strong>message</strong> on a kafka topic.</p>

<p>Run the Avro Console consumer:  (using the excellent <a href="https://stedolan.github.io/jq/">jq</a> for easy formatting of the JSON)</p>

<pre><code>./bin/kafka-avro-console-consumer \
--bootstrap-server localhost:9092 \
--property schema.registry.url=http://localhost:8081 \
--topic fullfillment.sakila.customer \
--from-beginning | jq '.'
</code></pre>

<p>This will show the current contents of the topic. Leave the above command running, and in a separate window make a change to the table in MySQL, for example, an update:</p>

<pre><code>mysql&gt; UPDATE CUSTOMER SET FIRST_NAME='Rick' WHERE CUSTOMER_ID=603;
</code></pre>

<p>In the Kafka consumer you&rsquo;ll see the change record come through pretty much instantaneously.</p>

<script src="https://asciinema.org/a/vzt7YhIBHdcuYz9Zp4UsYPuaS.js" id="asciicast-vzt7YhIBHdcuYz9Zp4UsYPuaS" async></script>

<p>The records from Debezium look like this:</p>

<pre><code>{
  &quot;before&quot;: null,
  &quot;after&quot;: {
    &quot;fullfillment.sakila.rental.Value&quot;: {
      &quot;rental_id&quot;: 13346,
      &quot;rental_date&quot;: 1124483301000,
      &quot;inventory_id&quot;: 4541,
      &quot;customer_id&quot;: 131,
      &quot;return_date&quot;: {
        &quot;long&quot;: 1125188901000
      },
      &quot;staff_id&quot;: 2,
      &quot;last_update&quot;: &quot;2006-02-15T21:30:53Z&quot;
    }
  },
  &quot;source&quot;: {
    &quot;name&quot;: &quot;fullfillment&quot;,
    &quot;server_id&quot;: 0,
    &quot;ts_sec&quot;: 0,
    &quot;gtid&quot;: null,
    &quot;file&quot;: &quot;mysql-bin.000002&quot;,
    &quot;pos&quot;: 832,
    &quot;row&quot;: 0,
    &quot;snapshot&quot;: {
      &quot;boolean&quot;: true
    },
    &quot;thread&quot;: null,
    &quot;db&quot;: {
      &quot;string&quot;: &quot;sakila&quot;
    },
    &quot;table&quot;: {
      &quot;string&quot;: &quot;rental&quot;
    }
  },
  &quot;op&quot;: &quot;c&quot;,
  &quot;ts_ms&quot;: {
    &quot;long&quot;: 1518190060267
  }
}
</code></pre>

<p>Note the structure of the messages - you get an <code>before</code> and <code>after</code> view of the record, plus a bunch of metadata (<code>source</code>, <code>op</code>, <code>ts_ms</code>). Depending on what you&rsquo;re using the CDC events for, you&rsquo;ll want to retain some or all of this structure.</p>

<h3 id="event-message-flattening-with-single-message-transform">Event Message Flattening with Single Message Transform</h3>

<p>For simply streaming into Kafka the <em>current</em> state of the record, it can be useful to take just the <code>after</code> section of the message. Kafka Connect includes functionality called Single Message Transform (SMT). As the name suggests, it enables you to transform single messages! You can read more about it and examples of its usage <a href="https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-3/">here</a>. As well as the <a href="http://kafka.apache.org/documentation.html#connect_transforms">Transforms that ship with Apache Kafka</a>, you can write your own using the <a href="https://kafka.apache.org/10/javadoc/org/apache/kafka/connect/transforms/Transformation.html">documented API</a>. This is exactly what the Debezium project have done, shipping their own SMT as part of it, providing an easy way to <a href="http://debezium.io/docs/configuration/event-flattening/">flatten the events that Debezium emits</a>.</p>

<p>Using SMT you can amend the message inbound/outbound from Kafka to show just the new record:</p>

<pre><code>{
  &quot;c1&quot;: {
    &quot;int&quot;: 100
  },
  &quot;c2&quot;: {
    &quot;string&quot;: &quot;wibble&quot;
  },
  &quot;create_ts&quot;: &quot;2018-01-23T22:47:09Z&quot;,
  &quot;update_ts&quot;: &quot;2018-02-09T15:35:48Z&quot;
}
</code></pre>

<p>instead of the full change:</p>

<pre><code>{
  &quot;before&quot;: {
    &quot;fullfillment.demo.foobar.Value&quot;: {
      &quot;c1&quot;: {
        &quot;int&quot;: 100
      },
      &quot;c2&quot;: {
        &quot;string&quot;: &quot;bar&quot;
      },
      &quot;create_ts&quot;: &quot;2018-01-23T22:47:09Z&quot;,
      &quot;update_ts&quot;: &quot;2018-01-23T22:47:09Z&quot;
    }
  },
  &quot;after&quot;: {
    &quot;fullfillment.demo.foobar.Value&quot;: {
      &quot;c1&quot;: {
        &quot;int&quot;: 100
      },
      &quot;c2&quot;: {
        &quot;string&quot;: &quot;wibble&quot;
      },
      &quot;create_ts&quot;: &quot;2018-01-23T22:47:09Z&quot;,
      &quot;update_ts&quot;: &quot;2018-02-09T15:35:48Z&quot;
    }
  },
  &quot;source&quot;: {
    &quot;name&quot;: &quot;fullfillment&quot;,
    &quot;server_id&quot;: 42,
    &quot;ts_sec&quot;: 1518190548,
    &quot;gtid&quot;: null,
    &quot;file&quot;: &quot;mysql-bin.000002&quot;,
    &quot;pos&quot;: 1025,
    &quot;row&quot;: 3,
    &quot;snapshot&quot;: null,
    &quot;thread&quot;: {
      &quot;long&quot;: 11
    },
    &quot;db&quot;: {
      &quot;string&quot;: &quot;demo&quot;
    },
    &quot;table&quot;: {
      &quot;string&quot;: &quot;foobar&quot;
    }
  },
  &quot;op&quot;: &quot;u&quot;,
  &quot;ts_ms&quot;: {
    &quot;long&quot;: 1518190548539
  }
}
</code></pre>

<p>SMT can also be used to modify the target topic (which unmodified is <code>server.database.table</code>), using the <code>RegexRouter</code> transform.</p>

<p>With these two SMT included, this is how our configuration looks now:</p>

<pre><code>{
  &quot;name&quot;: &quot;mysql-connector-flattened&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.hostname&quot;: &quot;localhost&quot;,
    &quot;database.port&quot;: &quot;3306&quot;,
    &quot;database.user&quot;: &quot;debezium&quot;,
    &quot;database.password&quot;: &quot;dbz&quot;,
    &quot;database.server.id&quot;: &quot;42&quot;,
    &quot;database.server.name&quot;: &quot;fullfillment&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;localhost:9092&quot;,
    &quot;database.history.kafka.topic&quot;: &quot;dbhistory.fullfillment&quot; ,
    &quot;include.schema.changes&quot;: &quot;true&quot; ,
    &quot;transforms&quot;: &quot;unwrap,changetopic&quot;,
    &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
    &quot;transforms.changetopic.type&quot;:&quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
    &quot;transforms.changetopic.regex&quot;:&quot;(.*)&quot;,
    &quot;transforms.changetopic.replacement&quot;:&quot;$1-smt&quot;
  }
}
</code></pre>

<hr />

<p>To see how streaming events from a RDBMS such as MySQL into Kafka can be even more powerful when combined with KSQL for stream processing check out <a href="https://www.confluent.io/blog/ksql-in-action-enriching-csv-events-with-data-from-rdbms-into-AWS/">KSQL in Action: Enriching CSV Events with Data from RDBMS into AWS</a>.</p>

</article>

		</main>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr />

<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt" /></p>

<p>Robin Moffatt is a Developer Advocate at Confluent, and Oracle Groundbreaker Ambassador. He also likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</p>

				
			
		
		</div>
		
	</div>

		
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.github.io/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2019 Robin Moffatt
			</p>
		</footer>
		
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-75492960-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	</body>
</html>
