<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Streaming data from Oracle into Kafka (December 2018)</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.github.io/index.xml">
		<link rel="canonical" href="https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/">
		
		<link rel="shortcut icon" type="image/png" href="https://rmoff.github.io/apple-touch-icon-precomposed.png">
		
		
		<meta name="generator" content="Hugo 0.52" />

		
		<meta name="og:title" content="Streaming data from Oracle into Kafka (December 2018)" />
		<meta name="og:type" content="article" />
		<meta name="og:image" content="https://rmoff.github.io/images/2018/12/IMG_7464.jpg" />
		<meta name="og:description" content="" />
		<meta name="og:url" content="https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/" />
		<meta name="og:site_name" content="Streaming data from Oracle into Kafka (December 2018)" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.github.io/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.github.io/css/descartes.css" />
		
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		

		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.github.io/js/story.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.github.io/images/2018/12/IMG_7464.jpg'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.github.io" title="Home">
						<img src="https://rmoff.github.io/img/logo.jpg" class="w2 h2 br-100" alt="rmoff&#39;s random ramblings" />
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="/about-me/">about</a>
						<a class="link f5 ml2 dim near-white" href="/talks/">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://twitter.com/rmoff/"><i class="fab fa-twitter-square"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.github.io/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.github.io/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">Streaming data from Oracle into Kafka (December 2018)</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2018-12-12T09:49:04Z">Dec 12, 2018</time>
								<span class="display-print">by Robin Moffatt</span>
								 in <a href="https://rmoff.github.io/categories//oracle" class="no-underline category near-white dim">Oracle</a>, <a href="https://rmoff.github.io/categories//cdc" class="no-underline category near-white dim">Cdc</a>, <a href="https://rmoff.github.io/categories//debezium" class="no-underline category near-white dim">Debezium</a>, <a href="https://rmoff.github.io/categories//goldengate" class="no-underline category near-white dim">Goldengate</a>, <a href="https://rmoff.github.io/categories//xstream" class="no-underline category near-white dim">Xstream</a>, <a href="https://rmoff.github.io/categories//logminer" class="no-underline category near-white dim">Logminer</a>, <a href="https://rmoff.github.io/categories//flashback" class="no-underline category near-white dim">Flashback</a>, <a href="https://rmoff.github.io/categories//licence" class="no-underline category near-white dim">Licence</a>, <a href="https://rmoff.github.io/categories//ksql" class="no-underline category near-white dim">Ksql</a>
								<span class="display-print">at https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/</span>
							
						
					</h2>
				</div>

				
				
				
				

			</div>
		</header>
		
		<main role="main">
		
<article class="center bg-white br-3 pv1 ph4 nested-copy-line-height lh-copy f4 nested-links mw-100 measure-wide">
	

<p><em>This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018. For a more detailed background to why and how at a broader level for all databases (not just Oracle) see <a href="http://cnfl.io/kafka-cdc">this blog</a> and <a href="https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka">these slides</a>.</em></p>

<h3 id="what-techniques-tools-are-there">What techniques &amp; tools are there?</h3>

<p>As of December 2018, this is what the line-up looks like:</p>

<ul>
<li><strong>Query-based CDC</strong>

<ul>
<li>The <a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/source_config_options.html">JDBC Connector</a> for Kafka Connect, polls the database for new or changed data based on an incrementing ID column and/or update timestamp</li>
</ul></li>

<li><p><strong>Log-based CDC</strong></p>

<ul>
<li><p><strong>Oracle GoldenGate for Big Data</strong> (license <a href="https://www.oracle.com/assets/technology-price-list-070617.pdf">$20k per CPU</a>). Supports three &ldquo;handlers&rdquo;:</p>

<ul>
<li><a href="https://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-handler.htm#GADBD449">Kafka</a></li>
<li><a href="https://docs.oracle.com/goldengate/bd123110/gg-bd/GADBD/using-kafka-connect-handler.htm#GADBD-GUID-81730248-AC12-438E-AF82-48C7002178EC">Kafka Connect</a> (runs in the OGG runtime, not a Connect worker. It doesn&rsquo;t support the full Connect API, including Single Message Transforms.)</li>
<li><a href="https://docs.oracle.com/goldengate/bd123210/gg-bd/GADBD/using-kafka-rest-proxy-handler.htm">Kafka REST Proxy</a></li>
</ul></li>

<li><p><strong>Oracle XStream</strong>  (requires <strong>Oracle GoldenGate</strong> license <a href="https://www.oracle.com/assets/technology-price-list-070617.pdf">$17.5k per CPU</a>).</p>

<ul>
<li>Built on top of LogMiner. Oracle&rsquo;s API for third-party applications wanting to stream events from the database.</li>
<li>Currently beta implementation by <a href="https://debezium.io/docs/connectors/oracle/"><strong>Debezium</strong></a> (0.9) with Kafka Connect</li>
</ul></li>

<li><p><strong>Oracle Log Miner</strong> No special license required (even available in Oracle XE).</p>

<ul>
<li><a href="https://issues.jboss.org/browse/DBZ-20">Being</a> <a href="https://issues.jboss.org/browse/DBZ-137?_sscc=t">considered</a> by Debezium, and also implemented by <a href="https://github.com/erdemcer/kafka-connect-oracle">community connector here</a></li>
<li>Available commercially from Attunity, SQData, HVR, StreamSets, Striim etc</li>
<li>DBVisit Replicate is no longer developed.</li>
</ul></li>
</ul></li>

<li><p><strong>Triggers</strong> to capture changes made to a table, write details of those changes to another database table, ingest that table into Kafka (e.g. with JDBC connector).</p></li>

<li><p><strong>Flashback</strong> to show all changes to a given table between two points in time. <a href="https://blog.pythian.com/streaming-oracle-kafka-stories-message-bus-stop/">Implemented as a PoC by Stewart Bryson and Bj√∂rn Rost</a>.</p></li>
</ul>

<h3 id="what-do-they-look-like-in-action">What do they look like in action?</h3>

<p>I did a recent talk at UK Oracle User Group TECH18 conference, presenting my talk &ldquo;<a href="https://speakerdeck.com/rmoff/no-more-silos-integrating-databases-and-apache-kafka">No More Silos: Integrating Databases and Apache Kafka</a>&rdquo;. As part of this I did a live demo showing the difference between using the JDBC Connector (query-based CDC) and the new Debezium/XStream option (log-based CDC). Here I&rsquo;ll try and replicate the discussion and examples. You can also see previous articles that I&rsquo;ve written showing <a href="https://rmoff.net/tag/goldengate/">GoldenGate in action</a>.</p>

<p>You can find all of the code on the <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/no-more-silos-oracle.adoc">demo-scene</a> repository, runnable through Docker and Docker Compose. Simply clone the repo, and then run</p>

<pre><code>cd docker-compose
./scripts/setup.sh
</code></pre>

<p>The setup script does all of the rest, including bringing up Confluent Platform, and configuring the connectors. <em>You do have to <a href="https://github.com/oracle/docker-images/blob/master/OracleDatabase/SingleInstance/README.md">build the Oracle database docker image</a> first</em>.</p>

<h4 id="setup">Setup</h4>

<p>Some notes on setup of each option:</p>

<ul>
<li>JDBC connector

<ul>
<li>The main thing you need here is the Oracle JDBC driver in the correct folder for the Kafka Connect JDBC connector.</li>
<li>In the Docker Compose I use a pass-through volume (<code>db-leach</code>) mounted from the database container to <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/docker-compose.yml#L77-L83">copy the JDBC driver directly from the database container onto the Kafka Connect container</a>.</li>
<li>You also need to make sure that the source table has an incrementing ID column and/or update timestamp column that can be used to identify changed rows. Without that you can only do a bulk load of the data each time.</li>
</ul></li>
<li>Debezium connector

<ul>
<li>Requires a bunch of libraries (instant client and others), <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/docker-compose.yml#L146-L165">copied from the database container using the same pass-through volume as above</a>.</li>
<li>This requires config work on the database, covered by the <a href="https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle">Debezium docs</a> and done by the Docker script <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-setup-scripts/01_xstreams-setup.sh">here</a></li>
<li>Each table needs to be configured (script <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-startup-scripts/01_create_customers.sh#L36-L37">here</a>)</li>
<li>I hit problems with the Capture stopping with permission errors so <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/ora-startup-scripts/03_restart_capture.sh">automated its restart</a> (hacky, I know)</li>
</ul></li>
</ul>

<p>The actual config of the two connectors is done in separate calls to Kafka Connect&rsquo;s REST API (<a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/create-ora-source-jdbc.sh">JDBC</a> / <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/create-ora-source-debezium-xstream.sh">Debezium</a>). I run separate instances of Kafka Connect (in distributed mode, single node) just to keep troubleshooting simple, but in theory they could be in the same worker.</p>

<p>The invocation of the above REST configuration scripts is managed by the master <code>setup.sh</code> script, with <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/docker-compose/scripts/setup.sh#L14-L18">some logic in it</a> to wait until Kafka Connect is available before launching the config.</p>

<p>You can validate that each connector is running by querying the REST API for the two Kafka Connect worker instances:</p>

<pre><code>$ curl -s &quot;http://localhost:18083/connectors&quot;| jq '.[]'| xargs -I{connector_name} curl -s &quot;http://localhost:18083/connectors/&quot;{connector_name}&quot;/status&quot;| jq -c -M '[.name,.connector.state,.tasks[].state]|join(&quot;:|:&quot;)'| column -s : -t| sed 's/\&quot;//g'| sort
ora-source-jdbc  |  RUNNING  |  RUNNING


$ curl -s &quot;http://localhost:8083/connectors&quot;| jq '.[]'| xargs -I{connector_name} curl -s &quot;http://localhost:8083/connectors/&quot;{connector_name}&quot;/status&quot;| jq -c -M '[.name,.connector.state,.tasks[].state]|join(&quot;:|:&quot;)'| column -s : -t| sed 's/\&quot;//g'| sort
ora-source-debezium-xstream  |  RUNNING  |  RUNNING
</code></pre>

<h4 id="initial-data-load">Initial data load</h4>

<p>In Oracle, check the source data:</p>

<pre><code>COL FIRST_NAME FOR A15
COL LAST_NAME FOR A15
COL ID FOR 999
COL CLUB_STATUS FOR A12
COL UPDATE_TS FOR A29
SET LINESIZE 200
SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS, UPDATE_TS FROM CUSTOMERS;

  ID FIRST_NAME      LAST_NAME       CLUB_STATUS  UPDATE_TS
---- --------------- --------------- ------------ -----------------------------
  1 Rica            Blaisdell       bronze       11-DEC-18 05.16.00.000000 PM
  2 Ruthie          Brockherst      platinum     11-DEC-18 05.16.00.000000 PM
  3 Mariejeanne     Cocci           bronze       11-DEC-18 05.16.00.000000 PM
  4 Hashim          Rumke           platinum     11-DEC-18 05.16.00.000000 PM
  5 Hansiain        Coda            platinum     11-DEC-18 05.16.00.000000 PM
</code></pre>

<p>Now let&rsquo;s see what&rsquo;s in Kafka. I&rsquo;m using KSQL here to inspect the data; you could use other Kafka console consumers if you&rsquo;d rather.</p>

<p>Launch KSQL:</p>

<pre><code>docker-compose exec ksql-cli ksql http://ksql-server:8088
</code></pre>

<p>Inspect the topics on the Kafka cluster:</p>

<pre><code>ksql&gt; LIST TOPICS;

Kafka Topic               | Registered | Partitions | Partition Replicas | Consumers | ConsumerGroups
-------------------------------------------------------------------------------------------------------
asgard.DEBEZIUM.CUSTOMERS | false      | 1          | 1                  | 0         | 0
ora-CUSTOMERS-jdbc        | false      | 1          | 1                  | 0         | 0
‚Ä¶
</code></pre>

<p>The two topics listed are for the same table (<code>CUSTOMERS</code>) from the Debezium and JDBC connectors respectively.</p>

<p>Dump the contents:</p>

<ul>
<li><p>Debezium/XStreams:</p>

<pre><code>ksql&gt; PRINT 'asgard.DEBEZIUM.CUSTOMERS' FROM BEGINNING;
Format:AVRO
12/11/18 5:16:40 PM UTC, , {&quot;before&quot;: null, &quot;after&quot;: {&quot;ID&quot;: 1, &quot;FIRST_NAME&quot;: &quot;Rica&quot;, &quot;LAST_NAME&quot;: &quot;Blaisdell&quot;, &quot;EMAIL&quot;: &quot;rblaisdell0@rambler.ru&quot;, &quot;GENDER&quot;: &quot;Female&quot;, &quot;CLUB_STATUS&quot;: &quot;bronze&quot;, &quot;COMMENTS&quot;: &quot;Universal optimal hierarchy&quot;, &quot;CREATE_TS&quot;: 1544548560283613, &quot;UPDATE_TS&quot;: 1544548560000000}, &quot;source&quot;: {&quot;version&quot;: &quot;0.9.0.Alpha2&quot;, &quot;connector&quot;: &quot;oracle&quot;, &quot;name&quot;: &quot;asgard&quot;, &quot;ts_ms&quot;: 1544548595164, &quot;txId&quot;: null, &quot;scn&quot;: 3014605, &quot;snapshot&quot;: true}, &quot;op&quot;: &quot;r&quot;, &quot;ts_ms&quot;: 1544548595189, &quot;messagetopic&quot;: &quot;asgard.DEBEZIUM.CUSTOMERS&quot;, &quot;messagesource&quot;: &quot;Debezium CDC from Oracle on asgard&quot;}
‚Ä¶
</code></pre></li>

<li><p>JDBC connector</p>

<pre><code>ksql&gt; PRINT 'ora-CUSTOMERS-jdbc' FROM BEGINNING;
Format:AVRO
12/11/18 5:16:55 PM UTC, null, {&quot;ID&quot;: 1, &quot;FIRST_NAME&quot;: &quot;Rica&quot;, &quot;LAST_NAME&quot;: &quot;Blaisdell&quot;, &quot;EMAIL&quot;: &quot;rblaisdell0@rambler.ru&quot;, &quot;GENDER&quot;: &quot;Female&quot;, &quot;CLUB_STATUS&quot;: &quot;bronze&quot;, &quot;COMMENTS&quot;: &quot;Universal optimal hierarchy&quot;, &quot;CREATE_TS&quot;: 1544548560283, &quot;UPDATE_TS&quot;: 1544548560000, &quot;messagetopic&quot;: &quot;ora-CUSTOMERS-jdbc&quot;, &quot;messagesource&quot;: &quot;JDBC Source Connector from Oracle on asgard&quot;}
‚Ä¶
</code></pre></li>
</ul>

<p>Each has the full contents of the source table (5 records, only first is shown above). We can actually use KSQL to easily query the topic directly if we want. First we declare each topic as the source for a stream:</p>

<pre><code>SET 'auto.offset.reset' = 'earliest';
CREATE STREAM CUSTOMERS_STREAM_DBZ_SRC WITH (KAFKA_TOPIC='asgard.DEBEZIUM.CUSTOMERS', VALUE_FORMAT='AVRO');
CREATE STREAM CUSTOMERS_STREAM_JDBC_SRC WITH (KAFKA_TOPIC='ora-CUSTOMERS-jdbc', VALUE_FORMAT='AVRO');
</code></pre>

<p>and then query the JDBC-sourced Kafka topic:</p>

<pre><code>ksql&gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC LIMIT 5;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
</code></pre>

<p>and the one from Debezium:</p>

<pre><code>ksql&gt; SELECT AFTER-&gt;ID AS ID, AFTER-&gt;FIRST_NAME AS FIRST_NAME, AFTER-&gt;LAST_NAME AS LAST_NAME, AFTER-&gt;CLUB_STATUS AS CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
3 | Mariejeanne | Cocci | bronze
4 | Hashim | Rumke | platinum
5 | Hansiain | Coda | platinum
</code></pre>

<p>Note that I&rsquo;m accessing nested attributes of the <code>AFTER</code> object here using the <code>-&gt;</code> operator.</p>

<p>The schema for both topics come from the Schema Registry, in which Kafka Connect automatically stores the schema for the data coming from Oracle and serialises the data into Avro. The great thing about this is in a consuming application, such as KSQL, the schema is already available and doesn&rsquo;t have to be manually entered.</p>

<h4 id="insert">INSERT</h4>

<p>Insert a row in the Oracle database:</p>

<pre><code>SQL&gt; SET AUTOCOMMIT ON;
SQL&gt;
SQL&gt; INSERT INTO CUSTOMERS (FIRST_NAME,LAST_NAME,CLUB_STATUS) VALUES ('Rick','Astley','Bronze');

1 row created.

Commit complete.
</code></pre>

<p>Straight away in the Kafka topics you&rsquo;ll see a new row (in fact, if you have left the above <code>SELECT</code> running you won&rsquo;t need to rerun this, it&rsquo;ll show the new row already):</p>

<ul>
<li><p>JDBC</p>

<pre><code>ksql&gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
42 | Rick | Astley | Bronze
</code></pre></li>

<li><p>Debezium/XStream</p>

<pre><code>ksql&gt; SELECT AFTER-&gt;ID AS ID, AFTER-&gt;FIRST_NAME AS FIRST_NAME, AFTER-&gt;LAST_NAME AS LAST_NAME, AFTER-&gt;CLUB_STATUS AS CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
3 | Mariejeanne | Cocci | bronze
4 | Hashim | Rumke | platinum
5 | Hansiain | Coda | platinum
42 | Rick | Astley | Bronze
</code></pre></li>
</ul>

<p>So far, so same. Each captures an insert. Debezium from XStream and the database&rsquo;s redo log, JDBC by polling the database for any rows with a newer <code>UPDATE_TS</code> or higher <code>ID</code> than the previous request.</p>

<h4 id="update">UPDATE</h4>

<p>This is where things get interesting. Let&rsquo;s update the row in Oracle that we just created:</p>

<pre><code>SQL&gt; UPDATE CUSTOMERS SET CLUB_STATUS = 'Platinum' where ID=42;

1 row updated.

Commit complete.
SQL&gt;
</code></pre>

<p>Now check out the data in Kafka.</p>

<ul>
<li><p>JDBC is as before; the changed data row is available to us:</p>

<pre><code>ksql&gt; SELECT ID, FIRST_NAME, LAST_NAME, CLUB_STATUS FROM CUSTOMERS_STREAM_JDBC_SRC;
1 | Rica | Blaisdell | bronze
2 | Ruthie | Brockherst | platinum
5 | Hansiain | Coda | platinum
4 | Hashim | Rumke | platinum
3 | Mariejeanne | Cocci | bronze
42 | Rick | Astley | Bronze
42 | Rick | Astley | Platinum
</code></pre></li>

<li><p>Debezium/XStream now comes into its own. As well as the new row of data, we can see what it was previously, through the <code>BEFORE</code> nested object:</p>

<pre><code>ksql&gt; SELECT OP, AFTER-&gt;ID, BEFORE-&gt;CLUB_STATUS, AFTER-&gt;CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
r | 1 | null | bronze
r | 2 | null | platinum
r | 3 | null | bronze
r | 4 | null | platinum
r | 5 | null | platinum
c | 42 | null | Bronze
u | 42 | Bronze | Platinum
</code></pre>

<p>I&rsquo;m just showing the before/after <code>CLUB_STATUS</code> but all the other fields are also available. There&rsquo;s also metadata about the change, including the type of operation in the <code>OP</code> field (<code>r</code>=read, i.e the initial snapshot, <code>c</code>=create, <code>u</code>=update)</p>

<p>Let&rsquo;s look at the full payload of each message sent to Kafka:</p>

<pre><code>{
  &quot;before&quot;: {
    &quot;ID&quot;: 42,
    &quot;FIRST_NAME&quot;: &quot;Rick&quot;,
    &quot;LAST_NAME&quot;: &quot;Astley&quot;,
    &quot;EMAIL&quot;: null,
    &quot;GENDER&quot;: null,
    &quot;CLUB_STATUS&quot;: &quot;Bronze&quot;,
    &quot;COMMENTS&quot;: null,
    &quot;CREATE_TS&quot;: 1544000706681769,
    &quot;UPDATE_TS&quot;: 1544000706000000
  },
  &quot;after&quot;: {
    &quot;ID&quot;: 42,
    &quot;FIRST_NAME&quot;: &quot;Rick&quot;,
    &quot;LAST_NAME&quot;: &quot;Astley&quot;,
    &quot;EMAIL&quot;: null,
    &quot;GENDER&quot;: null,
    &quot;CLUB_STATUS&quot;: &quot;Platinum&quot;,
    &quot;COMMENTS&quot;: null,
    &quot;CREATE_TS&quot;: 1544000706681769,
    &quot;UPDATE_TS&quot;: 1544000742000000
  },
  &quot;source&quot;: {
    &quot;version&quot;: &quot;0.9.0.Alpha2&quot;,
    &quot;connector&quot;: &quot;oracle&quot;,
    &quot;name&quot;: &quot;asgard&quot;,
    &quot;ts_ms&quot;: 1544000742000,
    &quot;txId&quot;: &quot;6.26.734&quot;,
    &quot;scn&quot;: 2796831,
    &quot;snapshot&quot;: false
  },
  &quot;op&quot;: &quot;u&quot;,
  &quot;ts_ms&quot;: 1544000745823,
  &quot;messagetopic&quot;: &quot;asgard.DEBEZIUM.CUSTOMERS&quot;,
  &quot;messagesource&quot;: &quot;Debezium CDC from Oracle on asgard&quot;
}
</code></pre>

<p>So each time a change is made in the database, you get a full before/after snapshot of the record, plus a bunch of other metadata. This is great for applications processing inbound changes that need to know not just that something changed (<em>here&rsquo;s the new record</em>) but also exactly <em>what</em> changed (before/after payloads) as well as <em>how</em> (insert/update/etc.)</p></li>
</ul>

<h4 id="delete">DELETE</h4>

<p>Delete a record from the source system</p>

<pre><code>SQL&gt; DELETE FROM CUSTOMERS WHERE ID=42;

1 row deleted.

Commit complete.
</code></pre>

<p>Now check out the data in Kafka.</p>

<p>JDBC is unchanged; it&rsquo;s not captured any change to the source table. If you think about it, this is perfectly reasonable. How you query a database for a row that doesn&rsquo;t exist?
Debezium/XStream, on the other hand, reports the data change precisely:</p>

<pre><code>ksql&gt; SELECT OP, AFTER-&gt;ID, BEFORE-&gt;CLUB_STATUS, AFTER-&gt;CLUB_STATUS FROM CUSTOMERS_STREAM_DBZ_SRC;
r | 1 | null | bronze
r | 2 | null | platinum
r | 3 | null | bronze
r | 4 | null | platinum
r | 5 | null | platinum
c | 42 | null | Bronze
u | 42 | Bronze | Platinum
d | null | Platinum | null
</code></pre>

<p>Note the <code>d</code> record on the last row. This has captured the <code>DELETE</code> operation perfectly. The <code>null</code> in the right-most column is the current value for <code>AFTER-&gt;CLUB_STATUS</code>, and since the record is deleted, it has no value. We can see this even more clearly if we look at the raw payload for the whole record:</p>

<pre><code>{
  &quot;before&quot;: {
    &quot;ID&quot;: 42,
    &quot;FIRST_NAME&quot;: &quot;Rick&quot;,
    &quot;LAST_NAME&quot;: &quot;Astley&quot;,
    &quot;EMAIL&quot;: null,
    &quot;GENDER&quot;: null,
    &quot;CLUB_STATUS&quot;: &quot;Platinum&quot;,
    &quot;COMMENTS&quot;: null,
    &quot;CREATE_TS&quot;: 1544562543660463,
    &quot;UPDATE_TS&quot;: 1544562791000000
  },
  &quot;after&quot;: null,
  &quot;source&quot;: {
    &quot;version&quot;: &quot;0.9.0.Alpha2&quot;,
    &quot;connector&quot;: &quot;oracle&quot;,
    &quot;name&quot;: &quot;asgard&quot;,
    &quot;ts_ms&quot;: 1544563479000,
    &quot;txId&quot;: &quot;9.32.712&quot;,
    &quot;scn&quot;: 3042804,
    &quot;snapshot&quot;: true
  },
  &quot;op&quot;: &quot;d&quot;,
  &quot;ts_ms&quot;: 1544563482682,
  &quot;messagetopic&quot;: &quot;asgard.DEBEZIUM.CUSTOMERS&quot;,
  &quot;messagesource&quot;: &quot;Debezium CDC from Oracle on asgard&quot;
}
</code></pre>

<p>The full record that has been deleted is present in the <code>BEFORE</code> object, but <code>AFTER</code> is null‚Äîit&rsquo;s been deleted, it no longer exists. It is an ex-record.</p>

<hr />

<p>Bonus KSQL :</p>

<p>We&rsquo;re working with data in a Kafka topic. As it happens, KSQL is kinda useful for interogating that data, but at the end of the day it&rsquo;s still just a Kafka topic. We can use KSQL to also help monitor the lag between the event in the source system (<code>source-&gt;ms_ms</code> as provided by Debezium) and the time recorded on the Kafka broker (the Kafka message timestamp, exposed in <code>ROWTIME</code>):</p>

<pre><code>ksql&gt; SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss Z'), \
&gt;         OP, \
&gt;         ROWTIME - SOURCE-&gt;TS_MS AS LAG_MS \
&gt; FROM CUSTOMERS_STREAM_DBZ_SRC;
2018-12-11 17:16:40 +0000 | r | 5829
2018-12-11 17:16:40 +0000 | r | 5806
2018-12-11 17:16:40 +0000 | r | 5802
2018-12-11 17:16:41 +0000 | r | 5805
2018-12-11 17:16:41 +0000 | r | 5805
2018-12-11 21:09:07 +0000 | c | 4104
2018-12-11 21:13:51 +0000 | u | 40734
2018-12-11 21:28:10 +0000 | d | 211438
</code></pre>

<p>Some of these lag times are pretty high; <a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1018">DBZ-1018 Oracle connector is laggy</a> is a JIRA currently tracking it.</p>

<p>You can get the same data out of the JDBC connector, based on the <code>UPDATE_TS</code> of the record itself:</p>

<pre><code>ksql&gt; SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss Z'), \
&gt;          ROWTIME - UPDATE_TS AS LAG_MS \
&gt; FROM CUSTOMERS_STREAM_JDBC_SRC;
2018-12-11 17:16:55 +0000 | 55612
2018-12-11 17:16:55 +0000 | 55613
2018-12-11 17:16:55 +0000 | 55614
2018-12-11 17:16:55 +0000 | 55615
2018-12-11 17:16:55 +0000 | 55615
2018-12-11 21:09:04 +0000 | 1330
2018-12-11 21:13:12 +0000 | 1384
</code></pre>

<p>You&rsquo;ll note here no available <code>OP</code> information, and no row for the corresponding <code>DELETE</code> action in the source database.</p>

<hr />

<h3 id="ecosystem">Ecosystem</h3>

<p>When you&rsquo;re bringing data into Kafka, you need to remember the bigger picture. Dumping it into a topic alone is not enough. Well, it is, but your wider community of developers won&rsquo;t thank you.</p>

<p>You want to ensure that the schema of the source data is preserved, and that you&rsquo;re using a serialisation method for the data that is suitable. Doing this means that developers can use the data without being tightly coupled to the producer of the data to understand how to use it.</p>

<p>However you do this, it should be in a way that integrates with the broader Kafka and Confluent Platform ecosystem. One option is the Schema Registry and Avro. <strong>If you&rsquo;re using Kafka Connect then this is available by default</strong>, since you just select the Avro converter when you set up Kafka Connect.</p>

<hr />

<h3 id="overview-of-the-pros-and-cons-of-each-technique">Overview of the Pros and Cons of each technique</h3>

<p><em>Some of these are objective, others subjective. Others may indeed be plain false ;-) Discussion, comments, and corrections in the comment function below welcomed!</em></p>

<ul>
<li><strong>Query-based CDC</strong>

<ul>
<li>Pros:

<ul>
<li>Easy to set up</li>
<li>Minimal privileges required</li>
</ul></li>
<li>Cons:

<ul>
<li>No capture of DELETEs</li>
<li>No capture of before-state in an UPDATE</li>
<li>No guarantee that <em>all</em> events are captured; only the state at the time of polling</li>
<li>Increased load on the source DB due to polling (and/or unacceptable latency in capturing the events if polling interval too high)</li>
</ul></li>
</ul></li>
</ul>

<p><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">Another con one could add to query-based CDC is that it needs support by the model (update column); I&#39;ve blogged about here: <a href="https://t.co/DDYV62DIVF">https://t.co/DDYV62DIVF</a>. Log-based CDC also can give you additional metadata like TX ids, causing queries (for some DBs) etc.</p>&mdash; Gunnar Morling (@gunnarmorling) <a href="https://twitter.com/gunnarmorling/status/1073323155370987521?ref_src=twsrc%5Etfw">December 13, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<ul>
<li><p><strong>Log-based CDC</strong></p>

<ul>
<li><p>Oracle GoldenGate for Big Data (&ldquo;OGGBD&rdquo;)</p>

<ul>
<li><p>Pros:</p>

<ul>
<li>Low latency</li>
<li>Minimal impact on the source</li>
<li>Captures every event</li>
<li>Capture before/after state in UPDATEs</li>
<li>Captures DELETEs include prior state</li>
</ul></li>

<li><p>Cons:</p>

<ul>
<li>Requires sysadmin privileges to install</li>
<li>Relatively complex to set up (compared to JDBC connector)</li>
<li>License cost</li>
<li>Kafka Connect support is not fully compliant with the Kafka Connect API which may matter if you want to use things like custom converters, Single Message Transform, and so on.</li>
</ul></li>
</ul></li>

<li><p>Oracle XStream</p>

<ul>
<li><p>Pros:</p>

<ul>
<li>Marginally cheaper than OGGBD</li>
<li>Debezium is open source and is under active development</li>
<li>Low latency</li>
<li>Minimal impact on the source</li>
<li>Captures every event</li>
<li>Capture before/after state in UPDATEs</li>
<li>Captures DELETEs include prior state</li>
</ul></li>

<li><p>Cons:</p>

<ul>
<li>Requires sysadmin privileges to install</li>
<li>Relatively complex to set up (compared to JDBC connector)</li>
<li>License cost</li>
<li>Some issues with current Debezium implementation:

<ul>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1022">DBZ-1022 org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for field insertion, found: null</a></li>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1019">DBZ-1019 java.lang.IllegalArgumentException: timeout value is negative</a></li>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1018">DBZ-1018 Oracle connector is laggy</a></li>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1014">DBZ-1014 Oracle connector requires libaio to be installed</a></li>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1013">DBZ-1013 Include Instant Client in Docker build for Oracle</a></li>
<li><a href="https://issues.jboss.org/projects/DBZ/issues/DBZ-1012">DBZ-1012 Debezium doesn&rsquo;t report absence of Instant Client</a></li>
</ul></li>
</ul></li>
</ul></li>

<li><p>Oracle Log Miner</p>

<ul>
<li><p>Pros:</p>

<ul>
<li>No additional license cost</li>
<li>Full access to events?</li>
</ul></li>

<li><p>Cons</p>

<ul>
<li>Relatively complex to set up (compared to JDBC connector)</li>
<li>Requires code to parse events. How infallible is this?</li>
<li>Could be inefficient if only capturing events from a small proportion of the DB activity (has to scan all REDO log still). Is this also a problem with XStream?</li>
</ul></li>
</ul></li>
</ul></li>

<li><p><strong>Triggers</strong></p>

<ul>
<li><p>Pros:</p>

<ul>
<li>No licence cost</li>
<li>Entirely customisable</li>
</ul></li>

<li><p>Cons:</p>

<ul>
<li>Completely bespoke code to develop and maintain.</li>
<li>Tightly coupled to source application</li>
</ul></li>
</ul></li>

<li><p><strong>Flashback</strong></p>

<ul>
<li><p>Pro + Con:</p>

<ul>
<li>Requires EE licence‚Äîbut this is something users are more likely to have already than OGG/OGGBD</li>
</ul></li>

<li><p>Unknown:</p>

<ul>
<li>What granularity of data can be retrieved?</li>
<li>Impact on the DB from polling?</li>
<li>Unclear how much bespoke coding this would require per integration?</li>
</ul></li>
</ul></li>
</ul>

<p><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">There are 2 flashback features:<br>Flashback transaction query shows transactions from redo, similar to log miner.<br>Flashback version query shows previous versions, from undo, within undo_retention. Allowed in all editions. I think this is the one you mention.</p>&mdash; Franck Pachot (@FranckPachot) <a href="https://twitter.com/FranckPachot/status/1073323013750317056?ref_src=twsrc%5Etfw">December 13, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<h3 id="references">References</h3>

<ul>
<li><p>Oracle GoldenGate for Big Data</p>

<ul>
<li><a href="https://www.oracle.com/middleware/data-integration/goldengate/big-data/">https://www.oracle.com/middleware/data-integration/goldengate/big-data/</a></li>
<li><a href="https://rmoff.net/tag/goldengate/">https://rmoff.net/tag/goldengate/</a></li>
<li><a href="https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/">https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/</a></li>
<li><a href="https://www.confluent.io/connector/oracle-goldengate/">https://www.confluent.io/connector/oracle-goldengate/</a></li>
</ul></li>

<li><p>Oracle XStream / Debezium</p>

<ul>
<li><a href="https://docs.oracle.com/en/database/oracle/oracle-database/18/xstrm/index.html">https://docs.oracle.com/en/database/oracle/oracle-database/18/xstrm/index.html</a></li>
<li><a href="https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle">https://github.com/debezium/debezium-examples/blob/master/tutorial/README.md#using-oracle</a></li>
</ul></li>

<li><p>LogMiner</p>

<ul>
<li><a href="https://docs.oracle.com/en/database/oracle/oracle-database/18/sutil/oracle-logminer-utility.html#GUID-2EAA593B-DC09-4D30-87EB-34819FC68B3D">https://docs.oracle.com/en/database/oracle/oracle-database/18/sutil/oracle-logminer-utility.html#GUID-2EAA593B-DC09-4D30-87EB-34819FC68B3D</a></li>
</ul></li>
</ul>

<h3 id="try-it-out">Try it out!</h3>

<p>You can find all of the code used in this article <a href="https://github.com/confluentinc/demo-scene/blob/master/no-more-silos-oracle/">on github here</a>.</p>

<h3 id="feedback">Feedback?</h3>

<p>Some of these are objective, others subjective. Others may indeed be plain false ;-) Discussion, comments, and corrections in the comment function below welcomed!</p>

<p>For <em>help</em> in getting this working, the best place to head is the Confluent Community:</p>

<ul>
<li>Mailing list: <a href="https://groups.google.com/forum/#!forum/confluent-platform">https://groups.google.com/forum/#!forum/confluent-platform</a></li>
<li>Slack group: <a href="https://slackpass.io/confluentcommunity">https://slackpass.io/confluentcommunity</a></li>
</ul>

<p>There&rsquo;s also a good Debezium community:</p>

<ul>
<li>Mailing list: <a href="https://groups.google.com/forum/#!forum/debezium">https://groups.google.com/forum/#!forum/debezium</a></li>
<li>Gitter: <a href="https://gitter.im/debezium/user">https://gitter.im/debezium/user</a></li>
</ul>

<h3 id="updates-comments">Updates &amp; Comments</h3>

<ul>
<li><p>Adam Leszczy≈Ñski has built a (non-Kafka Connect) source: <a href="https://www.bersler.com/blog/openlogreplicator-first-log-based-open-source-oracle-to-kafka-replication/">https://www.bersler.com/blog/openlogreplicator-first-log-based-open-source-oracle-to-kafka-replication/</a></p></li>

<li><p>From Tanel Poder:</p>

<p><blockquote class="twitter-tweet" data-conversation="none" data-lang="en"><p lang="en" dir="ltr">1) Materialized view logs<br><br>2) JDBC table change notifications (limited use as it only stores a few rowids at fine grained before switching to table level notifications)<br><br>Btw i thought attunity had a binary reader?</p>&mdash; Tanel Poder (@TanelPoder) <a href="https://twitter.com/TanelPoder/status/1073335673455894531?ref_src=twsrc%5Etfw">December 13, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p></li>
</ul>

</article>

		</main>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr />

<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt" /></p>

<p>Robin Moffatt is a Developer Advocate at Confluent, and Oracle Groundbreaker Ambassador. He also likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</p>

				
			
		
		</div>
		
	</div>

		
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.github.io/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2019 Robin Moffatt
			</p>
		</footer>
		
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-75492960-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	
	</body>
</html>
