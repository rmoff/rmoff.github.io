<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Single Message Transform on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/single-message-transform/</link>
    <description>Recent content in Single Message Transform on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Dec 2020 20:00:18 +0000</lastBuildDate><atom:link href="https://rmoff.net/categories/single-message-transform/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 2: ValueToKey and ExtractField</title>
      <link>https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/</link>
      <pubDate>Wed, 09 Dec 2020 20:00:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/09/twelve-days-of-smt-day-2-valuetokey-and-extractfield/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Setting the key of a Kafka message is important as it ensures correct logical processing when consumed across multiple partitions, as well as being a requirement when joining to messages in other topics. When using Kafka Connect the connector may already set the key, which is great. If not, you can use these two Single Message Transforms (SMT) to set it as part of the pipeline based on a field in the value part of the message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/valuetokey.html&#34;&gt;&lt;code&gt;ValueToKey&lt;/code&gt;&lt;/a&gt; Single Message Transform specify the name of the field (&lt;code&gt;id&lt;/code&gt;) that you want to copy from the value to the key:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                    &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;copyIdToKey&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.copyIdToKey.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;   &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.ValueToKey&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.copyIdToKey.fields&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;id&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 1: InsertField (timestamp)</title>
      <link>https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/</link>
      <pubDate>Tue, 08 Dec 2020 22:23:18 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/12/08/twelve-days-of-smt-day-1-insertfield-timestamp/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can use the &lt;a href=&#34;https://docs.confluent.io/platform/current/connect/transforms/insertfield.html&#34;&gt;&lt;code&gt;InsertField&lt;/code&gt;&lt;/a&gt; Single Message Transform (SMT) to add the message timestamp into each message that Kafka Connect sends to a sink.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To use the Single Message Transform specify the name of the field (&lt;code&gt;timestamp.field&lt;/code&gt;) that you want to add to hold the message timestamp:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;                         &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;insertTS&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertTS.type&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;           &lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;org.apache.kafka.connect.transforms.InsertField$Value&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;,&lt;/span&gt;
&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;transforms.insertTS.timestamp.field&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;background-color: #f8f8f8&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;messageTS&lt;/span&gt;&lt;span style=&#34;color: #d14&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Streaming XML messages from IBM MQ into Kafka into MongoDB</title>
      <link>https://rmoff.net/2020/10/05/streaming-xml-messages-from-ibm-mq-into-kafka-into-mongodb/</link>
      <pubDate>Mon, 05 Oct 2020 10:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/05/streaming-xml-messages-from-ibm-mq-into-kafka-into-mongodb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Letâ€™s imagine we have XML data on a queue in IBM MQ, and we want to ingest it into Kafka to then use downstream, perhaps in an application or maybe to stream to a NoSQL store like MongoDB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock note&#34;&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
This same pattern for ingesting XML will work with other connectors such as &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-jms&#34;&gt;JMS&lt;/a&gt; and &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-activemq&#34;&gt;ActiveMQ&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Ingesting XML data into Kafka - Option 2: Kafka Connect plus Single Message Transform</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/</link>
      <pubDate>Thu, 01 Oct 2020 14:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-2-kafka-connect-plus-single-message-transform/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;We previously looked at the background to &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/&#34;&gt;getting XML into Kafka&lt;/a&gt;, and potentially &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/&#34;&gt;how [not] to do it&lt;/a&gt;. Now letâ€™s look at the &lt;em&gt;proper&lt;/em&gt; way to build a streaming ingestion pipeline for XML into Kafka, using Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;If youâ€™re unfamiliar with Kafka Connect, check out this &lt;a href=&#34;https://rmoff.dev/what-is-kafka-connect&#34;&gt;quick intro to Kafka Connect here&lt;/a&gt;. Kafka Connectâ€™s excellent plugable architecture means that we can pair any &lt;strong&gt;source connector&lt;/strong&gt; to read XML from wherever we have it (for example, a flat file, or a MQ, or anywhere else), with a &lt;strong&gt;Single Message Transform&lt;/strong&gt; to transform the XML into a payload with a schema, and finally a &lt;strong&gt;converter&lt;/strong&gt; to serialise the data in a form that we would like to use such as Avro or Protobuf.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Primitive Keys in ksqlDB</title>
      <link>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</link>
      <pubDate>Fri, 07 Feb 2020 10:58:06 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ksqlDB 0.7 will add support for message keys as primitive data types beyond just &lt;code&gt;STRING&lt;/code&gt; (which is all weâ€™ve had to date). That means that Kafka messages are going to be much easier to work with, and require less wrangling to get into the form in which you need them. Take an example of a database table that youâ€™ve ingested into a Kafka topic, and want to join to a stream of events. Previously youâ€™d have had to take the Kafka topic into which the table had been ingested and run a ksqlDB processor to re-key the messages such that ksqlDB could join on them. &lt;em&gt;Friends, I am here to tell you that this is no longer needed!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
