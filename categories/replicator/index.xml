<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Replicator on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/replicator/</link>
    <description>Recent content in Replicator on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Apr 2020 13:55:46 +0100</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/replicator/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Confluent Cloud when there is no Cloud (or internet)</title>
      <link>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</link>
      <pubDate>Mon, 20 Apr 2020 13:55:46 +0100</pubDate>
      <guid>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://confluent.cloud/signup&#34;&gt;☁️Confluent Cloud&lt;/a&gt; is a great solution for a hosted and managed Apache Kafka service, with the additional benefits of Confluent Platform such as ksqlDB and managed Kafka Connect connectors. But as a developer, you won’t always have a reliable internet connection. Train, planes, and automobiles—not to mention crappy hotel or conference Wi-Fi. Wouldn’t it be useful if you could have a replica of your Cloud data on your local machine? That just pulled down new data automagically, without needing to be restarted each time you got back on the network?&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Resetting a Consumer Group in Kafka</title>
      <link>https://rmoff.net/2019/08/09/resetting-a-consumer-group-in-kafka/</link>
      <pubDate>Fri, 09 Aug 2019 16:32:46 +0200</pubDate>
      <guid>https://rmoff.net/2019/08/09/resetting-a-consumer-group-in-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;I’ve been using &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-replicator/index.html&#34;&gt;Replicator&lt;/a&gt; as a powerful way to copy data from my Kafka rig at home onto my laptop’s Kafka environment. It means that when I’m on the road I can continue to work with the same set of data and develop pipelines etc. With a VPN back home I can even keep them in sync directly if I want to.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;I hit a problem the other day where Replicator was running, but I had no data in my target topics on my laptop. After a bit of head-scratching I realised that my local Kafka environment had been rebuilt (I use Docker Compose so complete rebuilds to start from scratch are easy), hence no data in the topic. But, even after restarting the Replicator Kafka Connect worker, I still had no data loaded into the empty topics. What was going on? Well Replicator acts as a consumer from the source Kafka cluster (on my home server), and so far as that Kafka cluster was concerned, Replicator had already read the messages. It thought that because even though I’d rebuilt everything on my laptop, Replicator was using the same connector name as before, and the connector name is used as the Consumer group name - which is how the &lt;em&gt;source&lt;/em&gt; Kafka cluster keeps track of the offsets. So my &amp;#34;new&amp;#34; Kafka environment was going back to the source, which viewed it as the existing &amp;#34;old&amp;#34; one, which had already received the messages.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
  </channel>
</rss>
