<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafkacat on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/categories/kafkacat/</link>
    <description>Recent content in kafkacat on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Sep 2019 10:43:45 +0200</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/categories/kafkacat/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Copying data between Kafka clusters with Kafkacat</title>
      <link>https://rmoff.github.io/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</link>
      <pubDate>Sun, 29 Sep 2019 10:43:45 +0200</pubDate>
      
      <guid>https://rmoff.github.io/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafkacat_gives_you_kafka_super_powers&#34;&gt;kafkacat gives you Kafka super powers üòé&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I&amp;#8217;ve &lt;a href=&#34;https://rmoff.net/categories/kafkacat/&#34;&gt;written before&lt;/a&gt; about &lt;a href=&#34;https://github.com/edenhill/kafkacat&#34;&gt;kafkacat&lt;/a&gt; and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in &lt;a href=&#34;https://talks.rmoff.net/8Oruwt/on-track-with-apache-kafka-building-a-streaming-etl-solution-with-rail-data#s9tMEWG&#34;&gt;a recent demo&lt;/a&gt; that I built in which data needed manipulating in a way that I couldn&amp;#8217;t easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it&amp;#8217;s getting data from &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; down to a local cluster.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Reset Kafka Connect Source Connector Offsets</title>
      <link>https://rmoff.github.io/2019/08/15/reset-kafka-connect-source-connector-offsets/</link>
      <pubDate>Thu, 15 Aug 2019 10:42:34 +0100</pubDate>
      
      <guid>https://rmoff.github.io/2019/08/15/reset-kafka-connect-source-connector-offsets/</guid>
      <description>Kafka Connect in distributed mode uses Kafka itself to persist the offsets of any source connectors. This is a great way to do things as it means that you can easily add more workers, rebuild existing ones, etc without having to worry about where the state is persisted. I personally always recommend using distributed mode, even if just for a single worker instance - it just makes things easier, and more standard.</description>
    </item>
    
    <item>
      <title>Manually delete a connector from Kafka Connect</title>
      <link>https://rmoff.github.io/2019/06/23/manually-delete-a-connector-from-kafka-connect/</link>
      <pubDate>Sun, 23 Jun 2019 11:39:46 +0200</pubDate>
      
      <guid>https://rmoff.github.io/2019/06/23/manually-delete-a-connector-from-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect has as &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST API&lt;/a&gt; through which all config should be done, including removing connectors that have been created. Sometimes though, you might have reason to want to manually do this‚Äîand since Kafka Connect running in distributed mode uses Kafka as its persistent data store, you can achieve this by manually writing to the topic yourself.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Deleting a Connector in Kafka Connect without the REST API</title>
      <link>https://rmoff.github.io/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</link>
      <pubDate>Wed, 22 May 2019 10:32:10 +0100</pubDate>
      
      <guid>https://rmoff.github.io/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect exposes a &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST interface&lt;/a&gt; through which all config and monitoring operations can be done. You can create connectors, delete them, restart them, check their status, and so on. But, I found a situation recently in which I needed to delete a connector and couldn&amp;#8217;t do so with the REST API. Here&amp;#8217;s another way to do it, by amending the configuration Kafka topic that Kafka Connect in distributed mode uses to persist configuration information for connectors. Note that this is not a recommended way of working with Kafka Connect‚Äîthe REST API is there for a good reason :)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Sending multiline messages to Kafka</title>
      <link>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</link>
      <pubDate>Tue, 04 Sep 2018 08:26:51 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</guid>
      <description>(SO answer repost)
You can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):
kafkacat -b kafka:29092 \ -t test_topic_01 \ -D/ \ -P &amp;lt;&amp;lt;EOF this is a string message with a line break/this is another message with two line breaks! EOF  Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140</description>
    </item>
    
    <item>
      <title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
      <link>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
      <pubDate>Thu, 10 May 2018 12:56:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
      <description>tl;dr Use curl to pull data from the Mockaroo REST endpoint, and pipe it into kafkacat, thus:
curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \ kafkacat -b localhost:9092 -t purchases -P  Three things I love‚Ä¶Kafka, kafkacat, and Mockaroo. And in this post I get to show all three üòÅ
Mockaroo is a very cool online service that lets you quickly mock up test data. What sets it apart from SELECT RANDOM(100) FROM DUMMY; is that it has lots of different classes of test data for you to choose from.</description>
    </item>
    
  </channel>
</rss>