<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafkacat on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/kafkacat/</link>
    <description>Recent content in kafkacat on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Oct 2020 13:09:41 +0100</lastBuildDate><atom:link href="https://rmoff.net/categories/kafkacat/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ingesting XML data into Kafka - Option 1: The Dirty Hack</title>
      <link>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/</link>
      <pubDate>Thu, 01 Oct 2020 13:09:41 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-option-1-the-dirty-hack/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;üëâ &lt;a href=&#34;https://rmoff.net/2020/10/01/ingesting-xml-data-into-kafka-introduction/&#34;&gt;Ingesting XML data into Kafka - Introduction&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;What would a blog post on &lt;code&gt;rmoff.net&lt;/code&gt; be if it didn‚Äôt include the dirty hack option? üòÅ&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;em&gt;The secret to dirty hacks is that they are often rather effective and when needs must, they can suffice. If you‚Äôre prototyping and need to &lt;a href=&#34;https://www.urbandictionary.com/define.php?term=JFDI&#34;&gt;&lt;strong&gt;JFDI&lt;/strong&gt;&lt;/a&gt;, a dirty hack is just fine. If you‚Äôre looking for code to run in Production, then a dirty hack probably is not fine.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Setting key value when piping from `jq` to `kafkacat`</title>
      <link>https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/</link>
      <pubDate>Wed, 30 Sep 2020 20:54:09 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/30/setting-key-value-when-piping-from-jq-to-kafkacat/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;One of my favourite hacks for getting data into Kafka is using kafkacat and &lt;code&gt;stdin&lt;/code&gt;, often from &lt;code&gt;jq&lt;/code&gt;. You can see this in action with &lt;a href=&#34;https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/&#34;&gt;Wi-Fi data&lt;/a&gt;, &lt;a href=&#34;https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/&#34;&gt;IoT data&lt;/a&gt;, and data from a &lt;a href=&#34;https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/&#34;&gt;REST endpoint&lt;/a&gt;. This is fine for getting values into a Kafka message - but Kafka messages are &lt;strong&gt;key&lt;/strong&gt;/value, and being able to specify a key is can often be important.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Here‚Äôs a way to do that, using a separator and some &lt;code&gt;jq&lt;/code&gt; magic. Note that at the moment &lt;a href=&#34;https://github.com/edenhill/kafkacat/issues/140&#34;&gt;kafkacat only supports single byte separator characters&lt;/a&gt;, so you need to choose carefully. If you pick a separator that also appears in your data, it‚Äôs possibly going to have unintended consequences.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Counting the number of messages in a Kafka topic</title>
      <link>https://rmoff.net/2020/09/08/counting-the-number-of-messages-in-a-kafka-topic/</link>
      <pubDate>Tue, 08 Sep 2020 10:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/09/08/counting-the-number-of-messages-in-a-kafka-topic/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;There‚Äôs ways, and then there‚Äôs ways, to count the number of records/events/messages in a Kafka topic. Most of them are potentially inaccurate, or inefficient, or both. Here‚Äôs one that falls into the &lt;em&gt;potentially inefficient&lt;/em&gt; category, using &lt;code&gt;kafkacat&lt;/code&gt; to read all the messages and pipe to &lt;code&gt;wc&lt;/code&gt; which with the &lt;code&gt;-l&lt;/code&gt; will tell you how many lines there are, and since each message is a line, how many messages you have in the Kafka topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;rouge highlight&#34; style=&#34;background-color: #f8f8f8&#34;&gt;&lt;code data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color: #008080&#34;&gt;$ &lt;/span&gt;kafkacat &lt;span style=&#34;color: #000080&#34;&gt;-b&lt;/span&gt; broker:29092 &lt;span style=&#34;color: #000080&#34;&gt;-t&lt;/span&gt; mytestopic &lt;span style=&#34;color: #000080&#34;&gt;-C&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-e&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-q&lt;/span&gt;| &lt;span style=&#34;color: #0086B3&#34;&gt;wc&lt;/span&gt; &lt;span style=&#34;color: #000080&#34;&gt;-l&lt;/span&gt;
       3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Why JSON isn&#39;t the same as JSON Schema in Kafka Connect converters (Viewing Kafka messages bytes as hex)</title>
      <link>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</link>
      <pubDate>Fri, 03 Jul 2020 08:16:36 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve been playing around with the new SerDes (serialisers/deserialisers) that shipped with Confluent Platform 5.5 - &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/serdes-develop/index.html&#34;&gt;Protobuf, and JSON Schema&lt;/a&gt; (these were added to the existing support for Avro). The serialisers (and associated &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/connect.html&#34;&gt;Kafka Connect converters&lt;/a&gt;) take a payload and serialise it into bytes for sending to Kafka, and I was interested in what those bytes look like. For that I used my favourite Kafka swiss-army knife: kafkacat.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>How to install kafkacat on Fedora</title>
      <link>https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/</link>
      <pubDate>Mon, 20 Apr 2020 10:25:32 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/20/how-to-install-kafkacat-on-fedora/</guid>
      <description>kafkacat is one of my go-to tools when working with Kafka. It‚Äôs a producer and consumer, but also a swiss-army knife of debugging and troubleshooting capabilities. So when I built a new Fedora server recently, I needed to get it installed. Unfortunately there‚Äôs no pre-packed install available on yum, so here‚Äôs how to do it manually.
 Pre-requisite installs We‚Äôll need some packages from the Confluent repo so set this up for yum first by creating /etc/yum.</description>
    </item>
    
    <item>
      <title>A quick and dirty way to monitor data arriving on Kafka</title>
      <link>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</link>
      <pubDate>Thu, 16 Apr 2020 00:51:18 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve been poking around recently with &lt;a href=&#34;https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/&#34;&gt;capturing Wi-Fi packet data&lt;/a&gt; and streaming it into Apache Kafka, from where I‚Äôm processing and analysing it. Kafka itself is rock-solid - because I‚Äôm using &lt;a href=&#34;https://confluent.cloud/signup&#34;&gt;‚òÅÔ∏èConfluent Cloud&lt;/a&gt; and someone else worries about provisioning it, scaling it, and keeping it running for me. But whilst Kafka works just great, my side of the setup‚Äî&lt;code&gt;tshark&lt;/code&gt; running on a Raspberry Pi‚Äîis less than stable. For whatever reason it sometimes stalls and I have to restart the Raspberry Pi and restart the capture process.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Streaming Wi-Fi trace data from Raspberry Pi to Apache Kafka with Confluent Cloud</title>
      <link>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</link>
      <pubDate>Wed, 11 Mar 2020 11:58:13 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</guid>
      <description>Wi-fi is now ubiquitous in most populated areas, and the way the devices communicate leaves a lot of &amp;#39;digital exhaust&amp;#39;. Usually a computer will have a Wi-Fi device that‚Äôs configured to connect to a given network, but often these devices can be configured instead to pick up the background Wi-Fi chatter of surrounding devices.
 There are good reasons‚Äîand bad‚Äîfor doing this. Just like taking apart equipment to understand how it works teaches us things, so being able to dissect and examine protocol traffic lets us learn about this.</description>
    </item>
    
    <item>
      <title>Primitive Keys in ksqlDB</title>
      <link>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</link>
      <pubDate>Fri, 07 Feb 2020 10:58:06 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ksqlDB 0.7 will add support for message keys as primitive data types beyond just &lt;code&gt;STRING&lt;/code&gt; (which is all we‚Äôve had to date). That means that Kafka messages are going to be much easier to work with, and require less wrangling to get into the form in which you need them. Take an example of a database table that you‚Äôve ingested into a Kafka topic, and want to join to a stream of events. Previously you‚Äôd have had to take the Kafka topic into which the table had been ingested and run a ksqlDB processor to re-key the messages such that ksqlDB could join on them. &lt;em&gt;Friends, I am here to tell you that this is no longer needed!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Notes on getting data into InfluxDB from Kafka with Kafka Connect</title>
      <link>https://rmoff.net/2020/01/23/notes-on-getting-data-into-influxdb-from-kafka-with-kafka-connect/</link>
      <pubDate>Thu, 23 Jan 2020 12:01:35 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/23/notes-on-getting-data-into-influxdb-from-kafka-with-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You can download the InfluxDB connector for Kafka Connect &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-influxdb&#34;&gt;here&lt;/a&gt;. Documentation for it is &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-influxdb/influx-db-sink-connector/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;When a message from your source Kafka topic is written to InfluxDB the InfluxDB values are set thus:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Timestamp&lt;/strong&gt; is taken from the Kafka message timestamp (which is either set by your producer, or the time at which it was received by the broker)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tag(s)&lt;/strong&gt; are taken from the &lt;code&gt;tags&lt;/code&gt; field in the message. This field must be a &lt;code&gt;map&lt;/code&gt; type - see below&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Value&lt;/strong&gt; fields are taken from the rest of the message, and must be numeric or boolean&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Measurement name&lt;/strong&gt; can be specified as a field of the message, or hardcoded in the connector config.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Monitoring Sonos with ksqlDB, InfluxDB, and Grafana</title>
      <link>https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/</link>
      <pubDate>Tue, 21 Jan 2020 22:47:35 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/21/monitoring-sonos-with-ksqldb-influxdb-and-grafana/</guid>
      <description>I‚Äôm quite a fan of Sonos audio equipment but recently had some trouble with some of the devices glitching and even cutting out whilst playing. Under the covers Sonos stuff is running Linux (of course) and exposes some diagnostics through a rudimentary frontend that you can access at http://&amp;lt;sonos player IP&amp;gt;:1400/support/review:
   Whilst this gives you the current state, you can‚Äôt get historical data on it. It felt like the problems were happening &amp;#34;all the time&amp;#34;, but were they actually?</description>
    </item>
    
    <item>
      <title>Using Kafka Connect and Debezium with Confluent Cloud</title>
      <link>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</link>
      <pubDate>Wed, 16 Oct 2019 16:29:34 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is based on using &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; to provide your managed Kafka and Schema Registry. All that you run yourself is the Kafka Connect worker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Optionally, you can use this &lt;a href=&#34;https://github.com/rmoff/debezium-ccloud/blob/master/docker-compose.yml&#34;&gt;Docker Compose&lt;/a&gt; to run the worker and a sample MySQL database.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Skipping bad records with the Kafka Connect JDBC sink connector</title>
      <link>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</link>
      <pubDate>Tue, 15 Oct 2019 09:58:38 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</guid>
      <description>&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The Kafka Connect framework provides generic &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;error handling and dead-letter queue capabilities&lt;/a&gt; which are available for problems with [de]serialisation and Single Message Transforms. When it comes to errors that a connector may encounter doing the actual &lt;code&gt;pull&lt;/code&gt; or &lt;code&gt;put&lt;/code&gt; of data from the source/target system, it‚Äôs down to the connector itself to implement logic around that. For example, the Elasticsearch sink connector provides configuration (&lt;code&gt;behavior.on.malformed.documents&lt;/code&gt;) that can be set so that a single bad record won‚Äôt halt the pipeline. Others, such as the JDBC Sink connector, don‚Äôt provide this &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-jdbc/issues/721&#34;&gt;yet&lt;/a&gt;. That means that if you hit this problem, you need to manually unblock it yourself. One way is to manually move the offset of the consumer on past the bad message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; : You can use &lt;code&gt;kafka-consumer-groups --reset-offsets --to-offset &amp;lt;x&amp;gt;&lt;/code&gt; to manually move the connector past a bad message&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Copying data between Kafka clusters with Kafkacat</title>
      <link>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</link>
      <pubDate>Sun, 29 Sep 2019 10:43:45 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafkacat_gives_you_kafka_super_powers&#34;&gt;kafkacat gives you Kafka super powers üòé&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve &lt;a href=&#34;https://rmoff.net/categories/kafkacat/&#34;&gt;written before&lt;/a&gt; about &lt;a href=&#34;https://github.com/edenhill/kafkacat&#34;&gt;kafkacat&lt;/a&gt; and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in &lt;a href=&#34;https://talks.rmoff.net/8Oruwt/on-track-with-apache-kafka-building-a-streaming-etl-solution-with-rail-data#s9tMEWG&#34;&gt;a recent demo&lt;/a&gt; that I built in which data needed manipulating in a way that I couldn‚Äôt easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it‚Äôs getting data from &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; down to a local cluster.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Reset Kafka Connect Source Connector Offsets</title>
      <link>https://rmoff.net/2019/08/15/reset-kafka-connect-source-connector-offsets/</link>
      <pubDate>Thu, 15 Aug 2019 10:42:34 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/08/15/reset-kafka-connect-source-connector-offsets/</guid>
      <description>Kafka Connect in distributed mode uses Kafka itself to persist the offsets of any source connectors. This is a great way to do things as it means that you can easily add more workers, rebuild existing ones, etc without having to worry about where the state is persisted. I personally always recommend using distributed mode, even if just for a single worker instance - it just makes things easier, and more standard.</description>
    </item>
    
    <item>
      <title>Manually delete a connector from Kafka Connect</title>
      <link>https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/</link>
      <pubDate>Sun, 23 Jun 2019 11:39:46 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect has as &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST API&lt;/a&gt; through which all config should be done, including removing connectors that have been created. Sometimes though, you might have reason to want to manually do this‚Äîand since Kafka Connect running in distributed mode uses Kafka as its persistent data store, you can achieve this by manually writing to the topic yourself.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Deleting a Connector in Kafka Connect without the REST API</title>
      <link>https://rmoff.net/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</link>
      <pubDate>Wed, 22 May 2019 10:32:10 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/05/22/deleting-a-connector-in-kafka-connect-without-the-rest-api/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka Connect exposes a &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;REST interface&lt;/a&gt; through which all config and monitoring operations can be done. You can create connectors, delete them, restart them, check their status, and so on. But, I found a situation recently in which I needed to delete a connector and couldn‚Äôt do so with the REST API. Here‚Äôs another way to do it, by amending the configuration Kafka topic that Kafka Connect in distributed mode uses to persist configuration information for connectors. Note that this is not a recommended way of working with Kafka Connect‚Äîthe REST API is there for a good reason :)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Sending multiline messages to Kafka</title>
      <link>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</link>
      <pubDate>Tue, 04 Sep 2018 08:26:51 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</guid>
      <description>(SO answer repost)
You can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):
kafkacat -b kafka:29092 \ -t test_topic_01 \ -D/ \ -P &amp;lt;&amp;lt;EOF this is a string message with a line break/this is another message with two line breaks! EOF  Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140</description>
    </item>
    
    <item>
      <title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
      <link>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
      <pubDate>Thu, 10 May 2018 12:56:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
      <description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Use &lt;code&gt;curl&lt;/code&gt; to pull data from the Mockaroo REST endpoint, and pipe it into &lt;code&gt;kafkacat&lt;/code&gt;, thus:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \
kafkacat -b localhost:9092 -t purchases -P
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
