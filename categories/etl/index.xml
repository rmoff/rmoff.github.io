<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Etl on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/etl/</link>
    <description>Recent content in Etl on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 10:01:56 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/etl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building a data pipeline with DuckDB</title>
      <link>https://rmoff.net/2025/03/20/building-a-data-pipeline-with-duckdb/</link>
      <pubDate>Thu, 20 Mar 2025 10:01:56 +0000</pubDate>
      <guid>https://rmoff.net/2025/03/20/building-a-data-pipeline-with-duckdb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;In this blog post I’m going to explore how as a data engineer in the field today I might go about putting together a rudimentary data pipeline.&#xA;I’ll take some operational data, and wrangle it into a form that makes it easily pliable for analytics work.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;After a somewhat fevered and nightmarish period during which people walked around declaring &amp;#34;Schema on Read&amp;#34; was the future, that &amp;#34;Data is the new oil&amp;#34;, and &amp;#34;Look at the size of my big data&amp;#34;, the path that is history in IT is somewhat coming back on itself to a more sensible approach to things.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;As they say:&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;quoteblock&#34;&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;What’s old is new&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;This is good news for me, because I am old and what I knew then is &amp;#39;new&amp;#39; now ;)&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Why Do We Need Streaming ETL?</title>
      <link>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This is an expanded version of the intro to an article I posted over on the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog&lt;/a&gt;. Here I get to be as verbose as I like &lt;code&gt;;)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe. From there it was checked and loaded, and then reports generated on it. This was nearly twenty years ago as my greying beard will attest—and not a lot has changed in the large majority of reporting and analytics systems since then. COBOL is maybe less common, but what has remained constant is the batch-driven nature of processing. Sometimes batches are run more frequently, and get given fancy names like intra-day ETL or even micro-batching. But batch processing it is, and as such latency is built into our reporting &lt;em&gt;by design&lt;/em&gt;. When we opt for batch processing we voluntarily inject delays into the availability of data to our end users. Much better is to build our systems around a streaming platform instead.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oracle 11g - How to force a sql_id to use a plan_hash_value using SQL Baselines</title>
      <link>https://rmoff.net/2011/06/28/oracle-11g-how-to-force-a-sql_id-to-use-a-plan_hash_value-using-sql-baselines/</link>
      <pubDate>Tue, 28 Jun 2011 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2011/06/28/oracle-11g-how-to-force-a-sql_id-to-use-a-plan_hash_value-using-sql-baselines/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s a scenario that&amp;rsquo;ll be depressingly familiar to most reading this: after ages of running fine, and no changes to the code, a query suddenly starts running for magnitudes longer than it used to.&lt;/p&gt;&#xA;&lt;p&gt;In this instance it was an ETL step which used to take c.1 hour, and was now at 5 hours and counting. Since it still hadn&amp;rsquo;t finished, and the gods had conspired to bring down Grid too (unrelated), I generated a SQL Monitor report to see what was happening: [sourcecode language=&amp;ldquo;sql&amp;rdquo;] select DBMS_SQLTUNE.REPORT_SQL_MONITOR( type=&amp;gt;&amp;lsquo;HTML&amp;rsquo;, report_level=&amp;gt;&amp;lsquo;ALL&amp;rsquo;,sql_id=&amp;gt;&amp;lsquo;939abmqmvcc4d&amp;rsquo;) as report FROM dual; [/sourcecode] (h/t to &lt;a href=&#34;http://twitter.com/martinberx/status/85295030713073664&#34;&gt;Martin Berger&lt;/a&gt; for this)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Warehousing and Statistics in Oracle 11g - incremental global statistics</title>
      <link>https://rmoff.net/2010/12/30/data-warehousing-and-statistics-in-oracle-11g-incremental-global-statistics/</link>
      <pubDate>Thu, 30 Dec 2010 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2010/12/30/data-warehousing-and-statistics-in-oracle-11g-incremental-global-statistics/</guid>
      <description>&lt;p&gt;This is a series of posts where I hope to humbly plug some gaps in the information available (or which has escaped my &lt;a href=&#34;http://www.urbandictionary.com/define.php?term=google-fu&#34;&gt;google-fu&lt;/a&gt;) regarding statistics management in Oracle 11g specific to Data Warehousing.&lt;/p&gt;&#xA;&lt;p&gt;Incremental Global Statistics is new functionality in Oracle 11g (and 10.2.0.4?) and is explained in depth in several places including:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://download.oracle.com/docs/cd/B28359_01/server.111/b28274/stats.htm#i42218&#34;&gt;Oracle® Database Performance Tuning Guide - Statistics on Partitioned Objects&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://structureddata.org/2008/07/16/oracle-11g-incremental-global-statistics-on-partitioned-tables/&#34;&gt;Greg Rahn - Oracle 11g: Incremental Global Statistics On Partitioned Tables&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://blogs.oracle.com/optimizer/2009/02/maintaining_statistics_on_large_partitioned_tables.html&#34;&gt;Inside the Oracle Optimiser - Maintaining statistics on large partitioned tables&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.oraclegeek.net/downloads/One_Pass_Distinct_Sampling.ppt&#34;&gt;Amit Poddar - One Pass Distinct Sampling&lt;/a&gt; (ppt - slides 52 onwards are most relevant)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In essence, Oracle maintains information about each partition when statistics is gathered on the partition, and it uses this to work out the global statistics - without having to scan the whole table. For a more detailed description, see the above links. It&amp;rsquo;s important to note that this is not the same as aggregated global statistics (which Doug Burns &lt;a href=&#34;http://oracledoug.com/serendipity/index.php?/archives/1590-Statistics-on-Partitioned-Tables-Contents.html&#34;&gt;covers in detail here&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Analysing ODI batch performance</title>
      <link>https://rmoff.net/2010/11/03/analysing-odi-batch-performance/</link>
      <pubDate>Wed, 03 Nov 2010 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2010/11/03/analysing-odi-batch-performance/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been involved with some performance work around an ODI DWH load batch. The batch comprises well over 1000 tasks in ODI, and whilst the Operator console is not a bad interface, it&amp;rsquo;s not very easy to spot the areas consuming the most runtime.&lt;/p&gt;&#xA;&lt;p&gt;Here&amp;rsquo;s a set of SQL statements to run against the ODI work repository tables to help you methodically find the steps of most interest for tuning efforts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mark Rittman&#39;s OBIEE repository for DAC</title>
      <link>https://rmoff.net/2009/07/23/mark-rittmans-obiee-repository-for-dac/</link>
      <pubDate>Thu, 23 Jul 2009 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2009/07/23/mark-rittmans-obiee-repository-for-dac/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.rittmanmead.com/2009/01/30/analyzing-bi-apps-etl-runs-using-obiee-and-the-dac-repository/&#34;&gt;Mark Rittman has an excellent article&lt;/a&gt; about querying the DAC repository database tables, including a &lt;a href=&#34;http://www.rittmanmead.com/files/DAC%20Analysis.rpd&#34;&gt;downloadable RPD file&lt;/a&gt;. Being new to working with RPDs I thought it would be good practise to explore this as well as hopefully get some useful information about our current ETL deployment.&lt;/p&gt;&#xA;&lt;p&gt;I downloaded the RPD to c:\OracleBI\server\Repository and opened it up in the Admin tool (Administrator/Administrator).&lt;br&gt;&#xA;First off I changed the connection pool to point to my DAC repository database, having setup a TNS entry for it first.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
