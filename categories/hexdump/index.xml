<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hexdump on rmoff&#39;s random ramblings2</title>
    <link>https://rmoff.net/categories/hexdump/</link>
    <description>Recent content in hexdump on rmoff&#39;s random ramblings2</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Feb 2021 22:45:36 +0000</lastBuildDate><atom:link href="https://rmoff.net/categories/hexdump/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Loading delimited data into Kafka - quick &amp; dirty (but effective)</title>
      <link>https://rmoff.net/2021/02/26/loading-delimited-data-into-kafka-quick-dirty-but-effective/</link>
      <pubDate>Fri, 26 Feb 2021 22:45:36 +0000</pubDate>
      
      <guid>https://rmoff.net/2021/02/26/loading-delimited-data-into-kafka-quick-dirty-but-effective/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Whilst Apache Kafka is an event streaming platform designed for, well, &lt;em&gt;streams&lt;/em&gt; of events, it’s perfectly valid to use it as a store of data which perhaps changes only occasionally (or even never). I’m thinking here of reference data (lookup data) that’s used to enrich regular streams of events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;You might well get your reference data from a database where it resides and do so effectively &lt;a href=&#34;https://rmoff.dev/no-more-silos&#34;&gt;using CDC&lt;/a&gt; - but sometimes it comes down to those pesky CSV files that we all know and love/hate. Simple, awful, but effective. I wrote previously about &lt;a href=&#34;https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/&#34;&gt;loading CSV data into Kafka from files that are updated frequently&lt;/a&gt;, but here I want to look at CSV files that are not changing. Kafka Connect simplifies getting data in to (and out of) Kafka but even Kafka Connect becomes a bit of an overhead when you just have a single file that you want to load into a topic and then never deal with again. I spent this afternoon wrangling with a couple of CSV-ish files, and building on my previous article about &lt;a href=&#34;https://rmoff.net/2021/02/02/performing-a-group-by-on-data-in-bash/&#34;&gt;neat tricks you can do in bash with data&lt;/a&gt;, I have some more to share with you here :)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Why JSON isn&#39;t the same as JSON Schema in Kafka Connect converters and ksqlDB (Viewing Kafka messages bytes as hex)</title>
      <link>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-and-ksqldb-viewing-kafka-messages-bytes-as-hex/</link>
      <pubDate>Fri, 03 Jul 2020 08:16:36 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-and-ksqldb-viewing-kafka-messages-bytes-as-hex/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I’ve been playing around with the new SerDes (serialisers/deserialisers) that shipped with Confluent Platform 5.5 - &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/serdes-develop/index.html&#34;&gt;Protobuf, and JSON Schema&lt;/a&gt; (these were added to the existing support for Avro). The serialisers (and associated &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/connect.html&#34;&gt;Kafka Connect converters&lt;/a&gt;) take a payload and serialise it into bytes for sending to Kafka, and I was interested in what those bytes look like. For that I used my favourite Kafka swiss-army knife: kafkacat.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Replacing UTF8 non-breaking-space with bash/sed on the Mac</title>
      <link>https://rmoff.net/2019/01/21/replacing-utf8-non-breaking-space-with-bash/sed-on-the-mac/</link>
      <pubDate>Mon, 21 Jan 2019 14:01:24 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/01/21/replacing-utf8-non-breaking-space-with-bash/sed-on-the-mac/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A script I’d batch-run on my Markdown files had inserted a UTF-8 non-breaking-space between Markdown heading indicator and the text, which meant that &lt;code&gt;&lt;mark&gt;#&lt;/mark&gt; My title&lt;/code&gt; actually got rendered as that, instead of an H3 title.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Looking at the file contents, I could see it wasn’t just a space between the &lt;code&gt;#&lt;/code&gt; and the text, but a non-breaking space.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
