<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>jdbc sink on rmoff&#39;s random ramblings2</title>
    <link>https://rmoff.net/categories/jdbc-sink/</link>
    <description>Recent content in jdbc sink on rmoff&#39;s random ramblings2</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Mar 2021 12:16:16 +0000</lastBuildDate><atom:link href="https://rmoff.net/categories/jdbc-sink/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Connect JDBC Sink deep-dive: Working with Primary Keys</title>
      <link>https://rmoff.net/2021/03/12/kafka-connect-jdbc-sink-deep-dive-working-with-primary-keys/</link>
      <pubDate>Fri, 12 Mar 2021 12:16:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2021/03/12/kafka-connect-jdbc-sink-deep-dive-working-with-primary-keys/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The Kafka Connect JDBC Sink can be used to stream data from a Kafka topic to a database such as Oracle, Postgres, MySQL, DB2, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;It supports many permutations of configuration around how &lt;strong&gt;primary keys&lt;/strong&gt; are handled. The &lt;a href=&#34;https://docs.confluent.io/kafka-connect-jdbc/current/sink-connector/sink_config_options.html#data-mapping?utm_source=rmoff&amp;amp;utm_medium=blog&amp;amp;utm_campaign=tm.devx_ch.rmoff_jdbc-sink-primary-keys&amp;amp;utm_term=rmoff-devx&#34;&gt;documentation&lt;/a&gt; details these. This article aims to illustrate and expand on this.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Connect - SQLSyntaxErrorException: BLOB/TEXT column … used in key specification without a key length</title>
      <link>https://rmoff.net/2021/03/11/kafka-connect-sqlsyntaxerrorexception-blob/text-column-used-in-key-specification-without-a-key-length/</link>
      <pubDate>Thu, 11 Mar 2021 11:25:57 +0000</pubDate>
      
      <guid>https://rmoff.net/2021/03/11/kafka-connect-sqlsyntaxerrorexception-blob/text-column-used-in-key-specification-without-a-key-length/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I got the error &lt;code&gt;SQLSyntaxErrorException: BLOB/TEXT column &amp;#39;MESSAGE_KEY&amp;#39; used in key specification without a key length&lt;/code&gt; with &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html&#34;&gt;Kafka Connect JDBC Sink connector&lt;/a&gt; (v10.0.2) and MySQL (8.0.23)&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kafka Connect JDBC Sink - setting the key field name</title>
      <link>https://rmoff.net/2020/02/25/kafka-connect-jdbc-sink-setting-the-key-field-name/</link>
      <pubDate>Tue, 25 Feb 2020 14:37:12 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/02/25/kafka-connect-jdbc-sink-setting-the-key-field-name/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I wanted to get some data from a Kafka topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ksql&lt;span style=&#34;color:#666&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PRINT&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PERSON_STATS&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;BEGINNING;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;Key&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;format:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;KAFKA&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;(STRING)&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;Value&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;format:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;AVRO&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;rowtime:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;25&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;20&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;12&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;51&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PM&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;UTC,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;key&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;robin,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;value:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;PERSON&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;robin&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;LOCATION_CHANGES&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;&amp;#34;UNIQUE_LOCATIONS&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;&#34;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;into Postgres, so did the easy thing and used Kafka Connect with the &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html&#34;&gt;JDBC Sink connector&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Skipping bad records with the Kafka Connect JDBC sink connector</title>
      <link>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</link>
      <pubDate>Tue, 15 Oct 2019 09:58:38 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/</guid>
      <description>&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The Kafka Connect framework provides generic &lt;a href=&#34;https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues&#34;&gt;error handling and dead-letter queue capabilities&lt;/a&gt; which are available for problems with [de]serialisation and Single Message Transforms. When it comes to errors that a connector may encounter doing the actual &lt;code&gt;pull&lt;/code&gt; or &lt;code&gt;put&lt;/code&gt; of data from the source/target system, it’s down to the connector itself to implement logic around that. For example, the Elasticsearch sink connector provides configuration (&lt;code&gt;behavior.on.malformed.documents&lt;/code&gt;) that can be set so that a single bad record won’t halt the pipeline. Others, such as the JDBC Sink connector, don’t provide this &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-jdbc/issues/721&#34;&gt;yet&lt;/a&gt;. That means that if you hit this problem, you need to manually unblock it yourself. One way is to manually move the offset of the consumer on past the bad message.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; : You can use &lt;code&gt;kafka-consumer-groups --reset-offsets --to-offset &amp;lt;x&amp;gt;&lt;/code&gt; to manually move the connector past a bad message&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Flatten CDC records in KSQL</title>
      <link>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</link>
      <pubDate>Thu, 11 Oct 2018 15:13:59 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</guid>
      <description>&lt;h3 id=&#34;the-problem---nested-messages-in-kafka&#34;&gt;The problem - nested messages in Kafka&lt;/h3&gt;
&lt;p&gt;Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
