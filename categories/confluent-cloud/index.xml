<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Confluent Cloud on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/confluent-cloud/</link>
    <description>Recent content in Confluent Cloud on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Apr 2020 13:55:46 +0100</lastBuildDate><atom:link href="https://rmoff.net/categories/confluent-cloud/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Confluent Cloud when there is no Cloud (or internet)</title>
      <link>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</link>
      <pubDate>Mon, 20 Apr 2020 13:55:46 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/20/using-confluent-cloud-when-there-is-no-cloud-or-internet/</guid>
      <description>‚òÅÔ∏èConfluent Cloud is a great solution for a hosted and managed Apache Kafka service, with the additional benefits of Confluent Platform such as ksqlDB and managed Kafka Connect connectors. But as a developer, you won‚Äôt always have a reliable internet connection. Train, planes, and automobiles‚Äînot to mention crappy hotel or conference Wi-Fi. Wouldn‚Äôt it be useful if you could have a replica of your Cloud data on your local machine? That just pulled down new data automagically, without needing to be restarted each time you got back on the network?</description>
    </item>
    
    <item>
      <title>Streaming Wi-Fi trace data from Raspberry Pi to Apache Kafka with Confluent Cloud</title>
      <link>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</link>
      <pubDate>Wed, 11 Mar 2020 11:58:13 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/</guid>
      <description>Wi-fi is now ubiquitous in most populated areas, and the way the devices communicate leaves a lot of &amp;#39;digital exhaust&amp;#39;. Usually a computer will have a Wi-Fi device that‚Äôs configured to connect to a given network, but often these devices can be configured instead to pick up the background Wi-Fi chatter of surrounding devices.
 There are good reasons‚Äîand bad‚Äîfor doing this. Just like taking apart equipment to understand how it works teaches us things, so being able to dissect and examine protocol traffic lets us learn about this.</description>
    </item>
    
    <item>
      <title>Streaming data from SQL Server to Kafka to Snowflake ‚ùÑÔ∏è with Kafka Connect</title>
      <link>https://rmoff.net/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</link>
      <pubDate>Wed, 20 Nov 2019 17:59:50 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/20/streaming-data-from-sql-server-to-kafka-to-snowflake-with-kafka-connect/</guid>
      <description>Snowflake is the data warehouse built for the cloud, so let‚Äôs get all ‚òÅÔ∏è cloudy and stream some data from Kafka running in Confluent Cloud to Snowflake!
 What I‚Äôm showing also works just as well for an on-premises Kafka cluster. I‚Äôm using SQL Server as an example data source, with Debezium to capture and stream and changes from it into Kafka.
   I‚Äôm assuming that you‚Äôve signed up for Confluent Cloud and Snowflake and are the proud owner of credentials for both.</description>
    </item>
    
    <item>
      <title>Running Dockerised Kafka Connect worker on GCP</title>
      <link>https://rmoff.net/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</link>
      <pubDate>Tue, 12 Nov 2019 14:45:43 +0000</pubDate>
      
      <guid>https://rmoff.net/2019/11/12/running-dockerised-kafka-connect-worker-on-gcp/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I &lt;a href=&#34;http://talks.rmoff.net/&#34;&gt;talk and write about Kafka and Confluent Platform&lt;/a&gt; a lot, and more and more of the demos that I‚Äôm building are around &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt;. This means that I don‚Äôt have to run or manage my own Kafka brokers, Zookeeper, Schema Registry, KSQL servers, etc which makes things a ton easier. Whilst there are managed connectors on Confluent Cloud (S3 etc), I need to run my own Kafka Connect worker for those connectors not yet provided. An example is the MQTT source connector that I use in &lt;a href=&#34;https://rmoff.dev/kssf19-ksql-video&#34;&gt;this demo&lt;/a&gt;. Up until now I‚Äôd either run this worker locally, or manually build a cloud VM. Locally is fine, as it‚Äôs all Docker, easily spun up in a single &lt;code&gt;docker-compose up -d&lt;/code&gt; command. I wanted something that would keep running whilst my laptop was off, but that was as close to my local build as possible‚Äîenter GCP and its functionality to run a container on a VM automagically.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;You can see &lt;a href=&#34;https://github.com/confluentinc/demo-scene/blob/master/mqtt-tracker/launch-worker-container_gcloud.sh&#34;&gt;the full script here&lt;/a&gt;&lt;/strong&gt;. The rest of this article just walks through the how and why.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using Kafka Connect and Debezium with Confluent Cloud</title>
      <link>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</link>
      <pubDate>Wed, 16 Oct 2019 16:29:34 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is based on using &lt;a href=&#34;https://confluent.cloud&#34;&gt;Confluent Cloud&lt;/a&gt; to provide your managed Kafka and Schema Registry. All that you run yourself is the Kafka Connect worker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Optionally, you can use this &lt;a href=&#34;https://github.com/rmoff/debezium-ccloud/blob/master/docker-compose.yml&#34;&gt;Docker Compose&lt;/a&gt; to run the worker and a sample MySQL database.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Copying data between Kafka clusters with Kafkacat</title>
      <link>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</link>
      <pubDate>Sun, 29 Sep 2019 10:43:45 +0200</pubDate>
      
      <guid>https://rmoff.net/2019/09/29/copying-data-between-kafka-clusters-with-kafkacat/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafkacat_gives_you_kafka_super_powers&#34;&gt;kafkacat gives you Kafka super powers üòé&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I‚Äôve &lt;a href=&#34;https://rmoff.net/categories/kafkacat/&#34;&gt;written before&lt;/a&gt; about &lt;a href=&#34;https://github.com/edenhill/kafkacat&#34;&gt;kafkacat&lt;/a&gt; and what a great tool it is for doing lots of useful things as a developer with Kafka. I used it too in &lt;a href=&#34;https://talks.rmoff.net/8Oruwt/on-track-with-apache-kafka-building-a-streaming-etl-solution-with-rail-data#s9tMEWG&#34;&gt;a recent demo&lt;/a&gt; that I built in which data needed manipulating in a way that I couldn‚Äôt easily elsewhere. Today I want share a very simple but powerful use for kafkacat as both a consumer and producer: copying data from one Kafka cluster to another. In this instance it‚Äôs getting data from &lt;a href=&#34;https://confluent.cloud/&#34;&gt;Confluent Cloud&lt;/a&gt; down to a local cluster.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Connecting KSQL to a Secured Schema Registry</title>
      <link>https://rmoff.net/2019/04/12/connecting-ksql-to-a-secured-schema-registry/</link>
      <pubDate>Fri, 12 Apr 2019 12:59:33 +0100</pubDate>
      
      <guid>https://rmoff.net/2019/04/12/connecting-ksql-to-a-secured-schema-registry/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;See also : &lt;a href=&#34;https://docs.confluent.io/current/ksql/docs/installation/server-config/security.html#configuring-ksql-for-secured-sr-long&#34; class=&#34;bare&#34;&gt;https://docs.confluent.io/current/ksql/docs/installation/server-config/security.html#configuring-ksql-for-secured-sr-long&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Confluent Cloud now includes a secured Schema Registry, which you can use from external applications, including KSQL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;To configure KSQL for it you need to set:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ksql.schema.registry.url&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;https://&amp;lt;Schema Registry endpoint&amp;gt;
ksql.schema.registry.basic.auth.credentials.source&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;USER_INFO
ksql.schema.registry.basic.auth.user.info&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&amp;lt;Schema Registry API Key&amp;gt;:&amp;lt;Schema Registry API Secret&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
