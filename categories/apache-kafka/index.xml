<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Kafka on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/apache-kafka/</link>
    <description>Recent content in Apache Kafka on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 14:20:08 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/apache-kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How we built the demo for the Current NOLA Day 2 keynote using Flink and AI</title>
      <link>https://rmoff.net/2025/11/06/how-we-built-the-demo-for-the-current-nola-day-2-keynote-using-flink-and-ai/</link>
      <pubDate>Thu, 06 Nov 2025 14:20:08 +0000</pubDate>
      <guid>https://rmoff.net/2025/11/06/how-we-built-the-demo-for-the-current-nola-day-2-keynote-using-flink-and-ai/</guid>
      <description>&lt;p&gt;At Current 2025 in New Orleans this year we built a demo for the &lt;a href=&#34;https://www.youtube.com/watch?v=q05yqzDcSCI&#34;&gt;Day 2 keynote&lt;/a&gt; that would automagically summarise what was happening in the room, as reported by members of the audience.&#xA;Here&amp;rsquo;s how we did it!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka to Iceberg - Exploring the Options</title>
      <link>https://rmoff.net/2025/08/18/kafka-to-iceberg-exploring-the-options/</link>
      <pubDate>Mon, 18 Aug 2025 13:43:31 +0000</pubDate>
      <guid>https://rmoff.net/2025/08/18/kafka-to-iceberg-exploring-the-options/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;You‚Äôve got data in &lt;a href=&#34;https://www.youtube.com/watch?v=9CrlA0Wasvk&#34;&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;You want to get that data into &lt;a href=&#34;https://www.youtube.com/watch?v=TsmhRZElPvM&#34;&gt;Apache Iceberg&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;What‚Äôs the best way to do it?&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;imageblock&#34;&gt;&#xA;&lt;div class=&#34;content&#34;&gt;&#xA;&lt;img src=&#34;https://rmoff.net/images/2025/08/kafka-to-iceberg.png&#34; alt=&#34;kafka to iceberg&#34;/&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Perhaps invariably, the answer is: &lt;strong&gt;IT DEPENDS&lt;/strong&gt;.&#xA;But fear not: here is a guide to help you navigate your way to choosing the best solution &lt;em&gt;for you&lt;/em&gt; ü´µ.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Writing to Apache Iceberg on S3 using Kafka Connect with Glue catalog</title>
      <link>https://rmoff.net/2025/07/04/writing-to-apache-iceberg-on-s3-using-kafka-connect-with-glue-catalog/</link>
      <pubDate>Fri, 04 Jul 2025 15:36:21 +0000</pubDate>
      <guid>https://rmoff.net/2025/07/04/writing-to-apache-iceberg-on-s3-using-kafka-connect-with-glue-catalog/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Without wanting to mix my temperature metaphors, Iceberg is the new hawtness, and getting data into it from other places is a common task.&#xA;I &lt;a href=&#34;https://rmoff.net/2025/06/24/writing-to-apache-iceberg-on-s3-using-flink-sql-with-glue-catalog/&#34;&gt;wrote previously about using Flink SQL to do this&lt;/a&gt;, and today I‚Äôm going to look at doing the same using Kafka Connect.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Kafka Connect can send data to Iceberg from any Kafka topic.&#xA;The source Kafka topic(s) can be populated by a Kafka Connect source connector (such as Debezium), or a regular application producing directly to it.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>It&#39;s Time We Talked About Time: Exploring Watermarks (And More) In Flink SQL</title>
      <link>https://rmoff.net/2025/04/25/its-time-we-talked-about-time-exploring-watermarks-and-more-in-flink-sql/</link>
      <pubDate>Fri, 25 Apr 2025 15:26:56 +0000</pubDate>
      <guid>https://rmoff.net/2025/04/25/its-time-we-talked-about-time-exploring-watermarks-and-more-in-flink-sql/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Whether you‚Äôre processing data in batch or as a stream, the concept of time is an important part of accurate processing logic.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Because we process data after it happens, there are a minimum of two different types of time to consider:&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;olist arabic&#34;&gt;&#xA;&lt;ol class=&#34;arabic&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;When it happened&lt;/strong&gt;, known as &lt;strong&gt;Event Time&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;When we process it&lt;/strong&gt;, known as &lt;strong&gt;Processing Time&lt;/strong&gt; (or &lt;em&gt;system time&lt;/em&gt; or &lt;em&gt;wall clock time&lt;/em&gt;)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Why is kcat showing the wrong topics?</title>
      <link>https://rmoff.net/2025/03/13/why-is-kcat-showing-the-wrong-topics/</link>
      <pubDate>Thu, 13 Mar 2025 10:42:11 +0000</pubDate>
      <guid>https://rmoff.net/2025/03/13/why-is-kcat-showing-the-wrong-topics/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Much as I love kcat (ü§´ &lt;em&gt;it‚Äôll always be kafkacat to me&lt;/em&gt;‚Ä¶), this morning I nearly fell out with it üëá&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;üòñ I thought I was going stir crazy, after listing topics on a broker &lt;strong&gt;and seeing topics from a different broker&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;üòµ WTF üòµ&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Sending Data to Apache Iceberg from Apache Kafka with Apache Flink</title>
      <link>https://rmoff.net/2024/07/18/sending-data-to-apache-iceberg-from-apache-kafka-with-apache-flink/</link>
      <pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2024/07/18/sending-data-to-apache-iceberg-from-apache-kafka-with-apache-flink/</guid>
      <description>&lt;div class=&#34;admonitionblock note&#34;&gt;&#xA;&lt;table&gt;&#xA;&lt;tbody&gt;&lt;tr&gt;&#xA;&lt;td class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fa icon-note&#34; title=&#34;Note&#34;&gt;&lt;/i&gt;&#xA;&lt;/td&gt;&#xA;&lt;td class=&#34;content&#34;&gt;&#xA;This post originally appeared on the &lt;a href=&#34;https://www.decodable.co/blog/kafka-to-iceberg-with-flink&#34;&gt;Decodable blog&lt;/a&gt;.&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;&lt;em&gt;Sometimes it‚Äôs not possible to have too much of a good thing, and whilst this blog may look at first-glance rather similar to the one that&lt;/em&gt; &lt;a href=&#34;https://rmoff.net/2024/06/18/how-to-get-data-from-apache-kafka-to-apache-iceberg-on-s3-with-decodable/&#34;&gt;I published just recently&lt;/a&gt; &lt;em&gt;, today we‚Äôre looking at a 100% pure Apache solution.&lt;/em&gt;&#xA;&lt;em&gt;Because who knows, maybe you prefer rolling your own tech stacks instead of letting Decodable do it for you üòâ.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>How to get data from Apache Kafka to Apache Iceberg on S3 with Decodable</title>
      <link>https://rmoff.net/2024/06/18/how-to-get-data-from-apache-kafka-to-apache-iceberg-on-s3-with-decodable/</link>
      <pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2024/06/18/how-to-get-data-from-apache-kafka-to-apache-iceberg-on-s3-with-decodable/</guid>
      <description>&lt;div class=&#34;admonitionblock note&#34;&gt;&#xA;&lt;table&gt;&#xA;&lt;tbody&gt;&lt;tr&gt;&#xA;&lt;td class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fa icon-note&#34; title=&#34;Note&#34;&gt;&lt;/i&gt;&#xA;&lt;/td&gt;&#xA;&lt;td class=&#34;content&#34;&gt;&#xA;This post originally appeared on the &lt;a href=&#34;https://www.decodable.co/blog/kafka-to-iceberg-with-decodable&#34;&gt;Decodable blog&lt;/a&gt;.&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://iceberg.apache.org/&#34;&gt;Apache Iceberg&lt;/a&gt;  is an open table format.&#xA;It combines the benefits of data lakes (open standards, cheap object storage) with the good things that data warehouses have, like first-class support for tables and SQL capabilities including updates to data in place, time-travel, and transactions.&#xA;With the recent  &lt;a href=&#34;https://www.databricks.com/company/newsroom/press-releases/databricks-agrees-acquire-tabular-company-founded-original-creators&#34;&gt;acquisition&lt;/a&gt;  by Databricks of Tabular‚Äîone of the main companies that contribute to Iceberg‚Äîit‚Äôs clear that Iceberg is winning out as one of the primary contenders in this space.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Using Apache Kafka with ngrok</title>
      <link>https://rmoff.net/2023/11/01/using-apache-kafka-with-ngrok/</link>
      <pubDate>Wed, 01 Nov 2023 10:07:58 +0000</pubDate>
      <guid>https://rmoff.net/2023/11/01/using-apache-kafka-with-ngrok/</guid>
      <description>&lt;p&gt;Sometimes you might want to access Apache Kafka that&amp;rsquo;s running on your local machine from another device not on the same network. I&amp;rsquo;m not sure I can think of a production use-case, but there are a dozen examples for sandbox, demo, and playground environments.&lt;/p&gt;&#xA;&lt;p&gt;In this post we&amp;rsquo;ll see how you can use &lt;a href=&#34;https://ngrok.com/&#34;&gt;ngrok&lt;/a&gt; to, in their words, &lt;code&gt;Put localhost on the internet&lt;/code&gt;. And specifically, your local Kafka broker on the internet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>üìå    üéÅ A collection of Kafka-related talks üíù</title>
      <link>https://rmoff.net/2020/09/23/a-collection-of-kafka-related-talks/</link>
      <pubDate>Wed, 23 Sep 2020 15:00:05 +0100</pubDate>
      <guid>https://rmoff.net/2020/09/23/a-collection-of-kafka-related-talks/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Here‚Äôs a collection of Kafka-related talks, &lt;em&gt;just for you.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Each one has üçøüé• a recording, üìî slides, and üëæ code to go and try out.¬†&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Flatten CDC records in KSQL</title>
      <link>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</link>
      <pubDate>Thu, 11 Oct 2018 15:13:59 +0000</pubDate>
      <guid>https://rmoff.net/2018/10/11/flatten-cdc-records-in-ksql/</guid>
      <description>&lt;h3 id=&#34;the-problem---nested-messages-in-kafka&#34;&gt;The problem - nested messages in Kafka&lt;/h3&gt;&#xA;&lt;p&gt;Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring JMX with jmxterm</title>
      <link>https://rmoff.net/2018/09/19/exploring-jmx-with-jmxterm/</link>
      <pubDate>Wed, 19 Sep 2018 08:11:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/09/19/exploring-jmx-with-jmxterm/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Check out the &lt;a href=&#34;https://github.com/jiaqi/jmxterm/&#34;&gt;jmxterm repository&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Download jmxterm from &lt;a href=&#34;https://docs.cyclopsgroup.org/jmxterm&#34;&gt;https://docs.cyclopsgroup.org/jmxterm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Accessing Kafka Docker containers&#39; JMX from host</title>
      <link>https://rmoff.net/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</link>
      <pubDate>Mon, 17 Sep 2018 15:29:48 +0000</pubDate>
      <guid>https://rmoff.net/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</guid>
      <description>&lt;p&gt;&lt;em&gt;See also &lt;a href=&#34;https://docs.confluent.io/current/installation/docker/docs/operations/monitoring.html&#34;&gt;docs&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;To help future Googlers‚Ä¶ with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables: &lt;code&gt;&amp;lt;x&amp;gt;_JMX_HOSTNAME&lt;/code&gt; and &lt;code&gt;&amp;lt;x&amp;gt;_JMX_PORT&lt;/code&gt;, prefixed by a component name.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;&amp;lt;x&amp;gt;_JMX_HOSTNAME&lt;/code&gt; - the hostname/IP of the &lt;em&gt;JMX host&lt;/em&gt; machine, &lt;em&gt;as accessible from the JMX Client&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This is used by the JMX client to connect back into JMX, so must be accessible from the &lt;em&gt;host machine running the JMX client&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sending multiline messages to Kafka</title>
      <link>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</link>
      <pubDate>Tue, 04 Sep 2018 08:26:51 +0000</pubDate>
      <guid>https://rmoff.net/2018/09/04/sending-multiline-messages-to-kafka/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://stackoverflow.com/questions/52151816/push-multiple-line-text-as-one-message-in-a-kafka-topic/52162998#52162998&#34;&gt;SO answer repost&lt;/a&gt;)&lt;/p&gt;&#xA;&lt;p&gt;You can use &lt;a href=&#34;https://docs.confluent.io/current/app-development/kafkacat-usage.html&#34;&gt;&lt;code&gt;kafkacat&lt;/code&gt;&lt;/a&gt; to send messages to Kafka that include line breaks. To do this, use its &lt;code&gt;-D&lt;/code&gt; operator to specify a custom message delimiter (in this example &lt;code&gt;/&lt;/code&gt;):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kafkacat -b kafka:29092 \&#xA;        -t test_topic_01 \&#xA;        -D/ \&#xA;        -P &amp;lt;&amp;lt;EOF&#xA;this is a string message &#xA;with a line break/this is &#xA;another message with two &#xA;line breaks!&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;&lt;em&gt;Note that the delimiter &lt;strong&gt;must&lt;/strong&gt; be a single byte - multi-byte chars will end up getting included in the resulting message &lt;a href=&#34;https://github.com/edenhill/kafkacat/issues/140&#34;&gt;See issue #140&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Listeners - Explained</title>
      <link>https://rmoff.net/2018/08/02/kafka-listeners-explained/</link>
      <pubDate>Thu, 02 Aug 2018 19:38:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/08/02/kafka-listeners-explained/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This was cross-posted on the &lt;a href=&#34;https://www.confluent.io/blog/kafka-listeners-explained&#34;&gt;Confluent.io blog&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;This question comes up on StackOverflow and such places a &lt;strong&gt;lot&lt;/strong&gt;, so here&amp;rsquo;s something to try and help.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; : You need to set &lt;code&gt;advertised.listeners&lt;/code&gt; (or &lt;code&gt;KAFKA_ADVERTISED_LISTENERS&lt;/code&gt; if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address‚Äìand if that&amp;rsquo;s not reachable then problems ensue.&lt;/p&gt;&#xA;&lt;p&gt;Put another way, courtesy of Spencer Ruport:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;code&gt;LISTENERS&lt;/code&gt; are what interfaces Kafka binds to. &lt;code&gt;ADVERTISED_LISTENERS&lt;/code&gt;  are how clients can connect.&lt;/p&gt;&#xA;&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      <guid>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://cnfl.io/syslogs-filtering&#34;&gt;this article&lt;/a&gt; I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/&#34;&gt;enrich streams&lt;/a&gt;. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana‚Äîand how KSQL again came into use for building some stream processing as a result of the discovery made.&lt;/p&gt;&#xA;&lt;p&gt;The data came from my home &lt;a href=&#34;https://www.ubnt.com/&#34;&gt;Ubiquiti&lt;/a&gt; router, and took two forms:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
      <link>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
      <pubDate>Thu, 10 May 2018 12:56:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
      <description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Use &lt;code&gt;curl&lt;/code&gt; to pull data from the Mockaroo REST endpoint, and pipe it into &lt;code&gt;kafkacat&lt;/code&gt;, thus:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \&#xA;kafkacat -b localhost:9092 -t purchases -P&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Tue, 27 Mar 2018 18:52:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>&lt;p&gt;&lt;em&gt;Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;mongodb-config---enabling-replica-sets&#34;&gt;MongoDB config - enabling replica sets&lt;/h3&gt;&#xA;&lt;p&gt;For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:&lt;/p&gt;&#xA;&lt;p&gt;Docs: &lt;a href=&#34;https://docs.mongodb.com/manual/replication/&#34;&gt;Replication&lt;/a&gt; / &lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/convert-standalone-to-replica-set/&#34;&gt;Convert a Standalone to a Replica Set&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Stop Mongo:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo service mongod stop&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add replica set config to &lt;code&gt;/etc/mongod.conf&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Sat, 24 Mar 2018 14:58:14 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://debezium.io/&#34;&gt;Debezium&lt;/a&gt; is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we&amp;rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.&lt;/p&gt;&#xA;&lt;p&gt;The software versions used here are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Confluent Platform 4.0&lt;/li&gt;&#xA;&lt;li&gt;Debezium 0.7.2&lt;/li&gt;&#xA;&lt;li&gt;MySQL 5.7.19 with &lt;a href=&#34;https://dev.mysql.com/doc/sakila/en/sakila-installation.html&#34;&gt;Sakila sample database&lt;/a&gt; installed&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;install-debezium&#34;&gt;Install Debezium&lt;/h3&gt;&#xA;&lt;p&gt;To use it, you need the relevant JAR for the source system (e.g. MySQL), and make that JAR available to Kafka Connect. Here we&amp;rsquo;ll set it up for MySQL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming data from Kafka into Elasticsearch</title>
      <link>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
      <pubDate>Tue, 06 Mar 2018 22:21:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
      <description>&lt;p&gt;&lt;em&gt;This article is part of a series exploring Streaming ETL in practice. You can read about &lt;a href=&#34;https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/&#34;&gt;setting up the ingest of realtime events from a standard Oracle platform&lt;/a&gt;, and &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;building streaming ETL using KSQL&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Installing the Python Kafka library from Confluent - troubleshooting some silly errors‚Ä¶</title>
      <link>https://rmoff.net/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:24 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</guid>
      <description>&lt;p&gt;System:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rmoff@proxmox01:~$ uname -a&#xA;Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux&#xA;&#xA;rmoff@proxmox01:~$ head -n1 /etc/os-release&#xA;PRETTY_NAME=&amp;#34;Debian GNU/Linux 8 (jessie)&amp;#34;&#xA;&#xA;rmoff@proxmox01:~$ python --version&#xA;Python 2.7.9&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Following:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/&#34;&gt;https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/confluentinc/confluent-kafka-python&#34;&gt;https://github.com/confluentinc/confluent-kafka-python&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Install &lt;code&gt;librdkafka&lt;/code&gt;, which is a pre-req for the Python library:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;wget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add -&#xA;sudo add-apt-repository &amp;quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main&amp;quot;&#xA;sudo apt-get install librdkafka-dev python-dev&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Setup virtualenv:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo apt-get install virtualenv&#xA;virtualenv kafka_push_notify&#xA;source ./kafka_push_notify/bin/activate.fish&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Try to install &lt;code&gt;confluent-kafka&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Do We Need Streaming ETL?</title>
      <link>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This is an expanded version of the intro to an article I posted over on the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog&lt;/a&gt;. Here I get to be as verbose as I like &lt;code&gt;;)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe. From there it was checked and loaded, and then reports generated on it. This was nearly twenty years ago as my greying beard will attest‚Äîand not a lot has changed in the large majority of reporting and analytics systems since then. COBOL is maybe less common, but what has remained constant is the batch-driven nature of processing. Sometimes batches are run more frequently, and get given fancy names like intra-day ETL or even micro-batching. But batch processing it is, and as such latency is built into our reporting &lt;em&gt;by design&lt;/em&gt;. When we opt for batch processing we voluntarily inject delays into the availability of data to our end users. Much better is to build our systems around a streaming platform instead.&lt;/p&gt;</description>
    </item>
    <item>
      <title>HOWTO: Oracle GoldenGate &#43; Apache Kafka &#43; Schema Registry &#43; Swingbench</title>
      <link>https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/</link>
      <pubDate>Thu, 01 Feb 2018 23:15:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is the detailed step-by-step if you want to recreate the process I describe in the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I used Oracle&amp;rsquo;s &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt;, which comes preinstalled with Oracle 12cR2. You can see the notes on &lt;a href=&#34;https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/&#34;&gt;how to do this here&lt;/a&gt;. These notes take you through installing and configuring:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Swingbench, to create a sample &amp;ldquo;Order Entry&amp;rdquo; schema and simulate events on the Oracle database&lt;/li&gt;&#xA;&lt;li&gt;Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I&amp;rsquo;m using Oracle GoldenGate 12.3.1 which includes the Kafka Connect handler as part of its distribution. A connector for earlier versions can be &lt;a href=&#34;http://www.oracle.com/technetwork/middleware/goldengate/oracle-goldengate-exchange-3805527.html&#34;&gt;found here&lt;/a&gt;. Some of the syntax may differ in the configuration below - if you hit problems then check out &lt;a href=&#34;&#34;&gt;an article that I wrote&lt;/a&gt; with an earlier version of the tool.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;OGG &lt;code&gt;extract&lt;/code&gt; from the Order Entry schema&lt;/li&gt;&#xA;&lt;li&gt;Confluent Platform&lt;/li&gt;&#xA;&lt;li&gt;KSQL&lt;/li&gt;&#xA;&lt;li&gt;Elasticsearch&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;From this point, I&amp;rsquo;ll now walk through configuring OGG-BD with the Kafka Connect handler&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available</title>
      <link>https://rmoff.net/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</link>
      <pubDate>Wed, 03 Jan 2018 11:26:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</guid>
      <description>&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;See also &lt;a href=&#34;https://rmoff.net/2018/08/02/kafka-listeners-explained/&#34;&gt;Kafka Listeners - Explained&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;A short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;WARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;KSQL was throwing a similar error:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;KSQL cannot initialize AdminCLient.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;I had correctly set the machine&amp;rsquo;s hostname in my Kafka &lt;code&gt;server.properties&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform</title>
      <link>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</link>
      <pubDate>Tue, 21 Nov 2017 17:31:00 +0000</pubDate>
      <guid>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</guid>
      <description>&lt;p&gt;&lt;em&gt;Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I used the &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt; for this - it&amp;rsquo;s preinstalled with Oracle 12cR2. &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;Big Data Lite&lt;/a&gt; is nice but currently has an older version of GoldenGate.&lt;/p&gt;&#xA;&lt;p&gt;Login to the VM (oracle/oracle) and then install some useful things:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm&#xA;sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop&#xA;sudo su -&#xA;cd /etc/yum.repos.d/&#xA;wget http://download.opensuse.org/repositories/shells:fish:release:2/CentOS_7/shells:fish:release:2.repo&#xA;yum install fish&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Check Oracle version etc:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Apache Kafka‚Ñ¢ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017</title>
      <link>https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</link>
      <pubDate>Wed, 20 Sep 2017 15:46:00 +0000</pubDate>
      <guid>https://rmoff.net/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s an impressive 19 sessions that cover Apache Kafka‚Ñ¢ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogoow17?search=kafka&amp;amp;showEnrolled=false&#34;&gt;OOW&lt;/a&gt;, &lt;a href=&#34;https://events.rainfocus.com/catalog/oracle/oow17/catalogjavaone17?search=kafka&amp;amp;showEnrolled=false&#34;&gt;JavaOne&lt;/a&gt;, and &lt;a href=&#34;http://www.oaktable.net/blog/oak-table-world-2017-oracle-open-world&#34;&gt;Oak Table World&lt;/a&gt;. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Check out the writeup of my previous visit to OOW including useful tips &lt;a href=&#34;https://www.rittmanmead.com/blog/2014/10/first-timer-tips-for-oracle-open-world/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configuring Kafka Connect to log REST HTTP messages to a separate file</title>
      <link>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</link>
      <pubDate>Mon, 12 Jun 2017 15:28:15 +0000</pubDate>
      <guid>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</guid>
      <description>&lt;p&gt;Kafka&amp;rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from &lt;a href=&#34;https://www.confluent.io/product/connectors/&#34;&gt;the many available&lt;/a&gt;, it&amp;rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the &lt;a href=&#34;https://www.confluent.io/product/control-center/&#34;&gt;Confluent Control Center&lt;/a&gt;, for both management and monitoring:&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://rmoff.net/images/2017/05/Control_Center.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;BUT &amp;hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured. What then? Well of course, it&amp;rsquo;s diagnostics time! And for diagnostics, you need logs. When you launch Kafka Connect it logs everything to &lt;code&gt;stdout&lt;/code&gt;, and this output includes content from the Kafka Connect &lt;a href=&#34;http://docs.confluent.io/current/connect/restapi.html&#34;&gt;REST interface&lt;/a&gt;. This REST interface is for configuration and control of the connectors (status/pause/resume) - and whilst Control Center is being used on the Connect configuration screens, you&amp;rsquo;ll notice that the REST interface gets polled frequently - every couple of seconds, with a greater number of requests the more connectors you have. All of this goes into the log:&lt;/p&gt;</description>
    </item>
    <item>
      <title>kafka.common.KafkaException: No key found on line 1</title>
      <link>https://rmoff.net/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</link>
      <pubDate>Fri, 12 May 2017 00:52:41 +0000</pubDate>
      <guid>https://rmoff.net/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</guid>
      <description>&lt;p&gt;A very silly &lt;a href=&#34;https://en.wiktionary.org/wiki/PEBCAK&#34;&gt;PEBCAK&lt;/a&gt; problem this one, but Google hits weren&amp;rsquo;t so helpful so here goes.&lt;/p&gt;&#xA;&lt;p&gt;Running a console producer, specifying keys:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;kafka-console-producer \&#xA;--broker-list localhost:9092 \&#xA;--topic test_topic \&#xA;--property parse.key=true \&#xA;--property key.seperator=,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Failed when I entered a key/value:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1,foo&#xA;kafka.common.KafkaException: No key found on line 1: 1,foo&#xA;        at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314)&#xA;        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55)&#xA;        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;kafka.common.KafkaException: No key found on line&lt;/strong&gt; &amp;hellip; but I specified the key, didn&amp;rsquo;t I?&lt;/p&gt;&#xA;&lt;p&gt;It would help if I could spell &amp;hellip;  &lt;code&gt;key.sep&lt;/code&gt;&lt;strong&gt;e&lt;/strong&gt;&lt;code&gt;rator&lt;/code&gt; isn&amp;rsquo;t a valid property to configure. &lt;code&gt;sep&lt;/code&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;code&gt;rator&lt;/code&gt; on the other hand, is:&lt;/p&gt;</description>
    </item>
    <item>
      <title>kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException</title>
      <link>https://rmoff.net/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-/-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</link>
      <pubDate>Fri, 02 Dec 2016 11:35:57 +0000</pubDate>
      <guid>https://rmoff.net/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-/-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</guid>
      <description>&lt;p&gt;By default, the &lt;code&gt;kafka-avro-console-producer&lt;/code&gt; will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 &lt;em&gt;already&lt;/em&gt;!&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[oracle@bigdatalite tmp]$ kafka-avro-console-producer \&#xA;&amp;gt;  --broker-list localhost:9092 --topic kudu_test \&#xA;&amp;gt;  --property value.schema=&amp;#39;{&amp;#34;type&amp;#34;:&amp;#34;record&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;myrecord&amp;#34;,&amp;#34;fields&amp;#34;:[{&amp;#34;name&amp;#34;:&amp;#34;id&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;int&amp;#34;},{&amp;#34;name&amp;#34;:&amp;#34;random_field&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;}]}&amp;#39;&#xA;&#xA;{&amp;#34;id&amp;#34;: 999, &amp;#34;random_field&amp;#34;: &amp;#34;foo&amp;#34;}&#xA;&#xA;org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {&amp;#34;type&amp;#34;:&amp;#34;record&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;myrecord&amp;#34;,&amp;#34;fields&amp;#34;:[{&amp;#34;name&amp;#34;:&amp;#34;id&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;int&amp;#34;},{&amp;#34;name&amp;#34;:&amp;#34;random_field&amp;#34;,&amp;#34;type&amp;#34;:&amp;#34;string&amp;#34;}]}&#xA;Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character (&amp;#39;&amp;lt;&amp;#39; (code 60)): expected a valid value (number, String, array, object, &amp;#39;true&amp;#39;, &amp;#39;false&amp;#39; or &amp;#39;null&amp;#39;)&#xA; at [Source: sun.net.www.protocol.http.HttpURLConnection$HttpInputStream@4e0ae11f; line: 1, column: 2]; error code: 50005&#xA;        at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:170)&#xA;        at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:187)&#xA;        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:238)&#xA;        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:230)&#xA;        at io.confluent.kafka.schemaregistry.client.rest.RestService.registerSchema(RestService.java:225)&#xA;        at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.registerAndGetId(CachedSchemaRegistryClient.java:59)&#xA;        at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.register(CachedSchemaRegistryClient.java:91)&#xA;        at io.confluent.kafka.serializers.AbstractKafkaAvroSerializer.serializeImpl(AbstractKafkaAvroSerializer.java:72)&#xA;        at io.confluent.kafka.formatter.AvroMessageReader.readMessage(AvroMessageReader.java:158)&#xA;        at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55)&#xA;        at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Solution? Make sure you specify the schema URL when you launch the producer, using &lt;code&gt;--property schema.registry.url=http://localhost:18081&lt;/code&gt; :&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect - java.lang.IncompatibleClassChangeError</title>
      <link>https://rmoff.net/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</link>
      <pubDate>Thu, 24 Nov 2016 20:58:44 +0000</pubDate>
      <guid>https://rmoff.net/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</guid>
      <description>&lt;p&gt;I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties&#xA;&#xA;[...]&#xA;Exception in thread &amp;#34;main&amp;#34; java.lang.IncompatibleClassChangeError: Implementing class&#xA;        at java.lang.ClassLoader.defineClass1(Native Method)&#xA;        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#xA;        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#xA;        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#xA;        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#xA;        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#xA;        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#xA;        at java.security.AccessController.doPrivileged(Native Method)&#xA;        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)&#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;        at java.lang.ClassLoader.defineClass1(Native Method)&#xA;        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The fix was to unset the &lt;code&gt;CLASSPATH&lt;/code&gt; first:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;unset CLASSPATH&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>OGG-15051 oracle.goldengate.util.GGException:  Class not found: &#34;kafkahandler&#34;</title>
      <link>https://rmoff.net/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</link>
      <pubDate>Fri, 29 Jul 2016 07:47:30 +0000</pubDate>
      <guid>https://rmoff.net/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</guid>
      <description>&lt;p&gt;Similar to the &lt;a href=&#34;https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/&#34;&gt;previous issue&lt;/a&gt;, the &lt;a href=&#34;http://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD457&#34;&gt;sample config&lt;/a&gt; in the docs causes another snafu:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;OGG-15051  Java or JNI exception:&#xA;oracle.goldengate.util.GGException:  Class not found: &amp;#34;kafkahandler&amp;#34;. kafkahandler&#xA; &#x9;Class not found: &amp;#34;kafkahandler&amp;#34;. kafkahandler&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This time it&amp;rsquo;s in the &lt;code&gt;kafka.props&lt;/code&gt; file:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;gg.handler.kafkahandler.Type = kafka&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Should be&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;gg.handler.kafkahandler.type = kafka&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;No capital T in Type!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;(Image credit: &lt;a href=&#34;https://unsplash.com/@vanschneider&#34;&gt;https://unsplash.com/@vanschneider&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>OGG -  Class not found: &#34;com.company.kafka.CustomProducerRecord&#34;</title>
      <link>https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</link>
      <pubDate>Thu, 28 Jul 2016 16:34:37 +0000</pubDate>
      <guid>https://rmoff.net/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</guid>
      <description>&lt;p&gt;In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there&amp;rsquo;s a &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD457&#34;&gt;helpful sample configuration&lt;/a&gt;, which isn&amp;rsquo;t so helpful &amp;hellip;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[...]&#xA;gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord&#xA;[...]&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This value for &lt;code&gt;gg.handler.kafkahandler.ProducerRecordClass&lt;/code&gt; will cause a failure when you start the replicat:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;[...]&#xA;Class not found: &amp;quot;com.company.kafka.CustomProducerRecord&amp;quot;&#xA;[...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;If you comment this configuration item out, it&amp;rsquo;ll use &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD455&#34;&gt;the default&lt;/a&gt; (&lt;code&gt;oracle.goldengate.handler.kafka.DefaultProducerRecord&lt;/code&gt;) and work swimingly!&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;(Image credit: &lt;a href=&#34;https://unsplash.com/@vanschneider&#34;&gt;https://unsplash.com/@vanschneider&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect JDBC - Oracle - Number of groups must be positive</title>
      <link>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</link>
      <pubDate>Wed, 27 Jul 2016 15:23:14 +0000</pubDate>
      <guid>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</guid>
      <description>&lt;p&gt;There are &lt;a href=&#34;https://groups.google.com/forum/#!searchin/confluent-platform/%22Number$20of$20groups$20must$20be$20positive%22&#34;&gt;various reasons for this error&lt;/a&gt;, but the one I hit was that &lt;strong&gt;the table name is case sensitive&lt;/strong&gt;, and returned from Oracle by the JDBC driver in uppercase.&lt;/p&gt;&#xA;&lt;p&gt;If you specify the tablename in your connecter config in lowercase, it won&amp;rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit &lt;code&gt;etc/kafka/connect-log4j.properties&lt;/code&gt; to set &lt;code&gt;log4j.rootLogger=DEBUG, stdout&lt;/code&gt;), and observe:  (&lt;em&gt;I&amp;rsquo;ve truncated some of the output for legibility&lt;/em&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required</title>
      <link>https://rmoff.net/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</link>
      <pubDate>Tue, 19 Jul 2016 14:36:52 +0000</pubDate>
      <guid>https://rmoff.net/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been doing some noodling around with Confluent&amp;rsquo;s Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you&amp;rsquo;re not familiar with Kafka Connect &lt;a href=&#34;http://docs.confluent.io/3.0.0/connect/design.html&#34;&gt;this page&lt;/a&gt; gives a good idea of the thinking behind it.&lt;/p&gt;&#xA;&lt;p&gt;One issue that I hit defeated my Google-fu so I&amp;rsquo;m recording it here to hopefully help out fellow n00bs.&lt;/p&gt;&#xA;&lt;p&gt;The pipeline that I&amp;rsquo;d set up looked like this:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/Eneco/kafka-connect-twitter&#34;&gt;Eneco&amp;rsquo;s Twitter Source&lt;/a&gt; streaming tweets to a Kafka topic&lt;/li&gt;&#xA;&lt;li&gt;Confluent&amp;rsquo;s &lt;a href=&#34;https://docs.confluent.io/current/connect/kafka-connect-hdfs/index.html&#34;&gt;HDFS Sink&lt;/a&gt; to stream tweets to HDFS and define Hive table automagically over them&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;It worked great, but only if I didn&amp;rsquo;t enable the Hive integration part. For me the integration with Hive to automatically define schemas was one of the key interests for this platform, so I wanted to see if I could get it to work. The error I got was&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
      <link>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
      <pubDate>Thu, 14 Apr 2016 22:51:43 +0000</pubDate>
      <guid>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
      <description>&lt;p&gt;Recently added to the &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;oracledi project over at java.net&lt;/a&gt; is &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;an adaptor&lt;/a&gt; enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;over at the Elastic blog&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (&lt;a href=&#34;https://rmoff.net/categories/elasticsearch/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.rittmanmead.com/tag/elasticsearch&#34;&gt;here&lt;/a&gt; for archives), as well as &lt;a href=&#34;https://speakerdeck.com/rmoff/data-discovery-and-systems-diagnostics-with-the-elk-stack&#34;&gt;spoken about&lt;/a&gt; - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with. So, being able to combine that with my &amp;ldquo;day job&amp;rdquo; focus of Oracle is fun. Let&amp;rsquo;s get started!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
      <link>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
      <pubDate>Tue, 12 Apr 2016 21:50:46 +0000</pubDate>
      <guid>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently been playing around with the ELK stack (&lt;a href=&#34;https://www.elastic.co/blog/heya-elastic-stack-and-x-pack&#34;&gt;now officially known as the Elastic stack&lt;/a&gt;) collecting data from &lt;a href=&#34;https://rmoff.net/2016/03/03/obihackers-irc-channel/&#34;&gt;an IRC channel&lt;/a&gt; with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and &lt;a href=&#34;https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;analysing it with Kibana&lt;/a&gt;. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.&lt;/p&gt;&#xA;&lt;p&gt;As I &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;wrote about last year&lt;/a&gt;, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;. Today I&amp;rsquo;m writing up a short real-world example of this in practice. There are three elements to the flexibility that I really want to highlight:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
      <link>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
      <pubDate>Wed, 16 Mar 2016 22:01:00 +0000</pubDate>
      <guid>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
      <description>&lt;p&gt;The Oracle by Example (ObE) &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/tutorials/obe/fmw/odi/odi_12c/DI_BDL_Guide/BigDataIntegration_Demo.html?cid=10235&amp;amp;ssid=0&#34;&gt;here&lt;/a&gt; demonstrating how to use &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GBDIN/intro_adapter.htm#GBDIN101&#34;&gt;Goldengate to replicate transactions big data targets&lt;/a&gt; such as HDFS is written for the BigDataLite &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite421-2843803.html&#34;&gt;4.2.1&lt;/a&gt;, and for me didn&amp;rsquo;t work on the current latest version, &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;4.4.0&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The OBE (and similar &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/odi/ODI_BigData_HOL.pdf&#34;&gt;Hands On Lab&lt;/a&gt; PDF) assume the presence of &lt;code&gt;pmov.prm&lt;/code&gt; and &lt;code&gt;pmov.properties&lt;/code&gt; in &lt;code&gt;/u01/ogg/dirprm/&lt;/code&gt;. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, &lt;code&gt;emov&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.4, with bells on because it includes &lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; too. And, who doesn&amp;rsquo;t like a bit of Kafka nowadays?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
