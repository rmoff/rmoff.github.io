<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Log4j on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/log4j/</link>
    <description>Recent content in Log4j on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2020 22:50:45 +0000</lastBuildDate><atom:link href="https://rmoff.net/categories/log4j/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Changing the Logging Level for Kafka Connect Dynamically</title>
      <link>https://rmoff.net/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/</link>
      <pubDate>Thu, 16 Jan 2020 22:50:45 +0000</pubDate>
      
      <guid>https://rmoff.net/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/</guid>
      <description>Logs are magical things. They tell us what an application is doing—or not doing. They help us debug problems. As it happens, they also underpin the entire philosophy of Apache Kafka, but that’s a story for another day. Today we’re talking about logs written by Kafka Connect, and how we can change the amount of detail written.
 By default, Kafka Connect will write logs at INFO and above. So when it starts up, the settings that it’s using, and any WARN or ERROR messages along the way - a missing configuration, a broken connector, and so on.</description>
    </item>
    
    <item>
      <title>Configuring Kafka Connect to log REST HTTP messages to a separate file</title>
      <link>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</link>
      <pubDate>Mon, 12 Jun 2017 15:28:15 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</guid>
      <description>Kafka&amp;rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from the many available, it&amp;rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the Confluent Control Center, for both management and monitoring:
BUT &amp;hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured.</description>
    </item>
    
    <item>
      <title>Kafka Connect JDBC - Oracle - Number of groups must be positive</title>
      <link>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</link>
      <pubDate>Wed, 27 Jul 2016 15:23:14 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</guid>
      <description>There are various reasons for this error, but the one I hit was that the table name is case sensitive, and returned from Oracle by the JDBC driver in uppercase.
If you specify the tablename in your connecter config in lowercase, it won&amp;rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit etc/kafka/connect-log4j.properties to set log4j.rootLogger=DEBUG, stdout), and observe: (I&amp;rsquo;ve truncated some of the output for legibility)</description>
    </item>
    
  </channel>
</rss>
