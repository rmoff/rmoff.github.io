<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Elasticsearch on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/elasticsearch/</link>
    <description>Recent content in Elasticsearch on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Nov 2020 17:14:33 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/elasticsearch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kafka Connect, ksqlDB, and Kafka Tombstone messages</title>
      <link>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</link>
      <pubDate>Tue, 03 Nov 2020 17:14:33 +0000</pubDate>
      <guid>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;As you may already realise, Kafka is not just a fancy message bus, or a pipe for big data. It’s an event streaming platform! If this is news to you, I’ll wait here whilst you &lt;a href=&#34;https://www.confluent.io/learn/kafka-tutorial/&#34;&gt;read this&lt;/a&gt; or &lt;a href=&#34;https://rmoff.dev/kafka101&#34;&gt;watch this&lt;/a&gt;…&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Streaming Geopoint data from Kafka to Elasticsearch</title>
      <link>https://rmoff.net/2020/11/03/streaming-geopoint-data-from-kafka-to-elasticsearch/</link>
      <pubDate>Tue, 03 Nov 2020 10:36:18 +0000</pubDate>
      <guid>https://rmoff.net/2020/11/03/streaming-geopoint-data-from-kafka-to-elasticsearch/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Streaming data from Kafka to Elasticsearch is easy with Kafka Connect - you can see how in this &lt;a href=&#34;https://rmoff.dev/kafka-elasticsearch&#34;&gt;tutorial&lt;/a&gt; and &lt;a href=&#34;https://rmoff.dev/kafka-elasticsearch-video&#34;&gt;video&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;One of the things that sometimes causes issues though is how to get location data correctly indexed into Elasticsearch as &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html&#34;&gt;&lt;code&gt;geo_point&lt;/code&gt;&lt;/a&gt; fields to enable all that lovely location analysis. Unlike data types like dates and numerics, Elasticsearch’s &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-field-mapping.html&#34;&gt;Dynamic Field Mapping&lt;/a&gt; won’t automagically pick up &lt;code&gt;geo_point&lt;/code&gt; data, and so you have to do two things:&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Adventures in the Cloud, Part 94: ECS</title>
      <link>https://rmoff.net/2020/02/13/adventures-in-the-cloud-part-94-ecs/</link>
      <pubDate>Thu, 13 Feb 2020 00:12:23 +0000</pubDate>
      <guid>https://rmoff.net/2020/02/13/adventures-in-the-cloud-part-94-ecs/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;My name’s Robin, and I’m a Developer Advocate. What that means in part is that I build a ton of demos, and Docker Compose is my jam. I love using Docker Compose for the same reasons that many people do:&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;ulist&#34;&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Spin up and tear down fully-functioning multi-component environments with ease. No bespoke builds, no cloning of VMs to preserve &amp;#34;that magic state where everything works&amp;#34;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Repeatability. It’s the same each time.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Portability. I can point someone at a &lt;code&gt;docker-compose.yml&lt;/code&gt; that I’ve written and they can run the same on their machine with the same results almost guaranteed.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Detecting and Analysing SSH Attacks with ksqlDB</title>
      <link>https://rmoff.net/2019/12/18/detecting-and-analysing-ssh-attacks-with-ksqldb/</link>
      <pubDate>Wed, 18 Dec 2019 17:23:40 +0000</pubDate>
      <guid>https://rmoff.net/2019/12/18/detecting-and-analysing-ssh-attacks-with-ksqldb/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;I’ve &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-part-1-filtering/&#34;&gt;written previously&lt;/a&gt; about ingesting Syslog into Kafka and using KSQL to analyse it. I want to revisit the subject since it’s nearly two years since I wrote about it and some things have changed since then.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;ksqlDB now includes the ability to define connectors from within it, which makes setting things up loads easier.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;You can find the &lt;a href=&#34;https://github.com/confluentinc/demo-scene/tree/master/syslog&#34;&gt;full rig to run this on GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;sect1&#34;&gt;&#xA;&lt;h2 id=&#34;_create_and_configure_the_syslog_connector&#34;&gt;Create and configure the Syslog connector&lt;/h2&gt;&#xA;&lt;div class=&#34;sectionbody&#34;&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;To start with, create a source connector:&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Kafka Connect and Elasticsearch</title>
      <link>https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/</link>
      <pubDate>Mon, 07 Oct 2019 15:44:59 +0100</pubDate>
      <guid>https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;I use the Elastic stack for a lot of my &lt;a href=&#34;https://talks.rmoff.net/&#34;&gt;talks&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/demo-scene/&#34;&gt;demos&lt;/a&gt; because it complements Kafka brilliantly. A few things have changed in recent releases and this blog is a quick note on some of the errors that you might hit and how to resolve them. It was inspired by a lot of the comments and discussion &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/314&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/confluentinc/kafka-connect-elasticsearch/issues/342&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Window Timestamps in KSQL / Integration with Elasticsearch</title>
      <link>https://rmoff.net/2018/09/03/window-timestamps-in-ksql-/-integration-with-elasticsearch/</link>
      <pubDate>Mon, 03 Sep 2018 16:16:30 +0000</pubDate>
      <guid>https://rmoff.net/2018/09/03/window-timestamps-in-ksql-/-integration-with-elasticsearch/</guid>
      <description>&lt;p&gt;KSQL provides the ability to create windowed aggregations. For example,&#xA;count the number of messages in a 1 minute window, grouped by a&#xA;particular column:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE TABLE RATINGS_BY_CLUB_STATUS AS \&#xA;SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \&#xA;FROM RATINGS_WITH_CUSTOMER_DATA \&#xA;     WINDOW TUMBLING (SIZE 1 MINUTES) \&#xA;GROUP BY CLUB_STATUS;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;How KSQL, and Kafka Streams, stores the window timestamp associated with&#xA;an aggregate, has recently changed. &lt;a href=&#34;https://github.com/confluentinc/ksql/issues/1497&#34;&gt;See #1497 for&#xA;details&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Whereas previously the &lt;em&gt;Kafka message timestamp&lt;/em&gt; (accessible through the&#xA;KSQL &lt;code&gt;ROWTIME&lt;/code&gt; system column) stored the start of the window for which&#xA;the aggregate had been calculated, this changed in July 2018 to instead&#xA;be the timestamp of the latest message to update that aggregate value.&#xA;This was in Apache Kafka 2.0 and Confluent Platform 5.0, and back-ported&#xA;to previous versions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      <guid>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://cnfl.io/syslogs-filtering&#34;&gt;this article&lt;/a&gt; I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/&#34;&gt;enrich streams&lt;/a&gt;. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.&lt;/p&gt;&#xA;&lt;p&gt;The data came from my home &lt;a href=&#34;https://www.ubnt.com/&#34;&gt;Ubiquiti&lt;/a&gt; router, and took two forms:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming data from Kafka into Elasticsearch</title>
      <link>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
      <pubDate>Tue, 06 Mar 2018 22:21:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
      <description>&lt;p&gt;&lt;em&gt;This article is part of a series exploring Streaming ETL in practice. You can read about &lt;a href=&#34;https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/&#34;&gt;setting up the ingest of realtime events from a standard Oracle platform&lt;/a&gt;, and &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;building streaming ETL using KSQL&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion</title>
      <link>https://rmoff.net/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</link>
      <pubDate>Fri, 13 May 2016 05:45:19 +0000</pubDate>
      <guid>https://rmoff.net/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</guid>
      <description>&lt;p&gt;Yesterday I wrote about &lt;a href=&#34;https://rmoff.net/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/&#34;&gt;Monitoring Logstash Ingest Rates with InfluxDB and Grafana&lt;/a&gt;, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/warkolm/&#34;&gt;Mark Walkom&lt;/a&gt; reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it&amp;rsquo;ll integrate back into the Elastic stack:&lt;/p&gt;&#xA;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/rmoff&#34;&gt;@rmoff&lt;/a&gt; nice, LS 5.0 will have APIs exposing metrics too. they’ll be integrated back into Marvel/Monitoring! :)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using R to Denormalise Data for Analysis in Kibana</title>
      <link>https://rmoff.net/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</link>
      <pubDate>Sun, 24 Apr 2016 12:22:12 +0000</pubDate>
      <guid>https://rmoff.net/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/products/kibana&#34;&gt;Kibana&lt;/a&gt; is a tool from &lt;a href=&#34;https://www.elastic.co/&#34;&gt;Elastic&lt;/a&gt; that makes analysis of data held in &lt;a href=&#34;https://www.elastic.co/products/elasticsearch&#34;&gt;Elasticsearch&lt;/a&gt; really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a &lt;a href=&#34;http://www.rittmanmead.com/2015/04/using-the-elk-stack-to-analyse-donors-choose-data/&#34;&gt;blog post last year&lt;/a&gt;, taking a CSV file and loading it into Elasticsearch via Logstash.&lt;/p&gt;&#xA;&lt;p&gt;This is all great, but the one real sticking point with analytics in Elasticsearch/Kibana is that it needs the data to be &lt;strong&gt;denormalised&lt;/strong&gt;. That is, you can&amp;rsquo;t give it a bunch of sources of data and it perform the joins for you in Kibana - it just doesn&amp;rsquo;t work like that. If you&amp;rsquo;re using Elasticsearch alone for analytics, maybe with a bespoke application, &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/current/relations.html&#34;&gt;there are ways of approaching it&lt;/a&gt;, but not through Kibana. Now, depending on where the data is coming from, this may not be a problem. For example, if you use the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html&#34;&gt;JDBC Logstash input&lt;/a&gt; to pull from an RDBMS source you can specify a complex SQL query going across multiple tables, so that the data when it hits Elasticsearch is nice and denormalised and ready for fun in Kibana. But, source data doesn&amp;rsquo;t always come this way, and it&amp;rsquo;s useful to have a way to work with the data still when it is like this.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
      <link>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
      <pubDate>Thu, 14 Apr 2016 22:51:43 +0000</pubDate>
      <guid>https://rmoff.net/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
      <description>&lt;p&gt;Recently added to the &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;oracledi project over at java.net&lt;/a&gt; is &lt;a href=&#34;https://java.net/projects/oracledi/&#34;&gt;an adaptor&lt;/a&gt; enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently &lt;a href=&#34;https://www.elastic.co/blog/visualising-oracle-performance-data-with-the-elastic-stack&#34;&gt;over at the Elastic blog&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (&lt;a href=&#34;https://rmoff.net/tag/elasticsearch/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.rittmanmead.com/tag/elasticsearch&#34;&gt;here&lt;/a&gt; for archives), as well as &lt;a href=&#34;https://speakerdeck.com/rmoff/data-discovery-and-systems-diagnostics-with-the-elk-stack&#34;&gt;spoken about&lt;/a&gt; - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with. So, being able to combine that with my &amp;ldquo;day job&amp;rdquo; focus of Oracle is fun. Let&amp;rsquo;s get started!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
      <link>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
      <pubDate>Tue, 12 Apr 2016 21:50:46 +0000</pubDate>
      <guid>https://rmoff.net/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently been playing around with the ELK stack (&lt;a href=&#34;https://www.elastic.co/blog/heya-elastic-stack-and-x-pack&#34;&gt;now officially known as the Elastic stack&lt;/a&gt;) collecting data from &lt;a href=&#34;https://rmoff.net/2016/03/03/obihackers-irc-channel/&#34;&gt;an IRC channel&lt;/a&gt; with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and &lt;a href=&#34;https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/&#34;&gt;analysing it with Kibana&lt;/a&gt;. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.&lt;/p&gt;&#xA;&lt;p&gt;As I &lt;a href=&#34;http://www.rittmanmead.com/2015/10/forays-into-kafka-enabling-flexible-data-pipelines/&#34;&gt;wrote about last year&lt;/a&gt;, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;. Today I&amp;rsquo;m writing up a short real-world example of this in practice. There are three elements to the flexibility that I really want to highlight:&lt;/p&gt;</description>
    </item>
    <item>
      <title>My latest IRC client : Kibana</title>
      <link>https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/</link>
      <pubDate>Thu, 24 Mar 2016 21:38:02 +0000</pubDate>
      <guid>https://rmoff.net/2016/03/24/my-latest-irc-client-kibana/</guid>
      <description>&lt;p&gt;OK, maybe that&amp;rsquo;s not entirely true. But my &lt;em&gt;read-only&lt;/em&gt; client, certainly.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://rmoff.net/images/2016/03/2016-03-24_21-15-30.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;I was perusing the &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/input-plugins.html&#34;&gt;Logstash input plugins&lt;/a&gt; recently when I noticed that there was one for &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/plugins-inputs-irc.html&#34;&gt;IRC&lt;/a&gt;. Being a fan of IRC and a regular on the &lt;a href=&#34;https://rmoff.net/2016/03/03/obihackers-irc-channel/&#34;&gt;#obihackers&lt;/a&gt; channel, I thought this could be fun and yet another great example of how easy &lt;a href=&#34;http://elastic.co&#34;&gt;the Elastic stack&lt;/a&gt; is to work with.&lt;/p&gt;&#xA;&lt;p&gt;Installation is a piece of cake:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget https://download.elastic.co/logstash/logstash/logstash-2.2.2.zip&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget https://download.elastic.co/kibana/kibana/kibana-4.4.2-linux-x64.tar.gz&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;unzip &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\*&lt;/span&gt;.zip&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tar -xf kibana-4.4.2-linux-x64.tar.gz&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mv elasticsearch-2.2.1 logstash-2.2.2 kibana-4.4.2-linux-x64 /opt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(you&amp;rsquo;ll also need Oracle JDK installed if not already, &lt;a href=&#34;http://www.jamescoyle.net/how-to/1897-download-oracle-java-from-the-terminal-with-wget&#34;&gt;here&amp;rsquo;s a handy way to get it from the CLI&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
      <link>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
      <pubDate>Wed, 16 Mar 2016 22:01:00 +0000</pubDate>
      <guid>https://rmoff.net/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
      <description>&lt;p&gt;The Oracle by Example (ObE) &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/tutorials/obe/fmw/odi/odi_12c/DI_BDL_Guide/BigDataIntegration_Demo.html?cid=10235&amp;amp;ssid=0&#34;&gt;here&lt;/a&gt; demonstrating how to use &lt;a href=&#34;https://docs.oracle.com/goldengate/bd1221/gg-bd/GBDIN/intro_adapter.htm#GBDIN101&#34;&gt;Goldengate to replicate transactions big data targets&lt;/a&gt; such as HDFS is written for the BigDataLite &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite421-2843803.html&#34;&gt;4.2.1&lt;/a&gt;, and for me didn&amp;rsquo;t work on the current latest version, &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;4.4.0&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The OBE (and similar &lt;a href=&#34;http://www.oracle.com/webfolder/technetwork/odi/ODI_BigData_HOL.pdf&#34;&gt;Hands On Lab&lt;/a&gt; PDF) assume the presence of &lt;code&gt;pmov.prm&lt;/code&gt; and &lt;code&gt;pmov.properties&lt;/code&gt; in &lt;code&gt;/u01/ogg/dirprm/&lt;/code&gt;. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, &lt;code&gt;emov&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.4, with bells on because it includes &lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Kafka&lt;/a&gt; too. And, who doesn&amp;rsquo;t like a bit of Kafka nowadays?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
