<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/categories/python/</link>
    <description>Recent content in Python on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Jun 2018 11:35:20 +0000</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.
The data came from my home Ubiquiti router, and took two forms:</description>
    </item>
    
    <item>
      <title>Installing the Python Kafka library from Confluent - troubleshooting some silly errors…</title>
      <link>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:24 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</guid>
      <description>System:
rmoff@proxmox01:~$ uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux rmoff@proxmox01:~$ head -n1 /etc/os-release PRETTY_NAME=&amp;quot;Debian GNU/Linux 8 (jessie)&amp;quot; rmoff@proxmox01:~$ python --version Python 2.7.9  Following:
 https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/ https://github.com/confluentinc/confluent-kafka-python  Install librdkafka, which is a pre-req for the Python library:
wget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add - sudo add-apt-repository &amp;quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main&amp;quot; sudo apt-get install librdkafka-dev python-dev  Setup virtualenv:</description>
    </item>
    
    <item>
      <title>boto / S3 errors</title>
      <link>https://rmoff.github.io/2016/10/14/boto-s3-errors/</link>
      <pubDate>Fri, 14 Oct 2016 08:41:30 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/10/14/boto-s3-errors/</guid>
      <description>Presented without comment, warranty, or context - other than these might help a wandering code hacker.
When using SigV4, you must specify a &amp;lsquo;host&amp;rsquo; parameter boto.s3.connection.HostRequiredError: BotoClientError: When using SigV4, you must specify a &#39;host&#39; parameter.  To fix, switch
conn_s3 = boto.connect_s3()  for
conn_s3 = boto.connect_s3(host=&#39;s3.amazonaws.com&#39;)  You can see a list of endpoints here.
boto.exception.S3ResponseError: S3ResponseError: 400 Bad Request Make sure you&amp;rsquo;re specifying the correct hostname (see above) for the bucket&amp;rsquo;s region.</description>
    </item>
    
    <item>
      <title>Reset Hue password</title>
      <link>https://rmoff.github.io/2016/07/05/reset-hue-password/</link>
      <pubDate>Tue, 05 Jul 2016 13:27:06 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/07/05/reset-hue-password/</guid>
      <description>(Ref)
The bit that caught me out was this kept failing with
Error: Password not present  and a Python stack trace that ended with
subprocess.CalledProcessError: Command &#39;/var/run/cloudera-scm-agent/process/78-hue-HUE_SERVER/altscript.sh sec-1-secret_key&#39; returned non-zero exit status 1  The answer (it seems) is to ensure that HUE_SECRET_KEY is set (to any value!)
Launch shell:
export HUE_SECRET_KEY=foobar /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/hue/build/env/bin/hue shell  Reset password for hue, activate account and make it superuser
from django.contrib.auth.models import User user = User.</description>
    </item>
    
  </channel>
</rss>