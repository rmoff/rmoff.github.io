<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ksql on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/categories/ksql/</link>
    <description>Recent content in Ksql on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jan 2019 12:12:11 +0000</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/categories/ksql/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KSQL REST API cheatsheet</title>
      <link>https://rmoff.github.io/post/ksql-rest-api-cheatsheet/</link>
      <pubDate>Thu, 17 Jan 2019 12:12:11 +0000</pubDate>
      
      <guid>https://rmoff.github.io/post/ksql-rest-api-cheatsheet/</guid>
      <description>Full reference is here
   Run a query
$ curl -X POST \  http://localhost:8088/query \  -H &amp;#39;content-type: application/vnd.ksql.v1+json; charset=utf-8&amp;#39; \  -d &amp;#39;{&amp;#34;ksql&amp;#34;:&amp;#34;SELECT * FROM COMPUTER_T;&amp;#34;, &amp;#34;streamsProperties&amp;#34;: { &amp;#34;ksql.streams.auto.offset.reset&amp;#34;: &amp;#34;earliest&amp;#34; }}&amp;#39; {&amp;#34;row&amp;#34;:{&amp;#34;columns&amp;#34;:[1547723561637,&amp;#34;100&amp;#34;,1,&amp;#34;100&amp;#34;,&amp;#34;e0:a1:d7:18:c2:72&amp;#34;]},&amp;#34;errorMessage&amp;#34;:null,&amp;#34;finalMessage&amp;#34;:null} {&amp;#34;row&amp;#34;:{&amp;#34;columns&amp;#34;:[1547723561637,&amp;#34;101&amp;#34;,2,&amp;#34;101&amp;#34;,&amp;#34;e1:a3:d7:18:c2:72&amp;#34;]},&amp;#34;errorMessage&amp;#34;:null,&amp;#34;finalMessage&amp;#34;:null} {&amp;#34;row&amp;#34;:{&amp;#34;columns&amp;#34;:[1547723561637,&amp;#34;102&amp;#34;,3,&amp;#34;102&amp;#34;,&amp;#34;e2:a4:d7:18:c2:72&amp;#34;]},&amp;#34;errorMessage&amp;#34;:null,&amp;#34;finalMessage&amp;#34;:null}  Remember, KSQL is a continuous query so this will keep executing; use LIMIT x if you just want the first x rows.
   Run a statement, use jq to format the output</description>
    </item>
    
    <item>
      <title>Docker Tips and Tricks with KSQL and Kafka</title>
      <link>https://rmoff.github.io/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</link>
      <pubDate>Sat, 15 Dec 2018 22:00:55 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</guid>
      <description>A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad…how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.</description>
    </item>
    
    <item>
      <title>Streaming data from Oracle into Kafka (December 2018)</title>
      <link>https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/</link>
      <pubDate>Wed, 12 Dec 2018 09:49:04 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/</guid>
      <description>This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018. For a more detailed background to why and how at a broader level for all databases (not just Oracle) see this blog and these slides.
What techniques &amp;amp; tools are there? As of December 2018, this is what the line-up looks like:
 Query-based CDC  The JDBC Connector for Kafka Connect, polls the database for new or changed data based on an incrementing ID column and/or update timestamp  Log-based CDC</description>
    </item>
    
    <item>
      <title>Flatten CDC records in KSQL</title>
      <link>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</link>
      <pubDate>Thu, 11 Oct 2018 15:13:59 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</guid>
      <description>The problem - nested messages in Kafka Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:
{ &amp;quot;SCN&amp;quot;: 12206116841348, &amp;quot;SEG_OWNER&amp;quot;: &amp;quot;KFKUSER&amp;quot;, &amp;quot;TABLE_NAME&amp;quot;: &amp;quot;CDCTAB2&amp;quot;, &amp;quot;TIMESTAMP&amp;quot;: 1539162785000, &amp;quot;SQL_REDO&amp;quot;: &amp;quot;insert into \&amp;quot;KFKUSER\&amp;quot;.\&amp;quot;CDCTAB2\&amp;quot;(\&amp;quot;ID\&amp;quot;,\&amp;quot;CITY\&amp;quot;,\&amp;quot;NATIONALITY\&amp;quot;) values (634789,&#39;AHMEDABAD&#39;,&#39;INDIA&#39;)&amp;quot;, &amp;quot;OPERATION&amp;quot;: &amp;quot;INSERT&amp;quot;, &amp;quot;data&amp;quot;: { &amp;quot;value&amp;quot;: { &amp;quot;ID&amp;quot;: 634789, &amp;quot;CITY&amp;quot;: { &amp;quot;string&amp;quot;: &amp;quot;AHMEDABAD&amp;quot; }, &amp;quot;NATIONALITY&amp;quot;: { &amp;quot;string&amp;quot;: &amp;quot;INDIA&amp;quot; } } }, &amp;quot;before&amp;quot;: null }  Note that the &amp;lsquo;payload&amp;rsquo; is nested under data-&amp;gt;value.</description>
    </item>
    
    <item>
      <title>Exploring JMX with jmxterm</title>
      <link>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</link>
      <pubDate>Wed, 19 Sep 2018 08:11:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</guid>
      <description>Check out the jmxterm repository / Download jmxterm from http://wiki.cyclopsgroup.org/jmxterm/
Launch:
java -jar ~/Downloads/jmxterm-1.0.0-uber.jar --url localhost:30002  You can pass the jmx host/port directly, or use the open command once jmxterm launches.
Once connected, use domains to list available domains
$&amp;gt;domains #following domains are available JMImplementation com.sun.management io.confluent.ksql.metrics io.confluent.rest java.lang java.nio java.util.logging kafka.admin.client kafka.consumer kafka.producer kafka.streams [...]  Switch to a particular domain:
$&amp;gt;domain io.confluent.ksql.metrics #domain is set to io.confluent.ksql.metrics  List the available MBeans in a the selected domain (you can also run this without choosing a domain first, to see every MBean, but it&amp;rsquo;s a long list):</description>
    </item>
    
    <item>
      <title>Window Timestamps in KSQL / Integration with Elasticsearch</title>
      <link>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</link>
      <pubDate>Mon, 03 Sep 2018 16:16:30 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</guid>
      <description>KSQL provides the ability to create windowed aggregations. For example, count the number of messages in a 1 minute window, grouped by a particular column:
CREATE TABLE RATINGS_BY_CLUB_STATUS AS \ SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \ FROM RATINGS_WITH_CUSTOMER_DATA \ WINDOW TUMBLING (SIZE 1 MINUTES) \ GROUP BY CLUB_STATUS;  How KSQL, and Kafka Streams, stores the window timestamp associated with an aggregate, has recently changed. See #1497 for details.
Whereas previously the Kafka message timestamp (accessible through the KSQL ROWTIME system column) stored the start of the window for which the aggregate had been calculated, this changed in July 2018 to instead be the timestamp of the latest message to update that aggregate value.</description>
    </item>
    
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.
The data came from my home Ubiquiti router, and took two forms:</description>
    </item>
    
    <item>
      <title>Stream-Table Joins in KSQL: Stream events must be timestamped after the Table messages</title>
      <link>https://rmoff.github.io/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</link>
      <pubDate>Thu, 17 May 2018 10:16:43 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/</guid>
      <description>(preserving this StackOverflow answer for posterity and future Googlers)
tl;dr When doing a stream-table join, your table messages must already exist (and must be timestamped) before the stream messages. If you re-emit your source stream messages, after the table topic is populated, the join will succeed.
Example data Use kafakcat to populate topics:
kafkacat -b localhost:9092 -P -t sessionDetails &amp;lt;&amp;lt;EOF {&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1} {&amp;quot;Media&amp;quot;:&amp;quot;Foo&amp;quot;,&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2} EOF kafkacat -b localhost:9092 -P -t voipDetails &amp;lt;&amp;lt;EOF {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1a&amp;quot;} {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:25:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:1,&amp;quot;Details&amp;quot;:&amp;quot;Bar1b&amp;quot;} {&amp;quot;SessionIdTime&amp;quot;:&amp;quot;2018-05-17 11:26:33 BST&amp;quot;,&amp;quot;SessionIdSeq&amp;quot;:2,&amp;quot;Details&amp;quot;:&amp;quot;Bar2&amp;quot;} EOF  Validate topic contents:</description>
    </item>
    
  </channel>
</rss>