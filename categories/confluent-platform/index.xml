<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Confluent Platform on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/confluent-platform/</link>
    <description>Recent content in Confluent Platform on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Mar 2018 22:18:00 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/confluent-platform/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why Do We Need Streaming ETL?</title>
      <link>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This is an expanded version of the intro to an article I posted over on the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog&lt;/a&gt;. Here I get to be as verbose as I like &lt;code&gt;;)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe. From there it was checked and loaded, and then reports generated on it. This was nearly twenty years ago as my greying beard will attestâ€”and not a lot has changed in the large majority of reporting and analytics systems since then. COBOL is maybe less common, but what has remained constant is the batch-driven nature of processing. Sometimes batches are run more frequently, and get given fancy names like intra-day ETL or even micro-batching. But batch processing it is, and as such latency is built into our reporting &lt;em&gt;by design&lt;/em&gt;. When we opt for batch processing we voluntarily inject delays into the availability of data to our end users. Much better is to build our systems around a streaming platform instead.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform</title>
      <link>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</link>
      <pubDate>Tue, 21 Nov 2017 17:31:00 +0000</pubDate>
      <guid>https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</guid>
      <description>&lt;p&gt;&lt;em&gt;Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I used the &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt; for this - it&amp;rsquo;s preinstalled with Oracle 12cR2. &lt;a href=&#34;http://www.oracle.com/technetwork/database/bigdata-appliance/oracle-bigdatalite-2104726.html&#34;&gt;Big Data Lite&lt;/a&gt; is nice but currently has an older version of GoldenGate.&lt;/p&gt;&#xA;&lt;p&gt;Login to the VM (oracle/oracle) and then install some useful things:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm&#xA;sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop&#xA;sudo su -&#xA;cd /etc/yum.repos.d/&#xA;wget http://download.opensuse.org/repositories/shells:fish:release:2/CentOS_7/shells:fish:release:2.repo&#xA;yum install fish&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Check Oracle version etc:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
