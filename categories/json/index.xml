<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>json on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/json/</link>
    <description>Recent content in json on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jul 2020 08:16:36 +0100</lastBuildDate><atom:link href="https://rmoff.net/categories/json/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why JSON isn&#39;t the same as JSON Schema in Kafka Connect converters (Viewing Kafka messages bytes as hex)</title>
      <link>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</link>
      <pubDate>Fri, 03 Jul 2020 08:16:36 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-viewing-kafka-messages-bytes-as-hex/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Iâ€™ve been playing around with the new SerDes (serialisers/deserialisers) that shipped with Confluent Platform 5.5 - &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/serdes-develop/index.html&#34;&gt;Protobuf, and JSON Schema&lt;/a&gt; (these were added to the existing support for Avro). The serialisers (and associated &lt;a href=&#34;https://docs.confluent.io/current/schema-registry/connect.html&#34;&gt;Kafka Connect converters&lt;/a&gt;) take a payload and serialise it into bytes for sending to Kafka, and I was interested in what those bytes look like. For that I used my favourite Kafka swiss-army knife: kafkacat.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Spark sqlContext.read.json - java.io.IOException: No input paths specified in job</title>
      <link>https://rmoff.net/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</link>
      <pubDate>Wed, 13 Jul 2016 04:50:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/</guid>
      <description>Trying to use SparkSQL to read a JSON file, from either pyspark or spark-shell, I got this error:
java.io.IOException: No input paths specified in job  scala&amp;gt; sqlContext.read.json(&amp;quot;/u02/custom/twitter/twitter.json&amp;quot;) java.io.IOException: No input paths specified in job at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202) Despite the reference articles that I found using this local path syntax (/u02/custom/twitter/twitter.json), it turned out that I needed to prefix it with file://:
scala&amp;gt; sqlContext.read.json(&amp;quot;file:///u02/custom/twitter/twitter.json&amp;quot;) res3: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: string, created_at: string, entities: struct&amp;lt;hashtags:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,media:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string,id:bigint,id_str:string,indices:array&amp;lt;bigint&amp;gt;,media_url:string,media_url_https:string,sizes:struct&amp;lt;large:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,medium:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,small:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;,thumb:struct&amp;lt;h:bigint,resize:string,w:bigint&amp;gt;&amp;gt;,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string&amp;gt;&amp;gt;,symbols:array&amp;lt;struct&amp;lt;indices:array&amp;lt;bigint&amp;gt;,text:string&amp;gt;&amp;gt;,urls:array&amp;lt;struct&amp;lt;display_url:string,expanded_url:string.</description>
    </item>
    
  </channel>
</rss>
