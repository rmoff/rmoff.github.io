<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Telegram on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/telegram/</link>
    <description>Recent content in Telegram on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jul 2020 15:00:05 +0100</lastBuildDate><atom:link href="https://rmoff.net/categories/telegram/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Telegram bot - BOT_COMMAND_INVALID</title>
      <link>https://rmoff.net/2020/07/23/telegram-bot-bot_command_invalid/</link>
      <pubDate>Thu, 23 Jul 2020 15:00:05 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/07/23/telegram-bot-bot_command_invalid/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A tiny snippet since I wasted 10 minutes going around the houses on this one…&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;tl;dr: If you try to create a command that is &lt;strong&gt;not in lower case&lt;/strong&gt; (e.g. &lt;code&gt;Alert&lt;/code&gt; not &lt;code&gt;alert&lt;/code&gt;) then the &lt;code&gt;setMyCommands&lt;/code&gt; API will return &lt;code&gt;BOT_COMMAND_INVALID&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Building a Telegram bot with Apache Kafka and ksqlDB</title>
      <link>https://rmoff.net/2020/05/18/building-a-telegram-bot-with-apache-kafka-and-ksqldb/</link>
      <pubDate>Mon, 18 May 2020 11:28:15 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/05/18/building-a-telegram-bot-with-apache-kafka-and-ksqldb/</guid>
      <description>Imagine you’ve got a stream of data; it’s not “big data,” but it’s certainly a lot. Within the data, you’ve got some bits you’re interested in, and of those bits, you’d like to be able to query information about them at any point. Sounds fun, right?
   What if you didn’t need any datastore other than Apache Kafka itself to be able to do this? What if you could ingest, filter, enrich, aggregate, and query data with just Kafka?</description>
    </item>
    
    <item>
      <title>A quick and dirty way to monitor data arriving on Kafka</title>
      <link>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</link>
      <pubDate>Thu, 16 Apr 2020 00:51:18 +0100</pubDate>
      
      <guid>https://rmoff.net/2020/04/16/a-quick-and-dirty-way-to-monitor-data-arriving-on-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;I’ve been poking around recently with &lt;a href=&#34;https://rmoff.net/2020/03/11/streaming-wi-fi-trace-data-from-raspberry-pi-to-apache-kafka-with-confluent-cloud/&#34;&gt;capturing Wi-Fi packet data&lt;/a&gt; and streaming it into Apache Kafka, from where I’m processing and analysing it. Kafka itself is rock-solid - because I’m using &lt;a href=&#34;https://confluent.cloud/signup&#34;&gt;☁️Confluent Cloud&lt;/a&gt; and someone else worries about provisioning it, scaling it, and keeping it running for me. But whilst Kafka works just great, my side of the setup—&lt;code&gt;tshark&lt;/code&gt; running on a Raspberry Pi—is less than stable. For whatever reason it sometimes stalls and I have to restart the Raspberry Pi and restart the capture process.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
