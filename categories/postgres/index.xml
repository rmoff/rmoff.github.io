<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Postgres on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/categories/postgres/</link>
    <description>Recent content in Postgres on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/categories/postgres/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Streaming Data from Postgres to Snowflake with CDC and Decodable</title>
      <link>https://rmoff.net/2024/11/19/streaming-data-from-postgres-to-snowflake-with-cdc-and-decodable/</link>
      <pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://rmoff.net/2024/11/19/streaming-data-from-postgres-to-snowflake-with-cdc-and-decodable/</guid>
      <description>&lt;div class=&#34;admonitionblock note&#34;&gt;&#xA;&lt;table&gt;&#xA;&lt;tbody&gt;&lt;tr&gt;&#xA;&lt;td class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fa icon-note&#34; title=&#34;Note&#34;&gt;&lt;/i&gt;&#xA;&lt;/td&gt;&#xA;&lt;td class=&#34;content&#34;&gt;&#xA;This post originally appeared on the &lt;a href=&#34;https://www.decodable.co/blog/streaming-data-from-postgres-to-snowflake&#34;&gt;Decodable blog&lt;/a&gt;.&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;In my  &lt;a href=&#34;https://rmoff.net/2024/10/15/why-do-i-need-cdc/&#34;&gt;last blog post&lt;/a&gt;  I looked at why you might need CDC.&#xA;In this post I’m going to put it into practice with probably the most common use case—extracting data from an operational transactional database to store somewhere else for analytics.&#xA;I’m going to show Postgres to Snowflake, but the pattern is the same for pretty much any combination, such as MySQL to BigQuery, SQL Server to Redshift, and so on.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Kafka Connect, ksqlDB, and Kafka Tombstone messages</title>
      <link>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</link>
      <pubDate>Tue, 03 Nov 2020 17:14:33 +0000</pubDate>
      <guid>https://rmoff.net/2020/11/03/kafka-connect-ksqldb-and-kafka-tombstone-messages/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;As you may already realise, Kafka is not just a fancy message bus, or a pipe for big data. It’s an event streaming platform! If this is news to you, I’ll wait here whilst you &lt;a href=&#34;https://www.confluent.io/learn/kafka-tutorial/&#34;&gt;read this&lt;/a&gt; or &lt;a href=&#34;https://rmoff.dev/kafka101&#34;&gt;watch this&lt;/a&gt;…&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Loading CSV data into Kafka</title>
      <link>https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/</link>
      <pubDate>Wed, 17 Jun 2020 17:57:18 +0100</pubDate>
      <guid>https://rmoff.net/2020/06/17/loading-csv-data-into-kafka/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;For whatever reason, CSV still exists as a ubiquitous data interchange format. It doesn’t get much simpler: chuck some plaintext with fields separated by commas into a file and stick &lt;code&gt;.csv&lt;/code&gt; on the end. If you’re feeling helpful you can include a header row with field names in.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-csv&#34; data-lang=&#34;csv&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;order_id&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;customer_id&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;order_total_usd&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;make&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;model&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;delivery_city&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;delivery_company&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;delivery_address&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;535&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;190899.73&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Dodge&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Ram Wagon B350&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Sheffield&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;DuBuque LLC&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;2810 Northland Avenue&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ba2121&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;671&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;33245.53&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Volkswagen&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Cabriolet&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Edinburgh&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;Bechtelar-VonRueden&lt;/span&gt;,&lt;span style=&#34;color:#ba2121&#34;&gt;1 Macpherson Crossing&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;In this article we’ll see how to load this CSV data into Kafka, without even needing to write any code&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Streaming messages from RabbitMQ into Kafka with Kafka Connect</title>
      <link>https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/</link>
      <pubDate>Wed, 08 Jan 2020 13:06:57 +0000</pubDate>
      <guid>https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;This was prompted by &lt;a href=&#34;https://stackoverflow.com/questions/59632068/kafka-connect-is-sending-a-malformed-json&#34;&gt;a question&lt;/a&gt; on StackOverflow to which I thought the answer would be straightforward, but turned out not to be so. And then I got a bit carried away and ended up with a nice example of how you can handle schema-less data coming from a system such as RabbitMQ and apply a schema to it.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;admonitionblock note&#34;&gt;&#xA;&lt;table&gt;&#xA;&lt;tbody&gt;&lt;tr&gt;&#xA;&lt;td class=&#34;icon&#34;&gt;&#xA;&lt;i class=&#34;fa icon-note&#34; title=&#34;Note&#34;&gt;&lt;/i&gt;&#xA;&lt;/td&gt;&#xA;&lt;td class=&#34;content&#34;&gt;&#xA;This same pattern for ingesting bytes and applying a schema will work with other connectors such as &lt;a href=&#34;https://www.confluent.io/hub/confluentinc/kafka-connect-mqtt&#34;&gt;MQTT&lt;/a&gt;&#xA;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&lt;/table&gt;&#xA;&lt;/div&gt;</description>
    </item>
  </channel>
</rss>
