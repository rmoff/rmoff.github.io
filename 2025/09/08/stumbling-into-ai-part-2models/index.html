<!DOCTYPE html>
<html lang="en">
	<head>
		<meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett">
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<title>Stumbling into AI: Part 2—Models</title>
		<link rel="alternate" type="application/rss+xml" title="RSS" href="https://rmoff.net/index.xml">
		<link rel="canonical" href="https://rmoff.net/2025/09/08/stumbling-into-ai-part-2models/">
		
		<script src="/js/copy-code.js"></script>
		
		
		
		

		
		<meta property="og:title" content="Stumbling into AI: Part 2—Models" />
		<meta property="og:type" content="article" />
		<meta property="og:image" content="https://rmoff.net/images/2025/09/h_IMG_2458.webp" />
		<meta property="og:description" content="" />
		<meta property="og:url" content="https://rmoff.net/2025/09/08/stumbling-into-ai-part-2models/" />
		<meta property="og:site_name" content="Stumbling into AI: Part 2—Models" />
		<meta name="twitter:card" content="summary_large_image" />
		<meta name="twitter:site" content="@" />


		
		<link rel="stylesheet" href="https://rmoff.net/css/tachyons.min.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/story.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/descartes.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/toc.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/retro-cards.css" />
		<link rel="stylesheet" href="https://rmoff.net/css/custom.css" />
		
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
		<link href="https://fonts.googleapis.com/css?family=Quattrocento+Sans:400,400i,700,700i|Quattrocento:400,700|Spectral:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
		
		
		<script>
			!function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey getNextSurveyStep identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
            posthog.init('phc_93NEP79Ju4xqXYWXnoLbr4HMW0Iaepj1BGOVoEXYX6P',{api_host:'https://eu.i.posthog.com', person_profiles: 'identified_only' 
                })
		</script>
		
		<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		
		<script src="https://rmoff.net/js/story.js"></script>
		<script src="https://rmoff.net/js/toc.js"></script>

	</head>
	<body class="ma0 bg-white section-post page-kind-page is-page-true ">
		
		<header class="cover bg-top" style="background-image: url('https://rmoff.net/images/2025/09/h_IMG_2458.webp'); background-position: center;">
			<div class="bg-black-30 bb bt">

				<nav class="hide-print sans-serif  border-box pa3 ph5-l">
					<a href="https://rmoff.net" title="Home">
						<link rel="preload" as="image" href="https://rmoff.net/img/repton.gif">
						<img
							src="https://rmoff.net/img/logo.jpg"
							class="w2 h2 br-100"
							alt="rmoff&#39;s random ramblings"
							onmouseover="this.src='https:\/\/rmoff.net\/img\/repton.gif';"
							onmouseout="this.src='https:\/\/rmoff.net\/img\/logo.jpg';"
						/>
					</a>
					<div class="fr h2 pv2 tr">
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net/bio">about</a>
						<a class="link f5 ml2 dim near-white" href="https://talks.rmoff.net">talks</a>
						<a class="link f5 ml2 dim near-white" href="https://bsky.app/profile/rmoff.net"><i class="fa-brands fa-bluesky"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://github.com/rmoff/"><i class="fab fa-github-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.youtube.com/c/rmoff"><i class="fab fa-youtube-square"></i></a>
						<a class="link f5 ml2 dim near-white" href="https://www.linkedin.com/in/robinmoffatt/"><i class="fab fa-linkedin"></i></a>
						<a class="link f5 ml2 dim near-white fas fa-rss-square" href="https://rmoff.net/index.xml" title="RSS Feed"></a>
						<a class="link f5 ml2 dim near-white fas fa-search" href="https://rmoff.net/search/" role="search" title="Search"></a>
					</div>
				</nav>

				<div id="hdr" class="tc-l pv4-ns pv5-l pv2 ph3 ph4-ns">
					<h1 class="near-white mt1-ns f2 fw3 mb0 mt0 lh-title">
						<span class="terminal-title">Stumbling into AI: Part 2—Models<span class="terminal-cursor"></span></span>
					</h1>
					<h2 class="near-white mt3-l mb4-l fw1 f6 f3-l measure-wide-l center lh-copy mt2 mb3">
						
						
							
								Published
								<time datetime="2025-09-08T08:10:34Z">Sep 8, 2025</time>
								<span class="display-print">by </span>
								 in <a href="https://rmoff.net/categories/ai" class="no-underline category near-white dim">AI</a>, <a href="https://rmoff.net/categories/raycast" class="no-underline category near-white dim">Raycast</a>, <a href="https://rmoff.net/categories/stumbling-into-ai" class="no-underline category near-white dim">Stumbling Into AI</a>
								<span class="display-print">at https://rmoff.net/2025/09/08/stumbling-into-ai-part-2models/</span>
							
						
					</h2>
				</div>

				
				
				
				<div class="w-100 cf hide-print">
					<a class="fr f6 ma0 pa2 link white-50 dim fas fa-camera" href="https://bsky.app/profile/rmoff.net" title="Photo Credit"></a>
				</div>
				
				

			</div>
		</header>
		
		<main role="main">
		
<div class="container-fluid docs">
  <div class="row">
    <main class="docs-content" role="main">

<article class="article">
	<div class="paragraph">
<p><em>A <a href="/categories/stumbling-into-ai">short series</a> of notes for myself as I learn more about the AI ecosystem as of September 2025.</em>
<em>The driver for all this is understanding more about Apache Flink’s <a href="https://github.com/apache/flink-agents"><strong>Flink Agents</strong></a> project, and Confluent’s <a href="https://www.confluent.io/product/streaming-agents/"><strong>Streaming Agents</strong></a>.</em></p>
</div>
<div class="paragraph">
<p>Having <a href="/2025/09/04/stumbling-into-ai-part-1mcp/">poked around MCP</a> and got a broad idea of what it is, I want to next look at Models.
What used to be as simple as &#34;<em>I used AI</em>&#34; actually boils down into several discrete areas, particularly when one starts looking at using LLMs beyond writing <a href="/images/2025/09/13d0418e1ddd2f60eef260aa512cb2a27aed080a4702fd7f01e73ef7b8ba5c2b.webp">a rap about Apache Kafka in the style of Monty Python</a> and using it to build agents (like the Flink Agents that prompted this exploration in the first place).</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/models.excalidraw.webp" alt="models.excalidraw"/></span></p>
</div>
<div class="sect1">
<h2 id="_models_large_language_ones_to_be_precise">Models (Large Language ones, to be precise)&nbsp;<a class="headline-hash" href="#_models_large_language_ones_to_be_precise">🔗</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>This is the what it’s all about right here.
Large Language Models (LLMs) are what piqued the interest of nerds outside the academic community in 2023 and the broader public a year or so later.
What used to be a &#34;<em>OMFG have you seen this</em>&#34; moment is now somewhat passé.
Of <em>course</em> I can ask my computer to write my homework assignment for me.
Of <em>course</em> I can use my phone to explain the nuances of the leg-before-wicket rule in Cricket.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/models1.excalidraw.webp" alt="models1.excalidraw"/></span></p>
</div>
<div class="paragraph">
<p>Without a model, the whole AI sandcastle collapses.
There are <a href="https://en.wikipedia.org/wiki/List_of_large_language_models">many dozens of LLMs</a>.
The most well-known ones are grouped into families and include <a href="https://platform.openai.com/docs/models">GPT</a>, <a href="https://docs.anthropic.com/en/docs/about-claude/models/overview#model-names">Claude</a>, and <a href="https://ai.google.dev/gemini-api/docs/models">Gemini</a>.
Within these there are different models, such as GPT-5, Claude 4.1, and so on.
Often these models themselves have variants, specific to certain tasks like writing software code, generating images, or understanding audio.</p>
</div>
<div class="sect2">
<h3 id="_companies">Companies&nbsp;<a class="headline-hash" href="#_companies">🔗</a> </h3>
<div class="paragraph">
<p>The big companies behind the models include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenAI (GPT)</p>
</li>
<li>
<p>Anthropic (Claude)</p>
</li>
<li>
<p>Google (Gemini)</p>
</li>
<li>
<p>Meta (Llama)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_does_it_work">How does it work?&nbsp;<a class="headline-hash" href="#_how_does_it_work">🔗</a> </h3>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Any sufficiently advanced technology is indistinguishable from magic.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>— <a href="https://en.wikipedia.org/wiki/Clarke%27s_three_laws#cite_note-:1-2">Arthur C. Clarke</a></p>
</div>
<div class="paragraph">
<p>Srsly tho, I’m not going to even pretend to try and understand how LLMs works.
Just as I couldn’t tell you how the CPU in my laptop works, but I know that it’s there and <em>waves hands</em> does stuff, the same is true for LLMs.</p>
</div>
<div class="paragraph">
<p>You give them text, they give you a response.</p>
</div>
<div class="paragraph">
<p>If you want a really good overview of LLMs, have a look at this excellent talk from one of the OGs in the LLM space, Andrej Karpathy:</p>
</div>
<div class="paragraph">

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/zjkBMFhNj_g" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

</div>
</div>
<div class="sect2">
<h3 id="_hallucinations">Hallucinations&nbsp;<a class="headline-hash" href="#_hallucinations">🔗</a> </h3>
<div class="paragraph">
<p>One of my favourite descriptions of LLMs likened them to an <strong>over-eager, hungover, intern</strong>.
They can do a lot, and know a lot of the words, but you’d never quite trust what they write.</p>
</div>
<div class="paragraph">
<p>As LLMs improve, it’s less likely you’ll get completely BS responses from them, but part of the risk is that they usually express themselves extremely confidently.
Here’s what an LLM told me about my home town:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Ilkley:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Known for its stunning lakes (Lythwaite Lake) and the Dung scale viaduct, which provides a breathtaking view of the surrounding landscape.</p>
</li>
<li>
<p>Has a rich history, including being part of the Lancashire cotton industry in the 1800s.</p>
</li>
</ul>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Sounds plausible, right?
But <code>Lythwaite Lake</code> and <code>Dung scale viaduct</code> <em>do not exist</em> (nor is there a lake or viaduct near the town).
And <a href="https://en.wikipedia.org/wiki/Wars_of_the_Roses">worse</a>, Ilkley is very much in Yorkshire, not Lancashire!</p>
</div>
<div class="paragraph">
<p>Of course, it’s easy to cherry-pick these examples.
If I ask a better model about Ilkley, it is <a href="https://en.wikipedia.org/wiki/Ilkley">completely right</a>:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Ilkley:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Known for its dramatic moorland (Ilkley Moor) and the Cow and Calf rocks, offering sweeping views over Wharfedale and inspiring the song “On Ilkla Moor Baht ’at.”</p>
</li>
<li>
<p>Has a rich history, from its Roman fort (Olicana) and medieval origins to becoming a Victorian spa town famed for hydrotherapy and elegant architecture.</p>
</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="sect2">
<h3 id="_tokens">Tokens&nbsp;<a class="headline-hash" href="#_tokens">🔗</a> </h3>
<div class="paragraph">
<p>The input and output of LLMs is measured—and in many cases, charged—on the basis of <em>tokens</em>.</p>
</div>
<div class="admonitionblock tip">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Just like the video above explaining how LLMs work, if you want to know about details of tokenisation check out this explainer:</p>
</div>
<div class="paragraph">

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/zduSFxRajkE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

</div>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>In some cases, the number of words is equivalent to the number of tokens:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight" style="color: #f8f8f2;background-color: #49483e"><code data-lang="bash"><span style="color: #f8f8f2">$ </span>ttok never gonna give you up
5</code></pre>
</div>
</div>
<div class="paragraph">
<p>but often not:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight" style="color: #f8f8f2;background-color: #49483e"><code data-lang="bash"><span style="color: #f8f8f2">$ </span>ttok apache flink
3
<span style="color: #f8f8f2">$ </span>ttok supercalifragilisticexpialidocious
11</code></pre>
</div>
</div>
<div class="paragraph">
<p>Different LLMs may use different tokenisation too.
You can use the <a href="https://github.com/simonw/ttok">ttok</a> tool (shown above) to explore tokenisation in more detail.
Some tools, such as Goose, will also show you how many tokens are used:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/441a2e716c8ec369ff81988cb0c369a67ffdfd10d292d87bd42d0c3bc65a770a.webp" alt="441a2e716c8ec369ff81988cb0c369a67ffdfd10d292d87bd42d0c3bc65a770a" width="600px"/></span></p>
</div>
<div class="paragraph">
<p>You’ll notice that as well as the token count, there’s a dollar amount next to it.
Since I’m running the model locally (using <a href="https://ollama.com/">Ollama</a>) there’s no direct cost for the invocation of it.
Where the token count matters is when you’re using remote models, like GPT or Claude.
These are <a href="https://platform.openai.com/docs/pricing?latest-pricing=standard#text-tokens">charged</a> based on the number of tokens used, often listed as a cost per 1M tokens.</p>
</div>
<div class="paragraph">
<p>Nine tokens might seem like a drop in the ocean of a million, but look at this:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/c4ad35ade6245a62812b3aa3026cd7e2765c76d781b2d08339bbbfa0923e8596.webp" alt="c4ad35ade6245a62812b3aa3026cd7e2765c76d781b2d08339bbbfa0923e8596"/></span></p>
</div>
<div class="paragraph">
<p>The same input prompt (<code>supercalifragilisticexpialidocious</code>) but somehow I just used nearly 10k tokens!
If you read my <a href="/2025/09/04/stumbling-into-ai-part-1mcp/">blog post about MCP</a> you’ll know that LLMs can make use of MCP servers (often generically referred to as &#34;tools&#34; or &#34;extensions&#34;).
They can be used to look up further information to support the user’s request (&#34;<em>what films have they rated the highest</em>&#34;), or even invoke actions (&#34;<em>book two tickets at the local cinema to see Top Gun on Monday at 8pm</em>&#34;).
So when I gave the agent the prompt <code>supercalifragilisticexpialidocious</code>, what it actually did was include information about all of the tools configured, so that the LLM could choose to use them or not—and this took up a lot of tokens, because there were several tools configured.</p>
</div>
<div class="paragraph">
<p>So if I disable the tools/MCP servers, the token count should be back to just that of the input expression?</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/1cb6fa8178df3c00a5e73f57459124f2afee02714fc43659881fd2baf3dde655.webp" alt="1cb6fa8178df3c00a5e73f57459124f2afee02714fc43659881fd2baf3dde655"/></span></p>
</div>
<div class="paragraph">
<p>Not so.
And that’s because most of the time you use an LLM you’re doing so with a particular purpose or framing, and so a <em>system prompt</em> will help focus it on what you want it to do.</p>
</div>
<div class="paragraph">
<p>For example, here is the same input, but with two different system prompts.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight" style="color: #f8f8f2;background-color: #49483e"><code data-lang="bash"><span style="color: #f8f8f2">$ </span><span style="color: #f8f8f2">echo</span> <span style="color: #e6db74">&#34;Internet&#34;</span> | <span style="color: #ae81ff">\ </span>                                        <i class="conum" data-value="1"></i><b>(1)</b>
    llm <span style="color: #f92672">-m</span> gpt-oss:latest <span style="color: #ae81ff">\</span>
        <span style="color: #f92672">-s</span> <span style="color: #e6db74">&#34;Define this word. Be concise.&#34;</span>                    <i class="conum" data-value="2"></i><b>(2)</b>
<span style="color: #66d9ef;font-weight: bold">**</span>Internet<span style="color: #66d9ef;font-weight: bold">**</span> – a global network of interconnected computers that exchange data using standardized protocols, enabling communication, information sharing, and services across the world.

<span style="color: #f8f8f2">$ </span><span style="color: #f8f8f2">echo</span> <span style="color: #e6db74">&#34;Internet&#34;</span> | <span style="color: #ae81ff">\ </span>                                        <i class="conum" data-value="1"></i><b>(1)</b>
    llm <span style="color: #f92672">-m</span> gpt-oss:latest <span style="color: #ae81ff">\</span>
        <span style="color: #f92672">-s</span> <span style="color: #e6db74">&#34;Define this word to a five year old. Be concise.&#34;</span> <i class="conum" data-value="2"></i><b>(2)</b>
The internet is like a giant invisible playground <span style="color: #66d9ef;font-weight: bold">for </span>computers. It lets them share pictures, videos, games, and messages so you can learn, play, and talk to friends from anywhere.</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>User input</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>System prompt</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>Ultimately the system prompt is just a bunch of tokens that get passed to the LLM; and that’s probably what we’re seeing in the screenshot above where the token count is higher than that of the input text alone.</p>
</div>
<div class="sect3">
<h4 id="_why_does_this_matter">Why does this matter?&nbsp;<a class="headline-hash" href="#_why_does_this_matter">🔗</a> </h4>
<div class="paragraph">
<p>Because someone has to pay for all this fun, and how many tokens you use determines how much you’ll pay.
You might be using the LLM provider’s API directly and thus directly exposed to the token cost, or you might be using a tool whose authoring company pays the API bills and in turn will cap your invocation through the tool at a certain point.
You might think a million tokens sounds a lot, but this can easily get burnt through with things like:
* MCP usage, in which the output from an API call might be a long JSON document - and often multiple API calls will get strung together to satisfy a single user request
* Coding help, when the LLM will have to be given reams of code across potentially many files</p>
</div>
</div>
<div class="sect3">
<h4 id="_context_window">Context Window&nbsp;<a class="headline-hash" href="#_context_window">🔗</a> </h4>
<div class="paragraph">
<p>When you interact with an LLM, it can &#39;remember&#39; what you’ve told it—and what it’s told you—before.
This is called the context window, and is measured in tokens.</p>
</div>
<div class="paragraph">
<p>Generally, the smaller the context window the faster a model will return, compared to a larger window.
Once the window is full you’ll see the model start to &#34;forget&#34; things, or just refuse to run.</p>
</div>
<div class="paragraph">
<p>Some AI tools will expose the current context window size, like Goose:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/e40e0845d3e3e37bd2448014a136da8709c6ea48287465e4d65f24cb45d98b08.webp" alt="e40e0845d3e3e37bd2448014a136da8709c6ea48287465e4d65f24cb45d98b08"/></span></p>
</div>
<div class="paragraph">
<p>You can also sometimes &#39;compact&#39; the context window, which will in effect summarise everything &#34;discussed&#34; so far with the LLM and start a new conversation.
Since the summary will be shorter than the dialogue from which it was created, the context window will be smaller.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_weights_parameters">Weights &amp; Parameters&nbsp;<a class="headline-hash" href="#_weights_parameters">🔗</a> </h3>
<div class="paragraph">
<p>After many years working with open source software, I was puzzled by the new terminology that I started to hear in relation to LLMs: &#34;Open Weight&#34;.</p>
</div>
<div class="paragraph">
<p>In terms of software alone, open source has <a href="https://opensource.org/osd">a strict set of definitions</a>, but one of the key ones from an end-user point of view is that I can access all the source code and in theory could build the program from scratch myself.</p>
</div>
<div class="paragraph">
<p>When it comes to LLMs it’s not quite so straightforward.
Watching <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Andrej Karpathy’s video</a> I’ve picked up the basic understanding that you’ve got the mega-expensive pre-training in which vast swathes of the internet and beyond are boiled down into a model.
He <a href="https://youtu.be/zjkBMFhNj_g?feature=shared&amp;t=258">gives the example</a> of Llama 2 costing $2M and taking 12 days to train.
The size of the model is defined by the number of parameters.
Broadly, the greater the number of parameters, the greater the accuracy of the LLM.
Fewer parameters means less computing power needed and potentially less accurate results—but depending on what you’re asking the LLM to do can sometimes be a good tradeoff.</p>
</div>
<div class="paragraph">
<p>Out of this pre-training is then a core model which is then trained further in what’s known as fine-tuning.
This is cheaper, and faster, to do.
It can be used to specialise the model towards particular tasks or domains.</p>
</div>
<div class="paragraph">
<p>Companies approach the sharing of models in different ways.
Some keep absolutely everything to themselves, giving the end user simply an API endpoint or web page with which to interact with the model that they’ve built.
Others will perhaps share the pre-trained model (but not the source data or code that went into training it), giving people the opportunity to then train it further with their own fine-tuning.
This is the &#34;Open Weight&#34; approach.</p>
</div>
<div class="paragraph">
<p>You can read more about <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama 4</a> and <a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">Llama 3</a> on the Meta AI blog, as well as <a href="https://openai.com/index/introducing-gpt-oss/">GPT-OSS from OpenAI</a>.
This post on Reddit is also interesting: <a href="https://www.reddit.com/r/LocalLLaMA/comments/1iw1xn7/the_paradox_of_open_weights_but_closed_source/">The Paradox of Open Weights, but Closed Source</a></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_clients">Clients&nbsp;<a class="headline-hash" href="#_clients">🔗</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>OK, so we’ve got our models.
They come in different shapes and sizes, and some are better than others.</p>
</div>
<div class="paragraph">
<p>To use an LLM, one needs a client.
Clients take various forms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Desktop and Web clients, specific to the AI company developing a family of LLMs.
These include <a href="https://chatgpt.com/">ChatGPT</a> and <a href="https://claude.ai/download">Claude</a>.</p>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/claudeandchatgpt.webp" alt="claudeandchatgpt"/></span></p>
</div>
</li>
<li>
<p>Tools built around AI functionality (e.g. Cursor) or with it bolted on whether you want it or not (<em>i.e. every bloody application out there these days</em> 😜).
Some of these will give you access to a set of models, whilst others will mask the model itself and just call it <del>&#34;magic&#34;</del>&#34;AI&#34;</p>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/79ab812d942ed692f1dc202e96075596a5578951d89e2f9c76123284b38b01e7.webp" alt="79ab812d942ed692f1dc202e96075596a5578951d89e2f9c76123284b38b01e7" width="600px"/></span></p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/cursor00.webp" alt="cursor00" width="600px"/></span></p>
</div>
</li>
<li>
<p>Model-agnostic interfaces, including:</p>
<div class="ulist">
<ul>
<li>
<p><a href="https://manual.raycast.com/ai">Raycast</a>, which as part of its application gives the user the option to interact with dozens of different LLMs</p>
</li>
<li>
<p>Simon Willison’s <a href="https://llm.datasette.io/en/stable/"><code>llm</code> CLI</a>:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight" style="color: #f8f8f2;background-color: #49483e"><code data-lang="bash"><span style="color: #75715e;font-style: italic"># Use GPT-OSS model</span>
<span style="color: #f8f8f2">$ </span>llm <span style="color: #f92672">-m</span> gpt-oss:latest <span style="color: #e6db74">&#39;What year was the world wide web invented? Be concise&#39;</span>
1989.

<span style="color: #75715e;font-style: italic"># Use Llama 3.1 model</span>
<span style="color: #f8f8f2">$ </span>llm <span style="color: #f92672">-m</span> llama3.1:latest <span style="color: #e6db74">&#39;What year was the world wide web invented? Be concise&#39;</span>
The World Wide Web <span style="color: #f92672;font-weight: bold">(</span>WWW<span style="color: #f92672;font-weight: bold">)</span> was invented <span style="color: #66d9ef;font-weight: bold">in </span>1989 by Tim Berners-Lee.</code></pre>
</div>
</div>
</li>
<li>
<p><a href="https://block.github.io/goose/">Goose</a>, which is an <em>an extensible open source AI agent</em>.
I’ve not used it a ton yet but at first glance it at least gives you a UI and CLI for interacting with LLMs and MCPs:</p>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/bb72744c933acfc7a85a9127f70f8161872462e7f95648fa66d47119718de9c0.webp" alt="bb72744c933acfc7a85a9127f70f8161872462e7f95648fa66d47119718de9c0"/></span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_where_the_model_runs">Where the model runs&nbsp;<a class="headline-hash" href="#_where_the_model_runs">🔗</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>Running LLMs takes some grunt, which is why they’re particularly well suited to being provided as hosted services since someone else can absorb the cost of provisioning the expensive hardware necessary to run them.</p>
</div>
<div class="paragraph">
<p>There are 3 broad options for getting access to running a model (assuming you’re using a client that has pluggable models; if you’re using something like ChatGPT then you just access the models through that alone and they run the models for you):</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>My cloud</strong></p>
<div class="ulist">
<ul>
<li>
<p>My laptop, my on-premises servers with some big fat GPUs, etc</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Their cloud</strong></p>
<div class="ulist">
<ul>
<li>
<p>Servers run by the model publishers themselves; <a href="https://platform.openai.com/docs/overview">OpenAI</a>, <a href="https://www.anthropic.com/api">Anthropic</a>, etc.
Usually they’ll only offer access to their own models.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Someone else’s cloud</strong></p>
<div class="ulist">
<ul>
<li>
<p>Models hosted by 3rd party providers, including <a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a>, <a href="https://ai.azure.com/">Azure AI Foundry</a>, <a href="https://openrouter.ai/">OpenRouter</a>, etc.
The big providers like Azure and Amazon will usually have partnerships with some model companies and provide access to their models, whilst others may only offer access to publicly-available models (basically what you or I could run on our own locally, but with the necessary hardware behind it to perform well).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>I’ve found <a href="https://openrouter.ai/">OpenRouter</a> particularly useful as it gives you access to free models, and the ability to run the same prompt across different models:</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/images/2025/09/0371e711472c4996419299c514fe5027c8963e680292df55dc7aafb1815bb2be.webp" alt="0371e711472c4996419299c514fe5027c8963e680292df55dc7aafb1815bb2be"/></span></p>
</div>
<div class="paragraph">
<p>It also has a good catalog of models and details of which provider offers them.</p>
</div>
<div class="paragraph">
<p>Finally, OpenRouter is a pragmatic way to make use of the free models; <code>gpt-oss:120b</code> might sound nice and make claims about being as good as some of the closed-weights GPT models, but it’s irrelevant if it won’t run locally.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_routers">Routers&nbsp;<a class="headline-hash" href="#_routers">🔗</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>The final piece of the puzzle, for now, is <strong>routers</strong>.</p>
</div>
<div class="paragraph">
<p>Given that there are multiple models, and multiple places in which to run them, how do you decide which one to call?
Different models are better at different tasks; or put another way, the big expensive models are usually good at everything but you may get a faster or cheaper (or perhaps even just more accurate) response from a specialised model.
You could take the artisanal approach, and curate your model access based on your in-depth understanding of all models each time you want to call one.</p>
</div>
<div class="paragraph">
<p>Alternatively, you use a router, which is a model itself and one that is specialised in understanding LLMs strengths, analysing the type of workload you want to run, and routing it to the most suitable one.</p>
</div>
<div class="paragraph">
<p>Some routers include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OpenRouter’s <a href="https://openrouter.ai/openrouter/auto">AutoRouter</a></p>
</li>
<li>
<p><a href="https://github.com/SomeOddCodeGuy/WilmerAI">WilmerAI</a></p>
</li>
<li>
<p><a href="https://www.aurelio.ai/semantic-router">Semantic Router</a></p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
You don’t have to use a router, but you’ll possibly see mention of them which is why I’m mentioning them here.
Also, because I got confused by <code>OpenRouter</code> also being a service provider, not just a router :)
</td>
</tr>
</tbody></table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_addendum_there_are_models_and_then_there_are_models_a_k_a_not_all_models_are_llms">Addendum: There are Models, and then there are <em>Models</em> (a.k.a. not all Models are LLMs)&nbsp;<a class="headline-hash" href="#_addendum_there_are_models_and_then_there_are_models_a_k_a_not_all_models_are_llms">🔗</a> </h2>
<div class="sectionbody">
<div class="paragraph">
<p>As I wrote <a href="/2025/09/12/stumbling-into-ai-part-3rag/">the third part</a> in this little voyage of discovery I realised that my understanding of models—as I wrote about them in this article—was incomplete.</p>
</div>
<div class="paragraph">
<p>There are different types of model, and there are different purposes to which a model is put.</p>
</div>
<div class="paragraph">
<p>LLMs as I’ve discussed in this blog post are <em>generative</em>.
They create (generate) new material based on their training over large datasets of text.
There are other <em>generative</em> models that are not LLMs.
These include those you might have also heard of like <a href="https://openai.com/index/dall-e-2/">DALL-E</a> and <a href="https://www.midjourney.com/home">Midjourney</a>, for generating images.
There are also models for generating <a href="https://huggingface.co/microsoft/VibeVoice-1.5B">speech</a>, <a href="https://ace-step.github.io/#RapMachine">music</a>, and <a href="https://huggingface.co/alibaba-pai/Wan2.2-VACE-Fun-A14B/blob/main/README_en.md#video-result">video</a>.</p>
</div>
<div class="paragraph">
<p>Other models that aren’t generative do tasks such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Creating embeddings (as used in <a href="/2025/09/12/stumbling-into-ai-part-3rag/">RAG</a>)</p>
</li>
<li>
<p>Sentiment analysis</p>
</li>
<li>
<p>Anomaly detection</p>
</li>
<li>
<p>Forecasting and prediction</p>
</li>
<li>
<p>Analysing video, image, or audio, e.g.</p>
<div class="ulist">
<ul>
<li>
<p>Detecting objects</p>
</li>
<li>
<p>Transcribing speech</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Regardless of the type, what you call a model to get it to do something, it’s called <em>inference</em>.
</td>
</tr>
</tbody></table>
</div>
</div>
</div>
	<hr>
	<div style="background-color: rgba(204, 234, 255, 0.25); margin-bottom:50px;margin-top:50px;padding: 20px; border-width: 2px; border-style: solid; border-color: darkorange;">
	
		<script src="https://giscus.app/client.js"
				data-repo="rmoff/rmoff-blog"
				data-repo-id="MDEwOlJlcG9zaXRvcnkxNTE3NDg2MTE="
				data-category="Announcements"
				data-category-id="DIC_kwDOCQuAA84CvP5T"
				data-mapping="pathname"
				data-strict="1"
				data-reactions-enabled="1"
				data-emit-metadata="0"
				data-input-position="bottom"
				data-theme="light"
				data-lang="en"
				crossorigin="anonymous"
				async>
		</script>
		
	</div>
		
				<div class="hide-print sans-serif f6 f5-l mt5 ph3 pb6 center nested-copy-line-height lh-copy nested-links mw-100 measure-wide">
		<div class="about-the-author">
		
			
			
				
					<hr>
<p><img src="/images/2018/05/ksldn18-01.jpg" alt="Robin Moffatt"></p>
<p><a href="https://bsky.app/profile/rmoff.net"><b class="fa-brands fa-bluesky"></b></a>  <em>Robin Moffatt works on the DevRel team at Confluent. He likes writing about himself in the third person, eating good breakfasts, and drinking good beer.</em></p>

				
			
		
		</div>
		
	</div>

		

</article>
      </main>
    
      
      <div class="docs-toc">
        <ul class="nav toc-top">
          <li><a href="#" id="back_to_top" class="docs-toc-title">Contents</a></li>
        </ul>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#_models_large_language_ones_to_be_precise">Models (Large Language ones, to be precise)</a>
      <ul>
        <li><a href="#_companies">Companies</a></li>
        <li><a href="#_how_does_it_work">How does it work?</a></li>
        <li><a href="#_hallucinations">Hallucinations</a></li>
        <li><a href="#_tokens">Tokens</a></li>
        <li><a href="#_weights_parameters">Weights &amp; Parameters</a></li>
      </ul>
    </li>
    <li><a href="#_clients">Clients</a></li>
    <li><a href="#_where_the_model_runs">Where the model runs</a></li>
    <li><a href="#_routers">Routers</a></li>
    <li><a href="#_addendum_there_are_models_and_then_there_are_models_a_k_a_not_all_models_are_llms">Addendum: There are Models, and then there are <em>Models</em> (a.k.a. not all Models are LLMs)</a></li>
  </ul>
</nav>
      </div>
      
      <div class="toc-mobile-label">TABLE OF CONTENTS</div>
      
    
    </div>
  </div>
</div>


		</main>
		
		
		<footer class="hide-print sans-serif f6 fw1 bg-black near-white bottom-0 w-100 pa3" role="contentinfo">
			<p class="w-50 fr tr">
			<a class="no-underline near-white" href="https://github.com/xaprb/story"><img class="dib" title="Made with Hugo and Story" alt="Story logo" src="https://rmoff.net/img/story-logo-white.svg" style="width: 1.5rem; height: 1.5rem" /></a>
			</p>
			<p class="w-50 near-white">
				&copy; 2025 
			</p>
		</footer>
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-75492960-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

	</body>
</html>
