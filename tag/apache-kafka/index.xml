<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Kafka on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/tag/apache-kafka/</link>
    <description>Recent content in Apache Kafka on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Jun 2018 11:35:20 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/tag/apache-kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      <guid>https://rmoff.net/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://cnfl.io/syslogs-filtering&#34;&gt;this article&lt;/a&gt; I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily &lt;a href=&#34;https://www.confluent.io/blog/real-time-syslog-processing-apache-kafka-ksql-enriching-events-with-external-data/&#34;&gt;enrich streams&lt;/a&gt;. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.&lt;/p&gt;&#xA;&lt;p&gt;The data came from my home &lt;a href=&#34;https://www.ubnt.com/&#34;&gt;Ubiquiti&lt;/a&gt; router, and took two forms:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Tue, 27 Mar 2018 18:52:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>&lt;p&gt;&lt;em&gt;Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;mongodb-config---enabling-replica-sets&#34;&gt;MongoDB config - enabling replica sets&lt;/h3&gt;&#xA;&lt;p&gt;For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:&lt;/p&gt;&#xA;&lt;p&gt;Docs: &lt;a href=&#34;https://docs.mongodb.com/manual/replication/&#34;&gt;Replication&lt;/a&gt; / &lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/convert-standalone-to-replica-set/&#34;&gt;Convert a Standalone to a Replica Set&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Stop Mongo:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo service mongod stop&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add replica set config to &lt;code&gt;/etc/mongod.conf&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Do We Need Streaming ETL?</title>
      <link>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/06/why-do-we-need-streaming-etl/</guid>
      <description>&lt;p&gt;&lt;em&gt;(This is an expanded version of the intro to an article I posted over on the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog&lt;/a&gt;. Here I get to be as verbose as I like &lt;code&gt;;)&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;My first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe. From there it was checked and loaded, and then reports generated on it. This was nearly twenty years ago as my greying beard will attest—and not a lot has changed in the large majority of reporting and analytics systems since then. COBOL is maybe less common, but what has remained constant is the batch-driven nature of processing. Sometimes batches are run more frequently, and get given fancy names like intra-day ETL or even micro-batching. But batch processing it is, and as such latency is built into our reporting &lt;em&gt;by design&lt;/em&gt;. When we opt for batch processing we voluntarily inject delays into the availability of data to our end users. Much better is to build our systems around a streaming platform instead.&lt;/p&gt;</description>
    </item>
    <item>
      <title>HOWTO: Oracle GoldenGate &#43; Apache Kafka &#43; Schema Registry &#43; Swingbench</title>
      <link>https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/</link>
      <pubDate>Thu, 01 Feb 2018 23:15:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/02/01/howto-oracle-goldengate--apache-kafka--schema-registry--swingbench/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is the detailed step-by-step if you want to recreate the process I describe in the &lt;a href=&#34;https://www.confluent.io/blog/ksql-in-action-real-time-streaming-etl-from-oracle-transactional-data&#34;&gt;Confluent blog here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I used Oracle&amp;rsquo;s &lt;a href=&#34;http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html&#34;&gt;Oracle Developer Days VM&lt;/a&gt;, which comes preinstalled with Oracle 12cR2. You can see the notes on &lt;a href=&#34;https://rmoff.net/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/&#34;&gt;how to do this here&lt;/a&gt;. These notes take you through installing and configuring:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Swingbench, to create a sample &amp;ldquo;Order Entry&amp;rdquo; schema and simulate events on the Oracle database&lt;/li&gt;&#xA;&lt;li&gt;Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith)&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I&amp;rsquo;m using Oracle GoldenGate 12.3.1 which includes the Kafka Connect handler as part of its distribution. A connector for earlier versions can be &lt;a href=&#34;http://www.oracle.com/technetwork/middleware/goldengate/oracle-goldengate-exchange-3805527.html&#34;&gt;found here&lt;/a&gt;. Some of the syntax may differ in the configuration below - if you hit problems then check out &lt;a href=&#34;&#34;&gt;an article that I wrote&lt;/a&gt; with an earlier version of the tool.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;OGG &lt;code&gt;extract&lt;/code&gt; from the Order Entry schema&lt;/li&gt;&#xA;&lt;li&gt;Confluent Platform&lt;/li&gt;&#xA;&lt;li&gt;KSQL&lt;/li&gt;&#xA;&lt;li&gt;Elasticsearch&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;From this point, I&amp;rsquo;ll now walk through configuring OGG-BD with the Kafka Connect handler&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
