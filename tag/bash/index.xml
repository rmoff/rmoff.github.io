<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bash on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/tag/bash/</link>
    <description>Recent content in Bash on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Dec 2018 14:50:45 +0000</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/tag/bash/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kafka Connect CLI tricks</title>
      <link>https://rmoff.github.io/2018/12/03/kafka-connect-cli-tricks/</link>
      <pubDate>Mon, 03 Dec 2018 14:50:45 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/12/03/kafka-connect-cli-tricks/</guid>
      <description>I do lots of work with Kafka Connect, almost entirely in Distributed modeâ€”even just with 1 node -&amp;gt; makes scaling out much easier when/if needed. Because I&amp;rsquo;m using Distributed mode, I use the Kafka Connect REST API to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.</description>
    </item>
    
    <item>
      <title>Simple export/import of Data Sources in Grafana</title>
      <link>https://rmoff.github.io/2017/08/08/simple-export-import-of-data-sources-in-grafana/</link>
      <pubDate>Tue, 08 Aug 2017 19:32:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2017/08/08/simple-export-import-of-data-sources-in-grafana/</guid>
      <description> Grafana API Reference
Export all Grafana data sources to data_sources folder mkdir -p data_sources &amp;amp;&amp;amp; curl -s &amp;quot;http://localhost:3000/api/datasources&amp;quot; -u admin:admin|jq -c -M &#39;.[]&#39;|split -l 1 - data_sources/  This exports each data source to a separate JSON file in the data_sources folder.
Load data sources back in from folder This submits every file that exists in the data_sources folder to Grafana as a new data source definition.
for i in data_sources/*; do \ curl -X &amp;quot;POST&amp;quot; &amp;quot;http://localhost:3000/api/datasources&amp;quot; \ -H &amp;quot;Content-Type: application/json&amp;quot; \ --user admin:admin \ --data-binary @$i done  </description>
    </item>
    
    <item>
      <title>Streaming data to InfluxDB from any bash command</title>
      <link>https://rmoff.github.io/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</link>
      <pubDate>Sat, 27 Feb 2016 21:05:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/</guid>
      <description>InfluxDB is a great time series database, that&amp;rsquo;s recently been rebranded as part of the &amp;ldquo;TICK&amp;rdquo; stack, including data collectors, visualisation, and ETL/Alerting. I&amp;rsquo;ve yet to really look at the other components, but InfluxDB alone works just great with my favourite visualisation/analysis tool for time series metrics, Grafana.
Getting data into InfluxDB is easy, with many tools supporting the native InfluxDB line input protocol, and those that don&amp;rsquo;t often supporting the carbon protocol (from Graphite), which InfluxDB also supports (along with others).</description>
    </item>
    
  </channel>
</rss>