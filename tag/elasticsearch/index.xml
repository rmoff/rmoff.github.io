<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Elasticsearch on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/tag/elasticsearch/</link>
    <description>Recent content in Elasticsearch on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Oct 2018 15:22:51 +0000</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/tag/elasticsearch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Streaming geopoint data from Kafka to Elasticsearch</title>
      <link>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</link>
      <pubDate>Fri, 05 Oct 2018 15:22:51 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</guid>
      <description>&lt;p&gt;Using the &lt;a href=&#34;https://www.confluent.io/connector/kafka-connect-elasticsearch/&#34;&gt;Elasticsearch Kafka Connect connector&lt;/a&gt; to stream events from a Kafka topic to Elasticsearch.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Window Timestamps in KSQL / Integration with Elasticsearch</title>
      <link>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</link>
      <pubDate>Mon, 03 Sep 2018 16:16:30 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/</guid>
      <description>KSQL provides the ability to create windowed aggregations. For example, count the number of messages in a 1 minute window, grouped by a particular column:
CREATE TABLE RATINGS_BY_CLUB_STATUS AS \ SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \ FROM RATINGS_WITH_CUSTOMER_DATA \ WINDOW TUMBLING (SIZE 1 MINUTES) \ GROUP BY CLUB_STATUS;  How KSQL, and Kafka Streams, stores the window timestamp associated with an aggregate, has recently changed. See #1497 for details.
Whereas previously the Kafka message timestamp (accessible through the KSQL ROWTIME system column) stored the start of the window for which the aggregate had been calculated, this changed in July 2018 to instead be the timestamp of the latest message to update that aggregate value.</description>
    </item>
    
    <item>
      <title>Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch</title>
      <link>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</link>
      <pubDate>Sun, 17 Jun 2018 11:35:20 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/</guid>
      <description>In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we&amp;rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana—and how KSQL again came into use for building some stream processing as a result of the discovery made.
The data came from my home Ubiquiti router, and took two forms:</description>
    </item>
    
    <item>
      <title>Streaming data from Kafka into Elasticsearch</title>
      <link>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
      <pubDate>Tue, 06 Mar 2018 22:21:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
      <description>This article is part of a series exploring Streaming ETL in practice. You can read about setting up the ingest of realtime events from a standard Oracle platform, and building streaming ETL using KSQL.
This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.</description>
    </item>
    
    <item>
      <title>Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion</title>
      <link>https://rmoff.github.io/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</link>
      <pubDate>Fri, 13 May 2016 05:45:19 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/</guid>
      <description>Yesterday I wrote about Monitoring Logstash Ingest Rates with InfluxDB and Grafana, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.
Mark Walkom reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it&amp;rsquo;ll integrate back into the Elastic stack:
@rmoff nice, LS 5.0 will have APIs exposing metrics too. they’ll be integrated back into Marvel/Monitoring!</description>
    </item>
    
    <item>
      <title>Using R to Denormalise Data for Analysis in Kibana</title>
      <link>https://rmoff.github.io/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</link>
      <pubDate>Sun, 24 Apr 2016 12:22:12 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/</guid>
      <description>Kibana is a tool from Elastic that makes analysis of data held in Elasticsearch really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a blog post last year, taking a CSV file and loading it into Elasticsearch via Logstash.</description>
    </item>
    
    <item>
      <title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
      <link>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
      <pubDate>Thu, 14 Apr 2016 22:51:43 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
      <description>Recently added to the oracledi project over at java.net is an adaptor enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently over at the Elastic blog.
Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (here and here for archives), as well as spoken about - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with.</description>
    </item>
    
    <item>
      <title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
      <link>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
      <pubDate>Tue, 12 Apr 2016 21:50:46 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
      <description>I&amp;rsquo;ve recently been playing around with the ELK stack (now officially known as the Elastic stack) collecting data from an IRC channel with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and analysing it with Kibana. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.
As I wrote about last year, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>My latest IRC client : Kibana</title>
      <link>https://rmoff.github.io/2016/03/24/my-latest-irc-client-kibana/</link>
      <pubDate>Thu, 24 Mar 2016 21:38:02 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/03/24/my-latest-irc-client-kibana/</guid>
      <description>OK, maybe that&amp;rsquo;s not entirely true. But my read-only client, certainly.
I was perusing the Logstash input plugins recently when I noticed that there was one for IRC. Being a fan of IRC and a regular on the #obihackers channel, I thought this could be fun and yet another great example of how easy the Elastic stack is to work with.
Installation is a piece of cake:
wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip wget https://download.</description>
    </item>
    
    <item>
      <title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
      <link>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
      <pubDate>Wed, 16 Mar 2016 22:01:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
      <description>The Oracle by Example (ObE) here demonstrating how to use Goldengate to replicate transactions big data targets such as HDFS is written for the BigDataLite 4.2.1, and for me didn&amp;rsquo;t work on the current latest version, 4.4.0.
The OBE (and similar Hands On Lab PDF) assume the presence of pmov.prm and pmov.properties in /u01/ogg/dirprm/. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, emov.
Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.</description>
    </item>
    
  </channel>
</rss>