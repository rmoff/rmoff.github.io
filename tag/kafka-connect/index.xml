<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka Connect on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/tag/kafka-connect/</link>
    <description>Recent content in Kafka Connect on rmoff&#39;s random ramblings</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Dec 2018 22:00:55 +0000</lastBuildDate>
    <atom:link href="https://rmoff.net/tag/kafka-connect/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Docker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka</title>
      <link>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</link>
      <pubDate>Sat, 15 Dec 2018 22:00:55 +0000</pubDate>
      <guid>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</guid>
      <description>&lt;p&gt;A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad‚Ä¶how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect CLI tricks</title>
      <link>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</link>
      <pubDate>Mon, 03 Dec 2018 14:50:45 +0000</pubDate>
      <guid>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</guid>
      <description>&lt;p&gt;I do lots of work with Kafka Connect, almost entirely in &lt;a href=&#34;https://docs.confluent.io/current/connect/concepts.html#distributed-workers&#34;&gt;Distributed mode&lt;/a&gt;‚Äîeven just with 1 node -&amp;gt; makes scaling out much easier when/if needed. Because I&amp;rsquo;m using Distributed mode, I use the &lt;a href=&#34;https://docs.confluent.io/current/connect/references/restapi.html&#34;&gt;Kafka Connect REST API&lt;/a&gt; to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;m showing the commands split with a line continuation character (&lt;code&gt;\&lt;/code&gt;) but you can of course run them on a single line. You might also choose to get fancy and set the Connect host and port as environment variables etc, but I leave that as an exercise for the reader :)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect and Oracle data types</title>
      <link>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</link>
      <pubDate>Mon, 21 May 2018 08:59:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://docs.confluent.io/current/connect/connect-jdbc/docs/source_connector.html&#34;&gt;Kafka Connect JDBC Connector&lt;/a&gt; by default does not cope so well with:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;NUMBER&lt;/code&gt; columns with no defined precision/scale. You may end up with apparent junk (&lt;code&gt;bytes&lt;/code&gt;) in the output, or just errors.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;TIMESTAMP WITH LOCAL TIME ZONE&lt;/code&gt;. Throws &lt;code&gt;JDBC type -102 not currently supported&lt;/code&gt; warning in the log.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Read more about &lt;code&gt;NUMBER&lt;/code&gt; data type in the &lt;a href=&#34;https://docs.oracle.com/database/121/SQLRF/sql_elements001.htm#SQLRF002220&#34;&gt;Oracle docs&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;tldr--how-do-i-make-it-work&#34;&gt;tl;dr : How do I make it work?&lt;/h3&gt;&#xA;&lt;p&gt;There are several options:&lt;/p&gt;&#xA;&lt;h4 id=&#34;new-in-confluent-platform-411--numericmapping&#34;&gt;New in Confluent Platform 4.1.1 : &lt;code&gt;numeric.mapping&lt;/code&gt;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In the connector configuration, set &lt;code&gt;&amp;quot;numeric.mapping&amp;quot;:&amp;quot;best_fit&amp;quot;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;New in Confluent Platform 4.1.1 (&lt;a href=&#34;https://docs.confluent.io/current/connect/connect-jdbc/docs/source_config_options.html#database&#34;&gt;Doc&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;avoid-the-problem-in-the-first-place&#34;&gt;Avoid the problem in the first place&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Change the DDL of the source object. For example:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;refine the &lt;code&gt;NUMBER&lt;/code&gt; &amp;rsquo;s precision and scale&lt;/li&gt;&#xA;&lt;li&gt;Use a &lt;code&gt;TIMESTAMP&lt;/code&gt; type that is supported&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;cast-the-datatypes-in-the-query&#34;&gt;CAST the datatypes in the &lt;code&gt;query&lt;/code&gt;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Pull from the object directly, and use &lt;code&gt;query&lt;/code&gt; in the JDBC connector (instead of &lt;code&gt;table.whitelist&lt;/code&gt;)‚Äîand cast the columns appropriately:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Tue, 27 Mar 2018 18:52:00 +0000</pubDate>
      <guid>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>&lt;p&gt;&lt;em&gt;Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;mongodb-config---enabling-replica-sets&#34;&gt;MongoDB config - enabling replica sets&lt;/h3&gt;&#xA;&lt;p&gt;For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:&lt;/p&gt;&#xA;&lt;p&gt;Docs: &lt;a href=&#34;https://docs.mongodb.com/manual/replication/&#34;&gt;Replication&lt;/a&gt; / &lt;a href=&#34;https://docs.mongodb.com/manual/tutorial/convert-standalone-to-replica-set/&#34;&gt;Convert a Standalone to a Replica Set&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Stop Mongo:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rmoff@proxmox01 ~&amp;gt; sudo service mongod stop&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add replica set config to &lt;code&gt;/etc/mongod.conf&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oracle GoldenGate / Kafka Connect Handler troubleshooting</title>
      <link>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</link>
      <pubDate>Tue, 12 Sep 2017 21:55:16 +0000</pubDate>
      <guid>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</guid>
      <description>&lt;p&gt;The Replicat was kapput:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GGSCI (localhost.localdomain) 3&amp;gt; info rkconnoe&#xA;&#xA;REPLICAT   RKCONNOE  Last Started 2017-09-12 17:06   Status ABENDED&#xA;Checkpoint Lag       00:00:00 (updated 00:46:34 ago)&#xA;Log Read Checkpoint  File /u01/app/ogg/dirdat/oe000000&#xA;                     First Record  RBA 0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So checking the OGG error log &lt;code&gt;ggserr.log&lt;/code&gt; showed&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2017-09-12T17:06:17.572-0400  ERROR   OGG-15051  Oracle GoldenGate Delivery, rkconnoe.prm:  Java or JNI exception:&#xA;                              oracle.goldengate.util.GGException: Error detected handling operation added event.&#xA;2017-09-12T17:06:17.572-0400  ERROR   OGG-01668  Oracle GoldenGate Delivery, rkconnoe.prm:  PROCESS ABENDING.&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So checking the replicat log &lt;code&gt;dirrpt/RKCONNOE_info_log4j.log&lt;/code&gt; showed:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kafka Connect - JsonDeserializer with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields</title>
      <link>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</link>
      <pubDate>Wed, 06 Sep 2017 12:00:25 +0000</pubDate>
      <guid>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</guid>
      <description>&lt;p&gt;An error that I see coming up frequently in the Kafka Connect community (e.g. &lt;a href=&#34;https://groups.google.com/forum/#!forum/confluent-platform&#34;&gt;mailing list&lt;/a&gt;, &lt;a href=&#34;https://slackpass.io/confluentcommunity&#34;&gt;Slack group&lt;/a&gt;, &lt;a href=&#34;https://stackoverflow.com/questions/tagged/apache-kafka-connect&#34;&gt;StackOverflow&lt;/a&gt;) is:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;JsonDeserializer with schemas.enable requires &amp;quot;schema&amp;quot; and &amp;quot;payload&amp;quot; fields and may not contain additional fields&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;No fields found using key and value schemas for table: foo-bar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;You can see an explanation, and solution, for the issue in my StackOverflow answer here: &lt;a href=&#34;https://stackoverflow.com/a/45940013/350613&#34;&gt;https://stackoverflow.com/a/45940013/350613&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;If you&amp;rsquo;re using &lt;code&gt;schemas.enable&lt;/code&gt; in the Connector configuration, you must have &lt;code&gt;schema&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt; as the root-level elements of your JSON message (&#xA;Which is pretty much verbatim what the error says üòÅ), like this:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oracle GoldenGate -&gt; Kafka Connect - &#34;Failed to serialize Avro data&#34;</title>
      <link>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</link>
      <pubDate>Tue, 29 Nov 2016 22:04:38 +0000</pubDate>
      <guid>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</guid>
      <description>&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; &lt;em&gt;Make sure that &lt;code&gt;key.converter.schema.registry.url&lt;/code&gt; and &lt;code&gt;value.converter.schema.registry.url&lt;/code&gt; are specified, and that there are no trailing whitespaces.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve been building on &lt;a href=&#34;https://www.confluent.io/blog/streaming-data-oracle-using-oracle-goldengate-kafka-connect/&#34;&gt;previous work&lt;/a&gt; I&amp;rsquo;ve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the &lt;a href=&#34;https://java.net/projects/oracledi/downloads/directory/GoldenGate/Oracle%20GoldenGate%20Adapter%20for%20Kafka%20Connect&#34;&gt;sample configuration&lt;/a&gt; gives.&lt;/p&gt;&#xA;&lt;p&gt;Simply changing the Kafka Connect OGG configuration file (&lt;code&gt;confluent.properties&lt;/code&gt;) from&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;value.converter=org.apache.kafka.connect.json.JsonConverter&#xA;key.converter=org.apache.kafka.connect.json.JsonConverter&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;to&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
