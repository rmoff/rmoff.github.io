<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafka connect on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.net/tag/kafka-connect/</link>
    <description>Recent content in kafka connect on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Dec 2018 22:00:55 +0000</lastBuildDate><atom:link href="https://rmoff.net/tag/kafka-connect/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Docker Tips and Tricks with Kafka Connect, ksqlDB, and Kafka</title>
      <link>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</link>
      <pubDate>Sat, 15 Dec 2018 22:00:55 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-kafka-connect-ksqldb-and-kafka/</guid>
      <description>A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad‚Ä¶how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.</description>
    </item>
    
    <item>
      <title>Kafka Connect CLI tricks</title>
      <link>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</link>
      <pubDate>Mon, 03 Dec 2018 14:50:45 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/12/03/kafka-connect-cli-tricks/</guid>
      <description>I do lots of work with Kafka Connect, almost entirely in Distributed mode‚Äîeven just with 1 node -&amp;gt; makes scaling out much easier when/if needed. Because I&amp;rsquo;m using Distributed mode, I use the Kafka Connect REST API to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.</description>
    </item>
    
    <item>
      <title>Kafka Connect and Oracle data types</title>
      <link>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</link>
      <pubDate>Mon, 21 May 2018 08:59:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/05/21/kafka-connect-and-oracle-data-types/</guid>
      <description>The Kafka Connect JDBC Connector by default does not cope so well with:
NUMBER columns with no defined precision/scale. You may end up with apparent junk (bytes) in the output, or just errors. TIMESTAMP WITH LOCAL TIME ZONE. Throws JDBC type -102 not currently supported warning in the log. Read more about NUMBER data type in the Oracle docs.
tl;dr : How do I make it work? There are several options:</description>
    </item>
    
    <item>
      <title>Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Tue, 27 Mar 2018 18:52:00 +0000</pubDate>
      
      <guid>https://rmoff.net/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I&amp;rsquo;m doing something wrong
MongoDB config - enabling replica sets For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:
Docs: Replication / Convert a Standalone to a Replica Set
Stop Mongo:
rmoff@proxmox01 ~&amp;gt; sudo service mongod stop Add replica set config to /etc/mongod.</description>
    </item>
    
    <item>
      <title>Oracle GoldenGate / Kafka Connect Handler troubleshooting</title>
      <link>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</link>
      <pubDate>Tue, 12 Sep 2017 21:55:16 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/12/oracle-goldengate-/-kafka-connect-handler-troubleshooting/</guid>
      <description>The Replicat was kapput:
GGSCI (localhost.localdomain) 3&amp;gt; info rkconnoe REPLICAT RKCONNOE Last Started 2017-09-12 17:06 Status ABENDED Checkpoint Lag 00:00:00 (updated 00:46:34 ago) Log Read Checkpoint File /u01/app/ogg/dirdat/oe000000 First Record RBA 0 So checking the OGG error log ggserr.log showed
2017-09-12T17:06:17.572-0400 ERROR OGG-15051 Oracle GoldenGate Delivery, rkconnoe.prm: Java or JNI exception: oracle.goldengate.util.GGException: Error detected handling operation added event. 2017-09-12T17:06:17.572-0400 ERROR OGG-01668 Oracle GoldenGate Delivery, rkconnoe.prm: PROCESS ABENDING. So checking the replicat log dirrpt/RKCONNOE_info_log4j.</description>
    </item>
    
    <item>
      <title>Kafka Connect - JsonDeserializer with schemas.enable requires &#34;schema&#34; and &#34;payload&#34; fields</title>
      <link>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</link>
      <pubDate>Wed, 06 Sep 2017 12:00:25 +0000</pubDate>
      
      <guid>https://rmoff.net/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/</guid>
      <description>An error that I see coming up frequently in the Kafka Connect community (e.g. mailing list, Slack group, StackOverflow) is:
JsonDeserializer with schemas.enable requires &amp;quot;schema&amp;quot; and &amp;quot;payload&amp;quot; fields and may not contain additional fields or
No fields found using key and value schemas for table: foo-bar You can see an explanation, and solution, for the issue in my StackOverflow answer here: https://stackoverflow.com/a/45940013/350613
If you&amp;rsquo;re using schemas.enable in the Connector configuration, you must have schema and payload as the root-level elements of your JSON message ( Which is pretty much verbatim what the error says üòÅ), like this:</description>
    </item>
    
    <item>
      <title>Oracle GoldenGate -&gt; Kafka Connect - &#34;Failed to serialize Avro data&#34;</title>
      <link>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</link>
      <pubDate>Tue, 29 Nov 2016 22:04:38 +0000</pubDate>
      
      <guid>https://rmoff.net/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/</guid>
      <description>tl;dr Make sure that key.converter.schema.registry.url and value.converter.schema.registry.url are specified, and that there are no trailing whitespaces.
I&amp;rsquo;ve been building on previous work I&amp;rsquo;ve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the sample configuration gives.
Simply changing the Kafka Connect OGG configuration file (confluent.properties) from
value.converter=org.apache.kafka.connect.json.JsonConverter key.</description>
    </item>
    
  </channel>
</rss>
